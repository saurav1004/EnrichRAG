Directory structure:
â””â”€â”€ neuml-txtai/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CITATION.cff
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ mkdocs.yml
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ .coveragerc
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ .pylintrc
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â””â”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ aws/
    â”‚   â”‚   â”œâ”€â”€ api.py
    â”‚   â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”‚   â””â”€â”€ workflow.py
    â”‚   â”œâ”€â”€ base/
    â”‚   â”‚   â””â”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ schedule/
    â”‚   â”‚   â””â”€â”€ Dockerfile
    â”‚   â””â”€â”€ workflow/
    â”‚       â””â”€â”€ Dockerfile
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ cloud.md
    â”‚   â”œâ”€â”€ examples.md
    â”‚   â”œâ”€â”€ faq.md
    â”‚   â”œâ”€â”€ further.md
    â”‚   â”œâ”€â”€ index.md
    â”‚   â”œâ”€â”€ install.md
    â”‚   â”œâ”€â”€ models.md
    â”‚   â”œâ”€â”€ observability.md
    â”‚   â”œâ”€â”€ poweredby.md
    â”‚   â”œâ”€â”€ usecases.md
    â”‚   â”œâ”€â”€ why.md
    â”‚   â”œâ”€â”€ agent/
    â”‚   â”‚   â”œâ”€â”€ configuration.md
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â””â”€â”€ methods.md
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”œâ”€â”€ cluster.md
    â”‚   â”‚   â”œâ”€â”€ configuration.md
    â”‚   â”‚   â”œâ”€â”€ customization.md
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ mcp.md
    â”‚   â”‚   â”œâ”€â”€ methods.md
    â”‚   â”‚   â”œâ”€â”€ openai.md
    â”‚   â”‚   â””â”€â”€ security.md
    â”‚   â”œâ”€â”€ embeddings/
    â”‚   â”‚   â”œâ”€â”€ format.md
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ indexing.md
    â”‚   â”‚   â”œâ”€â”€ methods.md
    â”‚   â”‚   â”œâ”€â”€ query.md
    â”‚   â”‚   â””â”€â”€ configuration/
    â”‚   â”‚       â”œâ”€â”€ ann.md
    â”‚   â”‚       â”œâ”€â”€ cloud.md
    â”‚   â”‚       â”œâ”€â”€ database.md
    â”‚   â”‚       â”œâ”€â”€ general.md
    â”‚   â”‚       â”œâ”€â”€ graph.md
    â”‚   â”‚       â”œâ”€â”€ index.md
    â”‚   â”‚       â”œâ”€â”€ scoring.md
    â”‚   â”‚       â””â”€â”€ vectors.md
    â”‚   â”œâ”€â”€ images/
    â”‚   â”‚   â”œâ”€â”€ agent.excalidraw
    â”‚   â”‚   â”œâ”€â”€ api.excalidraw
    â”‚   â”‚   â”œâ”€â”€ architecture.excalidraw
    â”‚   â”‚   â”œâ”€â”€ cloud.excalidraw
    â”‚   â”‚   â”œâ”€â”€ embeddings.excalidraw
    â”‚   â”‚   â”œâ”€â”€ examples.excalidraw
    â”‚   â”‚   â”œâ”€â”€ faq.excalidraw
    â”‚   â”‚   â”œâ”€â”€ flows.excalidraw
    â”‚   â”‚   â”œâ”€â”€ format.excalidraw
    â”‚   â”‚   â”œâ”€â”€ further.excalidraw
    â”‚   â”‚   â”œâ”€â”€ indexing.excalidraw
    â”‚   â”‚   â”œâ”€â”€ install.excalidraw
    â”‚   â”‚   â”œâ”€â”€ llm.excalidraw
    â”‚   â”‚   â”œâ”€â”€ models.excalidraw
    â”‚   â”‚   â”œâ”€â”€ pipeline.excalidraw
    â”‚   â”‚   â”œâ”€â”€ query.excalidraw
    â”‚   â”‚   â”œâ”€â”€ rag.excalidraw
    â”‚   â”‚   â”œâ”€â”€ schedule.excalidraw
    â”‚   â”‚   â”œâ”€â”€ search.excalidraw
    â”‚   â”‚   â”œâ”€â”€ task.excalidraw
    â”‚   â”‚   â”œâ”€â”€ why.excalidraw
    â”‚   â”‚   â””â”€â”€ workflow.excalidraw
    â”‚   â”œâ”€â”€ overrides/
    â”‚   â”‚   â””â”€â”€ main.html
    â”‚   â”œâ”€â”€ pipeline/
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ audio/
    â”‚   â”‚   â”‚   â”œâ”€â”€ audiomixer.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ audiostream.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ microphone.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ texttoaudio.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ texttospeech.md
    â”‚   â”‚   â”‚   â””â”€â”€ transcription.md
    â”‚   â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”‚   â”œâ”€â”€ filetohtml.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ htmltomd.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ segmentation.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ tabular.md
    â”‚   â”‚   â”‚   â””â”€â”€ textractor.md
    â”‚   â”‚   â”œâ”€â”€ image/
    â”‚   â”‚   â”‚   â”œâ”€â”€ caption.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ imagehash.md
    â”‚   â”‚   â”‚   â””â”€â”€ objects.md
    â”‚   â”‚   â”œâ”€â”€ text/
    â”‚   â”‚   â”‚   â”œâ”€â”€ entity.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ labels.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ llm.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ rag.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ similarity.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ summary.md
    â”‚   â”‚   â”‚   â””â”€â”€ translation.md
    â”‚   â”‚   â””â”€â”€ train/
    â”‚   â”‚       â”œâ”€â”€ hfonnx.md
    â”‚   â”‚       â”œâ”€â”€ mlonnx.md
    â”‚   â”‚       â””â”€â”€ trainer.md
    â”‚   â””â”€â”€ workflow/
    â”‚       â”œâ”€â”€ index.md
    â”‚       â”œâ”€â”€ schedule.md
    â”‚       â””â”€â”€ task/
    â”‚           â”œâ”€â”€ console.md
    â”‚           â”œâ”€â”€ export.md
    â”‚           â”œâ”€â”€ file.md
    â”‚           â”œâ”€â”€ image.md
    â”‚           â”œâ”€â”€ index.md
    â”‚           â”œâ”€â”€ retrieve.md
    â”‚           â”œâ”€â”€ service.md
    â”‚           â”œâ”€â”€ storage.md
    â”‚           â”œâ”€â”€ template.md
    â”‚           â”œâ”€â”€ url.md
    â”‚           â””â”€â”€ workflow.md
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ 01_Introducing_txtai.ipynb
    â”‚   â”œâ”€â”€ 02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb
    â”‚   â”œâ”€â”€ 03_Build_an_Embeddings_index_from_a_data_source.ipynb
    â”‚   â”œâ”€â”€ 04_Add_semantic_search_to_Elasticsearch.ipynb
    â”‚   â”œâ”€â”€ 05_Extractive_QA_with_txtai.ipynb
    â”‚   â”œâ”€â”€ 06_Extractive_QA_with_Elasticsearch.ipynb
    â”‚   â”œâ”€â”€ 07_Apply_labels_with_zero_shot_classification.ipynb
    â”‚   â”œâ”€â”€ 08_API_Gallery.ipynb
    â”‚   â”œâ”€â”€ 09_Building_abstractive_text_summaries.ipynb
    â”‚   â”œâ”€â”€ 10_Extract_text_from_documents.ipynb
    â”‚   â”œâ”€â”€ 11_Transcribe_audio_to_text.ipynb
    â”‚   â”œâ”€â”€ 12_Translate_text_between_languages.ipynb
    â”‚   â”œâ”€â”€ 13_Similarity_search_with_images.ipynb
    â”‚   â”œâ”€â”€ 14_Run_pipeline_workflows.ipynb
    â”‚   â”œâ”€â”€ 15_Distributed_embeddings_cluster.ipynb
    â”‚   â”œâ”€â”€ 16_Train_a_text_labeler.ipynb
    â”‚   â”œâ”€â”€ 17_Train_without_labels.ipynb
    â”‚   â”œâ”€â”€ 18_Export_and_run_models_with_ONNX.ipynb
    â”‚   â”œâ”€â”€ 19_Train_a_QA_model.ipynb
    â”‚   â”œâ”€â”€ 20_Extractive_QA_to_build_structured_data.ipynb
    â”‚   â”œâ”€â”€ 21_Export_and_run_other_machine_learning_models.ipynb
    â”‚   â”œâ”€â”€ 22_Transform_tabular_data_with_composable_workflows.ipynb
    â”‚   â”œâ”€â”€ 23_Tensor_workflows.ipynb
    â”‚   â”œâ”€â”€ 24_Whats_new_in_txtai_4_0.ipynb
    â”‚   â”œâ”€â”€ 25_Generate_image_captions_and_detect_objects.ipynb
    â”‚   â”œâ”€â”€ 26_Entity_extraction_workflows.ipynb
    â”‚   â”œâ”€â”€ 27_Workflow_scheduling.ipynb
    â”‚   â”œâ”€â”€ 28_Push_notifications_with_workflows.ipynb
    â”‚   â”œâ”€â”€ 29_Anatomy_of_a_txtai_index.ipynb
    â”‚   â”œâ”€â”€ 30_Embeddings_SQL_custom_functions.ipynb
    â”‚   â”œâ”€â”€ 31_Near_duplicate_image_detection.ipynb
    â”‚   â”œâ”€â”€ 32_Model_explainability.ipynb
    â”‚   â”œâ”€â”€ 33_Query_translation.ipynb
    â”‚   â”œâ”€â”€ 34_Build_a_QA_database.ipynb
    â”‚   â”œâ”€â”€ 35_Pictures_are_worth_a_thousand_words.ipynb
    â”‚   â”œâ”€â”€ 36_Run_txtai_in_native_code.ipynb
    â”‚   â”œâ”€â”€ 37_Embeddings_index_components.ipynb
    â”‚   â”œâ”€â”€ 38_Introducing_the_Semantic_Graph.ipynb
    â”‚   â”œâ”€â”€ 39_Classic_Topic_Modeling_with_BM25.ipynb
    â”‚   â”œâ”€â”€ 40_Text_to_Speech_Generation.ipynb
    â”‚   â”œâ”€â”€ 41_Train_a_language_model_from_scratch.ipynb
    â”‚   â”œâ”€â”€ 42_Prompt_driven_search_with_LLMs.ipynb
    â”‚   â”œâ”€â”€ 43_Embeddings_in_the_Cloud.ipynb
    â”‚   â”œâ”€â”€ 44_Prompt_templates_and_task_chains.ipynb
    â”‚   â”œâ”€â”€ 45_Customize_your_own_embeddings_database.ipynb
    â”‚   â”œâ”€â”€ 46_Whats_new_in_txtai_6_0.ipynb
    â”‚   â”œâ”€â”€ 47_Building_an_efficient_sparse_keyword_index_in_Python.ipynb
    â”‚   â”œâ”€â”€ 48_Benefits_of_hybrid_search.ipynb
    â”‚   â”œâ”€â”€ 49_External_database_integration.ipynb
    â”‚   â”œâ”€â”€ 50_All_about_vector_quantization.ipynb
    â”‚   â”œâ”€â”€ 51_Custom_API_Endpoints.ipynb
    â”‚   â”œâ”€â”€ 52_Build_RAG_pipelines_with_txtai.ipynb
    â”‚   â”œâ”€â”€ 53_Integrate_LLM_Frameworks.ipynb
    â”‚   â”œâ”€â”€ 54_API_Authorization_and_Authentication.ipynb
    â”‚   â”œâ”€â”€ 55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb
    â”‚   â”œâ”€â”€ 56_External_vectorization.ipynb
    â”‚   â”œâ”€â”€ 57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb
    â”‚   â”œâ”€â”€ 58_Advanced_RAG_with_graph_path_traversal.ipynb
    â”‚   â”œâ”€â”€ 59_Whats_new_in_txtai_7_0.ipynb
    â”‚   â”œâ”€â”€ 60_Advanced_RAG_with_guided_generation.ipynb
    â”‚   â”œâ”€â”€ 61_Integrate_txtai_with_Postgres.ipynb
    â”‚   â”œâ”€â”€ 62_RAG_with_llama_cpp_and_external_API_services.ipynb
    â”‚   â”œâ”€â”€ 63_How_RAG_with_txtai_works.ipynb
    â”‚   â”œâ”€â”€ 64_Embeddings_index_format_for_open_data_access.ipynb
    â”‚   â”œâ”€â”€ 65_Speech_to_Speech_RAG.ipynb
    â”‚   â”œâ”€â”€ 66_Generative_Audio.ipynb
    â”‚   â”œâ”€â”€ 67_Whats_new_in_txtai_8_0.ipynb
    â”‚   â”œâ”€â”€ 68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb
    â”‚   â”œâ”€â”€ 69_Granting_autonomy_to_agents.ipynb
    â”‚   â”œâ”€â”€ 70_Getting_started_with_LLM_APIs.ipynb
    â”‚   â”œâ”€â”€ 71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb
    â”‚   â”œâ”€â”€ 72_Parsing_the_stars_with_txtai.ipynb
    â”‚   â”œâ”€â”€ 73_Chunking_your_data_for_RAG.ipynb
    â”‚   â”œâ”€â”€ 74_OpenAI_Compatible_API.ipynb
    â”‚   â”œâ”€â”€ 75_Medical_RAG_Research_with_txtai.ipynb
    â”‚   â”œâ”€â”€ 76_Whats_new_in_txtai_9_0.ipynb
    â”‚   â”œâ”€â”€ 77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb
    â”‚   â”œâ”€â”€ 78_Accessing_Low_Level_Vector_APIs.ipynb
    â”‚   â”œâ”€â”€ article.py
    â”‚   â”œâ”€â”€ baseball.py
    â”‚   â”œâ”€â”€ benchmarks.py
    â”‚   â”œâ”€â”€ books.py
    â”‚   â”œâ”€â”€ images.py
    â”‚   â”œâ”€â”€ similarity.py
    â”‚   â”œâ”€â”€ wiki.py
    â”‚   â””â”€â”€ workflows.py
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ python/
    â”‚       â””â”€â”€ txtai/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ version.py
    â”‚           â”œâ”€â”€ agent/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ model.py
    â”‚           â”‚   â”œâ”€â”€ placeholder.py
    â”‚           â”‚   â””â”€â”€ tool/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ embeddings.py
    â”‚           â”‚       â”œâ”€â”€ factory.py
    â”‚           â”‚       â””â”€â”€ function.py
    â”‚           â”œâ”€â”€ ann/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ dense/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ annoy.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ faiss.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ ggml.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ hnsw.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ numpy.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ pgvector.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ sqlite.py
    â”‚           â”‚   â”‚   â””â”€â”€ torch.py
    â”‚           â”‚   â””â”€â”€ sparse/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ factory.py
    â”‚           â”‚       â”œâ”€â”€ ivfsparse.py
    â”‚           â”‚       â””â”€â”€ pgsparse.py
    â”‚           â”œâ”€â”€ api/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ application.py
    â”‚           â”‚   â”œâ”€â”€ authorization.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ cluster.py
    â”‚           â”‚   â”œâ”€â”€ extension.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ route.py
    â”‚           â”‚   â”œâ”€â”€ responses/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ json.py
    â”‚           â”‚   â”‚   â””â”€â”€ messagepack.py
    â”‚           â”‚   â””â”€â”€ routers/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ agent.py
    â”‚           â”‚       â”œâ”€â”€ caption.py
    â”‚           â”‚       â”œâ”€â”€ embeddings.py
    â”‚           â”‚       â”œâ”€â”€ entity.py
    â”‚           â”‚       â”œâ”€â”€ extractor.py
    â”‚           â”‚       â”œâ”€â”€ labels.py
    â”‚           â”‚       â”œâ”€â”€ llm.py
    â”‚           â”‚       â”œâ”€â”€ objects.py
    â”‚           â”‚       â”œâ”€â”€ openai.py
    â”‚           â”‚       â”œâ”€â”€ rag.py
    â”‚           â”‚       â”œâ”€â”€ reranker.py
    â”‚           â”‚       â”œâ”€â”€ segmentation.py
    â”‚           â”‚       â”œâ”€â”€ similarity.py
    â”‚           â”‚       â”œâ”€â”€ summary.py
    â”‚           â”‚       â”œâ”€â”€ tabular.py
    â”‚           â”‚       â”œâ”€â”€ textractor.py
    â”‚           â”‚       â”œâ”€â”€ texttospeech.py
    â”‚           â”‚       â”œâ”€â”€ transcription.py
    â”‚           â”‚       â”œâ”€â”€ translation.py
    â”‚           â”‚       â”œâ”€â”€ upload.py
    â”‚           â”‚       â””â”€â”€ workflow.py
    â”‚           â”œâ”€â”€ app/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â””â”€â”€ base.py
    â”‚           â”œâ”€â”€ archive/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ compress.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ tar.py
    â”‚           â”‚   â””â”€â”€ zip.py
    â”‚           â”œâ”€â”€ cloud/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ hub.py
    â”‚           â”‚   â””â”€â”€ storage.py
    â”‚           â”œâ”€â”€ console/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ __main__.py
    â”‚           â”‚   â””â”€â”€ base.py
    â”‚           â”œâ”€â”€ data/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ labels.py
    â”‚           â”‚   â”œâ”€â”€ questions.py
    â”‚           â”‚   â”œâ”€â”€ sequences.py
    â”‚           â”‚   â”œâ”€â”€ texts.py
    â”‚           â”‚   â””â”€â”€ tokens.py
    â”‚           â”œâ”€â”€ database/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ client.py
    â”‚           â”‚   â”œâ”€â”€ duckdb.py
    â”‚           â”‚   â”œâ”€â”€ embedded.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ rdbms.py
    â”‚           â”‚   â”œâ”€â”€ sqlite.py
    â”‚           â”‚   â”œâ”€â”€ encoder/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ image.py
    â”‚           â”‚   â”‚   â””â”€â”€ serialize.py
    â”‚           â”‚   â”œâ”€â”€ schema/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ orm.py
    â”‚           â”‚   â”‚   â””â”€â”€ statement.py
    â”‚           â”‚   â””â”€â”€ sql/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ aggregate.py
    â”‚           â”‚       â”œâ”€â”€ base.py
    â”‚           â”‚       â”œâ”€â”€ expression.py
    â”‚           â”‚       â””â”€â”€ token.py
    â”‚           â”œâ”€â”€ embeddings/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ index/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ action.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ autoid.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ configuration.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ documents.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ functions.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ indexes.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ indexids.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ reducer.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ stream.py
    â”‚           â”‚   â”‚   â””â”€â”€ transform.py
    â”‚           â”‚   â””â”€â”€ search/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ base.py
    â”‚           â”‚       â”œâ”€â”€ errors.py
    â”‚           â”‚       â”œâ”€â”€ explain.py
    â”‚           â”‚       â”œâ”€â”€ ids.py
    â”‚           â”‚       â”œâ”€â”€ query.py
    â”‚           â”‚       â”œâ”€â”€ scan.py
    â”‚           â”‚       â””â”€â”€ terms.py
    â”‚           â”œâ”€â”€ graph/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ networkx.py
    â”‚           â”‚   â”œâ”€â”€ query.py
    â”‚           â”‚   â”œâ”€â”€ rdbms.py
    â”‚           â”‚   â””â”€â”€ topics.py
    â”‚           â”œâ”€â”€ models/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ models.py
    â”‚           â”‚   â”œâ”€â”€ onnx.py
    â”‚           â”‚   â”œâ”€â”€ registry.py
    â”‚           â”‚   â”œâ”€â”€ tokendetection.py
    â”‚           â”‚   â””â”€â”€ pooling/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ base.py
    â”‚           â”‚       â”œâ”€â”€ cls.py
    â”‚           â”‚       â”œâ”€â”€ factory.py
    â”‚           â”‚       â”œâ”€â”€ late.py
    â”‚           â”‚       â”œâ”€â”€ mean.py
    â”‚           â”‚       â””â”€â”€ muvera.py
    â”‚           â”œâ”€â”€ pipeline/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ hfmodel.py
    â”‚           â”‚   â”œâ”€â”€ hfpipeline.py
    â”‚           â”‚   â”œâ”€â”€ nop.py
    â”‚           â”‚   â”œâ”€â”€ tensors.py
    â”‚           â”‚   â”œâ”€â”€ audio/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ audiomixer.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ audiostream.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ microphone.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ signal.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ texttoaudio.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ texttospeech.py
    â”‚           â”‚   â”‚   â””â”€â”€ transcription.py
    â”‚           â”‚   â”œâ”€â”€ data/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ filetohtml.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ htmltomd.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ segmentation.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ tabular.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ textractor.py
    â”‚           â”‚   â”‚   â””â”€â”€ tokenizer.py
    â”‚           â”‚   â”œâ”€â”€ image/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ caption.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ imagehash.py
    â”‚           â”‚   â”‚   â””â”€â”€ objects.py
    â”‚           â”‚   â”œâ”€â”€ llm/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ generation.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ huggingface.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ litellm.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ llama.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ llm.py
    â”‚           â”‚   â”‚   â””â”€â”€ rag.py
    â”‚           â”‚   â”œâ”€â”€ text/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ crossencoder.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ entity.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ labels.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ lateencoder.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ questions.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ reranker.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ similarity.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ summary.py
    â”‚           â”‚   â”‚   â””â”€â”€ translation.py
    â”‚           â”‚   â””â”€â”€ train/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ hfonnx.py
    â”‚           â”‚       â”œâ”€â”€ hftrainer.py
    â”‚           â”‚       â””â”€â”€ mlonnx.py
    â”‚           â”œâ”€â”€ scoring/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ bm25.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ pgtext.py
    â”‚           â”‚   â”œâ”€â”€ sif.py
    â”‚           â”‚   â”œâ”€â”€ sparse.py
    â”‚           â”‚   â”œâ”€â”€ terms.py
    â”‚           â”‚   â””â”€â”€ tfidf.py
    â”‚           â”œâ”€â”€ serialize/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ errors.py
    â”‚           â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”œâ”€â”€ messagepack.py
    â”‚           â”‚   â”œâ”€â”€ pickle.py
    â”‚           â”‚   â””â”€â”€ serializer.py
    â”‚           â”œâ”€â”€ util/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ resolver.py
    â”‚           â”‚   â”œâ”€â”€ sparsearray.py
    â”‚           â”‚   â””â”€â”€ template.py
    â”‚           â”œâ”€â”€ vectors/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ base.py
    â”‚           â”‚   â”œâ”€â”€ recovery.py
    â”‚           â”‚   â”œâ”€â”€ dense/
    â”‚           â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ external.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ factory.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ huggingface.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ litellm.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ llama.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ m2v.py
    â”‚           â”‚   â”‚   â”œâ”€â”€ sbert.py
    â”‚           â”‚   â”‚   â””â”€â”€ words.py
    â”‚           â”‚   â””â”€â”€ sparse/
    â”‚           â”‚       â”œâ”€â”€ __init__.py
    â”‚           â”‚       â”œâ”€â”€ base.py
    â”‚           â”‚       â”œâ”€â”€ factory.py
    â”‚           â”‚       â””â”€â”€ sbert.py
    â”‚           â””â”€â”€ workflow/
    â”‚               â”œâ”€â”€ __init__.py
    â”‚               â”œâ”€â”€ base.py
    â”‚               â”œâ”€â”€ execute.py
    â”‚               â”œâ”€â”€ factory.py
    â”‚               â””â”€â”€ task/
    â”‚                   â”œâ”€â”€ __init__.py
    â”‚                   â”œâ”€â”€ base.py
    â”‚                   â”œâ”€â”€ console.py
    â”‚                   â”œâ”€â”€ export.py
    â”‚                   â”œâ”€â”€ factory.py
    â”‚                   â”œâ”€â”€ file.py
    â”‚                   â”œâ”€â”€ image.py
    â”‚                   â”œâ”€â”€ retrieve.py
    â”‚                   â”œâ”€â”€ service.py
    â”‚                   â”œâ”€â”€ storage.py
    â”‚                   â”œâ”€â”€ stream.py
    â”‚                   â”œâ”€â”€ template.py
    â”‚                   â”œâ”€â”€ url.py
    â”‚                   â””â”€â”€ workflow.py
    â”œâ”€â”€ test/
    â”‚   â””â”€â”€ python/
    â”‚       â”œâ”€â”€ testagent.py
    â”‚       â”œâ”€â”€ testapp.py
    â”‚       â”œâ”€â”€ testarchive.py
    â”‚       â”œâ”€â”€ testcloud.py
    â”‚       â”œâ”€â”€ testconsole.py
    â”‚       â”œâ”€â”€ testembeddings.py
    â”‚       â”œâ”€â”€ testgraph.py
    â”‚       â”œâ”€â”€ testoptional.py
    â”‚       â”œâ”€â”€ testserialize.py
    â”‚       â”œâ”€â”€ testworkflow.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â”œâ”€â”€ testann/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testdense.py
    â”‚       â”‚   â””â”€â”€ testsparse.py
    â”‚       â”œâ”€â”€ testapi/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testapiagent.py
    â”‚       â”‚   â”œâ”€â”€ testapiembeddings.py
    â”‚       â”‚   â”œâ”€â”€ testapipipeline.py
    â”‚       â”‚   â”œâ”€â”€ testapiworkflow.py
    â”‚       â”‚   â”œâ”€â”€ testauthorization.py
    â”‚       â”‚   â”œâ”€â”€ testcluster.py
    â”‚       â”‚   â”œâ”€â”€ testencoding.py
    â”‚       â”‚   â”œâ”€â”€ testextension.py
    â”‚       â”‚   â”œâ”€â”€ testmcp.py
    â”‚       â”‚   â””â”€â”€ testopenai.py
    â”‚       â”œâ”€â”€ testdatabase/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testclient.py
    â”‚       â”‚   â”œâ”€â”€ testcustom.py
    â”‚       â”‚   â”œâ”€â”€ testdatabase.py
    â”‚       â”‚   â”œâ”€â”€ testduckdb.py
    â”‚       â”‚   â”œâ”€â”€ testencoder.py
    â”‚       â”‚   â”œâ”€â”€ testrdbms.py
    â”‚       â”‚   â”œâ”€â”€ testsql.py
    â”‚       â”‚   â””â”€â”€ testsqlite.py
    â”‚       â”œâ”€â”€ testmodels/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testmodels.py
    â”‚       â”‚   â””â”€â”€ testpooling.py
    â”‚       â”œâ”€â”€ testpipeline/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testaudio/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testaudiomixer.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testaudiostream.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testmicrophone.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testtexttoaudio.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testtexttospeech.py
    â”‚       â”‚   â”‚   â””â”€â”€ testtranscription.py
    â”‚       â”‚   â”œâ”€â”€ testdata/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testfiletohtml.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testtabular.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testtextractor.py
    â”‚       â”‚   â”‚   â””â”€â”€ testtokenizer.py
    â”‚       â”‚   â”œâ”€â”€ testimage/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testcaption.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testimagehash.py
    â”‚       â”‚   â”‚   â””â”€â”€ testobjects.py
    â”‚       â”‚   â”œâ”€â”€ testllm/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testgenerator.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testlitellm.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testllama.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testllm.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testrag.py
    â”‚       â”‚   â”‚   â””â”€â”€ testsequences.py
    â”‚       â”‚   â”œâ”€â”€ testtext/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testentity.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testlabels.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testreranker.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testsimilarity.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ testsummary.py
    â”‚       â”‚   â”‚   â””â”€â”€ testtranslation.py
    â”‚       â”‚   â””â”€â”€ testtrain/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”œâ”€â”€ testonnx.py
    â”‚       â”‚       â”œâ”€â”€ testquantization.py
    â”‚       â”‚       â””â”€â”€ testtrainer.py
    â”‚       â”œâ”€â”€ testscoring/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ testkeyword.py
    â”‚       â”‚   â””â”€â”€ testsparse.py
    â”‚       â””â”€â”€ testvectors/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ testdense/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ testcustom.py
    â”‚           â”‚   â”œâ”€â”€ testexternal.py
    â”‚           â”‚   â”œâ”€â”€ testhuggingface.py
    â”‚           â”‚   â”œâ”€â”€ testlitellm.py
    â”‚           â”‚   â”œâ”€â”€ testllama.py
    â”‚           â”‚   â”œâ”€â”€ testm2v.py
    â”‚           â”‚   â”œâ”€â”€ testsbert.py
    â”‚           â”‚   â”œâ”€â”€ testvectors.py
    â”‚           â”‚   â””â”€â”€ testwordvectors.py
    â”‚           â””â”€â”€ testsparse/
    â”‚               â”œâ”€â”€ __init__.py
    â”‚               â”œâ”€â”€ testsbert.py
    â”‚               â””â”€â”€ testvectors.py
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â”œâ”€â”€ build.yml
            â””â”€â”€ docs.yml

================================================
FILE: README.md
================================================
<p align="center">
    <img src="https://raw.githubusercontent.com/neuml/txtai/master/logo.png"/>
</p>

<p align="center">
    <b>All-in-one AI framework</b>
</p>

<p align="center">
    <a href="https://github.com/neuml/txtai/releases">
        <img src="https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success" alt="Version"/>
    </a>
    <a href="https://github.com/neuml/txtai">
        <img src="https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue" alt="GitHub last commit"/>
    </a>
    <a href="https://github.com/neuml/txtai/issues">
        <img src="https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success" alt="GitHub issues"/>
    </a>
    <a href="https://join.slack.com/t/txtai/shared_invite/zt-37c1zfijp-Y57wMty6YOx_hyIHEQvQJA">
        <img src="https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white" alt="Join Slack"/>
    </a>
    <a href="https://github.com/neuml/txtai/actions?query=workflow%3Abuild">
        <img src="https://github.com/neuml/txtai/workflows/build/badge.svg" alt="Build Status"/>
    </a>
    <a href="https://coveralls.io/github/neuml/txtai?branch=master">
        <img src="https://img.shields.io/coverallsCoverage/github/neuml/txtai" alt="Coverage Status">
    </a>
</p>

txtai is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

![architecture](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/architecture.png#gh-light-mode-only)
![architecture](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/architecture-dark.png#gh-dark-mode-only)

The key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.

This foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.

Build autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.

Summary of txtai features:

- ğŸ” Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing
- ğŸ“„ Create embeddings for text, documents, audio, images and video
- ğŸ’¡ Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more
- â†ªï¸ï¸ Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.
- ğŸ¤– Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems
- âš™ï¸ Web and Model Context Protocol (MCP) APIs. Bindings available for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).
- ğŸ”‹ Batteries included with defaults to get up and running fast
- â˜ï¸ Run local or scale out with container orchestration

txtai is built with Python 3.10+, [Hugging Face Transformers](https://github.com/huggingface/transformers), [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi). txtai is open-source under an Apache 2.0 license.

*Interested in an easy and secure way to run hosted txtai applications? Then join the [txtai.cloud](https://txtai.cloud) preview to learn more.*

## Why txtai?

![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why.png#gh-light-mode-only)
![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why-dark.png#gh-dark-mode-only)

New vector databases, LLM frameworks and everything in between are sprouting up daily. Why build with txtai?

- Up and running in minutes with [pip](https://neuml.github.io/txtai/install/) or [Docker](https://neuml.github.io/txtai/cloud/)
```python
# Get started in a couple lines
import txtai

embeddings = txtai.Embeddings()
embeddings.index(["Correct", "Not what we hoped"])
embeddings.search("positive", 1)
#[(0, 0.29862046241760254)]
```
- Built-in API makes it easy to develop applications using your programming language of choice
```yaml
# app.yml
embeddings:
    path: sentence-transformers/all-MiniLM-L6-v2
```
```bash
CONFIG=app.yml uvicorn "txtai.api:app"
curl -X GET "http://localhost:8000/search?query=positive"
```
- Run local - no need to ship data off to disparate remote services
- Work with micromodels all the way up to large language models (LLMs)
- Low footprint - install additional dependencies and scale up when needed
- [Learn by example](https://neuml.github.io/txtai/examples) - notebooks cover all available functionality

## Use Cases

The following sections introduce common txtai use cases. A comprehensive set of over 60 [example notebooks and applications](https://neuml.github.io/txtai/examples) are also available.

### Semantic Search

Build semantic/similarity/vector/neural search applications.

![demo](https://raw.githubusercontent.com/neuml/txtai/master/demo.gif)

Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.

![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search.png#gh-light-mode-only)
![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search-dark.png#gh-dark-mode-only)

Get started with the following examples.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=SIezMnVdmMs) | Overview of the functionality provided by txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |
| [Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) | Embed images and text into the same space for search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |
| [Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) | Question matching with semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |
| [Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) | Explore topics, data connectivity and run network analysis| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |

### LLM Orchestration

Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).

![llm](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/llm.png)

See below to learn more.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |
| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |
| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |

#### Agents

Agents connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems.

![agent](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/agent.png)

txtai agents are built on top of the [smolagents](https://github.com/huggingface/smolagents) framework. This supports all LLMs txtai supports (Hugging Face, llama.cpp, OpenAI / Claude / AWS Bedrock via LiteLLM).

See the link below to learn more.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |

#### Retrieval augmented generation

Retrieval augmented generation (RAG) reduces the risk of LLM hallucinations by constraining the output with a knowledge base as context. RAG is commonly used to "chat with your data".

![rag](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/rag.png#gh-light-mode-only)
![rag](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/rag-dark.png#gh-dark-mode-only)

A novel feature of txtai is that it can provide both an answer and source citation.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |
| [GraphRAG with Wikipedia and GPT OSS](https://github.com/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) | Deep graph search powered RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |

### Language Model Workflows

Language model workflows, also known as semantic workflows, connect language models together to build intelligent applications.

![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows.png#gh-light-mode-only)
![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows-dark.png#gh-dark-mode-only)

While LLMs are powerful, there are plenty of smaller, more specialized models that work better and faster for specific tasks. This includes models for extractive question-answering, automatic summarization, text-to-speech, transcription and translation.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=UBMPDCn1gEU) | Simple yet powerful constructs to efficiently process data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |
| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |
| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |
| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |

## Installation

![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install.png#gh-light-mode-only)
![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install-dark.png#gh-dark-mode-only)

The easiest way to install is via pip and PyPI

```
pip install txtai
```

Python 3.10+ is supported. Using a Python [virtual environment](https://docs.python.org/3/library/venv.html) is recommended.

See the detailed [install instructions](https://neuml.github.io/txtai/install) for more information covering [optional dependencies](https://neuml.github.io/txtai/install/#optional-dependencies), [environment specific prerequisites](https://neuml.github.io/txtai/install/#environment-specific-prerequisites), [installing from source](https://neuml.github.io/txtai/install/#install-from-source), [conda support](https://neuml.github.io/txtai/install/#conda) and how to [run with containers](https://neuml.github.io/txtai/cloud).

## Model guide

![models](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/models.png)

See the table below for the current recommended models. These models all allow commercial use and offer a blend of speed and performance.

| Component                                                                     | Model(s)                                                                 |
| ----------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| [Embeddings](https://neuml.github.io/txtai/embeddings)                        | [all-MiniLM-L6-v2](https://hf.co/sentence-transformers/all-MiniLM-L6-v2) | 
| [Image Captions](https://neuml.github.io/txtai/pipeline/image/caption)        | [BLIP](https://hf.co/Salesforce/blip-image-captioning-base)              |
| [Labels - Zero Shot](https://neuml.github.io/txtai/pipeline/text/labels)      | [BART-Large-MNLI](https://hf.co/facebook/bart-large)                     |
| [Labels - Fixed](https://neuml.github.io/txtai/pipeline/text/labels)          | Fine-tune with [training pipeline](https://neuml.github.io/txtai/pipeline/train/trainer)          |
| [Large Language Model (LLM)](https://neuml.github.io/txtai/pipeline/text/llm) | [Llama 3.1 Instruct](https://hf.co/meta-llama/Llama-3.1-8B-Instruct)     |
| [Summarization](https://neuml.github.io/txtai/pipeline/text/summary)          | [DistilBART](https://hf.co/sshleifer/distilbart-cnn-12-6)                |
| [Text-to-Speech](https://neuml.github.io/txtai/pipeline/audio/texttospeech)   | [ESPnet JETS](https://hf.co/NeuML/ljspeech-jets-onnx)                    |
| [Transcription](https://neuml.github.io/txtai/pipeline/audio/transcription)   | [Whisper](https://hf.co/openai/whisper-base)                             | 
| [Translation](https://neuml.github.io/txtai/pipeline/text/translation)        | [OPUS Model Series](https://hf.co/Helsinki-NLP)                          |

Models can be loaded as either a path from the Hugging Face Hub or a local directory. Model paths are optional, defaults are loaded when not specified. For tasks with no recommended model, txtai uses the default models as shown in the Hugging Face Tasks guide.

See the following links to learn more.

- [Hugging Face Tasks](https://hf.co/tasks)
- [Hugging Face Model Hub](https://hf.co/models)
- [MTEB Leaderboard](https://hf.co/spaces/mteb/leaderboard)
- [LMSYS LLM Leaderboard](https://chat.lmsys.org/?leaderboard)
- [Open LLM Leaderboard](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard)

## Powered by txtai

The following applications are powered by txtai.

![apps](https://raw.githubusercontent.com/neuml/txtai/master/apps.jpg)

| Application  | Description  |
|:------------ |:-------------|
| [rag](https://github.com/neuml/rag) | Retrieval Augmented Generation (RAG) application |
| [ragdata](https://github.com/neuml/ragdata) | Build knowledge bases for RAG |
| [paperai](https://github.com/neuml/paperai) | AI for medical and scientific papers |
| [annotateai](https://github.com/neuml/annotateai) | Automatically annotate papers with LLMs |

In addition to this list, there are also many other [open-source projects](https://github.com/neuml/txtai/network/dependents), [published research](https://scholar.google.com/scholar?q=txtai&hl=en&as_ylo=2022) and closed proprietary/commercial projects that have built on txtai in production.

## Further Reading

![further](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/further.png#gh-light-mode-only)
![further](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/further-ghdark.png#gh-dark-mode-only)

- [Introducing txtai, the all-in-one AI framework](https://medium.com/neuml/introducing-txtai-the-all-in-one-ai-framework-0660ecfc39d7)
- [Tutorial series on Hashnode](https://neuml.hashnode.dev/series/txtai-tutorial) | [dev.to](https://dev.to/neuml/tutorial-series-on-txtai-ibg)
- [What's new in txtai 9.0](https://medium.com/neuml/whats-new-in-txtai-9-0-d522bb150afa) | [8.0](https://medium.com/neuml/whats-new-in-txtai-8-0-2d7d0ab4506b) | [7.0](https://medium.com/neuml/whats-new-in-txtai-7-0-855ad6a55440) | [6.0](https://medium.com/neuml/whats-new-in-txtai-6-0-7d93eeedf804) | [5.0](https://medium.com/neuml/whats-new-in-txtai-5-0-e5c75a13b101) | [4.0](https://medium.com/neuml/whats-new-in-txtai-4-0-bbc3a65c3d1c)
- [Getting started with semantic search](https://medium.com/neuml/getting-started-with-semantic-search-a9fd9d8a48cf) | [workflows](https://medium.com/neuml/getting-started-with-semantic-workflows-2fefda6165d9) | [rag](https://medium.com/neuml/getting-started-with-rag-9a0cca75f748)
- [Running txtai at scale](https://medium.com/neuml/running-at-scale-with-txtai-71196cdd99f9)
- [Vector search & RAG Landscape: A review with txtai](https://medium.com/neuml/vector-search-rag-landscape-a-review-with-txtai-a7f37ad0e187)

## Documentation

[Full documentation on txtai](https://neuml.github.io/txtai) including configuration settings for embeddings, pipelines, workflows, API and a FAQ with common questions/issues is available.

## Contributing

For those who would like to contribute to txtai, please see [this guide](https://github.com/neuml/.github/blob/master/CONTRIBUTING.md).



================================================
FILE: CITATION.cff
================================================
cff-version: 1.2.0
date-released: 2020-08-11
message: "If you use this software, please cite it as below."
title: "txtai: the all-in-one AI framework"
abstract: "txtai is an all-in-one open-source AI framework for semantic search, LLM orchestration and language model workflows"
url: "https://github.com/neuml/txtai"
authors:
- family-names: "Mezzetti"
  given-names: "David"
  affiliation: NeuML
license: Apache-2.0



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   Copyright 2020- NeuML LLC

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================
FILE: Makefile
================================================
# Project utility scripts
.PHONY: test

# Setup environment
export SRC_DIR := ./src/python
export TEST_DIR := ./test/python
export PYTHONPATH := ${SRC_DIR}:${TEST_DIR}:${PYTHONPATH}
export PATH := ${TEST_DIR}:${PATH}
export PYTHONWARNINGS := ignore

# Disable tokenizer parallelism for tests
export TOKENIZERS_PARALLELISM := false

# Default python executable if not provided
PYTHON ?= python

# Check for wget
WGET := $(shell wget --version 2> /dev/null)
ifndef WGET
    $(error "Required binary `wget` not found, please install wget OS package")
endif

# Download test data
data:
	mkdir -p /tmp/txtai
	wget -N https://github.com/neuml/txtai/releases/download/v6.2.0/tests.tar.gz -P /tmp
	tar -xvzf /tmp/tests.tar.gz -C /tmp

# Unit tests
test:
	${PYTHON} -m unittest discover -v -s ${TEST_DIR}

# Run tests while calculating code coverage
coverage:
	coverage run -m unittest discover -v -k testagent -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testann -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testapi -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testapp -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testarchive -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testcloud -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testconsole -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testdatabase -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testembeddings -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testgraph -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testmodels -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testoptional -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testaudio -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testdata -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testimage -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testllm -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testtext -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testpipeline.testtrain -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testscoring -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testserialize -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testvectors -s ${TEST_DIR}
	coverage run -m unittest discover -v -k testworkflow -s ${TEST_DIR}
	coverage combine



================================================
FILE: mkdocs.yml
================================================
#
# txtai Material for MkDocs configuration
#

site_name: txtai
site_description: "txtai is an all-in-one open-source AI framework for semantic search, LLM orchestration and language model workflows"
repo_name: neuml/txtai
repo_url: https://github.com/neuml/txtai
copyright: Â© NeuML LLC, Apache-2.0 License
theme:
    name: material
    logo: images/logo.png
    favicon: images/logo.png
    custom_dir: docs/overrides
    icon:
        repo: fontawesome/brands/github
    palette:
    - media: "(prefers-color-scheme: light)"
      scheme: default
      primary: blue
      accent: blue
      toggle:
        icon: material/toggle-switch-off-outline
        name: Switch to dark mode
    - media: "(prefers-color-scheme: dark)"
      scheme: slate
      primary: light blue
      accent: light blue
      toggle:
        icon: material/toggle-switch
        name: Switch to light mode
    features:
        - navigation.indexes
        - navigation.instant
plugins:
    - search
    - mkdocstrings:
        handlers:
            python:
                options:
                    show_root_full_path: false
                    show_root_heading: true
                    show_root_toc_entry: false
    - redirects:
        redirect_maps:
            "pipeline/text/extractor.md": "pipeline/text/rag.md"
            "pipeline/text/generator.md": "pipeline/text/llm.md"
            "pipeline/text/sequences.md": "pipeline/text/llm.md"

markdown_extensions:
    - pymdownx.highlight
    - pymdownx.superfences
nav:
    - Home: index.md
    - Why txtai?: why.md
    - Use Cases: usecases.md
    - Installation: install.md
    - Model Guide: models.md
    - Embeddings:
        - embeddings/index.md
        - Configuration:
            - embeddings/configuration/index.md
            - ANN: embeddings/configuration/ann.md
            - Cloud: embeddings/configuration/cloud.md
            - Database: embeddings/configuration/database.md
            - General: embeddings/configuration/general.md
            - Graph: embeddings/configuration/graph.md
            - Scoring: embeddings/configuration/scoring.md
            - Vectors: embeddings/configuration/vectors.md
        - Index Format: embeddings/format.md
        - Index Guide: embeddings/indexing.md
        - Methods: embeddings/methods.md
        - Query Guide: embeddings/query.md
    - Agent:
        - agent/index.md
        - Configuration: agent/configuration.md
        - Methods: agent/methods.md
    - Pipeline:
        - pipeline/index.md
        - Audio:
            - Audio Mixer: pipeline/audio/audiomixer.md
            - Audio Stream: pipeline/audio/audiostream.md
            - Microphone: pipeline/audio/microphone.md
            - Text To Audio: pipeline/audio/texttoaudio.md
            - Text To Speech: pipeline/audio/texttospeech.md
            - Transcription: pipeline/audio/transcription.md
        - Data:
            - File To HTML: pipeline/data/filetohtml.md
            - HTML To Markdown: pipeline/data/htmltomd.md
            - Segmentation: pipeline/data/segmentation.md
            - Tabular: pipeline/data/tabular.md
            - Textractor: pipeline/data/textractor.md
        - Image:
            - Caption: pipeline/image/caption.md
            - Image Hash: pipeline/image/imagehash.md
            - Objects: pipeline/image/objects.md
        - Text:
            - Entity: pipeline/text/entity.md
            - Labels: pipeline/text/labels.md
            - LLM: pipeline/text/llm.md
            - RAG: pipeline/text/rag.md
            - Reranker: pipeline/text/reranker.md
            - Similarity: pipeline/text/similarity.md
            - Summary: pipeline/text/summary.md
            - Translation: pipeline/text/translation.md
        - Train:
            - HF ONNX: pipeline/train/hfonnx.md
            - ML ONNX: pipeline/train/mlonnx.md
            - Trainer: pipeline/train/trainer.md
    - Workflow:
        - workflow/index.md
        - Schedule: workflow/schedule.md
        - Tasks:
            - workflow/task/index.md
            - Console: workflow/task/console.md
            - Export: workflow/task/export.md
            - File: workflow/task/file.md
            - Image: workflow/task/image.md
            - Retrieve: workflow/task/retrieve.md
            - Service: workflow/task/service.md
            - Storage: workflow/task/storage.md
            - Template: workflow/task/template.md
            - Url: workflow/task/url.md
            - Workflow: workflow/task/workflow.md
    - API:
        - api/index.md
        - Cluster: api/cluster.md
        - Configuration: api/configuration.md
        - Customization: api/customization.md
        - Methods: api/methods.md
        - Model Context Protocol: api/mcp.md
        - OpenAI: api/openai.md
        - Security: api/security.md
    - Cloud: cloud.md
    - Examples: examples.md
    - FAQ: faq.md
    - Observability: observability.md
    - Powered by txtai: poweredby.md
    - Further Reading: further.md



================================================
FILE: pyproject.toml
================================================
[tool.black]
line-length = 150



================================================
FILE: setup.py
================================================
# pylint: disable = C0111
from setuptools import find_packages, setup

with open("README.md", "r", encoding="utf-8") as f:
    # Remove GitHub dark mode images
    DESCRIPTION = "".join([line for line in f if "gh-dark-mode-only" not in line])

# Required dependencies
install = ["faiss-cpu>=1.7.1.post2", "msgpack>=1.0.7", "torch>=2.4", "transformers>=4.46.3"]

# Required dependencies that are also base transformers dependencies
install += ["huggingface-hub>=0.34.0", "numpy>=1.18.4", "pyyaml>=5.3", "regex>=2022.8.17", "safetensors>=0.4.5"]

# Optional dependencies
extras = {}

# Development dependencies - not included in "all" install
extras["dev"] = [
    "black",
    "coverage",
    "coveralls",
    "httpx",
    "mkdocs-material",
    "mkdocs-redirects",
    "mkdocstrings[python]",
    "pre-commit",
    "pylint",
]

extras["agent"] = ["mcpadapt>=0.1.0", "smolagents>=1.17"]

extras["ann"] = [
    "annoy>=1.16.3",
    "bitsandbytes>=0.42.0",
    "ggml-py>=0.9.4",
    "hnswlib>=0.5.0",
    "pgvector>=0.4.1",
    "scikit-learn>=0.23.1",
    "scipy>=1.4.1",
    "sqlalchemy>=2.0.20",
    "sqlite-vec>=0.1.1",
]

extras["api"] = [
    "aiohttp>=3.8.1",
    "fastapi>=0.94.0",
    "fastapi-mcp>=0.2.0",
    "httpx>=0.28.1",
    "pillow>=7.1.2",
    "python-multipart>=0.0.7",
    "uvicorn>=0.12.1",
]

extras["cloud"] = ["apache-libcloud>=3.3.1", "fasteners>=0.14.1"]

extras["console"] = ["rich>=12.0.1"]

extras["database"] = ["duckdb>=0.7.1", "pillow>=7.1.2", "sqlalchemy>=2.0.20"]

extras["graph"] = ["grand-cypher>=0.6.0", "grand-graph>=0.6.0", "networkx>=2.7.1", "sqlalchemy>=2.0.20"]

extras["model"] = ["onnx>=1.11.0", "onnxruntime>=1.11.0"]

extras["pipeline-audio"] = [
    "onnx>=1.11.0",
    "onnxruntime>=1.11.0",
    "scipy>=1.4.1",
    "sounddevice>=0.5.0",
    "soundfile>=0.10.3.post1",
    "ttstokenizer>=1.1.0",
    "webrtcvad-wheels>=2.0.14",
]

extras["pipeline-data"] = ["beautifulsoup4>=4.9.3", "chonkie>=1.0.2", "docling>=2.8.2", "nltk>=3.5", "pandas>=1.1.0", "tika>=1.24"]

extras["pipeline-image"] = ["imagehash>=4.2.1", "pillow>=7.1.2", "timm>=0.4.12"]

extras["pipeline-llm"] = ["litellm>=1.37.16", "llama-cpp-python>=0.2.75"]

extras["pipeline-text"] = ["gliner>=0.2.16", "sentencepiece>=0.1.91", "staticvectors>=0.2.0"]

extras["pipeline-train"] = [
    "accelerate>=0.26.0",
    "bitsandbytes>=0.42.0",
    "onnx>=1.11.0",
    "onnxmltools>=1.9.1",
    "onnxruntime>=1.11.0",
    "peft>=0.8.1",
    "skl2onnx>=1.9.1",
]

extras["pipeline"] = (
    extras["pipeline-audio"]
    + extras["pipeline-data"]
    + extras["pipeline-image"]
    + extras["pipeline-llm"]
    + extras["pipeline-text"]
    + extras["pipeline-train"]
)

extras["scoring"] = ["sqlalchemy>=2.0.20"]

extras["vectors"] = [
    "litellm>=1.37.16",
    "llama-cpp-python>=0.2.75",
    "model2vec>=0.3.0",
    "scikit-learn>=0.23.1",
    "scipy>=1.4.1",
    "sentence-transformers>=5.0.0",
    "skops>=0.9.0",
    "staticvectors>=0.2.0",
]

extras["workflow"] = [
    "apache-libcloud>=3.3.1",
    "croniter>=1.2.0",
    "openpyxl>=3.0.9",
    "pandas>=1.1.0",
    "pillow>=7.1.2",
    "requests>=2.26.0",
    "xmltodict>=0.12.0",
]

# Backwards-compatible combination of ann and vectors extra
extras["similarity"] = extras["ann"] + extras["vectors"]

extras["all"] = (
    extras["agent"]
    + extras["api"]
    + extras["cloud"]
    + extras["console"]
    + extras["database"]
    + extras["graph"]
    + extras["model"]
    + extras["pipeline"]
    + extras["scoring"]
    + extras["similarity"]
    + extras["workflow"]
)

setup(
    name="txtai",
    version="9.2.0",
    author="NeuML",
    description="All-in-one open-source AI framework for semantic search, LLM orchestration and language model workflows",
    long_description=DESCRIPTION,
    long_description_content_type="text/markdown",
    url="https://github.com/neuml/txtai",
    project_urls={
        "Documentation": "https://github.com/neuml/txtai",
        "Issue Tracker": "https://github.com/neuml/txtai/issues",
        "Source Code": "https://github.com/neuml/txtai",
    },
    license="Apache 2.0: http://www.apache.org/licenses/LICENSE-2.0",
    packages=find_packages(where="src/python"),
    package_dir={"": "src/python"},
    keywords="search embedding machine-learning nlp",
    python_requires=">=3.10",
    install_requires=install,
    extras_require=extras,
    classifiers=[
        "License :: OSI Approved :: Apache Software License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Software Development",
        "Topic :: Text Processing :: Indexing",
        "Topic :: Utilities",
    ],
)



================================================
FILE: .coveragerc
================================================
[run]
source = src/python
concurrency = multiprocessing,thread
disable_warnings = no-data-collected
omit = **/__main__.py

[combine]
disable_warnings = no-data-collected



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/pycqa/pylint
    rev: v3.3.1
    hooks:
    - id: pylint
      args:
      - -d import-error
      - -d duplicate-code
      - -d too-many-positional-arguments
  - repo: https://github.com/ambv/black
    rev: 24.10.0
    hooks:
    - id: black
      language_version: python3



================================================
FILE: .pylintrc
================================================
[BASIC]
module-rgx=[a-z_][a-zA-Z0-9_]{2,30}$
method-rgx=[a-z_][a-zA-Z0-9_]{2,30}$
function-rgx=[a-z_][a-zA-Z0-9_]{2,30}$
argument-rgx=[a-z_][a-zA-Z0-9_]{0,30}$
variable-rgx=[a-z_][a-zA-Z0-9_]{0,30}$
attr-rgx=[a-z_][a-zA-Z0-9_]{0,30}$

[DESIGN]
max-args=10
max-locals=40
max-returns=10
max-attributes=20
min-public-methods=0

[FORMAT]
max-line-length=150



================================================
FILE: docker/api/Dockerfile
================================================
# Set base image
ARG BASE_IMAGE=neuml/txtai-cpu
FROM $BASE_IMAGE

# Copy configuration
COPY config.yml .

# Run local API instance to cache models in container
RUN python -c "from txtai.api import API; API('config.yml', False)"

# Start server and listen on all interfaces
ENV CONFIG "config.yml"
ENTRYPOINT ["uvicorn", "--host", "0.0.0.0", "txtai.api:app"]



================================================
FILE: docker/aws/api.py
================================================
"""
Lambda handler for a txtai API instance
"""

from mangum import Mangum

from txtai.api import app, start

# pylint: disable=C0103
# Create FastAPI application instance wrapped by Mangum
handler = None
if not handler:
    # Start application
    start()

    # Create handler
    handler = Mangum(app, lifespan="off")



================================================
FILE: docker/aws/Dockerfile
================================================
# Set base image
ARG BASE_IMAGE=neuml/txtai-cpu
FROM $BASE_IMAGE

# Application script to copy into image
ARG APP=api.py

# Install Lambda Runtime Interface Client and Mangum ASGI bindings
RUN pip install awslambdaric mangum

# Copy configuration
COPY config.yml .

# Run local API instance to cache models in container
RUN python -c "from txtai.api import API; API('config.yml', False)"

# Copy application
COPY $APP ./app.py

# Start runtime client using default application handler
ENV CONFIG "config.yml"
ENTRYPOINT ["python", "-m", "awslambdaric"]
CMD ["app.handler"]



================================================
FILE: docker/aws/workflow.py
================================================
"""
Lambda handler for txtai workflows
"""

import json

from txtai.api import API

APP = None


# pylint: disable=W0603,W0613
def handler(event, context):
    """
    Runs a workflow using input event parameters.

    Args:
        event: input event
        context: input context

    Returns:
        Workflow results
    """

    # Create (or get) global app instance
    global APP
    APP = APP if APP else API("config.yml")

    # Get parameters from event body
    event = json.loads(event["body"])

    # Run workflow and return results
    return {"statusCode": 200, "headers": {"Content-Type": "application/json"}, "body": list(APP.workflow(event["name"], event["elements"]))}



================================================
FILE: docker/base/Dockerfile
================================================
# Set base image
ARG BASE_IMAGE=python:3.10-slim
FROM $BASE_IMAGE

# Install GPU-enabled version of PyTorch if set
ARG GPU

# Target CPU architecture
ARG TARGETARCH

# Set Python version (i.e. 3, 3.10)
ARG PYTHON_VERSION=3

# List of txtai components to install
ARG COMPONENTS=[all]

# Locale environment variables
ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8

RUN \
    # Install required packages
    apt-get update && \
    apt-get -y --no-install-recommends install libgomp1 libportaudio2 libsndfile1 git gcc g++ python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python3-pip && \
    rm -rf /var/lib/apt/lists && \
    \
    # Install txtai project and dependencies
    ln -s /usr/bin/python${PYTHON_VERSION} /usr/bin/python && \
    python -m pip install --no-cache-dir -U pip wheel setuptools && \
    if [ -z ${GPU} ] && { [ -z ${TARGETARCH} ] || [ ${TARGETARCH} = "amd64" ] ;}; then pip install --no-cache-dir torch==2.9.0+cpu torchvision==0.24.0+cpu -f https://download.pytorch.org/whl/torch -f https://download.pytorch.org/whl/torchvision; fi && \
    python -m pip install --no-cache-dir txtai${COMPONENTS} && \
    python -c "import sys, importlib.util as util; 1 if util.find_spec('nltk') else sys.exit(); import nltk; nltk.download(['punkt', 'punkt_tab', 'averaged_perceptron_tagger_eng'])" && \
    \
    # Cleanup build packages
    apt-get -y purge git gcc g++ python${PYTHON_VERSION}-dev && apt-get -y autoremove

# Set default working directory
WORKDIR /app



================================================
FILE: docker/schedule/Dockerfile
================================================
# Set base image
ARG BASE_IMAGE=neuml/txtai-cpu
FROM $BASE_IMAGE

# Copy configuration
COPY config.yml .

# Run local API instance to cache models in container
RUN python -c "from txtai.api import API; API('config.yml', False)"

# Start application and wait for completion. Scheduled workflows can run indefinitely. 
ENTRYPOINT ["python", "-c", "from txtai.api import API; API('config.yml').wait()"]



================================================
FILE: docker/workflow/Dockerfile
================================================
# Set base image
ARG BASE_IMAGE=neuml/txtai-cpu
FROM $BASE_IMAGE

# Copy configuration
COPY config.yml .

# Run local API instance to cache models in container
RUN python -c "from txtai.api import API; API('config.yml', False)"

# Run workflow. Requires two command line arguments: name of workflow and input elements
ENTRYPOINT ["python", "-c", "import sys; from txtai.api import API\nfor _ in API('config.yml').workflow(sys.argv[1], sys.argv[2:]): pass"]
CMD ["workflow"]



================================================
FILE: docs/cloud.md
================================================
# Cloud

![cloud](images/cloud.png#only-light)
![cloud](images/cloud-dark.png#only-dark)

Scalable cloud-native applications can be built with txtai. The following cloud runtimes are supported.

- Container Orchestration Systems (i.e. Kubernetes)
- Docker Engine
- Serverless Compute
- txtai.cloud (planned for future)

Images for txtai are available on Docker Hub for [CPU](https://hub.docker.com/r/neuml/txtai-cpu) and [GPU](https://hub.docker.com/r/neuml/txtai-gpu) installs. The CPU install is recommended when GPUs aren't available given the image is significantly smaller.

The base txtai images have no models installed and models will be downloaded each time the container starts. Caching the models is recommended as that will significantly reduce container start times. This can be done a couple different ways.

- Create a container with the [models cached](#container-image-model-caching)
- Set the transformers cache environment variable and mount that volume when starting the image
    ```bash
    docker run -v <local dir>:/models -e TRANSFORMERS_CACHE=/models --rm -it <docker image>
    ```

## Build txtai images

The txtai images found on Docker Hub are configured to support most situations. This image can be locally built with different options as desired.

Examples build commands below.

```bash
# Get Dockerfile
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/base/Dockerfile

# Build Ubuntu 22.04 image running Python 3.10
docker build -t txtai --build-arg BASE_IMAGE=ubuntu:22.04 --build-arg PYTHON_VERSION=3.10 .

# Build image with GPU support
docker build -t txtai --build-arg GPU=1 .

# Build minimal image with the base txtai components
docker build -t txtai --build-arg COMPONENTS= .
```

## Container image model caching

As mentioned previously, model caching is recommended to reduce container start times. The following commands demonstrate this. In all cases, it is assumed a config.yml file is present in the local directory with the desired configuration set.

### API
This section builds an image that caches models and starts an API service. The config.yml file should be configured with the desired components to expose via the API.

The following is a sample config.yml file that creates an Embeddings API service.

```yaml
# config.yml
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
```

The next section builds the image and starts an instance.

```bash
# Get Dockerfile
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/api/Dockerfile

# CPU build
docker build -t txtai-api .

# GPU build
docker build -t txtai-api --build-arg BASE_IMAGE=neuml/txtai-gpu .

# Run
docker run -p 8000:8000 --rm -it txtai-api
```

### Service
This section builds a scheduled workflow service. [More on scheduled workflows can be found here.](../workflow/schedule)

```bash
# Get Dockerfile
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/service/Dockerfile

# CPU build
docker build -t txtai-service .

# GPU build
docker build -t txtai-service --build-arg BASE_IMAGE=neuml/txtai-gpu .

# Run
docker run --rm -it txtai-service
```

### Workflow
This section builds a single run workflow. [Example workflows can be found here.](../examples/#workflows)

```bash
# Get Dockerfile
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/workflow/Dockerfile

# CPU build
docker build -t txtai-workflow . 

# GPU build
docker build -t txtai-workflow --build-arg BASE_IMAGE=neuml/txtai-gpu .

# Run
docker run --rm -it txtai-workflow <workflow name> <workflow parameters>
```

## Serverless Compute

One of the most powerful features of txtai is building YAML-configured applications with the "build once, run anywhere" approach. API instances and workflows can run locally, on a server, on a cluster or serverless.

Serverless instances of txtai are supported on frameworks such as [AWS Lambda](https://aws.amazon.com/lambda/), [Google Cloud Functions](https://cloud.google.com/functions), [Azure Cloud Functions](https://azure.microsoft.com/en-us/services/functions/) and [Kubernetes](https://kubernetes.io/) with [Knative](https://knative.dev/docs/).

### AWS Lambda

The following steps show a basic example of how to build a serverless API instance with [AWS SAM](https://github.com/aws/serverless-application-model).

- Create config.yml and template.yml

```yaml
# config.yml
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
```

```yaml
# template.yml
Resources:
  txtai:
    Type: AWS::Serverless::Function
    Properties:
      PackageType: Image
      MemorySize: 3000
      Timeout: 20
      Events:
        Api:
          Type: Api
          Properties:
            Path: "/{proxy+}"
            Method: ANY
    Metadata:
      Dockerfile: Dockerfile
      DockerContext: ./
      DockerTag: api
```

- Install [AWS SAM](https://pypi.org/project/aws-sam-cli/)

- Run following

```bash
# Get Dockerfile and application
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/api.py
wget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/Dockerfile

# Build the docker image
sam build

# Start API gateway and Lambda instance locally
sam local start-api -p 8000 --warm-containers LAZY

# Verify instance running (should return 0)
curl http://localhost:8080/count
```

If successful, a local API instance is now running in a "serverless" fashion. This configuration can be deployed to AWS using SAM. [See this link for more information.](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-deploy.html)

### Kubernetes with Knative

txtai scales with container orchestration systems. This can be self-hosted or with a cloud provider such as [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/), [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine) and [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/). There are also other smaller providers with a managed Kubernetes offering.

A full example covering how to build a serverless txtai application on Kubernetes with Knative [can be found here](https://medium.com/neuml/serverless-vector-search-with-txtai-96f6163ab972).

## txtai.cloud

[txtai.cloud](https://txtai.cloud) is a planned effort that will offer an easy and secure way to run hosted txtai applications.



================================================
FILE: docs/examples.md
================================================
# Examples

![examples](images/examples.png#only-light)
![examples](images/examples-dark.png#only-dark)

See below for a comprehensive series of example notebooks and applications covering txtai.

## Semantic Search

Build semantic/similarity/vector/neural search applications.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=SIezMnVdmMs) | Overview of the functionality provided by txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |
| [Build an Embeddings index with Hugging Face Datasets](https://github.com/neuml/txtai/blob/master/examples/02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb) | Index and search Hugging Face Datasets | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb) |
| [Build an Embeddings index from a data source](https://github.com/neuml/txtai/blob/master/examples/03_Build_an_Embeddings_index_from_a_data_source.ipynb)  | Index and search a data source with word embeddings | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/03_Build_an_Embeddings_index_from_a_data_source.ipynb) |
| [Add semantic search to Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb)  | Add semantic search to existing search systems | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb) |
| [Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) | Embed images and text into the same space for search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |
| [Custom Embeddings SQL functions](https://github.com/neuml/txtai/blob/master/examples/30_Embeddings_SQL_custom_functions.ipynb) | Add user-defined functions to Embeddings SQL | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/30_Embeddings_SQL_custom_functions.ipynb) |
| [Model explainability](https://github.com/neuml/txtai/blob/master/examples/32_Model_explainability.ipynb) | Explainability for semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/32_Model_explainability.ipynb) |
| [Query translation](https://github.com/neuml/txtai/blob/master/examples/33_Query_translation.ipynb) | Domain-specific natural language queries with query translation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/33_Query_translation.ipynb) |
| [Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) | Question matching with semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |
| [Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) | Explore topics, data connectivity and run network analysis| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |
| [Topic Modeling with BM25](https://github.com/neuml/txtai/blob/master/examples/39_Classic_Topic_Modeling_with_BM25.ipynb) | Topic modeling backed by a BM25 index | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/39_Classic_Topic_Modeling_with_BM25.ipynb) |

## LLM

Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Prompt-driven search with LLMs](https://github.com/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) | Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) |
| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |
| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |
| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |
| [Generate knowledge with Semantic Graphs and RAG](https://github.com/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) | Knowledge exploration and discovery with Semantic Graphs and RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) |
| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |
| [Advanced RAG with graph path traversal](https://github.com/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) | Graph path traversal to collect complex sets of data for advanced RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) |
| [Advanced RAG with guided generation](https://github.com/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) | Retrieval Augmented and Guided Generation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) |
| [RAG with llama.cpp and external API services](https://github.com/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) | RAG with additional vector and LLM frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) |
| [How RAG with txtai works](https://github.com/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) | Create RAG processes, API services and Docker instances | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Getting started with LLM APIs](https://github.com/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) | Generate embeddings and run LLMs with OpenAI, Claude, Gemini, Bedrock and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |
| [Medical RAG Research with txtai](https://github.com/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) | Analyze PubMed article metadata with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) |
| [GraphRAG with Wikipedia and GPT OSS](https://github.com/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) | Deep graph search powered RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) |

## Pipelines

Transform data with language model backed pipelines.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Extractive QA with txtai](https://github.com/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) | Introduction to extractive question-answering with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) |
| [Extractive QA with Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) | Run extractive question-answering queries with Elasticsearch | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) |
| [Extractive QA to build structured data](https://github.com/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) | Build structured datasets using extractive question-answering | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) |
| [Apply labels with zero shot classification](https://github.com/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) | Use zero shot learning for labeling, classification and topic modeling | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) |
| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |
| [Extract text from documents](https://github.com/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) | Extract text from PDF, Office, HTML and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) |
| [Text to speech generation](https://github.com/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) | Generate speech from text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) |
| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |
| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |
| [Generate image captions and detect objects](https://github.com/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) | Captions and object detection for images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) |
| [Near duplicate image detection](https://github.com/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) | Identify duplicate and near-duplicate images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) |

## Workflows

Efficiently process data at scale.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=UBMPDCn1gEU) | Simple yet powerful constructs to efficiently process data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |
| [Transform tabular data with composable workflows](https://github.com/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) | Transform, index and search tabular data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) |
| [Tensor workflows](https://github.com/neuml/txtai/blob/master/examples/23_Tensor_workflows.ipynb) | Performant processing of large tensor arrays | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/23_Tensor_workflows.ipynb) |
| [Entity extraction workflows](https://github.com/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) | Identify entity/label combinations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) |
| [Workflow Scheduling](https://github.com/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) | Schedule workflows with cron expressions | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) |
| [Push notifications with workflows](https://github.com/neuml/txtai/blob/master/examples/28_Push_notifications_with_workflows.ipynb) | Generate and push notifications with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/28_Push_notifications_with_workflows.ipynb) |
| [Pictures are a worth a thousand words](https://github.com/neuml/txtai/blob/master/examples/35_Pictures_are_worth_a_thousand_words.ipynb) | Generate webpage summary images with DALL-E mini | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/35_Pictures_are_worth_a_thousand_words.ipynb) |
| [Run txtai with native code](https://github.com/neuml/txtai/blob/master/examples/36_Run_txtai_in_native_code.ipynb) | Execute workflows in native code with the Python C API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/36_Run_txtai_in_native_code.ipynb) |
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |

## Model Training

Train NLP models.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Train a text labeler](https://github.com/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) | Build text sequence classification models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) |
| [Train without labels](https://github.com/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) | Use zero-shot classifiers to train new models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) |
| [Train a QA model](https://github.com/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) | Build and fine-tune question-answering models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) |
| [Train a language model from scratch](https://github.com/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) | Build new language models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) |
| [Export and run models with ONNX](https://github.com/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) | Export models with ONNX, run natively in JavaScript, Java and Rust | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) |
| [Export and run other machine learning models](https://github.com/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) | Export and run models from scikit-learn, PyTorch and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) |

## API

Run distributed txtai, integrate with the API and cloud endpoints.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [API Gallery](https://github.com/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) | Using txtai in JavaScript, Java, Rust and Go | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) |
| [Distributed embeddings cluster](https://github.com/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) | Distribute an embeddings index across multiple data nodes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) |
| [Embeddings in the Cloud](https://github.com/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) | Load and use an embeddings index from the Hugging Face Hub | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) |
| [Custom API Endpoints](https://github.com/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) | Extend the API with custom endpoints | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) |
| [API Authorization and Authentication](https://github.com/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) | Add authorization, authentication and middleware dependencies to the API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) |
| [OpenAI Compatible API](https://github.com/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) | Connect to txtai with a standard OpenAI client library | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) |

## Architecture

Project architecture, data formats, external integrations, scale to production, benchmarks, and performance.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Anatomy of a txtai index](https://github.com/neuml/txtai/blob/master/examples/29_Anatomy_of_a_txtai_index.ipynb) | Deep dive into the file formats behind a txtai embeddings index | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/29_Anatomy_of_a_txtai_index.ipynb) |
| [Embeddings components](https://github.com/neuml/txtai/blob/master/examples/37_Embeddings_index_components.ipynb) | Composable search with vector, SQL and scoring components | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/37_Embeddings_index_components.ipynb) |
| [Customize your own embeddings database](https://github.com/neuml/txtai/blob/master/examples/45_Customize_your_own_embeddings_database.ipynb) | Ways to combine vector indexes with relational databases | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/45_Customize_your_own_embeddings_database.ipynb) |
| [Building an efficient sparse keyword index in Python](https://github.com/neuml/txtai/blob/master/examples/47_Building_an_efficient_sparse_keyword_index_in_Python.ipynb) | Fast and accurate sparse keyword indexing | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/47_Building_an_efficient_sparse_keyword_index_in_Python.ipynb) |
| [Benefits of hybrid search](https://github.com/neuml/txtai/blob/master/examples/48_Benefits_of_hybrid_search.ipynb) | Improve accuracy with a combination of semantic and keyword search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/48_Benefits_of_hybrid_search.ipynb) |
| [External database integration](https://github.com/neuml/txtai/blob/master/examples/49_External_database_integration.ipynb) | Store metadata in PostgreSQL, MariaDB, MySQL and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/49_External_database_integration.ipynb) |
| [All about vector quantization](https://github.com/neuml/txtai/blob/master/examples/50_All_about_vector_quantization.ipynb) | Benchmarking scalar and product quantization methods | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/50_All_about_vector_quantization.ipynb) |
| [External vectorization](https://github.com/neuml/txtai/blob/master/examples/56_External_vectorization.ipynb) | Vectorization with precomputed embeddings datasets and APIs | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/56_External_vectorization.ipynb) |
| [Integrate txtai with Postgres](https://github.com/neuml/txtai/blob/master/examples/61_Integrate_txtai_with_Postgres.ipynb) | Persist content, vectors and graph data in Postgres | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/61_Integrate_txtai_with_Postgres.ipynb) |
| [Embeddings index format for open data access](https://github.com/neuml/txtai/blob/master/examples/64_Embeddings_index_format_for_open_data_access.ipynb) | Platform and programming language independent data storage with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/64_Embeddings_index_format_for_open_data_access.ipynb) |
| [Accessing Low Level Vector APIs](https://github.com/neuml/txtai/blob/master/examples/78_Accessing_Low_Level_Vector_APIs.ipynb) | Build a vector database using txtai's low-level APIs | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/78_Accessing_Low_Level_Vector_APIs.ipynb) |

## Releases

New functionality added in major releases.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [What's new in txtai 4.0](https://github.com/neuml/txtai/blob/master/examples/24_Whats_new_in_txtai_4_0.ipynb) | Content storage, SQL, object storage, reindex and compressed indexes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/24_Whats_new_in_txtai_4_0.ipynb) |
| [What's new in txtai 6.0](https://github.com/neuml/txtai/blob/master/examples/46_Whats_new_in_txtai_6_0.ipynb) | Sparse, hybrid and subindexes for embeddings, LLM improvements | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/46_Whats_new_in_txtai_6_0.ipynb) |
| [What's new in txtai 7.0](https://github.com/neuml/txtai/blob/master/examples/59_Whats_new_in_txtai_7_0.ipynb) | Semantic graph 2.0, LoRA/QLoRA training and binary API support | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/59_Whats_new_in_txtai_7_0.ipynb) |
| [What's new in txtai 8.0](https://github.com/neuml/txtai/blob/master/examples/67_Whats_new_in_txtai_8_0.ipynb) | Agents with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/67_Whats_new_in_txtai_8_0.ipynb) |
| [What's new in txtai 9.0](https://github.com/neuml/txtai/blob/master/examples/76_Whats_new_in_txtai_9_0.ipynb) | Learned sparse vectors, late interaction models and rerankers | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/76_Whats_new_in_txtai_9_0.ipynb) |

## Applications

Series of example applications with txtai. Links to hosted versions on [Hugging Face Spaces](https://hf.co/spaces) are also provided, when available.

| Application  | Description  |       |
|:-------------|:-------------|------:|
| [Basic similarity search](https://github.com/neuml/txtai/blob/master/examples/similarity.py) | Basic similarity search example. Data from the original txtai demo. |[ğŸ¤—](https://hf.co/spaces/NeuML/similarity)|
| [Baseball stats](https://github.com/neuml/txtai/blob/master/examples/baseball.py) | Match historical baseball player stats using vector search. |[ğŸ¤—](https://hf.co/spaces/NeuML/baseball)|
| [Benchmarks](https://github.com/neuml/txtai/blob/master/examples/benchmarks.py) | Calculate performance metrics for the BEIR datasets. |*Local run only*|
| [Book search](https://github.com/neuml/txtai/blob/master/examples/books.py) | Book similarity search application. Index book descriptions and query using natural language statements. |*Local run only*|
| [Image search](https://github.com/neuml/txtai/blob/master/examples/images.py) | Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. |[ğŸ¤—](https://hf.co/spaces/NeuML/imagesearch)|
| [Retrieval Augmented Generation](https://github.com/neuml/rag/) | RAG with txtai embeddings databases. Ask questions and get answers from LLMs bound by a context. |*Local run only*|
| [Summarize an article](https://github.com/neuml/txtai/blob/master/examples/article.py) | Summarize an article. Workflow that extracts text from a webpage and builds a summary. |[ğŸ¤—](https://hf.co/spaces/NeuML/articlesummary)|
| [Wiki search](https://github.com/neuml/txtai/blob/master/examples/wiki.py) | Wikipedia search application. Queries Wikipedia API and summarizes the top result. |[ğŸ¤—](https://hf.co/spaces/NeuML/wikisummary)|
| [Workflow builder](https://github.com/neuml/txtai/blob/master/examples/workflows.py) | Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. |[ğŸ¤—](https://hf.co/spaces/NeuML/txtai)|



================================================
FILE: docs/faq.md
================================================
# FAQ

![faq](images/faq.png)

Below is a list of frequently asked questions and common issues encountered.

## Questions

----------

__Question__

What models are recommended?

__Answer__

See the [model guide](../models).

----------

__Question__

What is the best way to track the progress of an `embeddings.index` call?

__Answer__

Wrap the list or generator passed to the index call with tqdm. See [#478](https://github.com/neuml/txtai/issues/478) for more.

----------

__Question__

What is the best way to analyze and debug a txtai process?

__Answer__

See the [observability](../observability) section for more on how this can be enabled in txtai processes.

txtai also has a console application. [This article](https://medium.com/neuml/insights-from-the-txtai-console-d307c28e149e) has more details.

----------

__Question__

How can models be externally loaded and passed to embeddings and pipelines?

__Answer__

Embeddings example.

```python
from transformers import AutoModel, AutoTokenizer
from txtai import Embeddings

# Load model externally
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# Pass to embeddings instance
embeddings = Embeddings(path=model, tokenizer=tokenizer)
```

LLM pipeline example.

```python
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer
from txtai import LLM

# Load Phi 3.5-mini
path = "microsoft/Phi-3.5-mini-instruct"
model = AutoModelForCausalLM.from_pretrained(
  path,
  torch_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(path)

llm = LLM((model, tokenizer))
```

## Common issues

----------

__Issue__

Embeddings query errors like this:

```
SQLError: no such function: json_extract
```

__Solution__

Upgrade Python version as it doesn't have SQLite support for `json_extract`

----------

__Issue__

Segmentation faults and similar errors on macOS

__Solution__

Set the following environment parameters.

- OpenMP threading is handled internally on macOS platforms but it can be disabled via `export OMP_NUM_THREADS=1`
- Disable PyTorch MPS device via `export PYTORCH_MPS_DISABLE=1`
- Disable llama.cpp metal via `export LLAMA_NO_METAL=1`

For more details, refer to [this issue on GitHub](https://github.com/kyamagu/faiss-wheels/issues/100).

----------

__Issue__

Error running SQLite ANN on macOS

```
AttributeError: 'sqlite3.Connection' object has no attribute 'enable_load_extension'
```

__Solution__

See [this note](https://alexgarcia.xyz/sqlite-vec/python.html#macos-blocks-sqlite-extensions-by-default) for options on how to fix this.

----------

__Issue__

`ContextualVersionConflict` and/or package METADATA exception while running one of the [examples](../examples) notebooks on Google Colab

__Solution__

Restart the kernel. See issue [#409](https://github.com/neuml/txtai/issues/409) for more on this issue. 

----------

__Issue__

Error installing optional/extra dependencies such as `pipeline`

__Solution__

The default MacOS shell (zsh) and Windows PowerShell require escaping square brackets

```
pip install 'txtai[pipeline]'
```



================================================
FILE: docs/further.md
================================================
# Further reading

![further](images/further.png#only-light)
![further](images/further-dark.png#only-dark)

- [Introducing txtai, the all-in-one AI framework](https://medium.com/neuml/introducing-txtai-the-all-in-one-ai-framework-0660ecfc39d7)
- [Tutorial series on Hashnode](https://neuml.hashnode.dev/series/txtai-tutorial) | [dev.to](https://dev.to/neuml/tutorial-series-on-txtai-ibg)
- [What's new in txtai 9.0](https://medium.com/neuml/whats-new-in-txtai-9-0-d522bb150afa) | [8.0](https://medium.com/neuml/whats-new-in-txtai-8-0-2d7d0ab4506b) | [7.0](https://medium.com/neuml/whats-new-in-txtai-7-0-855ad6a55440) | [6.0](https://medium.com/neuml/whats-new-in-txtai-6-0-7d93eeedf804) | [5.0](https://medium.com/neuml/whats-new-in-txtai-5-0-e5c75a13b101) | [4.0](https://medium.com/neuml/whats-new-in-txtai-4-0-bbc3a65c3d1c)
- [Getting started with semantic search](https://medium.com/neuml/getting-started-with-semantic-search-a9fd9d8a48cf) | [workflows](https://medium.com/neuml/getting-started-with-semantic-workflows-2fefda6165d9) | [rag](https://medium.com/neuml/getting-started-with-rag-9a0cca75f748)
- [Running txtai at scale](https://medium.com/neuml/running-at-scale-with-txtai-71196cdd99f9)
- [Vector search & RAG Landscape: A review with txtai](https://medium.com/neuml/vector-search-rag-landscape-a-review-with-txtai-a7f37ad0e187)



================================================
FILE: docs/index.md
================================================
#

<p align="center">
    <img src="https://raw.githubusercontent.com/neuml/txtai/master/logo.png"/>
</p>

<p align="center">
    <b>All-in-one AI framework</b>
</p>

<p align="center">
    <a href="https://github.com/neuml/txtai/releases">
        <img src="https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success" alt="Version"/>
    </a>
    <a href="https://github.com/neuml/txtai">
        <img src="https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue" alt="GitHub last commit"/>
    </a>
    <a href="https://github.com/neuml/txtai/issues">
        <img src="https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success" alt="GitHub issues"/>
    </a>
    <a href="https://join.slack.com/t/txtai/shared_invite/zt-37c1zfijp-Y57wMty6YOx_hyIHEQvQJA">
        <img src="https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white" alt="Join Slack"/>
    </a>
    <a href="https://github.com/neuml/txtai/actions?query=workflow%3Abuild">
        <img src="https://github.com/neuml/txtai/workflows/build/badge.svg" alt="Build Status"/>
    </a>
    <a href="https://coveralls.io/github/neuml/txtai?branch=master">
        <img src="https://img.shields.io/coverallsCoverage/github/neuml/txtai" alt="Coverage Status">
    </a>
</p>

txtai is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

![architecture](images/architecture.png#gh-light-mode-only)
![architecture](images/architecture-dark.png#gh-dark-mode-only)

The key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.

This foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.

Build autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.

Summary of txtai features:

- ğŸ” Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing
- ğŸ“„ Create embeddings for text, documents, audio, images and video
- ğŸ’¡ Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more
- â†ªï¸ï¸ Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.
- ğŸ¤– Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems
- âš™ï¸ Web and Model Context Protocol (MCP) APIs. Bindings available for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).
- ğŸ”‹ Batteries included with defaults to get up and running fast
- â˜ï¸ Run local or scale out with container orchestration

txtai is built with Python 3.10+, [Hugging Face Transformers](https://github.com/huggingface/transformers), [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi). txtai is open-source under an Apache 2.0 license.

*Interested in an easy and secure way to run hosted txtai applications? Then join the [txtai.cloud](https://txtai.cloud) preview to learn more.*



================================================
FILE: docs/install.md
================================================
# Installation

![install](images/install.png#only-light)
![install](images/install-dark.png#only-dark)

The easiest way to install is via pip and PyPI

```
pip install txtai
```

Python 3.10+ is supported. Using a Python [virtual environment](https://docs.python.org/3/library/venv.html) is recommended.

## Optional dependencies

txtai has the following optional dependencies that can be installed as extras. The patterns below are supported
in setup.py install_requires sections.

_Note: Extras are provided for convenience. Alternatively, individual packages can be installed to limit dependencies._

### All

Install all dependencies.

```
pip install txtai[all]
```

### ANN

Additional ANN backends.

```
pip install txtai[ann]
```

### API

Serve txtai via a web API.

```
pip install txtai[api]
```

### Cloud

Interface with cloud compute.

```
pip install txtai[cloud]
```

### Console

Command line index query console.

```
pip install txtai[console]
```

### Database

Additional content storage options.

```
pip install txtai[database]
```

### Graph

Topic modeling, data connectivity and network analysis.

```
pip install txtai[graph]
```

### Model

Additional non-standard models.

```
pip install txtai[model]
```

### Pipeline

All pipelines - default install comes with most common pipelines.

```
pip install txtai[pipeline]
```

More granular extras are available for pipeline categories: `pipeline-audio`, `pipeline-data`, `pipeline-image`, `pipeline-llm`, `pipeline-text`, and `pipeline-train`.

### Scoring

Additional scoring methods.

```
pip install txtai[scoring]
```

### Vectors

Additional vector methods.

```
pip install txtai[vectors]
```

### Workflow

All workflow tasks - default install comes with most common workflow tasks.

```
pip install txtai[workflow]
```

### Combining dependencies

Multiple dependencies can be specified at the same time.

```
pip install txtai[pipeline,workflow]
```

## Environment specific prerequisites

Additional environment specific prerequisites are below.

### Linux

The AudioStream and Microphone pipelines require the [PortAudio](https://python-sounddevice.readthedocs.io/en/0.5.0/installation.html) system library. The Transcription pipeline requires the [SoundFile](https://github.com/bastibe/python-soundfile#installation) system library.

### macOS

Older versions of Faiss have a runtime dependency on `libomp` for macOS. Run `brew install libomp` in this case.

The AudioStream and Microphone pipelines require the [PortAudio](https://python-sounddevice.readthedocs.io/en/0.5.0/installation.html) system library. Run `brew install portaudio`.

### Windows

Optional dependencies require [C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

The [txtai build workflow](https://github.com/neuml/txtai/blob/master/.github/workflows/build.yml) occasionally has work arounds for other known but temporary dependency issues. The [FAQ](../faq) also has a list of common problems, including common installation issues.

## CPU-only

The default install adds PyTorch with GPU support. There are a number of dependencies that come with that. When running in a CPU-only environment or using Embeddings/LLM models without PyTorch (i.e. llama.cpp or API services), the CPU-only PyTorch package can be installed with txtai as follows.

```
pip install txtai torch==[version]+cpu \
-f https://download.pytorch.org/whl/torch
```

Where `[version]` is the version of PyTorch (such as 2.4.1). The [txtai-cpu](https://hub.docker.com/r/neuml/txtai-cpu) image on Docker Hub uses this method to reduce the image size.

## Install from source

txtai can also be installed directly from GitHub to access the latest, unreleased features.

```
pip install git+https://github.com/neuml/txtai
```

Extras can be installed from GitHub by adding `#egg=txtai[<name-of-extra>]` to the end of the above URL.

## Conda

A [community-supported txtai package](https://anaconda.org/conda-forge/txtai) is available via conda-forge.

```
conda install -c conda-forge txtai
```

## Run with containers

Docker images are available for txtai. [See this section](../cloud) for more information on container-based installs.



================================================
FILE: docs/models.md
================================================
# Model guide

![models](images/models.png)

See the table below for the current recommended models. These models all allow commercial use and offer a blend of speed and performance.

| Component                                            | Model(s)                                                                 |
| ---------------------------------------------------- | ------------------------------------------------------------------------ |
| [Embeddings](../embeddings)                          | [all-MiniLM-L6-v2](https://hf.co/sentence-transformers/all-MiniLM-L6-v2) | 
| [Image Captions](./pipeline/image/caption.md)        | [BLIP](https://hf.co/Salesforce/blip-image-captioning-base)              |
| [Labels - Zero Shot](./pipeline/text/labels.md)      | [BART-Large-MNLI](https://hf.co/facebook/bart-large)                     |
| [Labels - Fixed](./pipeline/text/labels.md)          | Fine-tune with [training pipeline](./pipeline/train/trainer.md)          |
| [Large Language Model (LLM)](./pipeline/text/llm.md) | [Llama 3.1 Instruct](https://hf.co/meta-llama/Llama-3.1-8B-Instruct)     |
| [Summarization](./pipeline/text/summary.md)          | [DistilBART](https://hf.co/sshleifer/distilbart-cnn-12-6)                |
| [Text-to-Speech](./pipeline/audio/texttospeech.md)   | [ESPnet JETS](https://hf.co/NeuML/ljspeech-jets-onnx)                    |
| [Transcription](./pipeline/audio/transcription.md)   | [Whisper](https://hf.co/openai/whisper-base)                             | 
| [Translation](./pipeline/text/translation.md)        | [OPUS Model Series](https://hf.co/Helsinki-NLP)                          |

Models can be loaded as either a path from the Hugging Face Hub or a local directory. Model paths are optional, defaults are loaded when not specified. For tasks with no recommended model, txtai uses the default models as shown in the Hugging Face Tasks guide.

See the following links to learn more.

- [Hugging Face Tasks](https://hf.co/tasks)
- [Hugging Face Model Hub](https://hf.co/models)
- [MTEB Leaderboard](https://hf.co/spaces/mteb/leaderboard)
- [LMSYS LLM Leaderboard](https://chat.lmsys.org/?leaderboard)
- [Open LLM Leaderboard](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard)



================================================
FILE: docs/observability.md
================================================
# Observability

![agent](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/agent.png)

Observability enables tracking the inner workings of a system without having to change the system. This makes it much easier to debug and evaluate overall performance.

`txtai` has an integration with [MLflow](https://mlflow.org) and it's [tracing module](https://mlflow.org/docs/latest/llms/tracing/index.html) to provide insights into each of the components in `txtai`.

## Examples

The following shows a number of examples on how to introduce observability into a `txtai` process.

### Initialization

Run the following sections first to initialize tracing.

```
# Install MLflow plugin for txtai
pip install mlflow-txtai

# Start a local MLflow service
mlflow server --host 127.0.0.1 --port 8000
```

```python
import mlflow

mlflow.set_tracking_uri(uri="http://localhost:8000")
mlflow.set_experiment("txtai")

# Enable txtai automatic tracing
mlflow.txtai.autolog()
```

### Textractor

The first example traces a [Textractor pipeline](../pipeline/data/textractor).

```python
from txtai.pipeline import Textractor

with mlflow.start_run():
    textractor = Textractor()
    textractor("https://github.com/neuml/txtai")
```

![textractor](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/textractor.png)

### Embeddings

Next, we'll trace an [Embeddings](../embeddings) query.

```python
from txtai import Embeddings

with mlflow.start_run():
    wiki = Embeddings()
    wiki.load(provider="huggingface-hub", container="neuml/txtai-wikipedia-slim")

    embeddings = Embeddings(content=True, graph=True)
    embeddings.index(wiki.search("SELECT id, text FROM txtai LIMIT 25"))

    embeddings.search("MATCH (A)-[]->(B) RETURN A")
```

![embeddings-load](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/embeddings-load.png)
![embeddings-index](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/embeddings-index.png)

### Retrieval Augmented Generation (RAG)

The next example traces a [RAG pipeline](../pipeline/text/rag).

```python
from txtai import Embeddings, RAG

with mlflow.start_run():
    wiki = Embeddings()
    wiki.load(provider="huggingface-hub", container="neuml/txtai-wikipedia-slim")

    # Define prompt template
    template = """
    Answer the following question using only the context below. Only include information
    specifically discussed.

    question: {question}
    context: {context} """

    # Create RAG pipeline
    rag = RAG(
        wiki,
        "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
        system="You are a friendly assistant. You answer questions from users.",
        template=template,
        context=10
    )

    rag("Tell me about the Roman Empire", maxlength=2048)
```

![rag](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/rag.png)

### Workflow

This example runs a [workflow](../workflow). This workflow runs an embeddings query and then translates each result to French. 

```python
from txtai import Embeddings, Workflow
from txtai.pipeline import Translation
from txtai.workflow import Task

with mlflow.start_run():
    wiki = Embeddings()
    wiki.load(provider="huggingface-hub", container="neuml/txtai-wikipedia-slim")

    # Translation instance
    translate = Translation()

    workflow = Workflow([
        Task(lambda x: [y[0]["text"] for y in wiki.batchsearch(x, 1)]),
        Task(lambda x: translate(x, "fr"))
    ])

    print(list(workflow(["Roman Empire", "Greek Empire", "Industrial Revolution"])))
```

![workflow](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/workflow.png)

### Agent

The last example runs a [txtai agent](../agent) designed to research questions on astronomy.

```python
from txtai import Agent, Embeddings

def search(query):
    """
    Searches a database of astronomy data.

    Make sure to call this tool only with a string input, never use JSON.    

    Args:
        query: concepts to search for using similarity search

    Returns:
        list of search results with for each match
    """

    return embeddings.search(
        "SELECT id, text, distance FROM txtai WHERE similar(:query)",
        10, parameters={"query": query}
    )

embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-astronomy")

agent = Agent(
    tools=[search],
    llm="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    max_iterations=10,
)

researcher = """
{command}

Do the following.
 - Search for results related to the topic.
 - Analyze the results
 - Continue querying until conclusive answers are found
 - Write a Markdown report
"""

with mlflow.start_run():
    agent(researcher.format(command="""
    Write a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.
    """), maxlength=16000)
```

![agent](https://raw.githubusercontent.com/neuml/mlflow-txtai/master/images/agent.png)

## Read more

Check out the [mlflow-txtai](https://github.com/neuml/mlflow-txtai) project to see more examples.



================================================
FILE: docs/poweredby.md
================================================
# Powered by txtai

The following applications are powered by txtai. 

![apps](https://raw.githubusercontent.com/neuml/txtai/master/apps.jpg)

| Application  | Description  |
|:------------ |:-------------|
| [rag](https://github.com/neuml/rag) | Retrieval Augmented Generation (RAG) application |
| [ragdata](https://github.com/neuml/ragdata) | Build knowledge bases for RAG |
| [paperai](https://github.com/neuml/paperai) | AI for medical and scientific papers |
| [annotateai](https://github.com/neuml/annotateai) | Automatically annotate papers with LLMs |

In addition to this list, there are also many other [open-source projects](https://github.com/neuml/txtai/network/dependents), [published research](https://scholar.google.com/scholar?q=txtai&hl=en&as_ylo=2022) and closed proprietary/commercial projects that have built on txtai in production.



================================================
FILE: docs/usecases.md
================================================
# Use Cases

The following sections introduce common txtai use cases. A comprehensive set of over 60 [example notebooks and applications](../examples) are also available.

## Semantic Search

Build semantic/similarity/vector/neural search applications.

![demo](https://raw.githubusercontent.com/neuml/txtai/master/demo.gif)

Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.

![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search.png#gh-light-mode-only)
![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search-dark.png#gh-dark-mode-only)

Get started with the following examples.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=SIezMnVdmMs) | Overview of the functionality provided by txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |
| [Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) | Embed images and text into the same space for search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |
| [Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) | Question matching with semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |
| [Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) | Explore topics, data connectivity and run network analysis| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |

## LLM Orchestration

Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).

![llm](images/llm.png)

See below to learn more.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |
| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |
| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |

### Agents

Agents connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems.

![agent](images/agent.png)

txtai agents are built on top of the [smolagents](https://github.com/huggingface/smolagents) framework. This supports all LLMs txtai supports (Hugging Face, llama.cpp, OpenAI / Claude / AWS Bedrock via LiteLLM).

See the link below to learn more.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |

### Retrieval augmented generation

Retrieval augmented generation (RAG) reduces the risk of LLM hallucinations by constraining the output with a knowledge base as context. RAG is commonly used to "chat with your data".

![rag](images/rag.png#gh-light-mode-only)
![rag](images/rag-dark.png#gh-dark-mode-only)

A novel feature of txtai is that it can provide both an answer and source citation.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |
| [GraphRAG with Wikipedia and GPT OSS](https://github.com/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) | Deep graph search powered RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |

## Language Model Workflows

Language model workflows, also known as semantic workflows, connect language models together to build intelligent applications.

![flows](images/flows.png#gh-light-mode-only)
![flows](images/flows-dark.png#gh-dark-mode-only)

While LLMs are powerful, there are plenty of smaller, more specialized models that work better and faster for specific tasks. This includes models for extractive question-answering, automatic summarization, text-to-speech, transcription and translation.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=UBMPDCn1gEU) | Simple yet powerful constructs to efficiently process data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |
| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |
| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |
| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |



================================================
FILE: docs/why.md
================================================
# Why txtai?

![why](images/why.png#only-light)
![why](images/why-dark.png#only-dark)

New vector databases, LLM frameworks and everything in between are sprouting up daily. Why build with txtai?

- Up and running in minutes with [pip](../install/) or [Docker](../cloud/)
```python
# Get started in a couple lines
import txtai

embeddings = txtai.Embeddings()
embeddings.index(["Correct", "Not what we hoped"])
embeddings.search("positive", 1)
#[(0, 0.29862046241760254)]
```
- Built-in API makes it easy to develop applications using your programming language of choice
```yaml
# app.yml
embeddings:
    path: sentence-transformers/all-MiniLM-L6-v2
```
```bash
CONFIG=app.yml uvicorn "txtai.api:app"
curl -X GET "http://localhost:8000/search?query=positive"
```
- Run local - no need to ship data off to disparate remote services
- Work with micromodels all the way up to large language models (LLMs)
- Low footprint - install additional dependencies and scale up when needed
- [Learn by example](../examples) - notebooks cover all available functionality



================================================
FILE: docs/agent/configuration.md
================================================
# Configuration

An agent takes two main arguments, an LLM and a list of tools.

The txtai agent framework is built with [smolagents](https://github.com/huggingface/smolagents). Additional options can be passed in the `Agent` constructor.

```python
from datetime import datetime

from txtai import Agent

wikipedia = {
    "name": "wikipedia",
    "description": "Searches a Wikipedia database",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-wikipedia"
}

arxiv = {
    "name": "arxiv",
    "description": "Searches a database of scientific papers",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-arxiv"
}

def today() -> str:
    """
    Gets the current date and time

    Returns:
        current date and time
    """

    return datetime.today().isoformat()

agent = Agent(
    model="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    tools=[today, wikipedia, arxiv, "websearch"],
)
```

## model

```yaml
model: string|llm instance
```

LLM model path or LLM pipeline instance. The `llm` parameter is also supported for backwards compatibility.

See the [LLM pipeline](../../pipeline/text/llm) for more information.

## tools

```yaml
tools: list
```

List of tools to supply to the agent. Supports the following configurations.

### function

A function tool takes the following dictionary fields.

| Field       | Description              |
|:------------|:-------------------------|
| name        | name of the tool         |
| description | tool description         |
| target      | target method / callable |

A function or callable method can also be directly supplied in the `tools` list. In this case, the fields are inferred from the method documentation.

### embeddings

Embeddings indexes have built-in support. Provide the following dictionary configuration to add an embeddings index as a tool.

| Field       | Description                                |
|:------------|:-------------------------------------------|
| name        | embeddings index name                      |
| description | embeddings index description               | 
| **kwargs    | Parameters to pass to [embeddings.load](../../embeddings/methods/#txtai.embeddings.Embeddings.load) |

### tool

A tool instance can be provided. Additionally, the following strings load tools directly.

| Tool        | Description                                               |
|:------------|:----------------------------------------------------------|
| http.*      | HTTP Path to a Model Context Protocol (MCP) server        |
| python      | Runs a Python action                                      |
| websearch   | Runs a websearch using the built-in websearch tool        |
| webview     | Extracts content from a web page                          |

## method

```yaml
method: code|tool
```

Sets the agent method. Supports either a `code` or `tool` calling agent (default). A code agent generates Python code and executes that. A tool calling agent generates JSON blocks and calls the agents within those blocks.

[Read more on this here](https://huggingface.co/docs/smolagents/v1.13.0/en/guided_tour#codeagent-and-toolcallingagent).



================================================
FILE: docs/agent/index.md
================================================
# Agent

![agent](../images/agent.png)

An agent automatically creates workflows to answer multi-faceted user requests. Agents iteratively prompt and/or interface with tools to
step through a process and ultimately come to an answer for a request.

Agents excel at complex tasks where multiple tools and/or methods are required. They incorporate a level of randomness similar to different
people working on the same task. When the request is simple and/or there is a rule-based process, other methods such as RAG and Workflows
should be explored.

The following code snippet defines a basic agent.

```python
from datetime import datetime

from txtai import Agent

wikipedia = {
    "name": "wikipedia",
    "description": "Searches a Wikipedia database",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-wikipedia"
}

arxiv = {
    "name": "arxiv",
    "description": "Searches a database of scientific papers",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-arxiv"
}

def today() -> str:
    """
    Gets the current date and time

    Returns:
        current date and time
    """

    return datetime.today().isoformat()

agent = Agent(
    model="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    tools=[today, wikipedia, arxiv, "websearch"],
    max_steps=10,
)
```

The agent above has access to two embeddings databases (Wikipedia and ArXiv) and the web. Given the user's input request, the agent decides the best tool to solve the task.

## Example

The first example will solve a problem with multiple data points. See below.

```python
agent("Which city has the highest population, Boston or New York?")
```

This requires looking up the population of each city before knowing how to answer the question. Multiple search requests are run to generate a final answer.

## Agentic RAG

Standard retrieval augmented generation (RAG) runs a single vector search to obtain a context and builds a prompt with the context + input question. Agentic RAG is a more complex process that goes through multiple iterations. It can also utilize multiple databases to come to a final conclusion.

The example below aggregates information from multiple sources and builds a report on a topic.

```python
researcher = """
You're an expert researcher looking to write a paper on {topic}.
Search for websites, scientific papers and Wikipedia related to the topic.
Write a report with summaries and references (with hyperlinks).
Write the text as Markdown.
"""

agent(researcher.format(topic="alien life"))
```

## Agent Teams

Agents can also be tools. This enables the concept of building "Agent Teams" to solve problems. The previous example can be rewritten as a list of agents.

```python
from txtai import Agent, LLM

llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

websearcher = Agent(
    model=llm,
    tools=["websearch"],
)

wikiman = Agent(
    model=llm,
    tools=[{
        "name": "wikipedia",
        "description": "Searches a Wikipedia database",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-wikipedia"
    }],
)

researcher = Agent(
    model=llm,
    tools=[{
        "name": "arxiv",
        "description": "Searches a database of scientific papers",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-arxiv"
    }],
)

agent = Agent(
    model=llm,
    tools=[{
        "name": "websearcher",
        "description": "I run web searches, there is no answer a web search can't solve!",
        "target": websearcher
    }, {
        "name": "wikiman",
        "description": "Wikipedia has all the answers, I search Wikipedia and answer questions",
        "target": wikiman
    }, {
        "name": "researcher",
        "description": "I'm a science guy. I search arXiv to get all my answers.",
        "target": researcher
    }],
    max_steps=10
)
```

This provides another level of intelligence to the process. Instead of just a single tool execution, each agent-tool combination has it's own reasoning engine.

```python
agent("""
Work with your team and build a comprehensive report on fundamental
concepts about Signal Processing.
Write the output in Markdown.
""")
```

# More examples

See the link below to learn more.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [What's new in txtai 8.0](https://github.com/neuml/txtai/blob/master/examples/67_Whats_new_in_txtai_8_0.ipynb) | Agents with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/67_Whats_new_in_txtai_8_0.ipynb) |
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |



================================================
FILE: docs/agent/methods.md
================================================
# Methods

## ::: txtai.agent.base.Agent.__init__
## ::: txtai.agent.base.Agent.__call__



================================================
FILE: docs/api/cluster.md
================================================
# Distributed embeddings clusters

The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below.

```yaml
cluster:
    shards:
        - http://127.0.0.1:8002
        - http://127.0.0.1:8003
```

This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters.

This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index.

See the link below for a detailed example covering distributed embeddings clusters.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Distributed embeddings cluster](https://github.com/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) | Distribute an embeddings index across multiple data nodes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) |



================================================
FILE: docs/api/configuration.md
================================================
# Configuration

Configuration is set through YAML. In most cases, YAML keys map to fields names in Python. The [example in the previous section](../) gave a full-featured example covering a wide array of configuration options.

Each section below describes the available configuration settings.

## Embeddings

The configuration parser expects a top level `embeddings` key to be present in the YAML. All [embeddings configuration](../../embeddings/configuration) is supported.

The following example defines an embeddings index.

```yaml
path: index path
writable: true

embeddings:
  path: vector model
  content: true
```

Three top level settings are available to control where indexes are saved and if an index is a read-only index.

### path
```yaml
path: string
```

Path to save and load the embeddings index. Each API instance can only access a single index at a time.

### writable
```yaml
writable: boolean
```

Determines if the input embeddings index is writable (true) or read-only (false). This allows serving a read-only index.

### cloud
[Cloud storage settings](../../embeddings/configuration/cloud) can be set under a `cloud` top level configuration group.

## Agent

Agents are defined under a top level `agent` key. Each key under the `agent` key is the name of the agent. Constructor parameters can be passed under this key.

The following example defines an agent.

```yaml
agent:
    researcher:
        tools:
            - websearch

llm:
    path: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
```

## Pipeline

Pipelines are loaded as top level configuration parameters. Pipeline names are automatically detected in the YAML configuration and created upon startup. All [pipelines](../../pipeline) are supported.

The following example defines a series of pipelines. Note that entries below are the lower-case names of the pipeline class.

```yaml
caption:

extractor:
  path: model path

labels:

summary:

tabular:

translation:
```

Under each pipeline name, configuration settings for the pipeline can be set.

## Workflow

Workflows are defined under a top level `workflow` key. Each key under the `workflow` key is the name of the workflow. Under that is a `tasks` key with each task definition.

The following example defines a workflow.

```yaml
workflow:
  sumtranslate:
    tasks:
        - action: summary
        - action: translation
```

### schedule

Schedules a workflow using a [cron expression](../../workflow/schedule).

```yaml
workflow:
  index:
    schedule:
      cron: 0/10 * * * * *
      elements: ["api params"] 
    tasks:
      - task: service
        url: api url
      - action: index
```

### tasks
```yaml
tasks: list
```

Expects a list of workflow tasks. Each element defines a single workflow task. All [task configuration](../../workflow/task) is supported.

A shorthand syntax for creating tasks is supported. This syntax will automatically map task strings to an `action:value` pair.

Example below.

```yaml
workflow:
  index:
    tasks:
      - action1
      - action2
```

Each task element supports the following additional arguments.

#### action
```yaml
action: string|list
```

Both single and multi-action tasks are supported.

The action parameter works slightly different when passed via configuration. The parameter(s) needs to be converted into callable method(s). If action is a pipeline that has been defined in the current configuration, it will use that pipeline as the action.

There are three special action names `index`, `upsert` and `search`. If `index` or `upsert` are used as the action, the task will collect workflow data elements and load them into defined the embeddings index. If `search` is used, the task will execute embeddings queries for each input data element.

Otherwise, the action must be a path to a callable object or function. The configuration parser will resolve the function name and use that as the task action.

#### task
```yaml
task: string
```

Optionally sets the type of task to create. For example, this could be a `file` task or a `retrieve` task. If this is not specified, a generic task is created. [The list of workflow tasks can be found here](../../workflow).

#### args
```yaml
args: list
```

Optional list of static arguments to pass to the workflow task. These are combined with workflow data to pass to each `__call__`.



================================================
FILE: docs/api/customization.md
================================================
# Customization

The txtai API has a number of features out of the box that are designed to help get started quickly. API services can also be augmented with custom code and functionality. The two main ways to do this are with extensions and dependencies.

Extensions add a custom endpoint. Dependencies add middleware that executes with each request. See the sections below for more.

## Extensions

While the API is extremely flexible and complex logic can be executed through YAML-driven workflows, some may prefer to create an endpoint in Python. API extensions define custom Python endpoints that interact with txtai applications. 

See the link below for a detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Custom API Endpoints](https://github.com/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) | Extend the API with custom endpoints | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) |

## Dependencies

txtai has a default API token authorization method that works well in many cases. Dependencies can also add custom logic with each request. This could be an additional authorization step and/or an authentication method. 

See the link below for a detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [API Authorization and Authentication](https://github.com/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) | Add authorization, authentication and middleware dependencies to the API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) |



================================================
FILE: docs/api/index.md
================================================
# API

![api](../images/api.png#only-light)
![api](../images/api-dark.png#only-dark)

txtai has a full-featured API, backed by [FastAPI](https://github.com/tiangolo/fastapi), that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API.

The following is an example configuration and startup script for the API.

Note: This configuration file enables all functionality. For memory-bound systems, splitting pipelines into multiple instances is a best practice.

```yaml
# Index file path
path: /tmp/index

# Allow indexing of documents
writable: True

# Enbeddings index
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2

# Extractive QA
extractor:
  path: distilbert-base-cased-distilled-squad

# Zero-shot labeling
labels:

# Similarity
similarity:

# Text segmentation
segmentation:
    sentences: true

# Text summarization
summary:

# Text extraction
textractor:
    paragraphs: true
    minlength: 100
    join: true

# Transcribe audio to text
transcription:

# Translate text between languages
translation:

# Workflow definitions
workflow:
    sumfrench:
        tasks:
            - action: textractor
              task: url
            - action: summary
            - action: translation
              args: ["fr"]
    sumspanish:
        tasks:
            - action: textractor
              task: url
            - action: summary
            - action: translation
              args: ["es"]
```

Assuming this YAML content is stored in a file named config.yml, the following command starts the API process.

```bash
CONFIG=config.yml uvicorn "txtai.api:app"
```

Uvicorn is a full-featured production-ready server. See the [Uvicorn deployment guide](https://www.uvicorn.org/deployment/) for more on configuration options.

## Connect to API

The default port for the API is 8000. See the uvicorn link above to change this.

txtai has a number of language bindings which abstract the API (see links below). Alternatively, code can be written to connect directly to the API. Documentation for a live running instance can be found at the `/docs` url (i.e. http://localhost:8000/docs). The following example runs a workflow using cURL.

```bash
curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"sumfrench", "elements": ["https://github.com/neuml/txtai"]}'
```

## Local instance

A local instance can be instantiated. In this case, a txtai application runs internally, without any network connections, providing the same consolidated functionality. This enables running txtai in Python with configuration.

The configuration above can be run in Python with:

```python
from txtai import Application

# Load and run workflow
app = Application(config.yml)
app.workflow("sumfrench", ["https://github.com/neuml/txtai"])
```

See this [link for a full list of methods](./methods).

## Run with containers

The API can be containerized and run. This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!

[See this section for more information](../cloud/#api).

## Supported language bindings

The following programming languages have bindings with the txtai API:

- [Python](https://github.com/neuml/txtai.py)
- [JavaScript](https://github.com/neuml/txtai.js)
- [Java](https://github.com/neuml/txtai.java)
- [Rust](https://github.com/neuml/txtai.rs)
- [Go](https://github.com/neuml/txtai.go)

The API also supports hosting [OpenAI-compatible](./openai) and [Model Context Protocol (MCP)](./mcp) endpoints.

See the links below for detailed examples covering the API.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [API Gallery](https://github.com/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) | Using txtai in JavaScript, Java, Rust and Go | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) |
| [Distributed embeddings cluster](https://github.com/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) | Distribute an embeddings index across multiple data nodes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) |
| [Embeddings in the Cloud](https://github.com/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) | Load and use an embeddings index from the Hugging Face Hub | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) |
| [Custom API Endpoints](https://github.com/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) | Extend the API with custom endpoints | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/51_Custom_API_Endpoints.ipynb) |
| [API Authorization and Authentication](https://github.com/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) | Add authorization, authentication and middleware dependencies to the API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/54_API_Authorization_and_Authentication.ipynb) |
| [OpenAI Compatible API](https://github.com/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) | Connect to txtai with a standard OpenAI client library | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) |



================================================
FILE: docs/api/mcp.md
================================================
# Model Context Protocol

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools.

The API can be configured to handle MCP requests. All enabled endpoints set in the API configuration are automatically added as MCP tools.

```yaml
mcp: True
```

Once this configuration option is added, a new route is added to the application `/mcp`. 

The [Model Context Protocol Inspector tool](https://www.npmjs.com/package/@modelcontextprotocol/inspector) is a quick way to explore how the MCP tools are exported through this interface.

Run the following and go to the local URL specified.

```
npx @modelcontextprotocol/inspector node build/index.js
```

Enter `http://localhost:8000/mcp` to see the full list of tools available.



================================================
FILE: docs/api/methods.md
================================================
# Methods

::: txtai.api.API
    options:
        inherited_members: true
        filters:
            - "!__del__"
            - "!flows"
            - "!function"
            - "!indexes"
            - "!limit"
            - "!pipes"
            - "!read"
            - "!resolve"
            - "!weights"



================================================
FILE: docs/api/openai.md
================================================
# OpenAI-compatible API

The API can be configured to serve an OpenAI-compatible API as shown below.

```yaml
openai: True
```

See the link below for a detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [OpenAI Compatible API](https://github.com/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) | Connect to txtai with a standard OpenAI client library | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/74_OpenAI_Compatible_API.ipynb) |



================================================
FILE: docs/api/security.md
================================================
# Security

The default implementation of an API service runs via HTTP and is fully open. If the service is being run as a prototype on an internal network, that may be fine. In most scenarios, the connection should at least be encrypted. Authorization is another built-in feature that requires a valid API token with each request. See below for more.

## HTTPS

The default API service command starts a Uvicorn server as a HTTP service on port 8000. To run a HTTPS service, consider the following options.

- [TLS Proxy Server](https://fastapi.tiangolo.com/deployment/https/). *Recommended choice*. With this configuration, the txtai API service runs as a HTTP service only accessible on the localhost/local network. The proxy server handles all encryption and redirects requests to local services. See this [example configuration](https://www.uvicorn.org/deployment/#running-behind-nginx) for more.

- [Uvicorn SSL Certificate](https://www.uvicorn.org/deployment/). Another option is setting the SSL certificate on the Uvicorn service. This works in simple situations but gets complex when hosting multiple txtai or other related services.

## Authorization

Authorization requires a valid API token with each API request. This token is sent as a HTTP `Authorization` header. 

*Server*
```bash
CONFIG=config.yml TOKEN=<sha256 encoded token> uvicorn "txtai.api:app"
```

*Client*
```bash
curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <token>" \ 
  -d '{"name":"sumfrench", "elements": ["https://github.com/neuml/txtai"]}'
```

It's important to note that HTTPS **must** be enabled using one of the methods mentioned above. Otherwise, tokens will be exchanged as clear text. 

Authentication and Authorization can be fully customized. See the [dependencies](../customization#dependencies) section for more.



================================================
FILE: docs/embeddings/format.md
================================================
# Index format

![format](../images/format.png#only-light)
![format](../images/format-dark.png#only-dark)

This section documents the txtai index format. Each component is designed to ensure open access to the underlying data in a programmatic and platform independent way

If an underlying library has an index format, that is used. Otherwise, txtai persists content with [MessagePack](https://msgpack.org/index.html) serialization.

To learn more about how these components work together, read the [Index Guide](../indexing) and [Query Guide](../query).

## ANN

Approximate Nearest Neighbor (ANN) index configuration for storing vector embeddings.

| Component                                                     | Storage Format                                                               |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| [Faiss](https://github.com/facebookresearch/faiss)            | Local file format provided by library                                        |
| [Hnswlib](https://github.com/nmslib/hnswlib)                  | Local file format provided by library                                        |
| [Annoy](https://github.com/spotify/annoy)                     | Local file format provided by library                                        |
| [NumPy](https://github.com/numpy/numpy)                       | Local NumPy array files via np.save / np.load                                |
| [Postgres via pgvector](https://github.com/pgvector/pgvector) | Vector tables in a Postgres database                                         |

## Core

Core embeddings index files.

| Component                                                     | Storage Format                                                               |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| [Configuration](https://www.json.org/)                        | Embeddings index configuration stored as JSON                                |
| [Index Ids](https://msgpack.org/index.html)                   | Embeddings index ids serialized with MessagePack. Only enabled when when content storage (database) is disabled. |

## Database

Databases store metadata, text and binary content.

| Component                                                     | Storage Format                                                               |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| [SQLite](https://www.sqlite.org/)                             | Local database files with SQLite                                             |
| [DuckDB](https://github.com/duckdb/duckdb)                    | Local database files with DuckDB                                             |
| [Postgres](https://www.postgresql.org/)                       | Postgres relational database via [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy). Supports additional databases via this library. |

## Graph

Graph nodes and edges for an embeddings index

| Component                                                     | Storage Format                                                                |
| ------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| [NetworkX](https://github.com/networkx/networkx)              | Nodes and edges exported to local file serialized with MessagePack            |
| [Postgres](https://github.com/aplbrain/grand)                 | Nodes and edges stored in a Postgres database. Supports additional databases. |

## Scoring

Sparse/keyword indexing

| Component                                                     | Storage Format                                                                |
| ------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| [Local index](https://www.sqlite.org/)                        | Metadata serialized with MessagePack. Terms stored in SQLite.                 |
| [Postgres](https://www.postgresql.org/docs/current/textsearch.html) | Text indexed with Postgres Full Text Search (FTS)                             |



================================================
FILE: docs/embeddings/index.md
================================================
# Embeddings

![embeddings](../images/embeddings.png#only-light)
![embeddings](../images/embeddings-dark.png#only-dark)

Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.

The following code snippet shows how to build and search an embeddings index.

```python
from txtai import Embeddings

# Create embeddings model, backed by sentence-transformers & transformers
embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2")

data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, " +
  "forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends " +
  "in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Index the list of text
embeddings.index(data)

print(f"{'Query':20} Best Match")
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war",
              "wildlife", "asia", "lucky", "dishonest junk"):
    # Extract uid of first result
    # search result format: (uid, score)
    uid = embeddings.search(query, 1)[0][0]

    # Print text
    print(f"{query:20} {data[uid]}")
```

## Build

An embeddings instance is [configuration-driven](configuration) based on what is passed in the constructor. Vectors are stored with the option to also [store content](configuration/database#content). Content storage enables additional filtering and data retrieval options.

The example above sets a specific embeddings vector model via the [path](configuration/vectors/#path) parameter. An embeddings instance with no configuration can also be created.

```python
embeddings = Embeddings()
```

In this case, when loading and searching for data, the [default transformers vector model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) is used to vectorize data. See the [model guide](../models) for current model recommentations.

## Index

After creating a new embeddings instance, the next step is adding data to it.

```python
embeddings.index(rows)
```

The index method takes an iterable and supports the following formats for each element.

- `(id, data, tags)` - default processing format

| Element     | Description                                                   |
| ----------- | ------------------------------------------------------------- |
| id          | unique record id                                              |
| data        | input data to index, can be text, a dictionary or object      |
| tags        | optional tags string, used to mark/label data as it's indexed |

- `(id, data)`

  Same as above but without tags.

- `data`

Single element to index. In this case, unique id's will automatically be generated. Note that for generated id's, [upsert](methods/#txtai.embeddings.base.Embeddings.upsert) and [delete](methods/#txtai.embeddings.base.Embeddings.delete) calls require a separate search to get the target ids.

When the data field is a dictionary, text is passed via the `text` key, binary objects via the `object` key. Note that [content](configuration/database#content) must be enabled to store metadata and [objects](configuration/database#objects) to store binary object data. The `id` and `tags` keys will be extracted, if provided.

The input iterable can be a list or generator. [Generators](https://wiki.python.org/moin/Generators) help with indexing very large datasets as only portions of the data is in memory at any given time.

More information on indexing can be found in the [index guide](indexing).

## Search

Once data is indexed, it is ready for search.

```python
embeddings.search(query, limit)
```

The search method takes two parameters, the query and query limit. The results format is different based on whether [content](configuration/database#content) is stored or not.

- List of `(id, score)` when content is _not_ stored
- List of `{**query columns}` when content is stored

Both natural language and SQL queries are supported. More information can be found in the [query guide](query).

## Resource management

Embeddings databases are context managers. The following blocks automatically [close](methods/#txtai.embeddings.base.Embeddings.close) and free resources upon completion.

```python
# Create a new Embeddings database, index data and save
with Embeddings() as embeddings:
  embeddings.index(rows)
  embeddings.save(path)

# Search a saved Embeddings database
with Embeddings().load(path) as embeddings:
  embeddings.search(query)
```

While calling `close` isn't always necessary (resources will be garbage collected), it's best to free shared resources like database connections as soon as they aren't needed.

## More examples

See [this link](../examples/#semantic-search) for a full list of embeddings examples.


================================================
FILE: docs/embeddings/indexing.md
================================================
# Index guide

![indexing](../images/indexing.png#only-light)
![indexing](../images/indexing-dark.png#only-dark)

This section gives an in-depth overview on how to index data with txtai. We'll cover vectorization, indexing/updating/deleting data and the various components of an embeddings database.

## Vectorization

The most compute intensive step in building an index is vectorization. The [path](../configuration/vectors#path) parameter sets the path to the vector model. There is logic to automatically detect the vector model [method](../configuration/vectors#method) but it can also be set directly.

The [batch](../configuration/vectors#batch) and [encodebatch](../configuration/vectors#encodebatch) parameters control the vectorization process. Larger values for `batch` will pass larger batches to the vectorization method. Larger values for `encodebatch` will pass larger batches for each vector encode call. In the case of GPU vector models, larger values will consume more GPU memory.

Data is buffered to temporary storage during indexing as embeddings vectors can be quite large (for example 768 dimensions of float32 is 768 * 4 = 3072 bytes per vector). Once vectorization is complete, a mmapped array is created with all vectors for [Approximate Nearest Neighbor (ANN)](../configuration/vectors#backend) indexing.

The terms `ANN` and `dense vector index` are used interchangeably throughout txtai's documentation.

## Setting a backend

As mentioned above, computed vectors are stored in an ANN. There are various index [backends](../configuration/ann#backend) that can be configured. Faiss is the default backend.

## Content storage

Embeddings indexes can optionally [store content](../configuration/database#content). When this is enabled, the input content is saved in a database alongside the computed vectors. This enables filtering on additional fields and content retrieval.

## Index vs Upsert

Data is loaded into an index with either an [index](../methods#txtai.embeddings.base.Embeddings.index) or [upsert](../methods#txtai.embeddings.base.Embeddings.upsert) call.

```python
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])
embeddings.upsert([(uid, text, None) for uid, text in enumerate(data)])
```

The `index` call will build a brand new index replacing an existing one. `upsert` will insert or update records. `upsert` ops do _not_ require a full index rebuild.

## Save

Indexes can be stored in a directory using the [save](../methods/#txtai.embeddings.base.Embeddings.save) method.

```python
embeddings.save("/path/to/save")
```

Compressed indexes are also supported.

```python
embeddings.save("/path/to/save/index.tar.gz")
```

In addition to saving indexes locally, they can also be persisted to [cloud storage](../configuration/cloud).

```python
embeddings.save("/path/to/save/index.tar.gz", cloud={...})
```

This is especially useful when running in a serverless context or otherwise running on temporary compute. Cloud storage is only supported with compressed indexes.

Embeddings indexes can be restored using the [load](../methods/#txtai.embeddings.base.Embeddings.load) method.

```python
embeddings.load("/path/to/load")
```

## Delete

Content can be removed from the index with the [delete](../methods#txtai.embeddings.base.Embeddings.delete) method. This method takes a list of ids to delete.

```python
embeddings.delete(ids)
```

## Reindex

When [content storage](../configuration/database#content) is enabled, [reindex](../methods#txtai.embeddings.base.Embeddings.reindex) can be called to rebuild the index with new settings. For example, the backend can be switched from faiss to hnsw or the vector model can be updated. This prevents having to go back to the original raw data. 

```python
embeddings.reindex(path="sentence-transformers/all-MiniLM-L6-v2", backend="hnsw")
```

## Graph

Enabling a [graph network](../configuration/graph) adds a semantic graph at index time as data is being vectorized. Vector embeddings are used to automatically create relationships in the graph. Relationships can also be manually specified at index time.

```python
# Manual relationships by id
embeddings.index([{"id": "0", "text": "...", "relationships": ["2"]}])

# Manual relationships with additional edge attributes
embeddings.index(["id": "0", "text": "...", "relationships": [
    {"id": "2", "type": "MEMBER_OF"}
]])
```

Additionally, graphs can be used for topic modeling. Dimensionality reduction with UMAP combined with HDBSCAN is a popular topic modeling method found in a number of libraries. txtai takes a different approach using community detection algorithms to build topic clusters.

This approach has the advantage of only having to vectorize data once. It also has the advantage of better topic precision given there isn't a dimensionality reduction operation (UMAP). Semantic graph examples are shown below.

Get a mapping of discovered topics to associated ids.

```python
embeddings.graph.topics
```

Show the most central nodes in the index.

```python
embeddings.graph.centrality()
```

Graphs are persisted alongside an embeddings index. Each save and load will also save and load the graph.

## Sparse vectors

Scoring instances can create a standalone [sparse keyword indexes](../configuration/general#keyword) (BM25, TF-IDF) and [sparse vector indexes](../configuration/general#sparse) (SPLADE). This enables [hybrid](../configuration/general/#hybrid) search when there is an available dense vector index.

The terms `sparse vector index`, `keyword index`, `terms index` and `scoring index` are used interchangeably throughout txtai's documentation.

See [this link](../../examples/#semantic-search) to learn more.

## Subindexes

An embeddings instance can optionally have associated [subindexes](../configuration/general/#indexes), which are also embeddings databases. This enables indexing additional fields, vector models and much more.

## Word vectors

When using [word vector backed models](../configuration/vectors#words) with scoring set, a separate call is required before calling `index` as follows:

```python
embeddings.score(rows)
embeddings.index(rows)
```

Both calls are required to support generator-backed iteration of data with word vectors models.



================================================
FILE: docs/embeddings/methods.md
================================================
# Methods

::: txtai.embeddings.Embeddings
    options:
        filters:
            - "!columns"
            - "!createann"
            - "!createcloud"
            - "!createdatabase"
            - "!creategraph"
            - "!createids"
            - "!createindexes"
            - "!createscoring"
            - "!checkarchive"
            - "!configure"
            - "!defaultallowed"
            - "!defaults"
            - "!initindex"
            - "!loadquery"
            - "!loadvectors"



================================================
FILE: docs/embeddings/query.md
================================================
# Query guide

![query](../images/query.png#only-light)
![query](../images/query-dark.png#only-dark)

This section covers how to query data with txtai. The simplest way to search for data is building a natural language string with the desired content to find. txtai also supports querying with SQL. We'll cover both methods here.

## Natural language queries

In the simplest case, the query is text and the results are index text that is most similar to the query text.

```python
embeddings.search("feel good story")
embeddings.search("wildlife")
```

The queries above [search](../methods#txtai.embeddings.base.Embeddings.search) the index for similarity matches on `feel good story` and `wildlife`. If content storage is enabled, a list of `{**query columns}` is returned. Otherwise, a list of `(id, score)` tuples are returned.

## SQL

txtai supports more complex queries with SQL. This is only supported if [content storage](../configuration/database#content) is enabled. txtai has a translation layer that analyzes input SQL statements and combines similarity results with content stored in a relational database.

SQL queries are run through `embeddings.search` like natural language queries but the examples below only show the SQL query for conciseness.

```python
embeddings.search("SQL query")
```

### Similar clause

The similar clause is a txtai function that enables similarity searches with SQL.

```sql
SELECT id, text, score FROM txtai WHERE similar('feel good story')
```

The similar clause takes the following arguments:

```sql
similar("query", "number of candidates", "index", "weights")
```

| Argument              | Description                            |
| --------------------- | ---------------------------------------|
| query                 | natural language query to run          |
| number of candidates  | number of candidate results to return  |
| index                 | target index name                      |
| weights               | hybrid score weights                   |

The txtai query layer joins results from two separate components, a relational store and a similarity index. With a similar clause, a similarity search is run and those ids are fed to the underlying database query.

The number of candidates should be larger than the desired number of results when applying additional filter clauses. This ensures that `limit` results are still returned after applying additional filters. If the number of candidates is not specified, it is defaulted as follows:

- For a single query filter clause, the default is the query limit
- With multiple filtering clauses, the default is 10x the query limit

The index name is only applicable when [subindexes](../configuration/general/#indexes) are enabled. This specifies the index to use for the query.

Weights sets the hybrid score weights when an index has both a sparse and dense index.

### Dynamic columns

Content can be indexed in multiple ways when content storage is enabled. [Remember that input documents](../#index) take the form of `(id, data, tags)` tuples. If data is a string or binary content, it's indexed and searchable with `similar()` clauses.

If data is a dictionary, then all fields in the dictionary are stored and available via SQL. The `text` field or [field specified in the index configuration](../configuration/general/#columns) is indexed and searchable with `similar()` clauses.

For example:

```python
embeddings.index([{"text": "text to index", "flag": True,
                   "actiondate": "2022-01-01"}])
```

With the above input data, queries can now have more complex filters.

```sql
SELECT text, flag, actiondate FROM txtai WHERE similar('query') AND flag = 1
AND actiondate >= '2022-01-01'
```

txtai's query layer automatically detects columns and translates queries into a format that can be understood by the underlying database.

Nested dictionaries/JSON is supported and can be escaped with bracket statements.

```python
embeddings.index([{"text": "text to index",
                   "parent": {"child element": "abc"}}])
```

```sql
SELECT text FROM txtai WHERE [parent.child element] = 'abc'
```

Note the bracket statement escaping the nested column with spaces in the name.

### Bind parameters

txtai has support for SQL bind parameters.

```python
# Query with a bind parameter for similar clause
query = "SELECT id, text, score FROM txtai WHERE similar(:x)"
results = embeddings.search(query, parameters={"x": "feel good story"})

# Query with a bind parameter for column filter
query = "SELECT text, flag, actiondate FROM txtai WHERE flag = :x"
results = embeddings.search(query, parameters={"x": 1})
```

### Aggregation queries

The goal of txtai's query language is to closely support all functions in the underlying database engine. The main challenge is ensuring dynamic columns are properly escaped into the engines native query function. 

Aggregation query examples.

```sql
SELECT count(*) FROM txtai WHERE similar('feel good story') AND score >= 0.15
SELECT max(length(text)) FROM txtai WHERE similar('feel good story')
AND score >= 0.15
SELECT count(*), flag FROM txtai GROUP BY flag ORDER BY count(*) DESC
```

## Binary objects

txtai has support for storing and retrieving binary objects. Binary objects can be retrieved as shown in the example below.

```python
# Create embeddings index with content and object storage enabled
embeddings = Embeddings(content=True, objects=True)

# Get an image
request = open("demo.gif", "rb")

# Insert record
embeddings.index([("txtai", {"text": "txtai executes machine-learning workflows.",
                             "object": request.read()})])

# Query txtai and get associated object
query = "SELECT object FROM txtai WHERE similar('machine learning') LIMIT 1"
result = embeddings.search(query)[0]["object"]

# Query binary content with a bind parameter
query = "SELECT object FROM txtai WHERE similar(:x) LIMIT 1"
results = embeddings.search(query, parameters={"x": request.read()})
```

## Custom SQL functions

Custom, user-defined SQL functions extend selection, filtering and ordering clauses with additional logic. For example, the following snippet defines a function that translates text using a translation pipeline.

```python
# Translation pipeline
translate = Translation()

# Create embeddings index
embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2",
                        content=True,
                        functions=[translate]})

# Run a search using a custom SQL function
embeddings.search("""
SELECT
  text,
  translation(text, 'de', null) 'text (DE)',
  translation(text, 'es', null) 'text (ES)',
  translation(text, 'fr', null) 'text (FR)'
FROM txtai WHERE similar('feel good story')
LIMIT 1
""")
```

## Query translation

Natural language queries with filters can be converted to txtai-compatible SQL statements with query translation. For example:

```python
embeddings.search("feel good story since yesterday")
```

can be converted to a SQL statement with a similar clause and date filter.

```sql
select id, text, score from txtai where similar('feel good story') and
entry >= date('now', '-1 day')
```

This requires setting a [query translation model](../configuration/database#query). The default query translation model is [t5-small-txtsql](https://huggingface.co/NeuML/t5-small-txtsql) but this can easily be finetuned to handle different use cases.

## Hybrid search

When an embeddings database has both a sparse and dense index, both indexes will be queried and the results will be equally weighted unless otherwise specified.

```python
embeddings.search("query", weights=0.5)
embeddings.search("SELECT id, text, score FROM txtai WHERE similar('query', 0.5)")
```

## Graph search

If an embeddings database has an associated graph network, graph searches can be run. The search syntax below uses [openCypher](https://github.com/opencypher/openCypher). Follow the preceding link to learn more about this syntax.

Additionally, standard embeddings searches can be returned as graphs.

```python
# Find all paths between id: 0 and id: 5 between 1 and 3 hops away
embeddings.graph.search("""
MATCH P=({id: 0})-[*1..3]->({id: 5})
RETURN P
""")

# Standard embeddings search as graph
embeddings.search("query", graph=True)
```

## Subindexes

Subindexes can be queried as follows:

```python
# Query with index parameter
embeddings.search("query", index="subindex1")

# Specify with SQL
embeddings.search("""
SELECT id, text, score FROM txtai
WHERE similar('query', 'subindex1')
""")
```

## Combined index architecture

txtai has multiple storage and indexing components. Content is stored in an underlying database along with an approximate nearest neighbor (ANN) index, keyword index and graph network. These components combine to deliver similarity search alongside traditional structured search.

The ANN index stores ids and vectors for each input element. When a natural language query is run, the query is translated into a vector and a similarity query finds the best matching ids. When a database is added into the mix, an additional step is executed. This step takes those ids and effectively inserts them as part of the underlying database query. The same steps apply with keyword indexes except a term frequency index is used to find the best matching ids.

Dynamic columns are supported via the underlying engine. For SQLite, data is stored as JSON and dynamic columns are converted into `json_extract` clauses. Client-server databases are supported via [SQLAlchemy](https://docs.sqlalchemy.org/en/20/dialects/) and dynamic columns are supported provided the underlying engine has [JSON](https://docs.sqlalchemy.org/en/20/core/type_basics.html#sqlalchemy.types.JSON) support.



================================================
FILE: docs/embeddings/configuration/ann.md
================================================
# ANN

Approximate Nearest Neighbor (ANN) index configuration for storing vector embeddings.

## backend
```yaml
backend: faiss|hnsw|annoy|ggml|numpy|torch|pgvector|sqlite|custom
```

Sets the ANN backend. Defaults to `faiss`. Additional backends are available via the [ann](../../../install/#ann) extras package. Set custom backends via setting this parameter to the fully resolvable class string.

Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). These are optional and set to defaults if omitted.

### faiss
```yaml
faiss:
    components: comma separated list of components - defaults to "IDMap,Flat" for small
                indices and "IVFx,Flat" for larger indexes where
                x = min(4 * sqrt(embeddings count), embeddings count / 39)
                automatically calculates number of IVF cells when omitted (supports "IVF,Flat")
    nprobe: search probe setting (int) - defaults to x/16 (as defined above)
            for larger indexes
    nflip: same as nprobe - only used with binary hash indexes
    quantize: store vectors with x-bit precision vs 32-bit (boolean|int)
              true sets 8-bit precision, false disables, int sets specified
              precision
    mmap: load as on-disk index (boolean) - trade query response time for a
          smaller RAM footprint, defaults to false
    sample: percent of data to use for model training (0.0 - 1.0)
            reduces indexing time for larger (>1M+ row) indexes, defaults to 1.0
```

Faiss supports both floating point and binary indexes. Floating point indexes are the default. Binary indexes are used when indexing scalar-quantized datasets.

See the following Faiss documentation links for more information.

- [Guidelines for choosing an index](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)
- [Index configuration summary](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)
- [Index Factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)
- [Binary Indexes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes)
- [Search Tuning](https://github.com/facebookresearch/faiss/wiki/Faster-search)

Note: For macOS users, an existing bug in an upstream package restricts the number of processing threads to 1. This limitation is managed internally to prevent system crashes.

### hnsw
```yaml
hnsw:
    efconstruction:  ef_construction param for init_index (int) - defaults to 200
    m: M param for init_index (int) - defaults to 16
    randomseed: random-seed param for init_index (int) - defaults to 100
    efsearch: ef search param (int) - defaults to None and not set
```

See [Hnswlib documentation](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) for more information on these parameters.

### annoy
```yaml
annoy:
    ntrees: number of trees (int) - defaults to 10
    searchk: search_k search setting (int) - defaults to -1
```

See [Annoy documentation](https://github.com/spotify/annoy#full-python-api) for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported.

### ggml
```yaml
ggml:
    gpu: enable GPU - defaults to True
    quantize: sets the tensor quantization - defaults to F32
    querysize: query buffer size - defaults to 64
```

The [GGML](https://github.com/ggml-org/ggml) backend is a k-nearest neighbors backend. It stores tensors using GGML and [GGUF](https://huggingface.co/docs/hub/en/gguf). It supports GPU-enabled operations and supports quantization. GGML is the framework used by [llama.cpp](https://github.com/ggml-org/llama.cpp).

[See this](https://github.com/ggml-org/ggml/blob/master/include/ggml.h#L379) for a list of quantization types.

### numpy

The NumPy backend is a k-nearest neighbors backend. It's designed for simplicity and works well with smaller datasets that fit into memory.

```yaml
numpy:
    safetensors: stores vectors using the safetensors format
                 defaults to NumPy array storage
```

### torch

The Torch backend is a k-nearest neighbors backend like NumPy. It supports GPU-enabled operations. It also has support for quantization which enables fitting larger arrays into GPU memory.

When quantization is enabled, vectors are _always_ stored in safetensors. _Note that macOS support for quantization is limited._

```yaml
torch:
    safetensors: stores vectors using the safetensors format - defaults
                 to NumPy array storage if quantization is disabled
    quantize:
        type: quantization type (fp4, nf4, int8)
        blocksize: quantization block size parameter
```

### pgvector
```yaml
pgvector:
    url: database url connection string, alternatively can be set via
         ANN_URL environment variable
    schema: database schema to store vectors - defaults to being
            determined by the database
    table: database table to store vectors - defaults to `vectors`
    precision: vector float precision (half or full) - defaults to `full`
    efconstruction:  ef_construction param (int) - defaults to 200
    m: M param for init_index (int) - defaults to 16
```

The pgvector backend stores embeddings in a Postgres database. See the [pgvector documentation](https://github.com/pgvector/pgvector-python?tab=readme-ov-file#sqlalchemy) for more information on these parameters. See the [SQLAlchemy](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls) documentation for more information on how to construct url connection strings.

### sqlite
```yaml
sqlite:
    quantize: store vectors with x-bit precision vs 32-bit (boolean|int)
              true sets 8-bit precision, false disables, int sets specified
              precision
    table: database table to store vectors - defaults to `vectors`
```

The SQLite backend stores embeddings in a SQLite database using [sqlite-vec](https://github.com/asg017/sqlite-vec). This backend supports 1-bit and 8-bit quantization at the storage level.

See [this note](https://alexgarcia.xyz/sqlite-vec/python.html#macos-blocks-sqlite-extensions-by-default) on how to run this ANN on MacOS.



================================================
FILE: docs/embeddings/configuration/cloud.md
================================================
# Cloud

The following describes parameters used to sync indexes with cloud storage. Cloud object storage, the [Hugging Face Hub](https://huggingface.co/models) and custom providers are all supported.

Parameters are set via the [embeddings.load](../../methods/#txtai.embeddings.base.Embeddings.load) and [embeddings.save](../../methods/#txtai.embeddings.base.Embeddings.save) methods.

## provider
```yaml
provider: string
```

Cloud provider. Can be one of the following:

- Cloud object storage. Set to one of these [providers](https://libcloud.readthedocs.io/en/stable/storage/supported_providers.html). Use the text shown in the `Provider Constant` column as lower case.

- Hugging Face Hub. Set to `huggingface-hub`.

- Custom providers. Set to the full class path of the custom provider.

## container
```yaml
container: string
```

Container/bucket/directory/repository name. Embeddings will be stored in the container with the filename specified by the `path` configuration.

## Cloud object storage configuration

In addition to the above common configuration, the cloud object storage provider has the following additional configuration parameters. Note that some cloud providers do not need any of these parameters and can use implicit authentication with service accounts.

See the [libcloud documentation](https://libcloud.readthedocs.io/en/stable/apidocs/libcloud.common.html#module-libcloud.common.base) for more information on these parameters.

### key
```yaml
key: string
```

Provider-specific access key. Can also be set via `ACCESS_KEY` environment variable. Ensure the configuration file is secured if added to the file. When using implicit authentication, set this to a value such as 'using-implicit-auth'.

### secret
```yaml
secret: string
```

Provider-specific access secret. Can also be set via `ACCESS_SECRET` environment variable. Ensure the configuration file is secured if added to the file. When using implicit authentication, this option is not required.

### prefix
```yaml
prefix: string
```

Optional object prefix. Object storage doesn't have the concept of a directory but a prefix is similar. For example, a prefix could be `base/dir`. This helps with organizing data in an object storage bucket.

More can be found at the following links.

- [Organizing objects using prefixes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html)
- [libcloud container method documentation](https://libcloud.readthedocs.io/en/stable/storage/api.html#libcloud.storage.base.StorageDriver.iterate_container_objects)

### host
```yaml
host: string
```

Optional server host name. Set when using a local cloud storage server.

### port
```yaml
port: int
```

Optional server port. Set when using a local cloud storage server.

### token
```yaml
token: string
```

Optional temporary session token

### region
```yaml
region: string
```

Optional parameter to specify the storage region, provider-specific.

## Hugging Face Hub configuration

The huggingface-hub provider supports the following additional configuration parameters. More on these parameters can be found in the [Hugging Face Hub's documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/overview).

### revision
```yaml
revision: string
```

Optional Git revision id which can be a branch name, a tag, or a commit hash

### cache
```yaml
cache: string
```

Path to the folder where cached files are stored

### token
```yaml
token: string|boolean
```

Token to be used for the download. If set to True, the token will be read from the Hugging Face config folder.



================================================
FILE: docs/embeddings/configuration/database.md
================================================
# Database

Databases store metadata, text and binary content.

## content
```yaml
content: boolean|sqlite|duckdb|client|url|custom
```

Enables content storage. When true, the default storage engine, `sqlite` will be used to save metadata.

Client-server connections are supported with either `client` or a full connection URL. When set to `client`, the CLIENT_URL environment variable must be set to the full connection URL. See the [SQLAlchemy](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls) documentation for more information on how to construct connection strings for client-server databases.

Add custom storage engines via setting this parameter to the fully resolvable class string.

Content storage specific settings are set with a corresponding configuration object having the same name as the content storage engine (i.e. duckdb or sqlite). These are optional and set to defaults if omitted.

### client
```yaml
schema:  default database schema for the session - defaults to being
         determined by the database
```

Additional settings for client-server databases. Also supported when the `content=url`.

### sqlite
```yaml
sqlite:
    wal: enable write-ahead logging - allows concurrent read/write operations,
         defaults to false
```

Additional settings for SQLite.

## objects
```yaml
objects: boolean|image|pickle
```

Enables object storage. Supports storing binary content. Requires content storage to also be enabled.

Object encoding options are:

- `standard`: Default encoder when boolean set. Encodes and decodes objects as byte arrays.
- `image`: Image encoder. Encodes and decodes objects as image objects.
- `pickle`: Pickle encoder. Encodes and decodes objects with the pickle module. Supports arbitrary objects.

## functions
```yaml
functions: list
```

List of functions with user-defined SQL functions, only used when [content](#content) is enabled. Each list element must be one of the following:

- function
- callable object
- dict with fields for name, argcount and function

[An example can be found here](../../query#custom-sql-functions).

## query
```yaml
query:
    path: sets the path for the query model - this can be any model on the
          Hugging Face Model Hub or a local file path.
    prefix: text prefix to prepend to all inputs
    maxlength: maximum generated sequence length
```

Query translation model. Translates natural language queries to txtai compatible SQL statements.



================================================
FILE: docs/embeddings/configuration/general.md
================================================
# General

General configuration options.

## keyword
```yaml
keyword: boolean|string
```

Enables sparse keyword indexing for this embeddings.

When set to a boolean, this parameter creates a BM25 index for full text search. When set to a string, it expects a [keyword method](../scoring#method).

It also implicitly disables the [defaults](#defaults) setting for vector search.

## sparse
```yaml
sparse: boolean|path
```

Enables sparse vector indexing for this embeddings.

When set to `True`, this parameter creates a sparse vector index using the [default sparse index model](https://huggingface.co/prithivida/Splade_PP_en_v2). When set to a string, it expects a local or Hugging Face model path.

It also implicitly disables the [defaults](#defaults) setting for vector search.

## dense
```yaml
dense: boolean|string
```

Alias for the [vector model path](../vectors/#path). When set to `True`, the [default transformers vector model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) is used.

## hybrid
```yaml
hybrid: boolean
```

Enables hybrid (sparse + dense) indexing for this embeddings.

When enabled, this parameter creates a BM25 index for full text search. It has no effect on the [defaults](#defaults) or [path](../vectors/#path) settings.

## defaults
```yaml
defaults: boolean
```

Uses default vector model path when enabled (default setting is True) and `path` is not provided. See [this link](../) for an example.

## indexes
```yaml
indexes: dict
```

Key value pairs defining subindexes for this embeddings. Each key is the index name and the value is the full configuration. This configuration can use any of the available configurations in a standard embeddings instance.

## autoid
```yaml
format: int|uuid function
```

Sets the auto id generation method. When this is not set, an autogenerated numeric sequence is used. This also supports [UUID generation functions](https://docs.python.org/3/library/uuid.html#uuid.uuid1). For example, setting this value to `uuid4` will generate random UUIDs. Setting this to `uuid5` will generate deterministic UUIDs for each input data row.

## columns
```yaml
columns:
    text: name of the text column
    object: name of the object column
```

Sets the `text` and `object` column names. Defaults to `text` and `object` if not provided.

## format
```yaml
format: json|pickle
```

Sets the configuration storage format. Defaults to `json`.



================================================
FILE: docs/embeddings/configuration/graph.md
================================================
# Graph

Enable graph storage via the `graph` parameter. This component requires the [graph](../../../install/#graph) extras package.

When enabled, a graph network is built using the embeddings index. Graph nodes are synced with each embeddings index operation (index/upsert/delete). Graph edges are created using the embeddings index upon completion of each index/upsert/delete embeddings index call.

## backend
```yaml
backend: networkx|rdbms|custom
```

Sets the graph backend. Defaults to `networkx`.

Add custom graph storage engines via setting this parameter to the fully resolvable class string.

The `rdbms` backend has the following additional settings.

### rdbms
```yaml
url: database url connection string, alternatively can be set via the
     GRAPH_URL environment variable
schema: database schema to store graph - defaults to being
        determined by the database
nodes: table to store node data, defaults to `nodes`
edges: table to store edge data, defaults to `edges`
```

## batchsize
```yaml
batchsize: int
```

Batch query size, used to query embeddings index - defaults to 256.

## limit
```yaml
limit: int
```

Maximum number of results to return per embeddings query - defaults to 15.

## minscore
```yaml
minscore: float
```

Minimum score required to consider embeddings query matches - defaults to 0.1.

## approximate
```yaml
approximate: boolean
```

When true, queries only run for nodes without edges - defaults to true.

## topics
```yaml
topics:
    algorithm: community detection algorithm (string), options are
               louvain (default), greedy, lpa
    level: controls number of topics (string), options are best (default) or first
    resolution: controls number of topics (int), larger values create more
                topics (int), defaults to 100
    labels: scoring index method used to build topic labels (string)
            options are bm25 (default), tfidf, sif
    terms: number of frequent terms to use for topic labels (int), defaults to 4
    stopwords: optional list of stop words to exclude from topic labels
    categories: optional list of categories used to group topics, allows
                granular topics with broad categories grouping topics
```

Enables topic modeling. Defaults are tuned so that in most cases these values don't need to be changed (except for categories). These parameters are available for advanced use cases where one wants full control over the community detection process.

## copyattributes
```yaml
copyattributes: boolean|list
```

Copy these attributes from input dictionaries in the `insert` method. If this is set to `True`, all attributes are copied. Otherwise, only the
attributes specified in this list are copied to the graph as attributes.



================================================
FILE: docs/embeddings/configuration/index.md
================================================
# Configuration

The following describes available embeddings configuration. These parameters are set in the [Embeddings constructor](../methods#txtai.embeddings.base.Embeddings.__init__) via either the `config` parameter or as keyword arguments.

Configuration is designed to be optional and set only when needed. Out of the box, sensible defaults are picked to get up and running fast. For example:

```python
from txtai import Embeddings

embeddings = Embeddings()
```

Creates a new embeddings instance, using [all-MiniLM-L6-v2](https://hf.co/sentence-transformers/all-MiniLM-L6-v2) as the vector model, [Faiss](https://faiss.ai/) as the ANN index backend and content disabled.

```python
from txtai import Embeddings

embeddings = Embeddings(content=True)
```

Is the same as above except it adds in [SQLite](https://www.sqlite.org/index.html) for content storage. 

The following sections link to all the available configuration options.

## [ANN](./ann)

The default vector index backend is Faiss.

## [Cloud](./cloud)

Embeddings databases can optionally be synced with cloud storage.

## [Database](./database)

Content storage is disabled by default. When enabled, SQLite is the default storage engine.

## [General](./general)

General configuration that doesn't fit elsewhere.

## [Graph](./graph)

An accomplying graph index can be created with an embeddings database. This enables topic modeling, path traversal and more. [NetworkX](https://github.com/networkx/networkx) is the default graph index.

## [Scoring](./scoring)

Sparse keyword indexing and word vectors term weighting.

## [Vectors](./vectors)

Vector search is enabled by converting text and other binary data into embeddings vectors. These vectors are then stored in an ANN index. The vector model is optional and a default model is used when not provided.



================================================
FILE: docs/embeddings/configuration/scoring.md
================================================
# Scoring

Enable scoring support via the `scoring` parameter.

This scoring instance can serve two purposes, depending on the settings.

One use case is building sparse/keyword indexes. This occurs when the `terms` parameter is set to `True`.

The other use case is with word vector term weighting. This feature has been available since the initial version but isn't quite as common anymore.

The following covers the available options.

## method
```yaml
method: bm25|tfidf|sif|pgtext|sparse|custom
```

Sets the scoring method. Add custom scoring via setting this parameter to the fully resolvable class string.

### pgtext
```yaml
schema: database schema to store keyword index - defaults to being
        determined by the database
```

Additional settings for Postgres full-text keyword indexes.

### sparse
```yaml
path: sparse vector model path
vectormethod: vector embeddings method
vectornormalize: enable vector embeddings normalization (boolean)
gpu: boolean|int|string|device
normalize: enable score normalization (boolean | float)
batch: Sets the transform batch size
encodebatch: Sets the encode batch size
vectors: additional model init args
encodeargs: additional encode() args
backend: ivfsparse|pgsparse
```

Sparse vector scoring options. The sparse scoring instance combines a sparse vector model with a sparse approximate nearest neighbor index (ANN). This method supports both vector normalization and score normalization.

Vector normalization normalizes all vectors to have a magnitude of 1. By extension, all generated scores will be 0 to 1.

Score normalization scales output scores by a scale factor. When `normalize` is set to `True` it uses a default scale factor. If `normalize` is an integer, then that is used as the scale factor. This scales the scores from 0 to 1.

#### ivfsparse
```yaml
ivfsparse:
  sample: percent of data to use for model training (0.0 - 1.0)
  nfeatures: top n features to use for model training (int)
  nlist: desired number of clusters (int)
  nprobe: search probe setting (int)
  minpoints: minimum number of points for a cluster (int)
```

Inverted file (IVF) index with flat vector file storage and sparse array support.

#### pgsparse

Sparse ANN backed by Postgres. Supports same options as the [pgvector](../ann/#pgvector) ANN.

## terms
```yaml
terms: boolean|dict
```

Enables term frequency sparse arrays for a scoring instance. This is the backend for sparse keyword indexes.

Supports a `dict` with the parameters `cachelimit` and `cutoff`.

`cachelimit` is the maximum amount of resident memory in bytes to use during indexing before flushing to disk. This parameter is an `int`.

`cutoff` is used during search to determine what constitutes a common term. This parameter is a `float`, i.e. 0.1 for a cutoff of 10%.

When `terms` is set to `True`, default parameters are used for the `cachelimit` and `cutoff`. Normally, these defaults are sufficient.

## normalize
```yaml
normalize: boolean
```

Enables normalized scoring (ranging from 0 to 1). When enabled, statistics from the index will be used to calculate normalized scores.



================================================
FILE: docs/embeddings/configuration/vectors.md
================================================
# Vectors

The following covers available vector model configuration options.

## path
```yaml
path: string
```

Sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the
[Hugging Face Hub](https://huggingface.co/models) or a local file path. Otherwise, it must be a local file path to a word embeddings model.

## method
```yaml
method: transformers|sentence-transformers|llama.cpp|litellm|model2vec|external|words
```

Embeddings method to use. If the method is not provided, it is inferred using the `path`.

`sentence-transformers`, `llama.cpp`, `litellm`, `model2vec` and `words` require the [vectors](../../../install/#vectors) extras package to be installed.

### transformers

Builds embeddings using a transformers model. While this can be any transformers model, it works best with
[models trained](https://huggingface.co/models?pipeline_tag=sentence-similarity) to build embeddings.

`mean`, `cls` and `late` pooling are supported and automatically inferred from the model. The pooling method can be overwritten by changing the method
from `transformers` to `meanpooling`, `clspooling` or `latepooling` respectively.

Setting `maxlength` to `True` enables truncating inputs to the `max_seq_length`. Setting `maxlength` to an integer will truncate inputs to that value. When omitted (default), the `maxlength` will be set to either the model or tokenizer maxlength.

### sentence-transformers

Same as transformers but loads models with the [sentence-transformers](https://github.com/UKPLab/sentence-transformers) library.

### llama.cpp

Builds embeddings using a [llama.cpp](https://github.com/abetlen/llama-cpp-python) model. Supports both local and remote GGUF paths on the HF Hub.

### litellm

Builds embeddings using a LiteLLM model. See the [LiteLLM documentation](https://litellm.vercel.app/docs/providers) for the options available with LiteLLM models.

### model2vec

Builds embeddings using a [Model2Vec](https://github.com/MinishLab/model2vec) model. Model2Vec is a knowledge-distilled version of a transformers model with static vectors.

### words

Builds embeddings using a word embeddings model and static vectors. While Transformers models are preferred in most cases, this method can be useful for low resource and historical languages where there isn't much linguistic data available.

#### pca
```yaml
pca: int
```

Removes _n_ principal components from generated embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single embedding, this method is applied.

### external

Embeddings are created via an external model or API. Requires setting the [transform](#transform) parameter to a function that translates data into embeddings.

#### transform
```yaml
transform: function
```

When method is `external`, this function transforms input content into embeddings. The input to this function is a list of data. This method must return either a numpy array or list of numpy arrays.

## gpu
```yaml
gpu: boolean|int|string|device
```

Set the target device. Supports true/false, device id, device string and torch device instance. This is automatically derived if omitted.

The `sentence-transformers` method supports encoding with multiple GPUs. This can be enabled by setting the gpu parameter to `all`.

## batch
```yaml
batch: int
```

Sets the transform batch size. This parameter controls how input streams are chunked and vectorized.

## encodebatch
```yaml
encodebatch: int
```

Sets the encode batch size. This parameter controls the underlying vector model batch size. This often corresponds to a GPU batch size, which controls GPU memory usage.

## dimensionality
```yaml
dimensionality: int
```

Enables truncation of vectors to this dimensionality. This is only useful for models trained to store more important information in earlier dimensions such as [Matryoshka Representation Learning (MRL)](https://huggingface.co/blog/matryoshka).

## quantize
```yaml
quantize: int|boolean
```

Enables scalar vector quantization at the specified precision. Supports 1-bit through 8-bit quantization. Scalar quantization transforms continuous floating point values to discrete unsigned integers. The `faiss`, `pgvector`, `numpy` and `torch` ANN backends support storing these vectors.

This parameter supports booleans for backwards compatability. When set to true/false, this flag sets [faiss.quantize](../ann/#faiss).

In addition to vector-level quantization, some ANN backends have the ability to quantize vectors at the storage layer. See the [ANN](../ann) configuration options for more.

## instructions
```yaml
instructions:
    query: prefix for queries
    data: prefix for indexing
```

Instruction-based models use prefixes to modify how embeddings are computed. This is especially useful with asymmetric search, which is when the query and indexed data are of vastly different lengths. In other words, short queries with long documents.

[E5-base](https://huggingface.co/intfloat/e5-base) is an example of a model that accepts instructions. It takes `query: ` and `passage: ` prefixes and uses those to generate embeddings that work well for asymmetric search.

## models
```yaml
models: dict
```

Loads and stores vector models in this cache. This is primarily used with subindexes but can be set on any embeddings instance. This prevents the same model from being loaded multiple times when working with multiple embeddings instances.

## tokenize
```yaml
tokenize: boolean
```

Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text. It's not recommended for use with recent vector models.

## vectors
```yaml
vectors: dict
```

Passes these additional parameters to the underlying vector model.



================================================
FILE: docs/images/agent.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "text",
      "version": 1781,
      "versionNonce": 1504167660,
      "isDeleted": false,
      "id": "yF7ftUwr3mnAwOlC59RMi",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1746.949005220418,
      "y": 32.708022189384565,
      "strokeColor": "#7950f2",
      "backgroundColor": "transparent",
      "width": 212.06666666666666,
      "height": 25.420547029684837,
      "seed": 376471794,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090696250,
      "link": null,
      "locked": false,
      "fontSize": 21.18378919140403,
      "fontFamily": 1,
      "text": "Tools and Functions",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Tools and Functions",
      "index": "a1",
      "frameId": null,
      "autoResize": true,
      "lineHeight": 1.2
    },
    {
      "type": "line",
      "version": 5394,
      "versionNonce": 1005349228,
      "isDeleted": false,
      "id": "aEfZn91vFxUsNoiS_231h",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1818.1429109642538,
      "y": -209.6883063766287,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fa5252",
      "width": 62.46534181999921,
      "height": 80.62144046844392,
      "seed": 283082348,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.20597861858336483,
          60.93324773178684
        ],
        [
          0.009639315807164919,
          67.87023338578067
        ],
        [
          3.2171032549927925,
          70.86761910642136
        ],
        [
          14.386936381131346,
          73.4043779800946
        ],
        [
          33.26711841453209,
          74.19428679833626
        ],
        [
          51.30588383530208,
          72.93308166143258
        ],
        [
          60.89001271988386,
          69.91693238712155
        ],
        [
          62.241724409786244,
          67.37428690837503
        ],
        [
          62.431567423392536,
          61.78937034530568
        ],
        [
          62.2825627324806,
          5.111959380832905
        ],
        [
          61.946658330498884,
          -0.24301167156979483
        ],
        [
          57.93575321694209,
          -3.2359402189020954
        ],
        [
          49.489643010028544,
          -4.969286064540119
        ],
        [
          30.242099023560478,
          -6.427153670107652
        ],
        [
          14.810450273983571,
          -5.557823934240758
        ],
        [
          2.6735633343171465,
          -2.609163541657122
        ],
        [
          -0.033774396606671454,
          -0.03661258119549431
        ],
        [
          0,
          0
        ]
      ],
      "index": "a3",
      "frameId": null
    },
    {
      "type": "line",
      "version": 3136,
      "versionNonce": 1274305516,
      "isDeleted": false,
      "id": "JSkCiTP6nfh3_0WXfktLt",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1821.286624574959,
      "y": -163.7331019656284,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 57.79381935156673,
      "height": 6.618955975035558,
      "seed": 465359084,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          1.6474005758537682,
          2.765528506120729
        ],
        [
          8.751958185556267,
          5.085781507915724
        ],
        [
          18.205684820748022,
          6.490912696060695
        ],
        [
          33.01612829072934,
          6.618955975035558
        ],
        [
          50.29940758591302,
          5.724372353349996
        ],
        [
          57.79381935156673,
          3.769506661359109
        ],
        [
          56.89849492771129,
          4.013173323127235
        ]
      ],
      "index": "a4",
      "frameId": null
    },
    {
      "type": "line",
      "version": 3222,
      "versionNonce": 727318124,
      "isDeleted": false,
      "id": "XbZa4_v84aJkwUbHNYtWo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1817.0033279434713,
      "y": -183.2009370605118,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 62.53013535500473,
      "height": 6.9378137498265104,
      "seed": 877384556,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          1.647400575853768,
          2.765528506120729
        ],
        [
          8.751958185556267,
          5.085781507915719
        ],
        [
          18.20568482074802,
          6.490912696060696
        ],
        [
          33.016128290729334,
          6.618955975035559
        ],
        [
          50.29940758591301,
          5.7243723533499935
        ],
        [
          60.393038010317625,
          2.4698973319836566
        ],
        [
          62.53013535500473,
          -0.3188577747909514
        ]
      ],
      "index": "a5",
      "frameId": null
    },
    {
      "type": "ellipse",
      "version": 6241,
      "versionNonce": 48713964,
      "isDeleted": false,
      "id": "WgZ_UNGw6chJCNh_YqBCg",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1818.1477872422097,
      "y": -216.22048622222513,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 62.06467888763865,
      "height": 12.55211437327341,
      "seed": 416712172,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "index": "a6",
      "frameId": null
    },
    {
      "type": "ellipse",
      "version": 1583,
      "versionNonce": 1378243436,
      "isDeleted": false,
      "id": "5zDzUgYAzqCuHz5y0fEbO",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1867.2486508066536,
      "y": -195.2355116320403,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 9.096173632176203,
      "height": 9.872132933859838,
      "seed": 778496108,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "index": "a7",
      "frameId": null
    },
    {
      "type": "ellipse",
      "version": 1632,
      "versionNonce": 392234476,
      "isDeleted": false,
      "id": "VtzlusDzgSLcDDBt5hG2x",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1867.2486508066536,
      "y": -173.56619840583764,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 9.096173632176203,
      "height": 9.872132933859838,
      "seed": 684129004,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "index": "a8",
      "frameId": null
    },
    {
      "type": "ellipse",
      "version": 1685,
      "versionNonce": 2007066732,
      "isDeleted": false,
      "id": "B_0DhVcOs-USQL4XV0QRo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1867.2486508066536,
      "y": -150.01453414636433,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 9.096173632176203,
      "height": 9.872132933859838,
      "seed": 847297900,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "index": "a9",
      "frameId": null
    },
    {
      "type": "text",
      "version": 1457,
      "versionNonce": 1722907372,
      "isDeleted": false,
      "id": "4zMdwTCbTrrgg-ITOMXC0",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1779.9259357205947,
      "y": -120.42699264611412,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 136.73556714551438,
      "height": 25.535793939087647,
      "seed": 1848588206,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1731090626713,
      "link": null,
      "locked": false,
      "fontSize": 21.27982828257304,
      "fontFamily": 1,
      "text": "Data Stores",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Data Stores",
      "index": "aA",
      "frameId": null,
      "autoResize": true,
      "lineHeight": 1.2
    },
    {
      "type": "arrow",
      "version": 3697,
      "versionNonce": 368624620,
      "isDeleted": false,
      "id": "iPTphyfJkvqyMptaZS0ge",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1814.6382080826127,
      "y": -173.3228158450134,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 182.4871709755887,
      "height": 81.80578637477413,
      "seed": 1965679388,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090676915,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": {
        "elementId": "V54vdkZmQeI13AEahvNPV",
        "focus": 0.1525458549344351,
        "gap": 4.034063550289034,
        "fixedPoint": null
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -182.4871709755887,
          81.80578637477413
        ]
      ],
      "index": "aB",
      "frameId": null
    },
    {
      "type": "text",
      "version": 1522,
      "versionNonce": 2052237012,
      "isDeleted": false,
      "id": "YCEwpQiKAjRS3zIG3_E8s",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1555.5109174458237,
      "y": -43.50932007711572,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 38.06666666666666,
      "height": 23.223174527172613,
      "seed": 676249636,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "fontSize": 19.35264543931051,
      "fontFamily": 1,
      "text": "LLM",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "LLM",
      "index": "aC",
      "frameId": null,
      "autoResize": true,
      "lineHeight": 1.2
    },
    {
      "type": "arrow",
      "version": 5294,
      "versionNonce": 1355936620,
      "isDeleted": false,
      "id": "leDc6Y5qdU0OH3j90C072",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1816.227500619918,
      "y": -9.184012279360502,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 182.7150865033991,
      "height": 52.05945128870996,
      "seed": 241264412,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090696850,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "qzVAVN6sXRxG16958WgBM",
        "focus": -0.358596672811187,
        "gap": 2.5501158014194516,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "V54vdkZmQeI13AEahvNPV",
        "focus": 0.704166265794774,
        "gap": 5.395440559783879,
        "fixedPoint": null
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -182.7150865033991,
          -52.05945128870996
        ]
      ],
      "index": "aD",
      "frameId": null
    },
    {
      "type": "line",
      "version": 2515,
      "versionNonce": 679390292,
      "isDeleted": false,
      "id": "aFBJSnHUVo8ZDTb0uAkQ2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1310.644313207792,
      "y": -121.5611895708457,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 99.33023905537627,
      "height": 88.43543879744573,
      "seed": 1963890941,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -53.27565065744737,
          16.38089846974506
        ],
        [
          -53.90255959959227,
          67.77711223685318
        ],
        [
          -6.4999879824429385,
          88.43543879744573
        ],
        [
          44.3351035833632,
          67.53847405928806
        ],
        [
          45.42767945578399,
          18.161010280772913
        ],
        [
          0,
          0
        ]
      ],
      "index": "aE",
      "frameId": null
    },
    {
      "type": "text",
      "version": 1046,
      "versionNonce": 235798996,
      "isDeleted": false,
      "id": "eqOUg003X-50uDJ_XGUnB",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1234.634080487935,
      "y": -28.17382387311136,
      "strokeColor": "#ffb13b",
      "backgroundColor": "#00e676",
      "width": 145.03034615947246,
      "height": 28.667631224488975,
      "seed": 1122464339,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "fontSize": 23.889692687074145,
      "fontFamily": 1,
      "text": "Agent Start",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Agent Start",
      "index": "aF",
      "frameId": null,
      "autoResize": true,
      "lineHeight": 1.2
    },
    {
      "type": "rectangle",
      "version": 2424,
      "versionNonce": 35039060,
      "isDeleted": false,
      "id": "ZTKEryJuyeKfeG4nAI4_e",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1519.3715280015788,
      "y": -97.22713220408534,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 16.103557439457987,
      "height": 40.2588935986449,
      "seed": 1188509340,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "roundness": null,
      "boundElements": [
        {
          "id": "Inol-LWi8GThocPMvSZGX",
          "type": "arrow"
        }
      ],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "index": "aG",
      "frameId": null
    },
    {
      "type": "rectangle",
      "version": 2566,
      "versionNonce": 741073492,
      "isDeleted": false,
      "id": "CckqOoWddQBHuvidbpk4P",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1552.7682727230922,
      "y": -135.31805500658663,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 16.103557439457962,
      "height": 80.51778719728983,
      "seed": 331107492,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "index": "aH",
      "frameId": null
    },
    {
      "type": "rectangle",
      "version": 2446,
      "versionNonce": 1386510292,
      "isDeleted": false,
      "id": "J8ox4BBdzEg-04DGRL-jh",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1583.2097012862876,
      "y": -111.1627188473997,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 16.103557439457973,
      "height": 56.36245103810287,
      "seed": 1316437916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090662600,
      "link": null,
      "locked": false,
      "index": "aI",
      "frameId": null
    },
    {
      "type": "rectangle",
      "version": 2640,
      "versionNonce": 739318484,
      "isDeleted": false,
      "id": "V54vdkZmQeI13AEahvNPV",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1612.013416117277,
      "y": -128.4099677663797,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 16.103557439457962,
      "height": 72.46600847756086,
      "seed": 1025829916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        },
        {
          "id": "5pptt-wBgcgbxd5LUka3S",
          "type": "arrow"
        },
        {
          "id": "iPTphyfJkvqyMptaZS0ge",
          "type": "arrow"
        }
      ],
      "updated": 1731090668783,
      "link": null,
      "locked": false,
      "index": "aJ",
      "frameId": null
    },
    {
      "type": "arrow",
      "version": 508,
      "versionNonce": 441289068,
      "isDeleted": false,
      "id": "Inol-LWi8GThocPMvSZGX",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1355.5263615107983,
      "y": -75.4835786217636,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "transparent",
      "width": 160.1111111111113,
      "height": 0.7777777777777999,
      "seed": 1433457683,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090662602,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": {
        "elementId": "ZTKEryJuyeKfeG4nAI4_e",
        "focus": -0.12143341039143216,
        "gap": 3.73405537966903,
        "fixedPoint": null
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          160.1111111111113,
          0.7777777777777999
        ]
      ],
      "index": "aK",
      "frameId": null
    },
    {
      "type": "line",
      "version": 2786,
      "versionNonce": 635507308,
      "isDeleted": false,
      "id": "8H60JYjdZgpvCjCSLf54L",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2069.873935895482,
      "y": -127.67713376387258,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 99.4022378882407,
      "height": 88.49954061012453,
      "seed": 1027783997,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1731090655648,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -53.31426714225508,
          16.39277205005967
        ],
        [
          -53.9416304949861,
          67.82624000521841
        ],
        [
          -6.504699453519805,
          88.49954061012453
        ],
        [
          44.36723957481235,
          67.58742885243016
        ],
        [
          45.4606073932546,
          18.174174162751758
        ],
        [
          0,
          0
        ]
      ],
      "index": "aL",
      "frameId": null
    },
    {
      "type": "text",
      "version": 1287,
      "versionNonce": 1131144428,
      "isDeleted": false,
      "id": "E_wil3E0D2OO7QeMf_xiu",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 2006.4263604935459,
      "y": -33.70532271267456,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 118.31123365004963,
      "height": 28.68841075757759,
      "seed": 754233971,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090655648,
      "link": null,
      "locked": false,
      "fontSize": 23.90700896464799,
      "fontFamily": 1,
      "text": "Agent End",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Agent End",
      "index": "aM",
      "frameId": null,
      "autoResize": true,
      "lineHeight": 1.2
    },
    {
      "type": "arrow",
      "version": 845,
      "versionNonce": 1896722900,
      "isDeleted": false,
      "id": "5pptt-wBgcgbxd5LUka3S",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1632.4152503996868,
      "y": -79.37246751065243,
      "strokeColor": "#00e676",
      "backgroundColor": "#34bbde",
      "width": 380.1111111111111,
      "height": 0.8888888888888005,
      "seed": 603518365,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1731090666260,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "V54vdkZmQeI13AEahvNPV",
        "focus": 0.35400627835344733,
        "gap": 4.29827684295185,
        "fixedPoint": null
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          380.1111111111111,
          -0.8888888888888005
        ]
      ],
      "index": "aN",
      "frameId": null
    },
    {
      "id": "qzVAVN6sXRxG16958WgBM",
      "type": "rectangle",
      "x": 1818.7776164213374,
      "y": -52.91854707012696,
      "width": 72.81644178885557,
      "height": 75.24632678271404,
      "angle": 0,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "seed": 1985394028,
      "version": 1013,
      "versionNonce": 627575148,
      "isDeleted": false,
      "boundElementIds": null,
      "index": "aW",
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1731090696250,
      "link": null,
      "locked": false
    },
    {
      "id": "z6DGOpk0Ie3KZbNy-UpRO",
      "type": "rectangle",
      "x": 1835.6278241812026,
      "y": -37.7505489614438,
      "width": 40.35005029973769,
      "height": 16.04584197060145,
      "angle": 0,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "seed": 2036669676,
      "version": 850,
      "versionNonce": 1651351148,
      "isDeleted": false,
      "boundElementIds": null,
      "index": "aZ",
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1731090696250,
      "link": null,
      "locked": false
    },
    {
      "id": "3SSoVnN124G8vYffp5E2K",
      "type": "rectangle",
      "x": 1835.6278241812026,
      "y": -9.064290762712005,
      "width": 40.35005029973769,
      "height": 16.04584197060145,
      "angle": 0,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "seed": 1326096236,
      "version": 892,
      "versionNonce": 1451684076,
      "isDeleted": false,
      "boundElementIds": null,
      "index": "aa",
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1731090696250,
      "link": null,
      "locked": false
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/api.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 3067,
      "versionNonce": 2074130101,
      "isDeleted": false,
      "id": "Ufh5VUA3qmvJowuFyEWz4",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 686.7515859513394,
      "y": 139.76834100985968,
      "strokeColor": "#d0d9dd",
      "backgroundColor": "transparent",
      "width": 647.3302177689409,
      "height": 598.4947884819552,
      "seed": 1218477931,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763581,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 834,
      "versionNonce": 351468315,
      "isDeleted": false,
      "id": "np05zgjgJ1cvT_UcHRO4Q",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 430.44757097692855,
      "y": 299.40554486158544,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 110.66704644097234,
      "height": 115.17917209201391,
      "seed": 112784715,
      "groupIds": [
        "MUxhY1x2GsxZKOmYcaGSB"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "_UxQXW5nkuKOXtaavoJ3q",
          "type": "arrow"
        }
      ],
      "updated": 1688312763581,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 3122,
      "versionNonce": 606214165,
      "isDeleted": false,
      "id": "GtS3MjQ4UtMxFXJO57RTt",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 436.4176538657067,
      "y": 346.76486653900156,
      "strokeColor": "#000000",
      "backgroundColor": "#4c6ef5",
      "width": 100.73332977294922,
      "height": 26,
      "seed": 1042420715,
      "groupIds": [
        "MUxhY1x2GsxZKOmYcaGSB"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763581,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "txtai.java",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "txtai.java",
      "lineHeight": 1.3,
      "baseline": 18
    },
    {
      "type": "ellipse",
      "version": 1432,
      "versionNonce": 951771067,
      "isDeleted": false,
      "id": "OwUTZKSXPIR8EHoAXUrAc",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 748.5117575144184,
      "y": 353.37666266846827,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 27.50940800286237,
      "height": 23.432583857740315,
      "seed": 616058027,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "wtQWWSoRs-w6Dtmax-Hb5",
          "type": "arrow"
        },
        {
          "id": "_UxQXW5nkuKOXtaavoJ3q",
          "type": "arrow"
        }
      ],
      "updated": 1688312763581,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1403,
      "versionNonce": 1823493493,
      "isDeleted": false,
      "id": "qfK-ulSZh44Bl6KC9Ii6F",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 746.9882589503762,
      "y": 513.0271342544165,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 27.50940800286237,
      "height": 23.432583857740315,
      "seed": 605129061,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "Iju4jXbzOeX44JCepZN3j",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1805,
      "versionNonce": 794636379,
      "isDeleted": false,
      "id": "xcu0cSh50RePPTdayuKwa",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 744.9160350012032,
      "y": 664.9580577695606,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 27.50940800286237,
      "height": 23.432583857740315,
      "seed": 726590347,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "QyCYDQ1akZks7nob5gQcb",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 1891,
      "versionNonce": 1687213781,
      "isDeleted": false,
      "id": "M3a040F3DuK1icxv47TKI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 888.8977161326809,
      "y": 311.8359502397928,
      "strokeColor": "#495057",
      "backgroundColor": "#ced4da",
      "width": 291.907455568772,
      "height": 237.38924779486172,
      "seed": 2113350923,
      "groupIds": [
        "OA8MrA83v3EseguJzvKpu"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          94.12373118273203,
          -0.35823119069767984
        ],
        [
          119.85817357723676,
          26.483994997226382
        ],
        [
          290.1826277821376,
          26.874356201934916
        ],
        [
          291.23331617134846,
          236.70336432335256
        ],
        [
          -0.6741393974235308,
          237.03101660416405
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 1544,
      "versionNonce": 179072251,
      "isDeleted": false,
      "id": "09DIdeixkMWbCga6IxuMh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 60,
      "angle": 0,
      "x": 896.3667273815456,
      "y": 321.85704123732336,
      "strokeColor": "#000000",
      "backgroundColor": "#ced4da",
      "width": 11.242087394797462,
      "height": 9.502075937111973,
      "seed": 1749242795,
      "groupIds": [
        "o22U873S7MmF-aAZOMXrC",
        "OA8MrA83v3EseguJzvKpu"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1579,
      "versionNonce": 374944821,
      "isDeleted": false,
      "id": "HvI6J1Tn27hWbdbb3G2PZ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 60,
      "angle": 0,
      "x": 911.3550504277948,
      "y": 321.3688660254814,
      "strokeColor": "#000000",
      "backgroundColor": "#ced4da",
      "width": 11.242087394797462,
      "height": 9.502075937111973,
      "seed": 261203531,
      "groupIds": [
        "o22U873S7MmF-aAZOMXrC",
        "OA8MrA83v3EseguJzvKpu"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1632,
      "versionNonce": 2116752795,
      "isDeleted": false,
      "id": "2zH7sdihf_U-DKWGnQEP-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 60,
      "angle": 0,
      "x": 927.1911747446115,
      "y": 320.8462868771415,
      "strokeColor": "#000000",
      "backgroundColor": "#ced4da",
      "width": 11.242087394797462,
      "height": 9.502075937111973,
      "seed": 1790747883,
      "groupIds": [
        "o22U873S7MmF-aAZOMXrC",
        "OA8MrA83v3EseguJzvKpu"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1672,
      "versionNonce": 502195605,
      "isDeleted": false,
      "id": "XUyFnqAl7kQAnMAoB7Cgx",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 935.2057099043777,
      "y": 359.9947885349467,
      "strokeColor": "#343a40",
      "backgroundColor": "#fefefe",
      "width": 189.85440623862308,
      "height": 166.25236270503373,
      "seed": 827248523,
      "groupIds": [
        "OA8MrA83v3EseguJzvKpu"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "wtQWWSoRs-w6Dtmax-Hb5",
          "type": "arrow"
        },
        {
          "id": "D904AMBJU1YvbvQMXyluo",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1200,
      "versionNonce": 688630331,
      "isDeleted": false,
      "id": "oYWPL1hYAJT2RGX0H8kwp",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 988.3296817129515,
      "y": 416.9111931412608,
      "strokeColor": "#343a40",
      "backgroundColor": "#fefefe",
      "width": 92.69999694824219,
      "height": 72,
      "seed": 857102411,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "API \nservice",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "API \nservice",
      "lineHeight": 1.2857142857142858,
      "baseline": 61
    },
    {
      "type": "arrow",
      "version": 3749,
      "versionNonce": 2122803957,
      "isDeleted": false,
      "id": "wtQWWSoRs-w6Dtmax-Hb5",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 948.4692485198284,
      "y": 398.09023585204204,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fefefe",
      "width": 169.2246874884429,
      "height": 34.21193675692433,
      "seed": 1918002661,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "XUyFnqAl7kQAnMAoB7Cgx",
        "focus": 0.5294260456922872,
        "gap": 1.5145847198102445
      },
      "endBinding": {
        "elementId": "OwUTZKSXPIR8EHoAXUrAc",
        "focus": -0.11436214558425631,
        "gap": 3.2793683958148367
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -41.176703496995515,
          -0.5128540090383922
        ],
        [
          -41.24791785008779,
          -33.26749312785438
        ],
        [
          -169.2246874884429,
          -34.21193675692433
        ]
      ]
    },
    {
      "type": "line",
      "version": 5941,
      "versionNonce": 2003095061,
      "isDeleted": false,
      "id": "_btUlJm1SELpFH74lhe5u",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1242.776436154969,
      "y": 396.44332526359824,
      "strokeColor": "#5f3dc4",
      "backgroundColor": "#7950f2",
      "width": 76.99810389727404,
      "height": 99.37827711605759,
      "seed": 576008901,
      "groupIds": [
        "nC_wnGn6V9Cg51lj82An8",
        "xEHH4p1jeHBH4PUaRrIni",
        "JG_8M8qPMZ6QPLNYCIjBV"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312766628,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.25390020469271946,
          75.10956320658954
        ],
        [
          0.011881933539363843,
          83.66046081728857
        ],
        [
          3.9655726433067358,
          87.35519793732486
        ],
        [
          17.734103263694266,
          90.48214189738948
        ],
        [
          41.00682018880677,
          91.45582553513545
        ],
        [
          63.24236222825351,
          89.90119697892055
        ],
        [
          75.05626943052894,
          86.1833309042851
        ],
        [
          76.72246117951802,
          83.04913080160064
        ],
        [
          76.95647177899504,
          76.16486549140681
        ],
        [
          76.77280066894052,
          6.301273122914296
        ],
        [
          76.35874703071867,
          -0.29954911620753677
        ],
        [
          71.41469198102897,
          -3.9887920872726426
        ],
        [
          61.00356715097897,
          -6.125406402086433
        ],
        [
          37.27802033642641,
          -7.922451580922152
        ],
        [
          18.256149021768273,
          -6.850869494392456
        ],
        [
          3.2955764171578545,
          -3.2161938062295943
        ],
        [
          -0.04163211827899763,
          -0.0451306156134037
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 6700,
      "versionNonce": 950438331,
      "isDeleted": false,
      "id": "afFg1c-VqjZ6VB_2cnCgV",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1242.2827684130289,
      "y": 387.6723986399642,
      "strokeColor": "#5f3dc4",
      "backgroundColor": "#7950f2",
      "width": 76.50422544892463,
      "height": 15.472404032124233,
      "seed": 1054869515,
      "groupIds": [
        "nC_wnGn6V9Cg51lj82An8",
        "xEHH4p1jeHBH4PUaRrIni",
        "JG_8M8qPMZ6QPLNYCIjBV"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312766628,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1907,
      "versionNonce": 1915503477,
      "isDeleted": false,
      "id": "ymMTsrfERD1IWhRqaTh_I",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1261.2853467746677,
      "y": 422.35935481481545,
      "strokeColor": "#5f3dc4",
      "backgroundColor": "#7950f2",
      "width": 17.116666793823242,
      "height": 36,
      "seed": 621580837,
      "groupIds": [
        "xEHH4p1jeHBH4PUaRrIni",
        "JG_8M8qPMZ6QPLNYCIjBV"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [
        {
          "id": "D904AMBJU1YvbvQMXyluo",
          "type": "arrow"
        }
      ],
      "updated": 1688312766628,
      "link": null,
      "locked": false,
      "fontSize": 29.219434366479078,
      "fontFamily": 3,
      "text": "{",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "{",
      "lineHeight": 1.2320567040578883,
      "baseline": 29
    },
    {
      "type": "text",
      "version": 1877,
      "versionNonce": 1212389979,
      "isDeleted": false,
      "id": "j26zNzxtmoRXOdiX0ecde",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1283.2962201204132,
      "y": 422.8360647867345,
      "strokeColor": "#5f3dc4",
      "backgroundColor": "#7950f2",
      "width": 17.21666717529297,
      "height": 35.94436319458793,
      "seed": 1834536619,
      "groupIds": [
        "xEHH4p1jeHBH4PUaRrIni",
        "JG_8M8qPMZ6QPLNYCIjBV"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312766628,
      "link": null,
      "locked": false,
      "fontSize": 29.384515916572173,
      "fontFamily": 3,
      "text": "}",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "}",
      "lineHeight": 1.2232416316348487,
      "baseline": 29
    },
    {
      "type": "text",
      "version": 1091,
      "versionNonce": 52301013,
      "isDeleted": false,
      "id": "RVGdpt8fu7DFpzBrOmhVl",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1257.5750007629395,
      "y": 489.46141030254137,
      "strokeColor": "#5f3dc4",
      "backgroundColor": "#7950f2",
      "width": 46.849998474121094,
      "height": 24,
      "seed": 1281426821,
      "groupIds": [
        "JG_8M8qPMZ6QPLNYCIjBV"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312766628,
      "link": null,
      "locked": false,
      "fontSize": 18.16360832146819,
      "fontFamily": 1,
      "text": "txtai",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "txtai",
      "lineHeight": 1.3213233612637187,
      "baseline": 17
    },
    {
      "type": "arrow",
      "version": 835,
      "versionNonce": 1915286293,
      "isDeleted": false,
      "id": "D904AMBJU1YvbvQMXyluo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1126.546873152304,
      "y": 442.58712580573655,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 118.78773358450212,
      "height": 0.33064816023818366,
      "seed": 1187727819,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "XUyFnqAl7kQAnMAoB7Cgx",
        "focus": -0.009650513305614256,
        "gap": 1.4886753513908815
      },
      "endBinding": {
        "elementId": "ymMTsrfERD1IWhRqaTh_I",
        "focus": -0.14572391577235705,
        "gap": 15.950740037861578
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          118.78773358450212,
          0.33064816023818366
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 906,
      "versionNonce": 1466119355,
      "isDeleted": false,
      "id": "y5QaZJpFTfZABVw2mHLpE",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 429.6664767795138,
      "y": 461.41041395399304,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 114.10871875077035,
      "height": 118.76116853994468,
      "seed": 1524581995,
      "groupIds": [
        "nGNW7tJf4LAc_8aa1gg6P"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "Iju4jXbzOeX44JCepZN3j",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 3201,
      "versionNonce": 723765365,
      "isDeleted": false,
      "id": "v_UF3QajlZu6wJg9yMLih",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 449.4691219935195,
      "y": 511.23981160653864,
      "strokeColor": "#000000",
      "backgroundColor": "#00e676",
      "width": 71.91666412353516,
      "height": 25.999999999999996,
      "seed": 396443915,
      "groupIds": [
        "nGNW7tJf4LAc_8aa1gg6P"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "fontSize": 19.322657964237056,
      "fontFamily": 1,
      "text": "txtai.js",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "txtai.js",
      "lineHeight": 1.3455705756486278,
      "baseline": 18
    },
    {
      "type": "rectangle",
      "version": 1005,
      "versionNonce": 1613948251,
      "isDeleted": false,
      "id": "bOQD-uSjLkWjJgytmZ4oa",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 431.6664767795138,
      "y": 619.410413953993,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 110.66704644097234,
      "height": 115.17917209201391,
      "seed": 347667589,
      "groupIds": [
        "efl0w5KGoAkFRTYqTi7-b"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "QyCYDQ1akZks7nob5gQcb",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 3264,
      "versionNonce": 484241877,
      "isDeleted": false,
      "id": "XLzbZJT2I8ZRyPvDGlp6q",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 448.63655966829197,
      "y": 667.7697356314092,
      "strokeColor": "#000000",
      "backgroundColor": "#ff7043",
      "width": 76.5,
      "height": 26,
      "seed": 869344229,
      "groupIds": [
        "efl0w5KGoAkFRTYqTi7-b"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763582,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "txtai.rs",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "txtai.rs",
      "lineHeight": 1.3,
      "baseline": 18
    },
    {
      "type": "rectangle",
      "version": 1804,
      "versionNonce": 1627619835,
      "isDeleted": false,
      "id": "NDWbJgM53uVck-LSQzD-0",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 428.6664767795138,
      "y": 141.41041395399304,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 110.66704644097234,
      "height": 115.17917209201391,
      "seed": 1934943141,
      "groupIds": [
        "6yaFhpagshturM5EK_Hay"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "095ylTvsupuqCzyoolf44",
          "type": "arrow"
        }
      ],
      "updated": 1688312763582,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 3123,
      "versionNonce": 1147793205,
      "isDeleted": false,
      "id": "DnwBnStWE5iZA-9cemLzh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 445.63655966829197,
      "y": 189.76973563140916,
      "strokeColor": "#000000",
      "backgroundColor": "#ffeb3b",
      "width": 78.13333129882812,
      "height": 52,
      "seed": 547563269,
      "groupIds": [
        "6yaFhpagshturM5EK_Hay"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "txtai.go\n",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "txtai.go\n",
      "lineHeight": 1.3,
      "baseline": 44
    },
    {
      "type": "ellipse",
      "version": 1434,
      "versionNonce": 47680155,
      "isDeleted": false,
      "id": "zt72wyfizv17KOzo8EnhU",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 748.2452959985687,
      "y": 186.2837080711298,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 27.50940800286237,
      "height": 23.432583857740315,
      "seed": 1191665131,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "wtQWWSoRs-w6Dtmax-Hb5",
          "type": "arrow"
        },
        {
          "id": "095ylTvsupuqCzyoolf44",
          "type": "arrow"
        }
      ],
      "updated": 1688312763583,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 198,
      "versionNonce": 1613774997,
      "isDeleted": false,
      "id": "l2-_RHbfroK1-yNNP9pWJ",
      "fillStyle": "cross-hatch",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 968.0163994686301,
      "y": 373.9539804419734,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#4c6ef5",
      "width": 186.00110275930913,
      "height": 179.95398042998312,
      "seed": 767528715,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -1.0163994686301048,
          -179.9539804299831
        ],
        [
          -186.0011027593091,
          -179.21342907018897
        ]
      ]
    },
    {
      "type": "line",
      "version": 251,
      "versionNonce": 711032635,
      "isDeleted": false,
      "id": "7sDk3itWmD2GE1P-EYplQ",
      "fillStyle": "cross-hatch",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 973.9999999999992,
      "y": 513.9999999998802,
      "strokeColor": "#ff7043",
      "backgroundColor": "#4c6ef5",
      "width": 197.9999999999992,
      "height": 166.00000000011983,
      "seed": 965983243,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -1.010204081632649,
          166.00000000011983
        ],
        [
          -197.9999999999992,
          166.00000000011983
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 121,
      "versionNonce": 1172661749,
      "isDeleted": false,
      "id": "095ylTvsupuqCzyoolf44",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 545.2882783422247,
      "y": 201.14030329253615,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ff7043",
      "width": 198.68164012474733,
      "height": 0.2787977820074161,
      "seed": 4899205,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "NDWbJgM53uVck-LSQzD-0",
        "focus": 0.038606073098519975,
        "gap": 5.954755121738572
      },
      "endBinding": {
        "elementId": "zt72wyfizv17KOzo8EnhU",
        "focus": -0.242073274518529,
        "gap": 4.559216593972247
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          198.68164012474733,
          -0.2787977820074161
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 67,
      "versionNonce": 33808347,
      "isDeleted": false,
      "id": "_UxQXW5nkuKOXtaavoJ3q",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 545.2448106156662,
      "y": 362.4655547515135,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#ff7043",
      "width": 195.75519089085594,
      "height": 1.5344470050185919,
      "seed": 180453803,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "np05zgjgJ1cvT_UcHRO4Q",
        "focus": 0.08624654954364566,
        "gap": 4.130193197765266
      },
      "endBinding": {
        "elementId": "OwUTZKSXPIR8EHoAXUrAc",
        "focus": 0.07905353643606809,
        "gap": 7.545863589033182
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          195.75519089085594,
          1.5344470050185919
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 146,
      "versionNonce": 2022629205,
      "isDeleted": false,
      "id": "Iju4jXbzOeX44JCepZN3j",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 547.8284140902045,
      "y": 520.9148281821533,
      "strokeColor": "#00e676",
      "backgroundColor": "#ff7043",
      "width": 195.16671561089288,
      "height": 3.1847497390683657,
      "seed": 1277347109,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "y5QaZJpFTfZABVw2mHLpE",
        "focus": -0.01448029053565506,
        "gap": 4.053218559920424
      },
      "endBinding": {
        "elementId": "qfK-ulSZh44Bl6KC9Ii6F",
        "focus": 0.03022905067490028,
        "gap": 4.0079533720564715
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          195.16671561089288,
          3.1847497390683657
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 551,
      "versionNonce": 1137781883,
      "isDeleted": false,
      "id": "QyCYDQ1akZks7nob5gQcb",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 548.0000000000001,
      "y": 679.9183352218894,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 190.09617369448893,
      "height": 1.4298124428980827,
      "seed": 1602091877,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "bOQD-uSjLkWjJgytmZ4oa",
        "focus": 0.05822088981723922,
        "gap": 5.666476779514028
      },
      "endBinding": {
        "elementId": "xcu0cSh50RePPTdayuKwa",
        "focus": -0.14162811640911155,
        "gap": 6.9174857452546
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          190.09617369448893,
          -1.4298124428980827
        ]
      ]
    },
    {
      "type": "line",
      "version": 139,
      "versionNonce": 2122460341,
      "isDeleted": false,
      "id": "HKLfE2F8l78a_hZEKhmnY",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 956,
      "y": 497,
      "strokeColor": "#00e676",
      "backgroundColor": "#ced4da",
      "width": 180,
      "height": 29,
      "seed": 597910187,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312763583,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -45,
          0
        ],
        [
          -44,
          29
        ],
        [
          -180,
          29
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/architecture.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 4858,
      "versionNonce": 83494474,
      "isDeleted": false,
      "id": "1iE4X4trkcbtJpCDC5ajY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 27.525089187647836,
      "y": -558.1200877100962,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 227.65938859429855,
      "height": 57.830603028426594,
      "seed": 1321467630,
      "groupIds": [
        "MnVd9k06uFuFRzqXmnEvR",
        "NFKrTdLl_zy4zOvDGWVBD"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "id": "qROQdhT3FWGzgS0JOQXlg",
          "type": "arrow"
        }
      ],
      "updated": 1691411991785,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 5224,
      "versionNonce": 1682430806,
      "isDeleted": false,
      "id": "ZiUda1q4lvcNM9G2Or_d7",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 39.103253497881326,
      "y": -548.6024245205953,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#343a40",
      "width": 210.71665954589844,
      "height": 40,
      "seed": 2050033394,
      "groupIds": [
        "NFKrTdLl_zy4zOvDGWVBD"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "qROQdhT3FWGzgS0JOQXlg",
          "type": "arrow"
        }
      ],
      "updated": 1691411991785,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Semantic, Keyword, Hybrid\n- Search with SQL",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Semantic, Keyword, Hybrid\n- Search with SQL",
      "lineHeight": 1.25,
      "baseline": 35
    },
    {
      "type": "rectangle",
      "version": 2527,
      "versionNonce": 2136048394,
      "isDeleted": false,
      "id": "i9TRMGNTF_ckjPiI7BrdL",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 358.6031681551546,
      "y": -598.4332496538902,
      "strokeColor": "#868e96",
      "backgroundColor": "#e9ecef",
      "width": 348.08489527005264,
      "height": 200.0037909410919,
      "seed": 1867912238,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "id": "viSfAjT-8KRTYvsrunjfn",
          "type": "arrow"
        },
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        },
        {
          "id": "sf5i6bxPYE2ykeh2wWm8m",
          "type": "arrow"
        }
      ],
      "updated": 1691411996548,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1915,
      "versionNonce": 211681430,
      "isDeleted": false,
      "id": "F1r0Plm7hDaeNndIwZAuf",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 368.61999967371725,
      "y": -538.0801566026939,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 160.26930429700263,
      "height": 62.605196991016655,
      "seed": 436869746,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "gw7zjxlDiAjodT1x1_fv_"
        },
        {
          "id": "qROQdhT3FWGzgS0JOQXlg",
          "type": "arrow"
        },
        {
          "id": "viSfAjT-8KRTYvsrunjfn",
          "type": "arrow"
        }
      ],
      "updated": 1691411991785,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1897,
      "versionNonce": 347110346,
      "isDeleted": false,
      "id": "gw7zjxlDiAjodT1x1_fv_",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 406.8879869357439,
      "y": -522.695916613224,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "width": 83.73332977294922,
      "height": 31.836717012076832,
      "seed": 1023300530,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991785,
      "link": null,
      "locked": false,
      "fontSize": 25.469373609661467,
      "fontFamily": 1,
      "text": "Sparse",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "F1r0Plm7hDaeNndIwZAuf",
      "originalText": "Sparse",
      "lineHeight": 1.25,
      "baseline": 22
    },
    {
      "type": "rectangle",
      "version": 2272,
      "versionNonce": 696090070,
      "isDeleted": false,
      "id": "55xWndBPWTiyUuLIS_J8b",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 369.8721036135376,
      "y": -469.46654385239594,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 157.76509641736197,
      "height": 57.59678123173532,
      "seed": 1317384558,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "Qaeh-leNa7YgcPsQbfBcq"
        },
        {
          "id": "qROQdhT3FWGzgS0JOQXlg",
          "type": "arrow"
        },
        {
          "id": "viSfAjT-8KRTYvsrunjfn",
          "type": "arrow"
        },
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691411991785,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2210,
      "versionNonce": 1434612362,
      "isDeleted": false,
      "id": "Qaeh-leNa7YgcPsQbfBcq",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 412.012986935744,
      "y": -456.59111621253714,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "width": 73.48332977294922,
      "height": 31.84592595201767,
      "seed": 1661259762,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991785,
      "link": null,
      "locked": false,
      "fontSize": 25.476740761614135,
      "fontFamily": 1,
      "text": "Dense",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "55xWndBPWTiyUuLIS_J8b",
      "originalText": "Dense",
      "lineHeight": 1.25,
      "baseline": 22
    },
    {
      "type": "rectangle",
      "version": 2356,
      "versionNonce": 268129046,
      "isDeleted": false,
      "id": "KSf88Asx9gne1WsRdsx6-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 535.1498236698217,
      "y": -469.46654385239594,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 156.51299247754164,
      "height": 57.59678123173532,
      "seed": 1016194418,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "7RaoYDeBRuz16CJo7jaVO"
        },
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691411991785,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2343,
      "versionNonce": 2120716618,
      "isDeleted": false,
      "id": "7RaoYDeBRuz16CJo7jaVO",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 576.8063214344714,
      "y": -456.5934785058263,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "width": 73.19999694824219,
      "height": 31.850650538596007,
      "seed": 1307924782,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991785,
      "link": null,
      "locked": false,
      "fontSize": 25.480520430876805,
      "fontFamily": 1,
      "text": "Graph",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "KSf88Asx9gne1WsRdsx6-",
      "originalText": "Graph",
      "lineHeight": 1.25,
      "baseline": 22
    },
    {
      "type": "text",
      "version": 1738,
      "versionNonce": 713850966,
      "isDeleted": false,
      "id": "gAcYCRjD6VZUsDZce-atm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 439.98992424347625,
      "y": -595.9290417742495,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 183.39999389648438,
      "height": 43.823637893711656,
      "seed": 1369566386,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991786,
      "link": null,
      "locked": false,
      "fontSize": 35.058910314969324,
      "fontFamily": 1,
      "text": "Embeddings",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Embeddings",
      "lineHeight": 1.25,
      "baseline": 31
    },
    {
      "type": "rectangle",
      "version": 2094,
      "versionNonce": 704590410,
      "isDeleted": false,
      "id": "UEE-Z2YT9Y5YvVCJefbd0",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 533.8977197300013,
      "y": -535.8280526628736,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 160.26930429700263,
      "height": 60.10098911137599,
      "seed": 1015954930,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "Y1uf_NH7fI6arCSGiICiX"
        },
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691412007529,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2084,
      "versionNonce": 1292240278,
      "isDeleted": false,
      "id": "Y1uf_NH7fI6arCSGiICiX",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 551.0823711155632,
      "y": -521.695916613224,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "width": 125.9000015258789,
      "height": 31.836717012076832,
      "seed": 867545010,
      "groupIds": [
        "cNL6-_r-I9wctWvuw8DqS"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991786,
      "link": null,
      "locked": false,
      "fontSize": 25.469373609661467,
      "fontFamily": 1,
      "text": "Database",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "UEE-Z2YT9Y5YvVCJefbd0",
      "originalText": "Database",
      "lineHeight": 1.25,
      "baseline": 22
    },
    {
      "type": "rectangle",
      "version": 4488,
      "versionNonce": 1920419530,
      "isDeleted": false,
      "id": "uV8iktEBp-zvkGceO0YNj",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 28.400291679657585,
      "y": -482.37846781152234,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 226.65938859429855,
      "height": 53.830603028426594,
      "seed": 511161586,
      "groupIds": [
        "lsnRF8t7e-8bA4k7ZMtQs",
        "XOEGmXNmM0vVhFi_aLf0h"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "id": "viSfAjT-8KRTYvsrunjfn",
          "type": "arrow"
        }
      ],
      "updated": 1691411991786,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 4865,
      "versionNonce": 144499414,
      "isDeleted": false,
      "id": "VU0ZFGV4tB5pR_8AMPehi",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 38.17389738827433,
      "y": -477.32443222094344,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#343a40",
      "width": 190.39999389648438,
      "height": 40,
      "seed": 660096690,
      "groupIds": [
        "XOEGmXNmM0vVhFi_aLf0h"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411991786,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Topics + Relationships\n- Multimodal Indexes",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Topics + Relationships\n- Multimodal Indexes",
      "lineHeight": 1.25,
      "baseline": 35
    },
    {
      "type": "arrow",
      "version": 3140,
      "versionNonce": 1465895050,
      "isDeleted": false,
      "id": "qROQdhT3FWGzgS0JOQXlg",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 357.5459819116054,
      "y": -474.380420640035,
      "strokeColor": "#868e96",
      "backgroundColor": "#e9ecef",
      "width": 101.91733646013165,
      "height": 51.182713481535075,
      "seed": 1412373166,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691412001500,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "55xWndBPWTiyUuLIS_J8b",
        "focus": -0.17593583244727756,
        "gap": 13.269493633705139
      },
      "endBinding": {
        "elementId": "ZiUda1q4lvcNM9G2Or_d7",
        "focus": -0.7240167179756973,
        "gap": 5.808732407693981
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -101.91733646013165,
          -51.182713481535075
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 2221,
      "versionNonce": 297446922,
      "isDeleted": false,
      "id": "viSfAjT-8KRTYvsrunjfn",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 355.69072075221885,
      "y": -473.6934438187176,
      "strokeColor": "#868e96",
      "backgroundColor": "#e9ecef",
      "width": 99.11528692741001,
      "height": 28.941684185339795,
      "seed": 769791342,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691412005709,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "55xWndBPWTiyUuLIS_J8b",
        "focus": 1.1614221413579826,
        "gap": 14.797915501333051
      },
      "endBinding": {
        "elementId": "uV8iktEBp-zvkGceO0YNj",
        "focus": 0.737345008582056,
        "gap": 1.5157535508527076
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -99.11528692741001,
          28.941684185339795
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 5939,
      "versionNonce": 1648005846,
      "isDeleted": false,
      "id": "CJaOip-NXs_WGJ3YnzrzX",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 780.2644916835155,
      "y": -444.3627069906215,
      "strokeColor": "#868e96",
      "backgroundColor": "#e9ecef",
      "width": 71.1891690063859,
      "height": 28.06038115974593,
      "seed": 1923467222,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411901454,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "SKqPLoIsGXgDL5mhOTqLv",
        "focus": -0.634641820715333,
        "gap": 5.063962132776453
      },
      "endBinding": {
        "elementId": "UEE-Z2YT9Y5YvVCJefbd0",
        "focus": -0.06665280707924877,
        "gap": 15.270023644140622
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -71.1891690063859,
          -28.06038115974593
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 3429,
      "versionNonce": 348832458,
      "isDeleted": false,
      "id": "7UwPwHCm_ZiMJb1o0x5A8",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 783.2131987136702,
      "y": -574.1983152922421,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 272.0848952700526,
      "height": 73.0037909410919,
      "seed": 155582550,
      "groupIds": [
        "kgkwY28q5i7W008rJksMR",
        "TqYTD07S3yHeAVJOG3yyi"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [],
      "updated": 1691411901455,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1591,
      "versionNonce": 72296150,
      "isDeleted": false,
      "id": "fFWZLl9F8IwK5QhXlAssA",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 792.6342330724652,
      "y": -569.5895346298003,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#343a40",
      "width": 77.1500015258789,
      "height": 20,
      "seed": 543556438,
      "groupIds": [
        "TqYTD07S3yHeAVJOG3yyi"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691411901455,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": " Prompt >",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": " Prompt >",
      "lineHeight": 1.25,
      "baseline": 15
    },
    {
      "type": "text",
      "version": 1341,
      "versionNonce": 450634262,
      "isDeleted": false,
      "id": "FgFcgIY3s9iRdPKwN4t_r",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 785.1996772966362,
      "y": -548.1590294764883,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#343a40",
      "width": 261.5,
      "height": 41.41205162166277,
      "seed": 423650058,
      "groupIds": [
        "TqYTD07S3yHeAVJOG3yyi"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691412007529,
      "link": null,
      "locked": false,
      "fontSize": 16.56482064866511,
      "fontFamily": 1,
      "text": "   Answer the following question\n   using the context below",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "   Answer the following question\n   using the context below",
      "lineHeight": 1.25,
      "baseline": 35
    },
    {
      "type": "rectangle",
      "version": 3467,
      "versionNonce": 1420811286,
      "isDeleted": false,
      "id": "_8pPaWBfdkvpd8B9PQ7wI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 784.2131987136702,
      "y": -487.1983152922421,
      "strokeColor": "#6741d9",
      "backgroundColor": "#6741d9",
      "width": 272.0848952700526,
      "height": 73.0037909410919,
      "seed": 2066566410,
      "groupIds": [
        "FtTm_MWbxe67f2n84saes",
        "3PAnqhix8z46z6JRJjAO7"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691411901455,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1602,
      "versionNonce": 1943803978,
      "isDeleted": false,
      "id": "apAxY8HSw8w2lfLe1pCXP",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 792.7130504007473,
      "y": -482.33339890257247,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#6741d9",
      "width": 75.73332977294922,
      "height": 20,
      "seed": 1870973578,
      "groupIds": [
        "3PAnqhix8z46z6JRJjAO7"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691411901455,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": " Search >",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": " Search >",
      "lineHeight": 1.25,
      "baseline": 15
    },
    {
      "type": "text",
      "version": 1069,
      "versionNonce": 1506273622,
      "isDeleted": false,
      "id": "SKqPLoIsGXgDL5mhOTqLv",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 785.3284538162922,
      "y": -458.46265045437633,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#6741d9",
      "width": 251.21665954589844,
      "height": 41.82578128069407,
      "seed": 1533237462,
      "groupIds": [
        "3PAnqhix8z46z6JRJjAO7"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "CJaOip-NXs_WGJ3YnzrzX",
          "type": "arrow"
        }
      ],
      "updated": 1691411901455,
      "link": null,
      "locked": false,
      "fontSize": 16.730312512277628,
      "fontFamily": 1,
      "text": "   SELECT ... FROM txtai\n   WHERE SIMILAR('question')",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "   SELECT ... FROM txtai\n   WHERE SIMILAR('question')",
      "lineHeight": 1.25,
      "baseline": 35
    },
    {
      "type": "arrow",
      "version": 5999,
      "versionNonce": 1002058634,
      "isDeleted": false,
      "id": "sf5i6bxPYE2ykeh2wWm8m",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 780.8522503409429,
      "y": -537.8502934180265,
      "strokeColor": "#868e96",
      "backgroundColor": "#e9ecef",
      "width": 72.32385186561339,
      "height": 64.0647548382936,
      "seed": 1598499018,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691412007529,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -72.32385186561339,
          64.0647548382936
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/cloud.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 3062,
      "versionNonce": 456539957,
      "isDeleted": false,
      "id": "Ufh5VUA3qmvJowuFyEWz4",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 570.7515859513394,
      "y": 59.768341009859626,
      "strokeColor": "#d0d9dd",
      "backgroundColor": "transparent",
      "width": 719.330217768941,
      "height": 100.49478848195537,
      "seed": 2109972213,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312603126,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1798,
      "versionNonce": 1560857173,
      "isDeleted": false,
      "id": "NDWbJgM53uVck-LSQzD-0",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 740.6664767795138,
      "y": 67.92614920483035,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 85,
      "height": 84.17917209201391,
      "seed": 1782446165,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "0VztU7lb7T9I5zoL2LjLt"
        }
      ],
      "updated": 1688312605190,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 385,
      "versionNonce": 1280754907,
      "isDeleted": false,
      "id": "0VztU7lb7T9I5zoL2LjLt",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 745.6664767795138,
      "y": 99.51573525083731,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 75,
      "height": 21,
      "seed": 2095457717,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312605190,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "Container",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "NDWbJgM53uVck-LSQzD-0",
      "originalText": "Container",
      "lineHeight": 1.3125,
      "baseline": 15
    },
    {
      "type": "rectangle",
      "version": 1874,
      "versionNonce": 847689653,
      "isDeleted": false,
      "id": "rr3YJsAyrrPqZbZ4qXh5x",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1175.874140249897,
      "y": 67.92614920483035,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 85,
      "height": 84.17917209201391,
      "seed": 1491183381,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "9NBdG6TyrBqjuDKvzAZXN",
          "type": "text"
        }
      ],
      "updated": 1688312605190,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 456,
      "versionNonce": 1820291451,
      "isDeleted": false,
      "id": "9NBdG6TyrBqjuDKvzAZXN",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1180.874140249897,
      "y": 99.51573525083731,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 75,
      "height": 21,
      "seed": 547748981,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312605190,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "Container",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "rr3YJsAyrrPqZbZ4qXh5x",
      "originalText": "Container",
      "lineHeight": 1.3125,
      "baseline": 15
    },
    {
      "type": "rectangle",
      "version": 1864,
      "versionNonce": 1889532181,
      "isDeleted": false,
      "id": "ji9UBE590YJ5e8DKUKbPG",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 598.874140249897,
      "y": 67.92614920483035,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 85,
      "height": 84.17917209201391,
      "seed": 200844757,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "2fdy7dRwF7ZVaYPrb1Q7p",
          "type": "text"
        }
      ],
      "updated": 1688312605190,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 446,
      "versionNonce": 673810971,
      "isDeleted": false,
      "id": "2fdy7dRwF7ZVaYPrb1Q7p",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 603.874140249897,
      "y": 99.51573525083731,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 75,
      "height": 21,
      "seed": 1039082293,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312605190,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "Container",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "ji9UBE590YJ5e8DKUKbPG",
      "originalText": "Container",
      "lineHeight": 1.3125,
      "baseline": 15
    },
    {
      "type": "rectangle",
      "version": 1805,
      "versionNonce": 281677429,
      "isDeleted": false,
      "id": "Kod3erHdgkFtJcsgqHeQQ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 889.874140249897,
      "y": 67.92614920483035,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 85,
      "height": 84.17917209201391,
      "seed": 1525609621,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "vA5QfvPySVZE9WnP6d7dp",
          "type": "text"
        }
      ],
      "updated": 1688312605190,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 390,
      "versionNonce": 1682411195,
      "isDeleted": false,
      "id": "vA5QfvPySVZE9WnP6d7dp",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 894.874140249897,
      "y": 99.51573525083731,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 75,
      "height": 21,
      "seed": 1588435445,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312605190,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "Container",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Kod3erHdgkFtJcsgqHeQQ",
      "originalText": "Container",
      "lineHeight": 1.3125,
      "baseline": 15
    },
    {
      "type": "rectangle",
      "version": 1916,
      "versionNonce": 1103787989,
      "isDeleted": false,
      "id": "8p_AIsxC6UbaWESkrwl4g",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1035.874140249897,
      "y": 67.92614920483035,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 85,
      "height": 84.17917209201391,
      "seed": 842044245,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "AceuLTPujneYjRp_njhIl",
          "type": "text"
        }
      ],
      "updated": 1688312605190,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 499,
      "versionNonce": 1132199771,
      "isDeleted": false,
      "id": "AceuLTPujneYjRp_njhIl",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1041.907473074604,
      "y": 99.51573525083731,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 72.93333435058594,
      "height": 21,
      "seed": 30236853,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312605190,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "Container",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "8p_AIsxC6UbaWESkrwl4g",
      "originalText": "Container",
      "lineHeight": 1.3125,
      "baseline": 15
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/embeddings.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 824,
      "versionNonce": 1352316119,
      "isDeleted": false,
      "id": "U2NgEIEiFpAlwmv5Xnyzr",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 50,
      "angle": 0,
      "x": 557.5,
      "y": 267.30499999999995,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 995.3286713286714,
      "height": 339.0000000000001,
      "seed": 1946478225,
      "groupIds": [
        "C_65R9XVeMQED2nMfIW2D"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788613149,
      "link": "",
      "locked": false
    },
    {
      "type": "text",
      "version": 607,
      "versionNonce": 590931993,
      "isDeleted": false,
      "id": "fbSO8bnZmAdvIXZxBAtHY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 572.7237762237763,
      "y": 281.50080419580416,
      "strokeColor": "#000",
      "backgroundColor": "#03a9f4",
      "width": 967,
      "height": 312,
      "seed": 1586673137,
      "groupIds": [
        "C_65R9XVeMQED2nMfIW2D"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788613149,
      "link": null,
      "locked": false,
      "fontSize": 20.27972027972028,
      "fontFamily": 1,
      "text": "Query                  Best Match\n----------------------------------------------------\nfeel good story        Maine man wins $1M from $25 lottery ticket\nclimate change         Canada's last fully intact ice shelf has suddenly collapsed, forming a \n                       Manhattan-sized iceberg\npublic health story     US tops 5 million confirmed virus cases\nwar                    Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nwildlife                 The National Park Service warns against sacrificing slower friends in a\n                        bear attack\nasia                   Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nlucky                   Maine man wins $1M from $25 lottery ticket\ndishonest junk          Make huge profits without work, earn up to $100,000 a day",
      "baseline": 304,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Query                  Best Match\n----------------------------------------------------\nfeel good story        Maine man wins $1M from $25 lottery ticket\nclimate change         Canada's last fully intact ice shelf has suddenly collapsed, forming a \n                       Manhattan-sized iceberg\npublic health story     US tops 5 million confirmed virus cases\nwar                    Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nwildlife                 The National Park Service warns against sacrificing slower friends in a\n                        bear attack\nasia                   Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nlucky                   Maine man wins $1M from $25 lottery ticket\ndishonest junk          Make huge profits without work, earn up to $100,000 a day"
    },
    {
      "type": "rectangle",
      "version": 990,
      "versionNonce": 2000421367,
      "isDeleted": false,
      "id": "UO6MS3wSDu7yg2421__LI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 935,
      "y": 109,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 214,
      "height": 49,
      "seed": 1629565989,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "8sp7H8ijWBlh6aMgZ0XTP"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        },
        {
          "id": "SJ0F0Y81z9hir5qQWAJjk",
          "type": "arrow"
        }
      ],
      "updated": 1673788613149,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1649,
      "versionNonce": 1978562009,
      "isDeleted": false,
      "id": "qYd3q0Vjks7VOHUC9RR51",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 551,
      "y": 109.5,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 219,
      "height": 52,
      "seed": 1441952427,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "WPeWn6N4rCHf0jY16N9Ge"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        }
      ],
      "updated": 1673788613149,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1336,
      "versionNonce": 1274169655,
      "isDeleted": false,
      "id": "WPeWn6N4rCHf0jY16N9Ge",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 597.5,
      "y": 117.5,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 126,
      "height": 36,
      "seed": 870516459,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788618421,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Vectorize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "qYd3q0Vjks7VOHUC9RR51",
      "originalText": "Vectorize"
    },
    {
      "type": "rectangle",
      "version": 1236,
      "versionNonce": 368709975,
      "isDeleted": false,
      "id": "5VuUdI_BsJ5pyE1nTqJUI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1334,
      "y": 110,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 218,
      "height": 49,
      "seed": 1044404613,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [
        {
          "id": "bJJ9SGsJsvT071qBBH0w5",
          "type": "text"
        },
        {
          "id": "SJ0F0Y81z9hir5qQWAJjk",
          "type": "arrow"
        }
      ],
      "updated": 1673788613149,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1420,
      "versionNonce": 1511737785,
      "isDeleted": false,
      "id": "bJJ9SGsJsvT071qBBH0w5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1397,
      "y": 116.5,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 92,
      "height": 36,
      "seed": 128953675,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788618421,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Search",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "5VuUdI_BsJ5pyE1nTqJUI",
      "originalText": "Search"
    },
    {
      "type": "text",
      "version": 981,
      "versionNonce": 1600260695,
      "isDeleted": false,
      "id": "8sp7H8ijWBlh6aMgZ0XTP",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1004,
      "y": 115.5,
      "strokeColor": "#000",
      "backgroundColor": "transparent",
      "width": 76,
      "height": 36,
      "seed": 1854823263,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788618422,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Index",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "UO6MS3wSDu7yg2421__LI",
      "originalText": "Index"
    },
    {
      "type": "text",
      "version": 420,
      "versionNonce": 1275651991,
      "isDeleted": false,
      "id": "jWJpSXHkTCzRTCA4tbAgv",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 550.5,
      "y": 189.30499999999995,
      "strokeColor": "#000",
      "backgroundColor": "#03a9f4",
      "width": 296,
      "height": 42,
      "seed": 1241563487,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788613150,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Transform input into numbers\n- Similar concepts have similar values",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Transform input into numbers\n- Similar concepts have similar values"
    },
    {
      "type": "text",
      "version": 419,
      "versionNonce": 51134809,
      "isDeleted": false,
      "id": "qEnmXs0P_MQE8r4c4OWGh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 933.5,
      "y": 188.20749999999992,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 230,
      "height": 42,
      "seed": 1038536465,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788613150,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Save vectors\n- Store content with vectors",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Save vectors\n- Store content with vectors"
    },
    {
      "type": "text",
      "version": 529,
      "versionNonce": 869653687,
      "isDeleted": false,
      "id": "1q8bzjK8lnKUZj8_A9v7D",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1245,
      "y": 191.20749999999992,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 333,
      "height": 42,
      "seed": 304472945,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": null,
      "boundElements": [],
      "updated": 1673788613150,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Find similar vectors with cosine similarity\n- Add rule-based filters using content",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Find similar vectors with cosine similarity\n- Add rule-based filters using content"
    },
    {
      "type": "arrow",
      "version": 1935,
      "versionNonce": 20011767,
      "isDeleted": false,
      "id": "Qzp41i_jzQIBlAB_qFKFH",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 772.5,
      "y": 131.8470411964629,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 158.1310513485223,
      "height": 0.5692601572380909,
      "seed": 660786897,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673788613309,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "qYd3q0Vjks7VOHUC9RR51",
        "focus": -0.15367587596362536,
        "gap": 2.5
      },
      "endBinding": {
        "elementId": "UO6MS3wSDu7yg2421__LI",
        "focus": 0.027437144815141,
        "gap": 4.3689486514776945
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          158.1310513485223,
          0.5692601572380909
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 2084,
      "versionNonce": 1180230679,
      "isDeleted": false,
      "id": "SJ0F0Y81z9hir5qQWAJjk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1151.5,
      "y": 134.67907617019108,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 181.5,
      "height": 1.5898915058209013,
      "seed": 899541905,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673788613309,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "UO6MS3wSDu7yg2421__LI",
        "focus": 0.08406032225724415,
        "gap": 2.5
      },
      "endBinding": {
        "elementId": "5VuUdI_BsJ5pyE1nTqJUI",
        "focus": 0.09327847520504394,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          181.5,
          -1.5898915058209013
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/examples.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 658,
      "versionNonce": 56883131,
      "isDeleted": false,
      "id": "28knmXv7UWxhQgfO1Ap_e",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 843.2544325440515,
      "y": 565.0365494150617,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 131.4290956134916,
      "height": 19.163298353951955,
      "seed": 333697800,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176184,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 759,
      "versionNonce": 1321514357,
      "isDeleted": false,
      "id": "NWB9SsDdfRnKFM7WSMiDD",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 843.094785904982,
      "y": 596.6192561492705,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 236.33086957943124,
      "height": 8.353548393013705,
      "seed": 1799143800,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176184,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 873,
      "versionNonce": 1965246555,
      "isDeleted": false,
      "id": "o8mRXU1Pv0Lht3Kvw3LjM",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 843.9010630546302,
      "y": 628.3064112463869,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 172.46335772731075,
      "height": 10.760364257767161,
      "seed": 84161032,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176184,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 852,
      "versionNonce": 1473789653,
      "isDeleted": false,
      "id": "7XXBg9rngZPBd8BY3k-Dk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 50,
      "angle": 0,
      "x": 820.9965701219508,
      "y": 409.40769144764795,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 772.0068597560986,
      "height": 326.7582126998559,
      "seed": 1645597304,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 820,
      "versionNonce": 1609532667,
      "isDeleted": false,
      "id": "1djoflJbPq1iZYxeG6vvs",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 843.094785904982,
      "y": 612.826386710884,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 236.33086957943124,
      "height": 8.353548393013705,
      "seed": 1354805512,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1047,
      "versionNonce": 2087951413,
      "isDeleted": false,
      "id": "61SvR173kGBlNiM2hYx-i",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 843.933979602177,
      "y": 666.8116705347015,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 141.1603070419808,
      "height": 35.19095464139164,
      "seed": 497337208,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 1
      },
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1179,
      "versionNonce": 1800088987,
      "isDeleted": false,
      "id": "KweZDYL4pV266i29UQo2O",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 873.1942926679951,
      "y": 677.4126832647471,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 63,
      "height": 17,
      "seed": 244666376,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.613804582094819,
      "fontFamily": 1,
      "text": "Show more",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Show more",
      "lineHeight": 1.3477297741025214,
      "baseline": 12
    },
    {
      "type": "line",
      "version": 1055,
      "versionNonce": 612126101,
      "isDeleted": false,
      "id": "6UHr1F9sbQjfc3Pn3AUC9",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.71238898038469,
      "x": 955.8234234136398,
      "y": 681.4576006557272,
      "strokeColor": "#495057",
      "backgroundColor": "transparent",
      "width": 14.84733876817561,
      "height": 7.91335492900009,
      "seed": 1278055544,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          7.737815540443359,
          7.91335492900009
        ],
        [
          14.84733876817561,
          0.6726390980766681
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 1593,
      "versionNonce": 711508539,
      "isDeleted": false,
      "id": "pQ7Helj-RkhLonwDn4uxq",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 878.2965111534407,
      "y": 493.103306996402,
      "strokeColor": "#000000",
      "backgroundColor": "#ff7043",
      "width": 12.482770172023958,
      "height": 31.20692543005986,
      "seed": 1223270520,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 806,
      "versionNonce": 522703605,
      "isDeleted": false,
      "id": "bI-A-gEMD-yvyhVvuajAH",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 865.549721104171,
      "y": 442.4040920739574,
      "strokeColor": "#000000",
      "backgroundColor": "#12b886",
      "width": 0,
      "height": 87.3793912041677,
      "seed": 1280885512,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          87.3793912041677
        ]
      ]
    },
    {
      "type": "line",
      "version": 816,
      "versionNonce": 640336603,
      "isDeleted": false,
      "id": "X3L53b-aDRBM9J_i7xfIb",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 865.3142359887239,
      "y": 530.6645290499368,
      "strokeColor": "#000000",
      "backgroundColor": "#12b886",
      "width": 181.0001674943474,
      "height": 0,
      "seed": 1422840184,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          181.0001674943474,
          0
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 1768,
      "versionNonce": 956017749,
      "isDeleted": false,
      "id": "Fk7rL2dsob9lBYBWtqIaE",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 909.5034365835013,
      "y": 461.80381850437016,
      "strokeColor": "#000000",
      "backgroundColor": "#ffeb3b",
      "width": 12.48277017202394,
      "height": 62.41385086011974,
      "seed": 1096491528,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1638,
      "versionNonce": 656650107,
      "isDeleted": false,
      "id": "V3UV1tCb1upEwCUx2NF_U",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 940.7103620135604,
      "y": 480.5279737624063,
      "strokeColor": "#000000",
      "backgroundColor": "#03a9f4",
      "width": 12.482770172023953,
      "height": 43.68969560208382,
      "seed": 1860559480,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1650,
      "versionNonce": 793292539,
      "isDeleted": false,
      "id": "H9TFct1zEuli3oFZ0J6Pi",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 971.9172874436208,
      "y": 474.28658867639376,
      "strokeColor": "#000000",
      "backgroundColor": "#7950f2",
      "width": 12.482770172023953,
      "height": 49.9310806880958,
      "seed": 1153016072,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314179109,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1806,
      "versionNonce": 1066797083,
      "isDeleted": false,
      "id": "Yvl5ZFMXGqZoKBjooHuuL",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1003.1242128736812,
      "y": 468.04520359038156,
      "strokeColor": "#000000",
      "backgroundColor": "#00e676",
      "width": 12.48277017202394,
      "height": 56.172465774107785,
      "seed": 905106296,
      "groupIds": [
        "0ClD8yewsWOTVzkWq2mkc"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 820,
      "versionNonce": 851598101,
      "isDeleted": false,
      "id": "k2RjGkR4842FWeQqwZNqP",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1140.5405571864028,
      "y": 441.0898168120742,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 134,
      "height": 46,
      "seed": 351902216,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 36,
      "fontFamily": 1,
      "text": "Results",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Results",
      "lineHeight": 1.2777777777777777,
      "baseline": 32
    },
    {
      "type": "line",
      "version": 387,
      "versionNonce": 359943355,
      "isDeleted": false,
      "id": "9BG_CdPIU12PIVg57JFjb",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1142.5011920133527,
      "y": 524.9312390735229,
      "strokeColor": "#000000",
      "backgroundColor": "#12b886",
      "width": 387.9702360583586,
      "height": 0.8663462515423723,
      "seed": 656392968,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          387.9702360583586,
          -0.8663462515423723
        ]
      ]
    },
    {
      "type": "text",
      "version": 959,
      "versionNonce": 1682976885,
      "isDeleted": false,
      "id": "ooEpA700FaTXoHICTApB5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1147.668463887201,
      "y": 537.9228771043508,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 10.29019548584314,
      "height": 17.493332325933352,
      "seed": 771215736,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972927,
      "fontFamily": 1,
      "text": "0",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "0",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1031,
      "versionNonce": 1818295643,
      "isDeleted": false,
      "id": "n9FAYaIZeMXggjqKvdYxs",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1147.927904475898,
      "y": 577.8396648224748,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 4.116078194337256,
      "height": 17.493332325933352,
      "seed": 725754376,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972932,
      "fontFamily": 1,
      "text": "1",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "1",
      "lineHeight": 1.3556693953949972,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1107,
      "versionNonce": 1144805845,
      "isDeleted": false,
      "id": "fQfYp0ljtX1ESRr5wPM9b",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1147.927904475895,
      "y": 625.2941282157817,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 10.29019548584314,
      "height": 17.493332325933352,
      "seed": 988734072,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "2",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "2",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1015,
      "versionNonce": 1873713659,
      "isDeleted": false,
      "id": "6-kghWfO7GdeQUDWs_zBe",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1146.639444338615,
      "y": 501.6706877015755,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 18.522351874517668,
      "height": 17.493332325933352,
      "seed": 1505796360,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972927,
      "fontFamily": 1,
      "text": "ID",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "ID",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1024,
      "versionNonce": 463230773,
      "isDeleted": false,
      "id": "UMowu2HWCm_mgorPKfHhX",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1246.2049260479205,
      "y": 537.9228771043508,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 54.53803607496866,
      "height": 17.493332325933352,
      "seed": 417059704,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "_____",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "_____",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1093,
      "versionNonce": 1012900507,
      "isDeleted": false,
      "id": "1_YJ1fjNPFhQdtKIp-DSI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1246.464366636618,
      "y": 577.8396648224748,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 54.53803607496866,
      "height": 17.493332325933352,
      "seed": 1377255432,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176185,
      "link": null,
      "locked": false,
      "fontSize": 12.90383362297293,
      "fontFamily": 1,
      "text": "_____",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "_____",
      "lineHeight": 1.3556693953949974,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1168,
      "versionNonce": 1960159381,
      "isDeleted": false,
      "id": "O5wbweMweVsQWwdNABtm7",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1246.4643666366173,
      "y": 625.2941282157817,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 43.2188210405412,
      "height": 17.493332325933352,
      "seed": 1845666936,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "____",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "____",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1068,
      "versionNonce": 723949371,
      "isDeleted": false,
      "id": "v3I-eUd4WMJ8VzvC_7rB_",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1246.204926047921,
      "y": 501.6706877015755,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 32.92862555469805,
      "height": 17.493332325933352,
      "seed": 1682046728,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972927,
      "fontFamily": 1,
      "text": "Text",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Text",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1119,
      "versionNonce": 1906167285,
      "isDeleted": false,
      "id": "sdGrFMITxSJsejpV0zIFY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1354.7619764110675,
      "y": 500.94934123472103,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 34.986664651866704,
      "height": 17.493332325933352,
      "seed": 161608312,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972929,
      "fontFamily": 1,
      "text": "Score",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Score",
      "lineHeight": 1.3556693953949976,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1042,
      "versionNonce": 1516632027,
      "isDeleted": false,
      "id": "5tPTHdBAi9oW2Cme01uST",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1356.7782004146586,
      "y": 539.0230645323406,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 34.986664651866704,
      "height": 17.493332325933352,
      "seed": 1702057736,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972929,
      "fontFamily": 1,
      "text": "0.851",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "0.851",
      "lineHeight": 1.3556693953949976,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1117,
      "versionNonce": 1627014997,
      "isDeleted": false,
      "id": "VkvcQQqVWAFSBPXeb-631",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1355.2070363053651,
      "y": 576.0677682813757,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 40.131762394788254,
      "height": 17.493332325933352,
      "seed": 1116261384,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972929,
      "fontFamily": 1,
      "text": "0.834",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "0.834",
      "lineHeight": 1.3556693953949976,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1189,
      "versionNonce": 2016429179,
      "isDeleted": false,
      "id": "r5V4P6WY1YTnZqynbGcPW",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1354.5812615574996,
      "y": 623.4026675162545,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 38.07372329761963,
      "height": 17.493332325933352,
      "seed": 754228488,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176186,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972924,
      "fontFamily": 1,
      "text": "0.653",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "0.653",
      "lineHeight": 1.355669395394998,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1164,
      "versionNonce": 356553909,
      "isDeleted": false,
      "id": "tplkVB19dZGWPH04eoN3-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1452.2813974443563,
      "y": 501.2965906346964,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 36.01568420045099,
      "height": 17.493332325933352,
      "seed": 633774600,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176187,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972922,
      "fontFamily": 1,
      "text": "Data",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Data",
      "lineHeight": 1.3556693953949983,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1091,
      "versionNonce": 1279786267,
      "isDeleted": false,
      "id": "-O_818hE0e37gLO8X85ED",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1456.3974756386945,
      "y": 536.2832552865634,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 57.6250947207216,
      "height": 17.493332325933352,
      "seed": 1316501624,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176187,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "{____}",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "{____}",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1118,
      "versionNonce": 2048275989,
      "isDeleted": false,
      "id": "q_5ogcScUemL3g6XJyETb",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1458.4555147358635,
      "y": 569.2118808412615,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 57.6250947207216,
      "height": 17.493332325933352,
      "seed": 649694840,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176187,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "{____}",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "{____}",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    },
    {
      "type": "text",
      "version": 1111,
      "versionNonce": 35627451,
      "isDeleted": false,
      "id": "hePUy_nbNvIpf6_6PBvbq",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1458.4555147358635,
      "y": 618.6048191733089,
      "strokeColor": "#000000",
      "backgroundColor": "transparent",
      "width": 57.6250947207216,
      "height": 17.493332325933352,
      "seed": 1966475272,
      "groupIds": [
        "RaIXn6GkWfnJt6NeXQ4tC"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688314176187,
      "link": null,
      "locked": false,
      "fontSize": 12.903833622972925,
      "fontFamily": 1,
      "text": "{____}",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "{____}",
      "lineHeight": 1.3556693953949979,
      "baseline": 12
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/faq.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "ellipse",
      "version": 236,
      "versionNonce": 266746677,
      "isDeleted": false,
      "id": "pLSV--mc1HnQu2oyARr4o",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 593.4166948358098,
      "y": -409.48426474916266,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 100,
      "height": 92,
      "seed": 513066133,
      "groupIds": [
        "agHSoiBea3xOkqCmOPa91",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 281,
      "versionNonce": 1964375707,
      "isDeleted": false,
      "id": "n2qByon4fJyq2-FZxQqM0",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 629.5250284234563,
      "y": -397.41226924952065,
      "strokeColor": "#ff7043",
      "backgroundColor": "#2e303e",
      "width": 29.78333282470704,
      "height": 79.856009000716,
      "seed": 852571451,
      "groupIds": [
        "agHSoiBea3xOkqCmOPa91",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false,
      "fontSize": 63.88480720057278,
      "fontFamily": 1,
      "text": "?",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "?",
      "lineHeight": 1.25,
      "baseline": 56
    },
    {
      "type": "ellipse",
      "version": 288,
      "versionNonce": 239275157,
      "isDeleted": false,
      "id": "QIRNCrcWupjspegQn6VDV",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 744.4166948358098,
      "y": -409.48426474916266,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 100,
      "height": 92,
      "seed": 1498649243,
      "groupIds": [
        "yonc0SR8jSwQLwuVvb9hG",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 372,
      "versionNonce": 1982180597,
      "isDeleted": false,
      "id": "a-CPMQS9ApW4f7p9FjS8m",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 780.5250284234563,
      "y": -397.41226924952065,
      "strokeColor": "#ffd43b",
      "backgroundColor": "#ffeb3b",
      "width": 29.78333282470704,
      "height": 79.856009000716,
      "seed": 1679591227,
      "groupIds": [
        "yonc0SR8jSwQLwuVvb9hG",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688313209123,
      "link": null,
      "locked": false,
      "fontSize": 63.88480720057278,
      "fontFamily": 1,
      "text": "?",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "?",
      "lineHeight": 1.25,
      "baseline": 56
    },
    {
      "type": "ellipse",
      "version": 326,
      "versionNonce": 1069462005,
      "isDeleted": false,
      "id": "nsYgl7js3PjMKKTKcLYf1",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 895.4166948358098,
      "y": -409.48426474916266,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 100,
      "height": 92,
      "seed": 384682581,
      "groupIds": [
        "M7MHx-aSMrw_q1mhaAuG6",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 371,
      "versionNonce": 641029083,
      "isDeleted": false,
      "id": "a_CBcLegs1ZMstjO9f4bx",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 931.5250284234563,
      "y": -397.41226924952065,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 29.78333282470704,
      "height": 79.856009000716,
      "seed": 1741404085,
      "groupIds": [
        "M7MHx-aSMrw_q1mhaAuG6",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false,
      "fontSize": 63.88480720057278,
      "fontFamily": 1,
      "text": "?",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "?",
      "lineHeight": 1.25,
      "baseline": 56
    },
    {
      "type": "ellipse",
      "version": 328,
      "versionNonce": 22981461,
      "isDeleted": false,
      "id": "l88Tj2DPIgTUA2Pij6hPA",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1046.4166948358097,
      "y": -409.48426474916266,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 100,
      "height": 92,
      "seed": 701954677,
      "groupIds": [
        "vFCiKG5iTbg2K5S2WQSsn",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 373,
      "versionNonce": 1236541563,
      "isDeleted": false,
      "id": "oqmBYU6SpmJhOl9d1JLJy",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1082.5250284234562,
      "y": -397.41226924952065,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 29.78333282470704,
      "height": 79.856009000716,
      "seed": 39445461,
      "groupIds": [
        "vFCiKG5iTbg2K5S2WQSsn",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false,
      "fontSize": 63.88480720057278,
      "fontFamily": 1,
      "text": "?",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "?",
      "lineHeight": 1.25,
      "baseline": 56
    },
    {
      "type": "ellipse",
      "version": 329,
      "versionNonce": 1937275061,
      "isDeleted": false,
      "id": "IFZTHrjZaEk7Avy2zkma-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1197.4166948358097,
      "y": -409.48426474916266,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 100,
      "height": 92,
      "seed": 894921429,
      "groupIds": [
        "70qQx7edpvpMfsvEta9mo",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 374,
      "versionNonce": 2029017371,
      "isDeleted": false,
      "id": "p0xoCeTYO1ZVy-xm2oET3",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1233.5250284234562,
      "y": -397.41226924952065,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 29.78333282470704,
      "height": 79.856009000716,
      "seed": 959268917,
      "groupIds": [
        "70qQx7edpvpMfsvEta9mo",
        "dKeNjYUtPsZHi90BFF7EY"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688313196175,
      "link": null,
      "locked": false,
      "fontSize": 63.88480720057278,
      "fontFamily": 1,
      "text": "?",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "?",
      "lineHeight": 1.25,
      "baseline": 56
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/flows.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "text",
      "version": 603,
      "versionNonce": 1883892902,
      "isDeleted": false,
      "id": "Buic2Lx427wuSIW8P_Rw5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 714,
      "y": 141,
      "strokeColor": "#000000",
      "backgroundColor": "#228be6",
      "width": 532,
      "height": 46,
      "seed": 373648901,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 36,
      "fontFamily": 1,
      "text": "What are semantic workflows?",
      "baseline": 32,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "What are semantic workflows?"
    },
    {
      "type": "rectangle",
      "version": 1671,
      "versionNonce": 51714234,
      "isDeleted": false,
      "id": "qYd3q0Vjks7VOHUC9RR51",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 425,
      "y": 339.5,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1441952427,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "WPeWn6N4rCHf0jY16N9Ge"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "arrow",
      "version": 323,
      "versionNonce": 921790438,
      "isDeleted": false,
      "id": "tgnQzXC9s8RY4oImBOXuB",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 562.5285999651802,
      "y": 272.4578674923631,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.4714000348197942,
      "height": 72.5421325076369,
      "seed": 650463755,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "B435ajoI5vAQBDvzkd8aY",
        "focus": 0.05963733900881362,
        "gap": 5.457867492363107
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          0.4714000348197942,
          72.5421325076369
        ]
      ]
    },
    {
      "type": "text",
      "version": 1359,
      "versionNonce": 2032529786,
      "isDeleted": false,
      "id": "WPeWn6N4rCHf0jY16N9Ge",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 430,
      "y": 344.5,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 870516459,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Translate",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "qYd3q0Vjks7VOHUC9RR51",
      "originalText": "Translate"
    },
    {
      "type": "rectangle",
      "version": 180,
      "versionNonce": 159873830,
      "isDeleted": false,
      "id": "IxMxusRKX2PpnJT1uY0cC",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 819,
      "y": 217.5,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 290.0000000000001,
      "height": 49,
      "seed": 1364021803,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "SSXjX_URKvcVC8h-Qgz4j"
        },
        {
          "id": "_YNRYTAnVzdhhKxFtkVyg",
          "type": "arrow"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 85,
      "versionNonce": 1360900666,
      "isDeleted": false,
      "id": "SSXjX_URKvcVC8h-Qgz4j",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 824,
      "y": 224,
      "strokeColor": "#000000",
      "backgroundColor": "#82c91e",
      "width": 280,
      "height": 36,
      "seed": 948915563,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Extract Text",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "IxMxusRKX2PpnJT1uY0cC",
      "originalText": "Extract Text"
    },
    {
      "type": "rectangle",
      "version": 217,
      "versionNonce": 511256166,
      "isDeleted": false,
      "id": "lATIKISgJPOGnUHleuFRH",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 821,
      "y": 338,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1256311595,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "21WiUuyDtpQ9FnJ74REpJ",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 96,
      "versionNonce": 2111789818,
      "isDeleted": false,
      "id": "21WiUuyDtpQ9FnJ74REpJ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 826,
      "y": 343,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 626266373,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Summarize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "lATIKISgJPOGnUHleuFRH",
      "originalText": "Summarize"
    },
    {
      "type": "arrow",
      "version": 162,
      "versionNonce": 1452501414,
      "isDeleted": false,
      "id": "_YNRYTAnVzdhhKxFtkVyg",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 962.2340210300499,
      "y": 270,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.2659789699500834,
      "height": 75,
      "seed": 101249451,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "IxMxusRKX2PpnJT1uY0cC",
        "focus": 0.012856281024868153,
        "gap": 3.5
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          0.2659789699500834,
          75
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 225,
      "versionNonce": 502131642,
      "isDeleted": false,
      "id": "6B3J0wJfi561c9gMsgtXG",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 824,
      "y": 455,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1164190923,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "qJDqmDEZF9Ewh2UrjXq5Z",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 147,
      "versionNonce": 1896039654,
      "isDeleted": false,
      "id": "qJDqmDEZF9Ewh2UrjXq5Z",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 829,
      "y": 460,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 1307943269,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Build Vector Index",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "6B3J0wJfi561c9gMsgtXG",
      "originalText": "Build Vector Index"
    },
    {
      "type": "arrow",
      "version": 215,
      "versionNonce": 124544122,
      "isDeleted": false,
      "id": "Q51e0Hav-kYfjX3b8tt-r",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 963.600775377322,
      "y": 387.5,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.29392358842710564,
      "height": 66.5,
      "seed": 59173285,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "lATIKISgJPOGnUHleuFRH",
        "focus": 0.015253485349599789,
        "gap": 3.5
      },
      "endBinding": {
        "elementId": "6B3J0wJfi561c9gMsgtXG",
        "focus": -0.039966641220930875,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -0.29392358842710564,
          66.5
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 1258,
      "versionNonce": 1062582310,
      "isDeleted": false,
      "id": "5VuUdI_BsJ5pyE1nTqJUI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1195,
      "y": 221,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1044404613,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "bJJ9SGsJsvT071qBBH0w5",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1443,
      "versionNonce": 1411237178,
      "isDeleted": false,
      "id": "bJJ9SGsJsvT071qBBH0w5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1200,
      "y": 226,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 128953675,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Run similarity query",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "5VuUdI_BsJ5pyE1nTqJUI",
      "originalText": "Run similarity query"
    },
    {
      "type": "rectangle",
      "version": 359,
      "versionNonce": 1029930726,
      "isDeleted": false,
      "id": "pSbgtf1qAB7tWl-pTld7e",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1197,
      "y": 343,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 210689733,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "pQzKSM3audka1kiQm_Sku",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        },
        {
          "id": "k67YzXNtt1GLh4i8Es5zJ",
          "type": "arrow"
        }
      ],
      "updated": 1673791251799,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 320,
      "versionNonce": 660337658,
      "isDeleted": false,
      "id": "pQzKSM3audka1kiQm_Sku",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1219.5,
      "y": 348,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 245,
      "height": 36,
      "seed": 1869028363,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791256722,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Send notifications",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "pSbgtf1qAB7tWl-pTld7e",
      "originalText": "Send notifications"
    },
    {
      "type": "arrow",
      "version": 329,
      "versionNonce": 251216122,
      "isDeleted": false,
      "id": "k67YzXNtt1GLh4i8Es5zJ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1339.1171359117898,
      "y": 265.5,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 1.0119492860442278,
      "height": 76.5,
      "seed": 1656816747,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673791249246,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": {
        "elementId": "pSbgtf1qAB7tWl-pTld7e",
        "focus": -0.010690950588675632,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          1.0119492860442278,
          76.5
        ]
      ]
    },
    {
      "type": "text",
      "version": 320,
      "versionNonce": 407191226,
      "isDeleted": false,
      "id": "kKPPLMCj8QQIJLbW-B5hm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1180,
      "y": 451,
      "strokeColor": "#000",
      "backgroundColor": "#fab005",
      "width": 296,
      "height": 52,
      "seed": 1079990731,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "- API bindings for JavaScript,\n  Rust, Go and Java",
      "baseline": 44,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- API bindings for JavaScript,\n  Rust, Go and Java"
    },
    {
      "type": "text",
      "version": 572,
      "versionNonce": 1757323750,
      "isDeleted": false,
      "id": "1AQ3rj-V4weRtPA8a5-z-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 437.5,
      "y": 449.5,
      "strokeColor": "#000",
      "backgroundColor": "#fab005",
      "width": 278,
      "height": 52,
      "seed": 836512075,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "- Build with Python or YAML\n- Run local or via API",
      "baseline": 44,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Build with Python or YAML\n- Run local or via API"
    },
    {
      "type": "rectangle",
      "version": 304,
      "versionNonce": 23156602,
      "isDeleted": false,
      "id": "B435ajoI5vAQBDvzkd8aY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 426,
      "y": 221,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 942981672,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "AoGnxEHn4x-zq2-0C0VrT",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        },
        {
          "id": "tgnQzXC9s8RY4oImBOXuB",
          "type": "arrow"
        }
      ],
      "updated": 1673791247706,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 183,
      "versionNonce": 1100838182,
      "isDeleted": false,
      "id": "AoGnxEHn4x-zq2-0C0VrT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 431,
      "y": 226,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 604886104,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791247706,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Summarize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "B435ajoI5vAQBDvzkd8aY",
      "originalText": "Summarize"
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#3030"
  },
  "files": {}
}


================================================
FILE: docs/images/format.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 564,
      "versionNonce": 555415043,
      "index": "aZV",
      "isDeleted": false,
      "id": "c8q6be5K81xmCJ-ukSRnZ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 518,
      "y": -130.82625000000002,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 261,
      "height": 183,
      "seed": 443859885,
      "groupIds": [
        "2ZFdM1ONmx5lh5aHlDXqH"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584360105,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 518,
      "versionNonce": 1253679021,
      "index": "aa",
      "isDeleted": false,
      "id": "uKvu2zcnLI9S3D-aFErJQ",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 616.6083333333333,
      "y": -129.82625000000002,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#03a9f4",
      "width": 53.78333333333333,
      "height": 35,
      "seed": 848196621,
      "groupIds": [
        "2ZFdM1ONmx5lh5aHlDXqH"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584360105,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 5,
      "text": "ANN",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "ANN",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "text",
      "version": 480,
      "versionNonce": 944506682,
      "index": "ab",
      "isDeleted": false,
      "id": "DCqImwTRCBh0nmPIv-3zR",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 536.8249969482422,
      "y": -85.17374999999998,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#03a9f4",
      "width": 213.35000610351562,
      "height": 125,
      "seed": 1618350307,
      "groupIds": [
        "2ZFdM1ONmx5lh5aHlDXqH"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724587151711,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 5,
      "text": "- Faiss\n- HNSWLib\n- Annoy\n- NumPy\n- Postgres (pgvector)",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Faiss\n- HNSWLib\n- Annoy\n- NumPy\n- Postgres (pgvector)",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "rectangle",
      "version": 780,
      "versionNonce": 1085123203,
      "index": "abV",
      "isDeleted": false,
      "id": "Jo5LO2bSebtz54pWb3XHT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 810.5,
      "y": -131.82625000000002,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 282.9999999999999,
      "height": 182,
      "seed": 848620899,
      "groupIds": [
        "hCyHVfABhzmE_8xEhUbGD"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584372798,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 717,
      "versionNonce": 708473645,
      "index": "ac",
      "isDeleted": false,
      "id": "wX4udz8nceRrNC3uUDLYf",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 886.2583312988281,
      "y": -121.82625000000002,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#03a9f4",
      "width": 131.48333740234375,
      "height": 35,
      "seed": 16206083,
      "groupIds": [
        "hCyHVfABhzmE_8xEhUbGD"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584372798,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 5,
      "text": "Database",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Database",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "text",
      "version": 722,
      "versionNonce": 1181998310,
      "index": "ad",
      "isDeleted": false,
      "id": "-5HdWsMbtE_7EnREtXBfc",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 831.1999969482422,
      "y": -77.17374999999998,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#03a9f4",
      "width": 241.60000610351562,
      "height": 75,
      "seed": 1976381603,
      "groupIds": [
        "hCyHVfABhzmE_8xEhUbGD"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724587158353,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 5,
      "text": "- SQLite\n- DuckDB\n- Postgres (SQLAlchemy)",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- SQLite\n- DuckDB\n- Postgres (SQLAlchemy)",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "rectangle",
      "version": 900,
      "versionNonce": 1747610221,
      "index": "adV",
      "isDeleted": false,
      "id": "AK1nt4FrnmjinS2uZVZ2g",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1125,
      "y": -132.32625000000002,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 266.9999999999999,
      "height": 180,
      "seed": 961514755,
      "groupIds": [
        "KOc5NUqUBL03A8qlCEUgw"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584372798,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 828,
      "versionNonce": 1440317155,
      "index": "ae",
      "isDeleted": false,
      "id": "yeFldcgggtNS8S44yWdtq",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1218.7916666666667,
      "y": -129.32625000000002,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 79.41666666666667,
      "height": 35,
      "seed": 41743523,
      "groupIds": [
        "KOc5NUqUBL03A8qlCEUgw"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584372798,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 5,
      "text": "Graph",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Graph",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "text",
      "version": 937,
      "versionNonce": 766285005,
      "index": "af",
      "isDeleted": false,
      "id": "FSHrZ4DMc-ubEPZmgY7M3",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1128.8999938964844,
      "y": -78.67374999999998,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 259.20001220703125,
      "height": 75,
      "seed": 2094819395,
      "groupIds": [
        "KOc5NUqUBL03A8qlCEUgw"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584372798,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 5,
      "text": "- NetworkX (MessagePack)\n- Postgres (grand-graph)\n",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- NetworkX (MessagePack)\n- Postgres (grand-graph)\n",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "rectangle",
      "version": 784,
      "versionNonce": 868973283,
      "index": "afV",
      "isDeleted": false,
      "id": "C_7EOPOUjQk0kE44hF9WC",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1423.5,
      "y": -130.82625000000002,
      "strokeColor": "#9775fa",
      "backgroundColor": "#9775fa",
      "width": 290.9999999999999,
      "height": 174,
      "seed": 606086243,
      "groupIds": [
        "-E2Q3MkgggNSCRYPrSvhr"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584154381,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 712,
      "versionNonce": 1558730957,
      "index": "ag",
      "isDeleted": false,
      "id": "nbfo2fj-5l1xNK7r9SnT8",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1520.2333335876465,
      "y": -127.82625000000002,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 97.53333282470703,
      "height": 35,
      "seed": 704880643,
      "groupIds": [
        "-E2Q3MkgggNSCRYPrSvhr"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584154381,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 5,
      "text": "Scoring",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Scoring",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "type": "text",
      "version": 832,
      "versionNonce": 2104267693,
      "index": "ah",
      "isDeleted": false,
      "id": "6a1UVKWBLqgnaAvPSuSfw",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1431.6750030517578,
      "y": -76.17374999999998,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 274.6499938964844,
      "height": 75,
      "seed": 154576803,
      "groupIds": [
        "-E2Q3MkgggNSCRYPrSvhr"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1724584347937,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 5,
      "text": "- Local index (MessagePack)\n- Postgres\n",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Local index (MessagePack)\n- Postgres\n",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/further.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 4646,
      "versionNonce": 1614521653,
      "isDeleted": false,
      "id": "8q8ph1hOgu4FaaN7bUDtj",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 570.5486656509088,
      "y": 207.02732362163974,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 971860443,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4695,
      "versionNonce": 245642395,
      "isDeleted": false,
      "id": "NLF6LLOv1mkjsqrOC4cKB",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 563.0012326151946,
      "y": 200.00723433592552,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 435779195,
      "groupIds": [
        "zukIlM5pM--6gudqqavs2",
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4793,
      "versionNonce": 1146828437,
      "isDeleted": false,
      "id": "b0FmtXHqO4i5gHXFgOxF6",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 553.6061433294792,
      "y": 191.75332808592574,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 35319579,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 3917,
      "versionNonce": 722004283,
      "isDeleted": false,
      "id": "XIBG5MqpMzO91s-zg9RV6",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 565.5925699163902,
      "y": 240.5005651332784,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 616846267,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 3943,
      "versionNonce": 356621301,
      "isDeleted": false,
      "id": "Ak-qKgUN0py0hmrGe0aKl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 567.4478221544045,
      "y": 209.03527783443982,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 1108878427,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 3967,
      "versionNonce": 5917339,
      "isDeleted": false,
      "id": "1CwtdEiRMjK88DqZoUvFn",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 567.3712226380351,
      "y": 276.4296521047969,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 1911087355,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4004,
      "versionNonce": 20545685,
      "isDeleted": false,
      "id": "EtF5vn14aeWzQBbD8KpKB",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 564.1871754565047,
      "y": 287.6728005130757,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 767101339,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 3970,
      "versionNonce": 237898203,
      "isDeleted": false,
      "id": "ETXFUxT0kF7UCijUSi0-u",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 564.4720183520706,
      "y": 224.09587938770105,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 448470587,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 3984,
      "versionNonce": 1942398453,
      "isDeleted": false,
      "id": "lLMO9OdzPiwVIPsiVJjmS",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 564.5795065482209,
      "y": 258.57920842354054,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 332504795,
      "groupIds": [
        "EGaO0Qgusro1bOGa76Zij"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 4612,
      "versionNonce": 232169435,
      "isDeleted": false,
      "id": "SfdOgKSw2Op0gU02ZRj6h",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 724.2986656509086,
      "y": 207.02732362163974,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1938754427,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4661,
      "versionNonce": 1481121621,
      "isDeleted": false,
      "id": "WZq_8J-B599FxSuSWhMOY",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 716.7512326151943,
      "y": 200.00723433592552,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 506848283,
      "groupIds": [
        "12jDSGpPx8NMUTpKIVTRd",
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4759,
      "versionNonce": 1196158075,
      "isDeleted": false,
      "id": "6UAiKf3P2B8oHIqxMXP9B",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 707.3561433294792,
      "y": 191.75332808592574,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1270393019,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 3883,
      "versionNonce": 353594549,
      "isDeleted": false,
      "id": "PtmQWP_l-8IxINtiwRUhu",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 719.3425699163902,
      "y": 240.5005651332784,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 297392475,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 3909,
      "versionNonce": 162440475,
      "isDeleted": false,
      "id": "Q3WDbscMJKM8bJF79W-YD",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 721.1978221544043,
      "y": 209.03527783443982,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 764147195,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 3934,
      "versionNonce": 701369877,
      "isDeleted": false,
      "id": "BS2q9beMoZf_-aYf5c8vt",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 721.1212226380351,
      "y": 276.42965210479707,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 238124699,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 3971,
      "versionNonce": 745621947,
      "isDeleted": false,
      "id": "oMYU6FaZy7M2x90y9S3Pt",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 717.9371754565047,
      "y": 287.67280051307563,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 509408059,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 3936,
      "versionNonce": 91063157,
      "isDeleted": false,
      "id": "PHkrZ4_UHjutk9XZElk_e",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 718.2220183520706,
      "y": 224.09587938770105,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1316017115,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375231,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 3951,
      "versionNonce": 1958107739,
      "isDeleted": false,
      "id": "Dorr3dO30jUkk79PopIf_",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 718.3295065482209,
      "y": 258.57920842354076,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1585422459,
      "groupIds": [
        "qd_DFmKp5LE3bMw3Mtfo2"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 4610,
      "versionNonce": 989091157,
      "isDeleted": false,
      "id": "Jt4NvPMeLg6mhAlXKpPrq",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 878.0486656509083,
      "y": 207.02732362163974,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1749801243,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4659,
      "versionNonce": 1562777211,
      "isDeleted": false,
      "id": "WgPx2gWYtu6RU7xJArs4i",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 870.5012326151941,
      "y": 200.00723433592552,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 692583867,
      "groupIds": [
        "Tzrs5VR8o2cJk5khXPvuR",
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4757,
      "versionNonce": 13165237,
      "isDeleted": false,
      "id": "O11wLzeZnprhHXZl10o-v",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 861.106143329479,
      "y": 191.75332808592574,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 211193435,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 3881,
      "versionNonce": 1991479067,
      "isDeleted": false,
      "id": "HZv1jZzJjUlbXWXKpVgPq",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 873.09256991639,
      "y": 240.5005651332784,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1963592443,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 3907,
      "versionNonce": 728024085,
      "isDeleted": false,
      "id": "z9PwounZ94sfL3PfhSiQ4",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 874.9478221544041,
      "y": 209.03527783443982,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 1193603995,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 3931,
      "versionNonce": 557651003,
      "isDeleted": false,
      "id": "Imd8yI2Lf6IQozPsseCce",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 874.8712226380349,
      "y": 276.4296521047971,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 857327675,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 3968,
      "versionNonce": 1811521781,
      "isDeleted": false,
      "id": "ZH0iHUr1LSzlOrGkrE_Bg",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 871.6871754565045,
      "y": 287.6728005130757,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 1053493467,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 3934,
      "versionNonce": 1028191163,
      "isDeleted": false,
      "id": "uboh1NQ170zSPaXGoSESZ",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 871.9720183520703,
      "y": 224.09587938770105,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 264975739,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312379126,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 3948,
      "versionNonce": 1717811797,
      "isDeleted": false,
      "id": "XLAGZPKHS8Xwt91UpHZ_0",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 872.0795065482207,
      "y": 258.57920842354076,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1969016347,
      "groupIds": [
        "oifR3K1EkgRm0woSpwQEe"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 4634,
      "versionNonce": 1854636411,
      "isDeleted": false,
      "id": "nZq35Q3L6H0QkeG3Vht31",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1031.7986656509083,
      "y": 207.02732362163974,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 846353083,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4683,
      "versionNonce": 855877557,
      "isDeleted": false,
      "id": "rrNUqmwxf5WDNs8Qq_9z0",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1024.251232615194,
      "y": 200.00723433592552,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 128861019,
      "groupIds": [
        "v87qT3LfamP0fU18OmjI3",
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4781,
      "versionNonce": 775133723,
      "isDeleted": false,
      "id": "hNZNlkh5OnGJBY8xlIc5t",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1014.8561433294788,
      "y": 191.75332808592574,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2103648251,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 3905,
      "versionNonce": 872343829,
      "isDeleted": false,
      "id": "KBAv7GqAYMFEu-wMY1mh0",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1026.8425699163897,
      "y": 240.5005651332784,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1838377115,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 3931,
      "versionNonce": 1049599675,
      "isDeleted": false,
      "id": "vBzGi3sNPrEnLnXkuTDui",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1028.697822154404,
      "y": 209.03527783443982,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 1307680059,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 3956,
      "versionNonce": 1791688309,
      "isDeleted": false,
      "id": "riv_87Cqm5BHH4qwhzob4",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1028.6212226380346,
      "y": 276.4296521047971,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 970997211,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 3993,
      "versionNonce": 977407835,
      "isDeleted": false,
      "id": "1xxX3pYb1rx-QZk3jub6u",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1025.4371754565043,
      "y": 287.6728005130757,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 1014090363,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 3958,
      "versionNonce": 237856725,
      "isDeleted": false,
      "id": "bU8iYByqD3dp76KNxsOad",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1025.72201835207,
      "y": 224.09587938770105,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1516815131,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 3973,
      "versionNonce": 1134390267,
      "isDeleted": false,
      "id": "VY-TfARiaY6LsV7-7rNWC",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1025.8295065482205,
      "y": 258.57920842354076,
      "strokeColor": "#7950f2",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1547151291,
      "groupIds": [
        "0RC_0jv9T44Fq-XFPghb3"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312375232,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 4712,
      "versionNonce": 628244187,
      "isDeleted": false,
      "id": "C72Ce5baIVqALdI-P3OM9",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1185.548665650909,
      "y": 207.0273236216395,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1476458587,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4761,
      "versionNonce": 1358557269,
      "isDeleted": false,
      "id": "zBGI80VXFpYU7OXFGK6J6",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1178.0012326151943,
      "y": 200.0072343359253,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2073507067,
      "groupIds": [
        "oYL8Phei164CeEqvr9yAQ",
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 4859,
      "versionNonce": 1209772923,
      "isDeleted": false,
      "id": "cfu4VQtjnsxUMcZwQ5sVW",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1168.6061433294788,
      "y": 191.75332808592574,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 377281947,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 3983,
      "versionNonce": 425926069,
      "isDeleted": false,
      "id": "LmvD8cAN6D1U7VGelLV01",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1180.5925699163897,
      "y": 240.50056513327817,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1675915835,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4009,
      "versionNonce": 2016314395,
      "isDeleted": false,
      "id": "uIr1YsstwF_3qU4vEy2_0",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1182.4478221544048,
      "y": 209.0352778344396,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 347108059,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4034,
      "versionNonce": 1258266389,
      "isDeleted": false,
      "id": "soobVx_Prpyrf1BH-snOZ",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1182.3712226380346,
      "y": 276.4296521047969,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 426991483,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4071,
      "versionNonce": 496847035,
      "isDeleted": false,
      "id": "wrQ8ziTQK2vkQdM0O4J3M",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1179.1871754565043,
      "y": 287.67280051307546,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 745519131,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4036,
      "versionNonce": 205845621,
      "isDeleted": false,
      "id": "yZFGBgkE8nXDy8E9XYuQO",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1179.47201835207,
      "y": 224.09587938770082,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 462945467,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4051,
      "versionNonce": 2114615643,
      "isDeleted": false,
      "id": "eReA_XFu0cSYQhyG-92nc",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1179.5795065482205,
      "y": 258.57920842354054,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1385082203,
      "groupIds": [
        "siLS3_RDwEMKtYUwaaxN-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688312381572,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/indexing.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "gu5_xiA8n2ahi-4mERgcR",
      "type": "rectangle",
      "x": 1278.0454545454545,
      "y": 234.17792970816078,
      "width": 319.99999999999983,
      "height": 286.9999999999999,
      "angle": 0,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 60,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "seed": 260819005,
      "version": 681,
      "versionNonce": 1445045779,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "ZyDOc33tL-6s4vwoFYHAq",
          "type": "arrow"
        },
        {
          "id": "5m7yE6eTRKFfSCbA82DrS",
          "type": "arrow"
        },
        {
          "id": "-D1yDTCnqxjTFsQU0jBjN",
          "type": "arrow"
        }
      ],
      "updated": 1641735984230
    },
    {
      "type": "line",
      "version": 5110,
      "versionNonce": 785100115,
      "isDeleted": false,
      "id": "BcQI3V6na893s7vMmTK-q",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1000.4754963515834,
      "y": 217.20853592127958,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 77.09201683999922,
      "height": 99.49948667804088,
      "seed": 597539171,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908376,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.2542098813493443,
          75.20117273657175
        ],
        [
          0.011896425679918422,
          83.76249969444815
        ],
        [
          3.970409367559332,
          87.46174320643391
        ],
        [
          17.75573317066317,
          90.59250103325854
        ],
        [
          41.05683533152865,
          91.56737225214069
        ],
        [
          63.319497586673116,
          90.01084754868091
        ],
        [
          75.14781395923075,
          86.28844687220405
        ],
        [
          76.81603792670788,
          83.15042405259751
        ],
        [
          77.05033394391478,
          76.25776215104557
        ],
        [
          76.86643881413028,
          6.3089586511537865
        ],
        [
          76.45188016352971,
          -0.2999144698665015
        ],
        [
          71.50179495549581,
          -3.9936571317850627
        ],
        [
          61.077971898861186,
          -6.132877429442784
        ],
        [
          37.32348754161154,
          -7.932114425900202
        ],
        [
          18.278415656797975,
          -6.859225353587373
        ],
        [
          3.2995959613238286,
          -3.2201165291205287
        ],
        [
          -0.04168289608444441,
          -0.045185660461322996
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "line",
      "version": 2808,
      "versionNonce": 923714909,
      "isDeleted": false,
      "id": "UFrJE_4GqAUPwzMDMeN7A",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 998.9994257898584,
      "y": 272.3348195790094,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 77.17198221193564,
      "height": 8.562348957853038,
      "seed": 960447053,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908377,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          2.033150371639873,
          3.413095389435587
        ],
        [
          10.801287372573954,
          6.276651055277943
        ],
        [
          22.468666942209353,
          8.010803051612635
        ],
        [
          40.747074201802775,
          8.168828515515864
        ],
        [
          62.077348233027564,
          7.0647721921469495
        ],
        [
          74.53446931782398,
          3.04824021069218
        ],
        [
          77.17198221193564,
          -0.3935204423371723
        ]
      ]
    },
    {
      "type": "line",
      "version": 2894,
      "versionNonce": 439388915,
      "isDeleted": false,
      "id": "fqsQ_Qaf-7qXlNZOtktUp",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 997.9675684673548,
      "y": 243.3707642437863,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 77.17198221193564,
      "height": 8.562348957853038,
      "seed": 1862702339,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908377,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          2.033150371639873,
          3.413095389435587
        ],
        [
          10.801287372573954,
          6.276651055277943
        ],
        [
          22.468666942209353,
          8.010803051612635
        ],
        [
          40.747074201802775,
          8.168828515515864
        ],
        [
          62.077348233027564,
          7.0647721921469495
        ],
        [
          74.53446931782398,
          3.04824021069218
        ],
        [
          77.17198221193564,
          -0.3935204423371723
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 5897,
      "versionNonce": 1193808317,
      "isDeleted": false,
      "id": "4l6IrZqyWS_r_TywqVTEW",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 996.9096194852727,
      "y": 208.07626729559803,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 76.59753601865496,
      "height": 15.49127539284798,
      "seed": 1511747757,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "bxuMGTzXLn7H-uBCptINx"
        }
      ],
      "updated": 1641735908377
    },
    {
      "type": "ellipse",
      "version": 1298,
      "versionNonce": 745059475,
      "isDeleted": false,
      "id": "vrSWcPDbYZk3CuUPgODtk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1033.4016065904952,
      "y": 232.55293497550144,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 11.226103154161754,
      "height": 12.183758484455605,
      "seed": 2081619107,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641735908377
    },
    {
      "type": "ellipse",
      "version": 1586,
      "versionNonce": 857264669,
      "isDeleted": false,
      "id": "ptDnFZABr-xnrQYrkdvCj",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1033.7527483744288,
      "y": 258.76954926256656,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 11.226103154161754,
      "height": 12.183758484455605,
      "seed": 1076024077,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641735908377
    },
    {
      "type": "ellipse",
      "version": 1470,
      "versionNonce": 1160423987,
      "isDeleted": false,
      "id": "2v5HdhgHuudJY2EmM0hV_",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.239590202363168,
      "x": 1034.4016065904952,
      "y": 287.53827583387294,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 11.226103154161754,
      "height": 12.183758484455605,
      "seed": 1718710339,
      "groupIds": [
        "ByeZbnpBUniBnZxL1XiMR",
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641735908377
    },
    {
      "type": "text",
      "version": 975,
      "versionNonce": 2010285693,
      "isDeleted": false,
      "id": "YJIBcqHtzJtKcCgFzxnQV",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 989.625,
      "y": 310.0375083418067,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 94,
      "height": 46,
      "seed": 131000685,
      "groupIds": [
        "YDUnsOO4AJOXnzopVjTfR"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641735908377,
      "fontSize": 17.4778970902999,
      "fontFamily": 1,
      "text": "Structured\nDatabase",
      "baseline": 39,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Structured\nDatabase"
    },
    {
      "type": "rectangle",
      "version": 1600,
      "versionNonce": 157084627,
      "isDeleted": false,
      "id": "YI607mwBteKrjS78gtTXD",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 50,
      "angle": 0,
      "x": 1281.9090909090908,
      "y": 473.0000000000002,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 315.00000000000006,
      "height": 40,
      "seed": 928276467,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "-D1yDTCnqxjTFsQU0jBjN",
          "type": "arrow"
        }
      ],
      "updated": 1641735908377
    },
    {
      "type": "rectangle",
      "version": 1299,
      "versionNonce": 2014752477,
      "isDeleted": false,
      "id": "UsRZ6Y682zf4FIZyNDOtq",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 50,
      "angle": 0,
      "x": 1280.9090909090908,
      "y": 238.6363636363636,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 316.1818181818184,
      "height": 47.00000000000002,
      "seed": 1663520371,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "5m7yE6eTRKFfSCbA82DrS",
          "type": "arrow"
        },
        {
          "id": "ZyDOc33tL-6s4vwoFYHAq",
          "type": "arrow"
        }
      ],
      "updated": 1641735908377
    },
    {
      "type": "line",
      "version": 6222,
      "versionNonce": 1024850291,
      "isDeleted": false,
      "id": "qtt-6VisYFKR2M8Cu45p-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1002.5946179731507,
      "y": 439.26150708178005,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 76.99810389727404,
      "height": 99.37827711605759,
      "seed": 89125379,
      "groupIds": [
        "XwpXytlIickMBY-WleyOp",
        "fZX3gzhfhBW0U-bYngONa",
        "q5UWDdBSxh3UYgBDbSFBQ"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908378,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.25390020469272123,
          75.10956320658954
        ],
        [
          0.011881933539366223,
          83.66046081728857
        ],
        [
          3.9655726433067375,
          87.35519793732486
        ],
        [
          17.73410326369428,
          90.48214189738947
        ],
        [
          41.00682018880676,
          91.45582553513545
        ],
        [
          63.24236222825349,
          89.90119697892054
        ],
        [
          75.05626943052894,
          86.18333090428511
        ],
        [
          76.72246117951802,
          83.04913080160064
        ],
        [
          76.95647177899504,
          76.16486549140681
        ],
        [
          76.77280066894052,
          6.301273122914302
        ],
        [
          76.35874703071867,
          -0.299549116207539
        ],
        [
          71.41469198102895,
          -3.9887920872726523
        ],
        [
          61.003567150978974,
          -6.125406402086429
        ],
        [
          37.27802033642641,
          -7.922451580922154
        ],
        [
          18.256149021768273,
          -6.850869494392455
        ],
        [
          3.2955764171578545,
          -3.2161938062295934
        ],
        [
          -0.04163211827899763,
          -0.0451306156134037
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 6957,
      "versionNonce": 1115081533,
      "isDeleted": false,
      "id": "xmFIoJeyaqzUfiLrt7yo1",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1002.1009502312106,
      "y": 430.490580458146,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 76.50422544892463,
      "height": 15.472404032124233,
      "seed": 761422765,
      "groupIds": [
        "XwpXytlIickMBY-WleyOp",
        "fZX3gzhfhBW0U-bYngONa",
        "q5UWDdBSxh3UYgBDbSFBQ"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "bxuMGTzXLn7H-uBCptINx"
        }
      ],
      "updated": 1641735908378
    },
    {
      "type": "text",
      "version": 2153,
      "versionNonce": 1570525971,
      "isDeleted": false,
      "id": "drtrHswdiUqZPuNgKc90d",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1020.1035285928494,
      "y": 465.17753663299726,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 18,
      "height": 36,
      "seed": 455653795,
      "groupIds": [
        "fZX3gzhfhBW0U-bYngONa",
        "q5UWDdBSxh3UYgBDbSFBQ"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908378,
      "fontSize": 29.219434366479078,
      "fontFamily": 3,
      "text": "<",
      "baseline": 29,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "<"
    },
    {
      "type": "text",
      "version": 2133,
      "versionNonce": 391404445,
      "isDeleted": false,
      "id": "Tof7f8O_qqAbmeVWbwNel",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1042.114401938595,
      "y": 465.6542466049163,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 18,
      "height": 36,
      "seed": 336815629,
      "groupIds": [
        "fZX3gzhfhBW0U-bYngONa",
        "q5UWDdBSxh3UYgBDbSFBQ"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908378,
      "fontSize": 29.384515916572173,
      "fontFamily": 3,
      "text": ">",
      "baseline": 29,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ">"
    },
    {
      "type": "text",
      "version": 1428,
      "versionNonce": 2070313139,
      "isDeleted": false,
      "id": "VzdNhewfEay7ILpboy3kh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 877.8181818181818,
      "y": 531.2795921207232,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 324,
      "height": 48,
      "seed": 1976012099,
      "groupIds": [
        "q5UWDdBSxh3UYgBDbSFBQ"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641735908378,
      "fontSize": 18.16360832146819,
      "fontFamily": 1,
      "text": "Approximate Nearest Neighbor (ANN)\nIndex",
      "baseline": 41,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Approximate Nearest Neighbor (ANN)\nIndex"
    },
    {
      "type": "rectangle",
      "version": 4844,
      "versionNonce": 786489341,
      "isDeleted": false,
      "id": "J8c0oXxyYGwX5-DrgmaWs",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 594.6310358331666,
      "y": 328.3910028343104,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 63.17951567316358,
      "height": 95.87140433060104,
      "seed": 982009293,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "eDQoIxWK3cF_GuzA8jGlK",
          "type": "arrow"
        },
        {
          "id": "UldaTf_SD8SLGi5U0s3az",
          "type": "arrow"
        }
      ],
      "updated": 1641735908378
    },
    {
      "type": "rectangle",
      "version": 4895,
      "versionNonce": 1411050067,
      "isDeleted": false,
      "id": "m9UEIWpFo5o-W_PZIjS5i",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 587.8843926619461,
      "y": 322.1157517367904,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 63.17951567316358,
      "height": 95.87140433060104,
      "seed": 1164020099,
      "groupIds": [
        "OQ3HaFukpFi0eOGW5R5UG",
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "eDQoIxWK3cF_GuzA8jGlK",
          "type": "arrow"
        },
        {
          "id": "UldaTf_SD8SLGi5U0s3az",
          "type": "arrow"
        }
      ],
      "updated": 1641735908378
    },
    {
      "type": "rectangle",
      "version": 4989,
      "versionNonce": 1058920541,
      "isDeleted": false,
      "id": "7d9_4d4sJrQ9DvGC8ybba",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 579.4861313362021,
      "y": 314.7375928350884,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 63.17951567316358,
      "height": 95.87140433060104,
      "seed": 1652032557,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1641735908378
    },
    {
      "type": "line",
      "version": 4113,
      "versionNonce": 747644915,
      "isDeleted": false,
      "id": "sbNllVyO-9z2sQtdPITdl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 590.173416729491,
      "y": 358.1987100051989,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 41.63767071747142,
      "height": 2.905130632707587,
      "seed": 605774115,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          36.135414139555195,
          0.14069349922795835
        ],
        [
          41.63767071747142,
          -2.7644371334796283
        ]
      ]
    },
    {
      "type": "line",
      "version": 4136,
      "versionNonce": 1104987325,
      "isDeleted": false,
      "id": "eUkNzCc4WrlKLnap75eGL",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 591.7725612633524,
      "y": 330.1315411841826,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 40.73266929000497,
      "height": 2.5058652970606046,
      "seed": 1184953997,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          15.046599363382079,
          -2.5058652970606046
        ],
        [
          40.73266929000497,
          -0.2927945794743634
        ]
      ]
    },
    {
      "type": "line",
      "version": 4162,
      "versionNonce": 1791799699,
      "isDeleted": false,
      "id": "RfDT1TO01chDwfw5Cbtzd",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 591.7870401126618,
      "y": 390.3595353798012,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 43.20811437101611,
      "height": 3.8264756613833097,
      "seed": 687424707,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          23.609890181258436,
          -0.22815162036966638
        ],
        [
          33.628485051192264,
          2.0697081845366485
        ],
        [
          43.20811437101611,
          -1.7567674768466617
        ]
      ]
    },
    {
      "type": "line",
      "version": 4198,
      "versionNonce": 2059347229,
      "isDeleted": false,
      "id": "eWFd9AM2v-w_xidHMXRZY",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 588.8193171598485,
      "y": 400.3650618336177,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 48.6343204088089,
      "height": 2.600928472214653,
      "seed": 1908186349,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          9.087460853677031,
          -1.042860552572967
        ],
        [
          14.419182407365316,
          -0.7529003472200484
        ],
        [
          41.35248119200923,
          0.5475638888872795
        ],
        [
          48.6343204088089,
          -2.053364583327373
        ]
      ]
    },
    {
      "type": "line",
      "version": 4165,
      "versionNonce": 1879508787,
      "isDeleted": false,
      "id": "x-HvHTslIzJHS5qwLLpGG",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 589.2138897559879,
      "y": 343.5358653753724,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 41.94947793197669,
      "height": 2.2130707175862407,
      "seed": 194622563,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.263408005353806,
          -0.5285512538564915
        ],
        [
          41.94947793197669,
          1.6845194637297491
        ]
      ]
    },
    {
      "type": "line",
      "version": 4180,
      "versionNonce": 1026980221,
      "isDeleted": false,
      "id": "0lRPNpWYP4Azt2euXF2xI",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 589.1700215474542,
      "y": 374.3250106316369,
      "strokeColor": "#343a40",
      "backgroundColor": "#fff",
      "width": 41.94947793197669,
      "height": 2.2130707175862407,
      "seed": 1532262221,
      "groupIds": [
        "BldQyeSIaH-oKkL4FMf2Q"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          7.2351635300143835,
          1.2764612775431747
        ],
        [
          16.263408005353806,
          -0.5285512538564915
        ],
        [
          41.94947793197669,
          1.6845194637297491
        ]
      ]
    },
    {
      "type": "text",
      "version": 470,
      "versionNonce": 849717459,
      "isDeleted": false,
      "id": "RZJSTAnmzzhRhyGEUMHmo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 557,
      "y": 432,
      "strokeColor": "#343a40",
      "backgroundColor": "transparent",
      "width": 117,
      "height": 26,
      "seed": 705534605,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Input Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Input Data"
    },
    {
      "type": "diamond",
      "version": 1413,
      "versionNonce": 1921270237,
      "isDeleted": false,
      "id": "TVvWBG3ghORc3yVIw7lIh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 783.6763173734828,
      "y": 422.8472597163642,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 73.64736525303452,
      "height": 24.04199580676619,
      "seed": 1762684669,
      "groupIds": [
        "sgYs6pusUBDCv2FmxIFXR"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908379
    },
    {
      "type": "diamond",
      "version": 1455,
      "versionNonce": 406787699,
      "isDeleted": false,
      "id": "Z9ZwzURs4p74Wb1ozju71",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 783.6763173734828,
      "y": 417.04700672905824,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 73.64736525303452,
      "height": 24.04199580676619,
      "seed": 1250133853,
      "groupIds": [
        "sgYs6pusUBDCv2FmxIFXR"
      ],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "CjwxjqXOKYE1zbZ-NBw5A",
          "type": "arrow"
        }
      ],
      "updated": 1641735908380
    },
    {
      "type": "diamond",
      "version": 1536,
      "versionNonce": 1606024765,
      "isDeleted": false,
      "id": "1eOQN_-IrMENzxR2quYRb",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 783.6763173734828,
      "y": 408.7893130516918,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 73.64736525303452,
      "height": 24.04199580676619,
      "seed": 1387566013,
      "groupIds": [
        "sgYs6pusUBDCv2FmxIFXR"
      ],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "UldaTf_SD8SLGi5U0s3az",
          "type": "arrow"
        },
        {
          "id": "CjwxjqXOKYE1zbZ-NBw5A",
          "type": "arrow"
        }
      ],
      "updated": 1641735908380
    },
    {
      "type": "diamond",
      "version": 1584,
      "versionNonce": 261738515,
      "isDeleted": false,
      "id": "UO_8s06e5WPXOXcLW1Gji",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 783.6763173734828,
      "y": 400.5962705882732,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 73.64736525303452,
      "height": 24.04199580676619,
      "seed": 1573196829,
      "groupIds": [
        "sgYs6pusUBDCv2FmxIFXR"
      ],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "UldaTf_SD8SLGi5U0s3az",
          "type": "arrow"
        },
        {
          "id": "CjwxjqXOKYE1zbZ-NBw5A",
          "type": "arrow"
        }
      ],
      "updated": 1641735908380
    },
    {
      "type": "text",
      "version": 595,
      "versionNonce": 733556381,
      "isDeleted": false,
      "id": "Rc3sIwmWEVTP-seViLAF-",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 776,
      "y": 456,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ffeb3b",
      "width": 90,
      "height": 26,
      "seed": 1267515533,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "CjwxjqXOKYE1zbZ-NBw5A",
          "type": "arrow"
        }
      ],
      "updated": 1641735908380,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Vectorize",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Vectorize"
    },
    {
      "type": "arrow",
      "version": 2357,
      "versionNonce": 1789679027,
      "isDeleted": false,
      "id": "UldaTf_SD8SLGi5U0s3az",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 662.8742386138015,
      "y": 381.70953615760675,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ffeb3b",
      "width": 115.84862824570905,
      "height": 39.38975678911993,
      "seed": 1103614243,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908380,
      "startBinding": {
        "elementId": "m9UEIWpFo5o-W_PZIjS5i",
        "focus": -0.05280483881151566,
        "gap": 11.810330278691765
      },
      "endBinding": {
        "elementId": "UO_8s06e5WPXOXcLW1Gji",
        "focus": -1.8872552019606,
        "gap": 9.60046258817383
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          115.84862824570905,
          39.38975678911993
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 852,
      "versionNonce": 624120573,
      "isDeleted": false,
      "id": "eDQoIxWK3cF_GuzA8jGlK",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 664.8519551997895,
      "y": 376.6331008210118,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#ffeb3b",
      "width": 330.68241691348294,
      "height": 116.55682104186076,
      "seed": 1088931821,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908380,
      "startBinding": {
        "elementId": "m9UEIWpFo5o-W_PZIjS5i",
        "focus": 0.3821910694005074,
        "gap": 13.788046864679757
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          330.68241691348294,
          -116.55682104186076
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 963,
      "versionNonce": 2023565139,
      "isDeleted": false,
      "id": "CjwxjqXOKYE1zbZ-NBw5A",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 863.6765930965225,
      "y": 445.6574693739797,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ffeb3b",
      "width": 137.0993469797395,
      "height": 39.74595185737945,
      "seed": 1654741539,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908380,
      "startBinding": {
        "elementId": "Rc3sIwmWEVTP-seViLAF-",
        "focus": -1.3712294571679748,
        "gap": 10.342530626020277
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          137.0993469797395,
          39.74595185737945
        ]
      ]
    },
    {
      "type": "text",
      "version": 879,
      "versionNonce": 1219095389,
      "isDeleted": false,
      "id": "jTu74ufZGB6NCYWR8F6cX",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1409.561741267423,
      "y": 407.263410502541,
      "strokeColor": "#343a40",
      "backgroundColor": "#868e96",
      "width": 65,
      "height": 25,
      "seed": 1925987693,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908381,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Search",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "rectangle",
      "version": 1230,
      "versionNonce": 1606599923,
      "isDeleted": false,
      "id": "94RaVBybGnyM0x9Gqgwk5",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0.7882111358327366,
      "x": 1433.0842846752632,
      "y": 375.5724519986199,
      "strokeColor": "#000000",
      "backgroundColor": "#868e96",
      "width": 70.38608542855415,
      "height": 6.900596610642562,
      "seed": 489839587,
      "groupIds": [
        "_mSpNqaGUToycQhKOY82U"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908381
    },
    {
      "type": "ellipse",
      "version": 1066,
      "versionNonce": 107902909,
      "isDeleted": false,
      "id": "x9CPx10TrNm9T_lgZgUAM",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1401.9154962081336,
      "y": 314.9184076792772,
      "strokeColor": "#000000",
      "backgroundColor": "#868e96",
      "width": 66.0077798878363,
      "height": 66.0077798878363,
      "seed": 1251739597,
      "groupIds": [
        "_mSpNqaGUToycQhKOY82U"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908381
    },
    {
      "type": "ellipse",
      "version": 1153,
      "versionNonce": 933473939,
      "isDeleted": false,
      "id": "EcCeRt1xfAokj2Az9b8OW",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1411.809595977187,
      "y": 325.8539916345475,
      "strokeColor": "#000000",
      "backgroundColor": "white",
      "width": 47.35405419542672,
      "height": 47.35405419542672,
      "seed": 1122166659,
      "groupIds": [
        "_mSpNqaGUToycQhKOY82U"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908381
    },
    {
      "type": "text",
      "version": 1087,
      "versionNonce": 1218607133,
      "isDeleted": false,
      "id": "haJCsmkSJ7-AvLSKoAVa0",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1293.0909090909092,
      "y": 248.72727272727272,
      "strokeColor": "#343a40",
      "backgroundColor": "#fd7e14",
      "width": 297,
      "height": 26,
      "seed": 1064787981,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "5m7yE6eTRKFfSCbA82DrS",
          "type": "arrow"
        },
        {
          "id": "ZyDOc33tL-6s4vwoFYHAq",
          "type": "arrow"
        }
      ],
      "updated": 1641735908381,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "SQL> SELECT {} FROM txtai",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "SQL> SELECT {} FROM txtai"
    },
    {
      "type": "text",
      "version": 1452,
      "versionNonce": 1660314675,
      "isDeleted": false,
      "id": "M5hw4wHqIGw3PXnaLCp6p",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1307.1818181818182,
      "y": 478.18181818181836,
      "strokeColor": "#343a40",
      "backgroundColor": "#fd7e14",
      "width": 261,
      "height": 26,
      "seed": 1708000109,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "-D1yDTCnqxjTFsQU0jBjN",
          "type": "arrow"
        }
      ],
      "updated": 1641735908381,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "> Natural Language Query",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "> Natural Language Query"
    },
    {
      "type": "arrow",
      "version": 1819,
      "versionNonce": 959933565,
      "isDeleted": false,
      "id": "5m7yE6eTRKFfSCbA82DrS",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1272.868226475393,
      "y": 268.6451583650937,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 188.8189376264454,
      "height": 219.36950558832018,
      "seed": 1269992291,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908382,
      "startBinding": {
        "elementId": "UsRZ6Y682zf4FIZyNDOtq",
        "focus": 0.900241545557177,
        "gap": 8.040864433697834
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -188.8189376264454,
          219.36950558832018
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 1276,
      "versionNonce": 698443219,
      "isDeleted": false,
      "id": "ZyDOc33tL-6s4vwoFYHAq",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1077.7543216412146,
      "y": 263.29298489476895,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#7950f2",
      "width": 199.76731534744795,
      "height": 0.40309918148193447,
      "seed": 343568205,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908382,
      "startBinding": null,
      "endBinding": {
        "elementId": "haJCsmkSJ7-AvLSKoAVa0",
        "focus": -0.17292767152914681,
        "gap": 15.569272102246714
      },
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          199.76731534744795,
          0.40309918148193447
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 1786,
      "versionNonce": 1206834397,
      "isDeleted": false,
      "id": "-D1yDTCnqxjTFsQU0jBjN",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.2935337071021,
      "y": 493.40248820420277,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 193.34252933148332,
      "height": 3.1975454283124236,
      "seed": 855220461,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641735908382,
      "startBinding": {
        "elementId": "YI607mwBteKrjS78gtTXD",
        "focus": -0.13787667355971328,
        "gap": 6.615557201988622
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -193.34252933148332,
          -3.1975454283124236
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/install.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 1435,
      "versionNonce": 1860045045,
      "isDeleted": false,
      "id": "LgVOdCQnXRH2GYHIGLwOx",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 764,
      "y": 123.99999999999991,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 20.000000000000092,
      "height": 199.99999999999991,
      "seed": 782310408,
      "groupIds": [
        "s5mCNZzJxsETmWhZoD3qR"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688311151676,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1534,
      "versionNonce": 2090527963,
      "isDeleted": false,
      "id": "GG_mFtlk6nEiYCQ8vhzjZ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 953,
      "y": 136.9999999999999,
      "strokeColor": "#ced4da",
      "backgroundColor": "#ced4da",
      "width": 17.99999999999998,
      "height": 173.99999999999994,
      "seed": 810177544,
      "groupIds": [
        "Rezl6DUXkEY8GT9kuDOQX"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688311151677,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 293,
      "versionNonce": 1618682453,
      "isDeleted": false,
      "id": "NLfZWFLgvJj3ccuF5yxD8",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 793,
      "y": 216.86805555555557,
      "strokeColor": "#343a40",
      "backgroundColor": "transparent",
      "width": 80,
      "height": 16,
      "seed": 297813880,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688311151677,
      "link": null,
      "locked": false,
      "fontSize": 12.242424242424246,
      "fontFamily": 1,
      "text": "Installing.......",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Installing.......",
      "lineHeight": 1.3069306930693065,
      "baseline": 11
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/llm.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "text",
      "version": 1159,
      "versionNonce": 947191573,
      "isDeleted": false,
      "id": "yF7ftUwr3mnAwOlC59RMi",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1475.7768832949857,
      "y": 232.08688631142672,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 268.6666564941406,
      "height": 33.6,
      "seed": 376471794,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335913281,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Workflow Processing",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Workflow Processing",
      "lineHeight": 1.2,
      "baseline": 24
    },
    {
      "type": "line",
      "version": 1539,
      "versionNonce": 1192947061,
      "isDeleted": false,
      "id": "yVN5CxzfpwAZ0J0GEsEim",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1595.7169087648438,
      "y": 44.55863019278951,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 170.54791263920666,
      "height": 165.88781924917737,
      "seed": 1276926237,
      "groupIds": [
        "ZD4Gm6iTkUS0-nxoYPoXN"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          0
        ],
        [
          -4.396962054402082,
          1.1646874339146507
        ],
        [
          -63.9402715195358,
          29.56596636773484
        ],
        [
          -63.9402715195358,
          29.56596636773484
        ],
        [
          -67.88778686931518,
          32.68804950762777
        ],
        [
          -70.0842642471441,
          37.216572224302446
        ],
        [
          -84.65888566945969,
          100.95916519983551
        ],
        [
          -84.65888566945969,
          100.95916519983551
        ],
        [
          -84.76907687141536,
          105.46565159474407
        ],
        [
          -83.05610475455529,
          109.6515846607256
        ],
        [
          -82.41499236471047,
          110.54913805780339
        ],
        [
          -41.27695016258076,
          161.6992265392742
        ],
        [
          -41.27695016258076,
          161.6992265392742
        ],
        [
          -37.31206815320823,
          164.8012680903417
        ],
        [
          -32.397540410633006,
          165.88781924917737
        ],
        [
          33.58360912709668,
          165.88781924917697
        ],
        [
          33.58360912709668,
          165.88781924917697
        ],
        [
          38.509483664717834,
          164.7592029175085
        ],
        [
          42.47369821555802,
          161.6511389543416
        ],
        [
          83.590366618572,
          110.49037113635723
        ],
        [
          83.590366618572,
          110.49037113635723
        ],
        [
          85.77883576779129,
          105.97187048787097
        ],
        [
          85.7274057950955,
          100.94848586332202
        ],
        [
          71.05662649840329,
          37.1524605394965
        ],
        [
          71.05662649840329,
          37.1524605394965
        ],
        [
          68.85613290539939,
          32.62393782282196
        ],
        [
          64.91262485436425,
          29.50185468292866
        ],
        [
          5.465486737767179,
          1.1700299681670856
        ],
        [
          5.465486737767179,
          1.1700299681670856
        ],
        [
          -0.053422158085595584,
          -3.469446951953614e-18
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "line",
      "version": 4960,
      "versionNonce": 2121044059,
      "isDeleted": false,
      "id": "aEfZn91vFxUsNoiS_231h",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1881.7866460270468,
      "y": -65.38351814097682,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fa5252",
      "width": 84.50536343081912,
      "height": 109.06758737884206,
      "seed": 1361500846,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.2786552913217619,
          82.43269138641281
        ],
        [
          0.013040413480105428,
          91.8172953399838
        ],
        [
          4.352213112689576,
          95.87226666144052
        ],
        [
          19.46316552700477,
          99.30408539980334
        ],
        [
          45.00495554826318,
          100.37270248918188
        ],
        [
          69.40844688139491,
          98.66649877125488
        ],
        [
          82.37420150566703,
          94.58614343882556
        ],
        [
          84.20284574710968,
          91.14636108917688
        ],
        [
          84.45967221106837,
          83.5908848820192
        ],
        [
          84.25809330040309,
          6.915642702567624
        ],
        [
          83.80366973142185,
          -0.3287549387484745
        ],
        [
          78.37757288423978,
          -4.377696435676519
        ],
        [
          66.95137090060692,
          -6.722629103443413
        ],
        [
          40.91260040265366,
          -8.694884889660191
        ],
        [
          20.036110369548158,
          -7.518824323429054
        ],
        [
          3.616892738261588,
          -3.529770380085963
        ],
        [
          -0.04569121975075156,
          -0.049530818049170484
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "line",
      "version": 2704,
      "versionNonce": 1487369941,
      "isDeleted": false,
      "id": "JSkCiTP6nfh3_0WXfktLt",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1887.3924109908396,
      "y": -3.2136630318863695,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 78.18555964094003,
      "height": 8.95436195346155,
      "seed": 1565257522,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          2.228662812409231,
          3.741306533812953
        ],
        [
          11.83996414096264,
          6.880228333571024
        ],
        [
          24.62930590722697,
          8.7811403955643
        ],
        [
          44.66540707207591,
          8.95436195346155
        ],
        [
          68.04684957381593,
          7.744137021248128
        ],
        [
          78.18555964094003,
          5.099524329682907
        ],
        [
          76.97433252487917,
          5.429165362754105
        ]
      ]
    },
    {
      "type": "line",
      "version": 2790,
      "versionNonce": 13618427,
      "isDeleted": false,
      "id": "XbZa4_v84aJkwUbHNYtWo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1881.5978130909527,
      "y": -29.55045127271626,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 84.59301845781613,
      "height": 9.385724231428382,
      "seed": 1628656878,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          2.228662812409231,
          3.7413065338129523
        ],
        [
          11.83996414096264,
          6.880228333571017
        ],
        [
          24.62930590722697,
          8.7811403955643
        ],
        [
          44.66540707207591,
          8.95436195346155
        ],
        [
          68.04684957381593,
          7.744137021248123
        ],
        [
          81.70187622537257,
          3.341366037466631
        ],
        [
          84.59301845781613,
          -0.43136227796683113
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 5809,
      "versionNonce": 170997243,
      "isDeleted": false,
      "id": "WgZ_UNGw6chJCNh_YqBCg",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1883.146078835665,
      "y": -74.22048622222516,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 83.96333219036016,
      "height": 16.980952253415385,
      "seed": 1814771954,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691336141044,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1151,
      "versionNonce": 1596584437,
      "isDeleted": false,
      "id": "5zDzUgYAzqCuHz5y0fEbO",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1949.5714949045796,
      "y": -45.83125704870746,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 12.305631190363249,
      "height": 13.3553768715004,
      "seed": 704022318,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691336146522,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1200,
      "versionNonce": 1743051285,
      "isDeleted": false,
      "id": "VtzlusDzgSLcDDBt5hG2x",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1949.5714949045796,
      "y": -16.516229929297197,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 12.305631190363249,
      "height": 13.3553768715004,
      "seed": 2080066226,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691336152276,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1253,
      "versionNonce": 1752124155,
      "isDeleted": false,
      "id": "B_0DhVcOs-USQL4XV0QRo",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1949.5714949045796,
      "y": 15.345309440526933,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 12.305631190363249,
      "height": 13.3553768715004,
      "seed": 1278833006,
      "groupIds": [
        "YlFNaYy8uxiVBIOqH078l"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691336157480,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1070,
      "versionNonce": 652075765,
      "isDeleted": false,
      "id": "4zMdwTCbTrrgg-ITOMXC0",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1779.4509372464736,
      "y": 51.50880129297349,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 298.8666687011719,
      "height": 33.6,
      "seed": 1848588206,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1691336802938,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Embeddings Database",
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Embeddings Database",
      "lineHeight": 1.2,
      "baseline": 24
    },
    {
      "type": "arrow",
      "version": 3186,
      "versionNonce": 1016072923,
      "isDeleted": false,
      "id": "iPTphyfJkvqyMptaZS0ge",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1878.6382080826127,
      "y": -7.322815845013395,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 202.4871709755887,
      "height": 104.80578637477413,
      "seed": 1965679388,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -202.4871709755887,
          104.80578637477413
        ]
      ]
    },
    {
      "type": "text",
      "version": 906,
      "versionNonce": 1278878741,
      "isDeleted": false,
      "id": "YCEwpQiKAjRS3zIG3_E8s",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1847.5109174458237,
      "y": 281.40729511478503,
      "strokeColor": "#7950f2",
      "backgroundColor": "transparent",
      "width": 159.76666259765625,
      "height": 33.6,
      "seed": 676249636,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335908714,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "LLM Models",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "LLM Models",
      "lineHeight": 1.2,
      "baseline": 24
    },
    {
      "type": "arrow",
      "version": 3832,
      "versionNonce": 1540672693,
      "isDeleted": false,
      "id": "leDc6Y5qdU0OH3j90C072",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1828.6470213908196,
      "y": 222.85602296508534,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 145.94742162238094,
      "height": 61.13637008509096,
      "seed": 241264412,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877141,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "ZTKEryJuyeKfeG4nAI4_e",
        "focus": -0.0827769297585387,
        "gap": 11.926065811794956
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": "arrow",
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -145.94742162238094,
          -61.13637008509096
        ]
      ]
    },
    {
      "type": "line",
      "version": 2313,
      "versionNonce": 303665589,
      "isDeleted": false,
      "id": "aFBJSnHUVo8ZDTb0uAkQ2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1294.8750417574363,
      "y": -49.56118957084571,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 116.42036295658872,
      "height": 103.65107323746608,
      "seed": 1963890941,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -62.44191743896485,
          19.19929080548739
        ],
        [
          -63.17668831316513,
          79.43840749607878
        ],
        [
          -7.618334228588694,
          103.65107323746608
        ],
        [
          51.963117173367294,
          79.15871076413049
        ],
        [
          53.24367464342358,
          21.28567723840068
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "text",
      "version": 876,
      "versionNonce": 1665749019,
      "isDeleted": false,
      "id": "eqOUg003X-50uDJ_XGUnB",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1186.6810892450637,
      "y": 59.89380735137763,
      "strokeColor": "#ffb13b",
      "backgroundColor": "#00e676",
      "width": 208.96665954589844,
      "height": 33.6,
      "seed": 1122464339,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Workflow Start",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Workflow Start",
      "lineHeight": 1.2,
      "baseline": 24
    },
    {
      "type": "rectangle",
      "version": 1789,
      "versionNonce": 1288147733,
      "isDeleted": false,
      "id": "ZTKEryJuyeKfeG4nAI4_e",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1840.5730872026145,
      "y": 200.79305844892008,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 23.299119994671287,
      "height": 58.247799986678125,
      "seed": 1188509340,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1934,
      "versionNonce": 2028890229,
      "isDeleted": false,
      "id": "CckqOoWddQBHuvidbpk4P",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1888.8925200747174,
      "y": 145.68194499341337,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 23.29911999467125,
      "height": 116.4955999733563,
      "seed": 331107492,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1815,
      "versionNonce": 568477019,
      "isDeleted": false,
      "id": "J8ox4BBdzEg-04DGRL-jh",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1932.9361110371237,
      "y": 180.63062498542024,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 23.29911999467127,
      "height": 81.54691998134939,
      "seed": 1316437916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2003,
      "versionNonce": 408130005,
      "isDeleted": false,
      "id": "V54vdkZmQeI13AEahvNPV",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1974.6102076249463,
      "y": 155.6767771383196,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 23.29911999467125,
      "height": 104.84603997602068,
      "seed": 1025829916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "arrow",
      "version": 279,
      "versionNonce": 1975968251,
      "isDeleted": false,
      "id": "Inol-LWi8GThocPMvSZGX",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1347.5263615107983,
      "y": 20.516421378236373,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "transparent",
      "width": 168.1111111111113,
      "height": 78.7777777777778,
      "seed": 1433457683,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          168.1111111111113,
          78.7777777777778
        ]
      ]
    },
    {
      "type": "line",
      "version": 2357,
      "versionNonce": 1477218101,
      "isDeleted": false,
      "id": "8H60JYjdZgpvCjCSLf54L",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1292.9959980885017,
      "y": 174.32286623612742,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 116.42036295658872,
      "height": 103.65107323746608,
      "seed": 1027783997,
      "groupIds": [],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -62.44191743896485,
          19.19929080548739
        ],
        [
          -63.17668831316513,
          79.43840749607878
        ],
        [
          -7.618334228588694,
          103.65107323746608
        ],
        [
          51.963117173367294,
          79.15871076413049
        ],
        [
          53.24367464342358,
          21.28567723840068
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "text",
      "version": 823,
      "versionNonce": 1747905179,
      "isDeleted": false,
      "id": "E_wil3E0D2OO7QeMf_xiu",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1194.426360493546,
      "y": 283.38308804490305,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 177.48333740234375,
      "height": 33.6,
      "seed": 754233971,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Workflow End",
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Workflow End",
      "lineHeight": 1.2,
      "baseline": 24
    },
    {
      "type": "arrow",
      "version": 385,
      "versionNonce": 645849237,
      "isDeleted": false,
      "id": "5pptt-wBgcgbxd5LUka3S",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1514.4152503996868,
      "y": 167.62753248934757,
      "strokeColor": "#00e676",
      "backgroundColor": "#34bbde",
      "width": 171.8888888888889,
      "height": 57.1111111111112,
      "seed": 603518365,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -171.8888888888889,
          57.1111111111112
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 1775,
      "versionNonce": 768401211,
      "isDeleted": false,
      "id": "GPI87MG84w7ACdrAaKDkG",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 40,
      "angle": 0,
      "x": 1553.9985837330207,
      "y": 157.35401003071968,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 28.340425531915425,
      "height": 27.553191489362284,
      "seed": 1372805341,
      "groupIds": [
        "Q6IEbA3c43OUEmWwOqoB-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1683,
      "versionNonce": 157287925,
      "isDeleted": false,
      "id": "x4O7WOdzCU_nAV1UPLb10",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 40,
      "angle": 0,
      "x": 1554.392200754299,
      "y": 90.61996747752698,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 28.340425531915475,
      "height": 25.978723404255863,
      "seed": 1393440061,
      "groupIds": [
        "Q6IEbA3c43OUEmWwOqoB-"
      ],
      "frameId": null,
      "roundness": {
        "type": 1
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "ellipse",
      "version": 1831,
      "versionNonce": 1090731995,
      "isDeleted": false,
      "id": "OAHRxshgLCmGZDCtG_FGl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 40,
      "angle": 0,
      "x": 1618.1581582011052,
      "y": 90.22635045624997,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 28.340425531915425,
      "height": 27.553191489362284,
      "seed": 896435613,
      "groupIds": [
        "Q6IEbA3c43OUEmWwOqoB-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 1777,
      "versionNonce": 867532629,
      "isDeleted": false,
      "id": "uB-LQPQxLXAYzpMkkFNId",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 40,
      "angle": 0,
      "x": 1583.4087576843299,
      "y": 102.97914403772018,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 35.87707425165013,
      "height": 0,
      "seed": 472238589,
      "groupIds": [
        "Q6IEbA3c43OUEmWwOqoB-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          35.87707425165013,
          0
        ]
      ]
    },
    {
      "type": "line",
      "version": 1658,
      "versionNonce": 736398459,
      "isDeleted": false,
      "id": "AP8DlX5fhd-adRn_4ictA",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 40,
      "angle": 0,
      "x": 1568.1322242882986,
      "y": 117.53634476915352,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 0,
      "height": 37.78723404255394,
      "seed": 1211466333,
      "groupIds": [
        "Q6IEbA3c43OUEmWwOqoB-"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1691335877136,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          37.78723404255394
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/models.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "line",
      "version": 1608,
      "versionNonce": 602661205,
      "isDeleted": false,
      "id": "yVN5CxzfpwAZ0J0GEsEim",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1702.2261496777674,
      "y": -5.340583377694145,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 191.85486538200286,
      "height": 186.6125755399512,
      "seed": 1276926237,
      "groupIds": [
        "ZD4Gm6iTkUS0-nxoYPoXN"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688309214322,
      "link": null,
      "locked": false,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          0
        ],
        [
          -4.946284888409469,
          1.310194580443298
        ],
        [
          -71.92848036094448,
          33.259712239166916
        ],
        [
          -71.92848036094448,
          33.259712239166916
        ],
        [
          -76.36916810848369,
          36.77184458512375
        ],
        [
          -78.84005658267724,
          41.86612631945928
        ],
        [
          -95.23551981468633,
          113.57223168992945
        ],
        [
          -95.23551981468633,
          113.57223168992945
        ],
        [
          -95.35947746324585,
          118.64172405287644
        ],
        [
          -93.4324997019961,
          123.35061560390292
        ],
        [
          -92.71129163005317,
          124.36030246259705
        ],
        [
          -46.433776844719105,
          181.9006920694186
        ],
        [
          -46.433776844719105,
          181.9006920694186
        ],
        [
          -41.973552779866964,
          185.39027898361584
        ],
        [
          -36.44504150185128,
          186.6125755399512
        ],
        [
          37.77928857887258,
          186.61257553995074
        ],
        [
          37.77928857887258,
          186.61257553995074
        ],
        [
          43.320564233786016,
          185.34295850958426
        ],
        [
          47.78003745293803,
          181.84659678914795
        ],
        [
          94.03350816005033,
          124.29419365112881
        ],
        [
          94.03350816005033,
          124.29419365112881
        ],
        [
          96.49538791875702,
          119.21118606558457
        ],
        [
          96.43753267834124,
          113.56021815873193
        ],
        [
          79.9338983420456,
          41.794005010746
        ],
        [
          79.9338983420456,
          41.794005010746
        ],
        [
          77.45849189744823,
          36.69972327641062
        ],
        [
          73.02231208993142,
          33.18759093045337
        ],
        [
          6.148302879201523,
          1.3162045700933498
        ],
        [
          6.148302879201523,
          1.3162045700933498
        ],
        [
          -0.060096314222328626,
          -3.469446951953614e-18
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 2051,
      "versionNonce": 216616981,
      "isDeleted": false,
      "id": "ZTKEryJuyeKfeG4nAI4_e",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2003.5691665051115,
      "y": 81.12265056081542,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 30.25969046240687,
      "height": 75.64922615601705,
      "seed": 1188509340,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688309215087,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2192,
      "versionNonce": 1390810587,
      "isDeleted": false,
      "id": "CckqOoWddQBHuvidbpk4P",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2066.323943610997,
      "y": 9.547190432552021,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 30.25969046240682,
      "height": 151.29845231203416,
      "seed": 331107492,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "leDc6Y5qdU0OH3j90C072",
          "type": "arrow"
        }
      ],
      "updated": 1688308769366,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2072,
      "versionNonce": 1184041301,
      "isDeleted": false,
      "id": "J8ox4BBdzEg-04DGRL-jh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2123.5254786767036,
      "y": 54.936726126162256,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 30.259690462406844,
      "height": 105.90891661842389,
      "seed": 1316437916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "5pptt-wBgcgbxd5LUka3S",
          "type": "arrow"
        }
      ],
      "updated": 1688308769366,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2259,
      "versionNonce": 1727832699,
      "isDeleted": false,
      "id": "V54vdkZmQeI13AEahvNPV",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2177.649637157211,
      "y": 22.527960822066646,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 30.25969046240682,
      "height": 136.16860708083075,
      "seed": 1025829916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688308769366,
      "link": null,
      "locked": false
    },
    {
      "type": "arrow",
      "version": 442,
      "versionNonce": 1922817787,
      "isDeleted": false,
      "id": "Inol-LWi8GThocPMvSZGX",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1374.656480958798,
      "y": 99.23064378133208,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 216.36132864000353,
      "height": 0.12505261106101684,
      "seed": 1653816315,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688309230624,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          216.36132864000353,
          -0.12505261106101684
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 917,
      "versionNonce": 151223483,
      "isDeleted": false,
      "id": "5pptt-wBgcgbxd5LUka3S",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1802.88085185622,
      "y": 95.10577155105176,
      "strokeColor": "#00e676",
      "backgroundColor": "#34bbde",
      "width": 184.23585514045453,
      "height": 3.170483019063738,
      "seed": 1717004827,
      "groupIds": [],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688309218767,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          184.23585514045453,
          3.170483019063738
        ]
      ]
    },
    {
      "type": "line",
      "version": 4226,
      "versionNonce": 789213499,
      "isDeleted": false,
      "id": "pikmP5MLN0MZlpnSik5KN",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1235.1617590461694,
      "y": 25.06522848370164,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 127.43688004487302,
      "height": 164.47752527518452,
      "seed": 844586005,
      "groupIds": [
        "xkmNAKuTv_iYXPLj0sxbd"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1688309232141,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.4202213858663673,
          124.31122212244284
        ],
        [
          0.019665374373081804,
          138.46351494441197
        ],
        [
          6.563281167657464,
          144.5785456702537
        ],
        [
          29.351096662508862,
          149.75384171231534
        ],
        [
          67.86895989535736,
          151.36535158937204
        ],
        [
          104.67023109801114,
          148.79234001109992
        ],
        [
          124.22301744981843,
          142.63902935804376
        ],
        [
          126.98067338289118,
          137.45172392705157
        ],
        [
          127.36797617588668,
          126.05781617962573
        ],
        [
          127.06398851859365,
          10.429017682904831
        ],
        [
          126.37870276277025,
          -0.4957733094390701
        ],
        [
          118.19596944321839,
          -6.601710860670866
        ],
        [
          100.96487933911183,
          -10.137946798406993
        ],
        [
          61.697553127560816,
          -13.11217368581247
        ],
        [
          30.215116414714352,
          -11.338635495813458
        ],
        [
          5.454393748609269,
          -5.323010354025768
        ],
        [
          -0.06890386898634299,
          -0.074694110077688
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 5067,
      "versionNonce": 1912378357,
      "isDeleted": false,
      "id": "sPZv-4m6hv0RYSW1Kzrrb",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1235.9555759014816,
      "y": 13.472824531333586,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 126.61947902597214,
      "height": 25.607837035548464,
      "seed": 194842997,
      "groupIds": [
        "xkmNAKuTv_iYXPLj0sxbd"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1688309232141,
      "link": null,
      "locked": false
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/pipeline.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 4788,
      "versionNonce": 1348021112,
      "isDeleted": false,
      "id": "9itz4bqXi6IMeZhGhc42J",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 793.1319708150991,
      "y": 84.01158837080237,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2026142776,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "sbyEF_hTthhrIaA-47du2",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1641741755942
    },
    {
      "type": "rectangle",
      "version": 4836,
      "versionNonce": 1572562952,
      "isDeleted": false,
      "id": "7HUYNqBGn8JbkXw6qt_jr",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 785.5845377793848,
      "y": 76.99149908508815,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1090159432,
      "groupIds": [
        "6R5o6OnVZ7fFC_87zCo7G",
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1641741755942
    },
    {
      "type": "rectangle",
      "version": 4932,
      "versionNonce": 2081750136,
      "isDeleted": false,
      "id": "mI_Mfw3cNZf0AuQhK5KUO",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 776.1894484936695,
      "y": 68.73759283508838,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1146631480,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1641741755942
    },
    {
      "type": "line",
      "version": 4055,
      "versionNonce": 958473992,
      "isDeleted": false,
      "id": "y3oKPiP36axXzEF0c7OiC",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 788.1758750805805,
      "y": 117.48482988244103,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1952111176,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755942,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4081,
      "versionNonce": 1110355320,
      "isDeleted": false,
      "id": "-52Udo1sfovtw-gxgUT91",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 790.0311273185948,
      "y": 86.01954258360246,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 923586104,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4106,
      "versionNonce": 1064764936,
      "isDeleted": false,
      "id": "7Tgba-Lgh4JYVwHG-MlYn",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 789.9545278022254,
      "y": 153.41391685395956,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 664293704,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4143,
      "versionNonce": 1542078072,
      "isDeleted": false,
      "id": "KsHbDZNNeZoS7zbhCYNon",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 786.770480620695,
      "y": 164.65706526223835,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 2043037496,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4108,
      "versionNonce": 1572406536,
      "isDeleted": false,
      "id": "R5nE2rgdgOpe93Q8VvYhl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.0553235162608,
      "y": 101.08014413686368,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 571402312,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4123,
      "versionNonce": 973844344,
      "isDeleted": false,
      "id": "lfWLpL8lTqrf2GMoqJY8y",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.1628117124112,
      "y": 135.5634731727032,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 2028511288,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 3006,
      "versionNonce": 23274504,
      "isDeleted": false,
      "id": "8X7QMyL8Lv5BuDjYinx6w",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 1013.7787705852069,
      "y": 125.15514857126914,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 50.7174766392476,
      "height": 12.698053371678215,
      "seed": 1906238776,
      "groupIds": [
        "3orQz7tgci8hTh4ECgIAF"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          1.3361877396713384,
          5.061656285093649
        ],
        [
          7.098613049589299,
          9.308339392337079
        ],
        [
          14.766422451441104,
          11.880105003906111
        ],
        [
          26.779003528407447,
          12.114458425450186
        ],
        [
          40.79727342221974,
          10.477131310135727
        ],
        [
          48.98410145879092,
          4.5205722256349645
        ],
        [
          50.7174766392476,
          -0.5835949462280285
        ]
      ]
    },
    {
      "type": "line",
      "version": 5373,
      "versionNonce": 349634680,
      "isDeleted": false,
      "id": "A4PSBtsGIGM5bIdhtvs22",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 1038.306180989564,
      "y": 66.05019201370396,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 52.317507746132115,
      "height": 154.56722543646003,
      "seed": 1241568072,
      "groupIds": [
        "3orQz7tgci8hTh4ECgIAF"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755943,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.1725162731731403,
          116.82107121890462
        ],
        [
          0.008073356596080744,
          130.12064288619618
        ],
        [
          2.694467356765587,
          135.86722334556717
        ],
        [
          12.049700419984944,
          140.7306911579349
        ],
        [
          27.86269432990675,
          142.2451023824676
        ],
        [
          42.97096432627199,
          139.82712302629713
        ],
        [
          50.998099415101294,
          134.0445691284302
        ],
        [
          52.13021819883883,
          129.16981553143583
        ],
        [
          52.28922018370778,
          118.46242736729553
        ],
        [
          52.16442211418855,
          9.800635828982735
        ],
        [
          51.88308720685254,
          -0.46590137319512337
        ],
        [
          48.52377541516831,
          -6.203936550968926
        ],
        [
          41.449781688403746,
          -9.527102901326515
        ],
        [
          25.329105770103325,
          -12.322123053992442
        ],
        [
          12.404412180527922,
          -10.655446243436785
        ],
        [
          2.239228448568725,
          -5.00228186200372
        ],
        [
          -0.028287562424331975,
          -0.07019354973772352
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "line",
      "version": 3140,
      "versionNonce": 141648648,
      "isDeleted": false,
      "id": "w17jKx2sTAqGcj5vlmOMy",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 1059.615995470046,
      "y": 125.12063304384947,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 50.57247907260371,
      "height": 11.002162473203482,
      "seed": 672606792,
      "groupIds": [
        "3orQz7tgci8hTh4ECgIAF"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755944,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          1.332367676378171,
          4.385645831062678
        ],
        [
          7.078318632616268,
          8.065162379820556
        ],
        [
          14.724206326638113,
          10.293455353024548
        ],
        [
          26.70244431044034,
          10.496509659421628
        ],
        [
          40.68063699304561,
          9.07785607393191
        ],
        [
          48.84405948536458,
          3.9168263545984514
        ],
        [
          50.57247907260371,
          -0.5056528137818528
        ]
      ]
    },
    {
      "type": "ellipse",
      "version": 6107,
      "versionNonce": 2081436024,
      "isDeleted": false,
      "id": "sxpRnxFiIOgK8uOIc81pk",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.5707963267948957,
      "x": 1105.8093248657638,
      "y": 119.64103777538094,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 51.27812853552538,
      "height": 22.797152568995934,
      "seed": 1744666168,
      "groupIds": [
        "3orQz7tgci8hTh4ECgIAF"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "bxuMGTzXLn7H-uBCptINx"
        },
        {
          "id": "ppSKhzeVi1x47qPOxgohP",
          "type": "arrow"
        },
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1641741755944
    },
    {
      "type": "text",
      "version": 903,
      "versionNonce": 828359176,
      "isDeleted": false,
      "id": "DAMG-SGC-r93uwzYgaFaM",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 961,
      "y": 165.3106382978724,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 213,
      "height": 80,
      "seed": 905916728,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "ppSKhzeVi1x47qPOxgohP",
          "type": "arrow"
        }
      ],
      "updated": 1641741755944,
      "fontSize": 16.51063829787236,
      "fontFamily": 3,
      "text": "[0.114, 0.344, 0.589],\n[0.894, 0.448, 0.385],\n[0.639, 0.785, 0.546],\n[0.789, 0.248, 0.187]",
      "baseline": 76,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "[0.114, 0.344, 0.589],\n[0.894, 0.448, 0.385],\n[0.639, 0.785, 0.546],\n[0.789, 0.248, 0.187]"
    },
    {
      "type": "rectangle",
      "version": 3994,
      "versionNonce": 1727742584,
      "isDeleted": false,
      "id": "0kwQuYpKaDvZuuKsvL7r5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1392.3414205264805,
      "y": 66.34139380028421,
      "strokeColor": "#00e676",
      "backgroundColor": "#d0d9dd",
      "width": 74.26505724978897,
      "height": 53.22792745407822,
      "seed": 1723506296,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "XV1prKL4Cu6Tu_cyztWSc"
        }
      ],
      "updated": 1641741755944
    },
    {
      "type": "text",
      "version": 3911,
      "versionNonce": 187318536,
      "isDeleted": false,
      "id": "4eS6gRMMGH5z1mWIMwlWa",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1406.4841911219582,
      "y": 64.65315479829201,
      "strokeColor": "#00e676",
      "backgroundColor": "#d0d9dd",
      "width": 47.44228316559144,
      "height": 24.299706011644403,
      "seed": 1855890696,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945,
      "fontSize": 18.51406172315763,
      "fontFamily": 1,
      "text": "Text",
      "baseline": 17.299706011644403,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Text"
    },
    {
      "type": "rectangle",
      "version": 3610,
      "versionNonce": 1146893176,
      "isDeleted": false,
      "id": "PVROaX-PjeUDmh4MQbTl8",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1250.6356656002554,
      "y": 86.0327898598672,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 224.03197101896893,
      "height": 53.22792745407822,
      "seed": 1271926648,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3670,
      "versionNonce": 1699536904,
      "isDeleted": false,
      "id": "ugOritv0_fPYbcZUm6Uu4",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1250.8971808651277,
      "y": 111.8916716422263,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 224.03197101896893,
      "height": 26.01198231582164,
      "seed": 613729288,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3765,
      "versionNonce": 175572088,
      "isDeleted": false,
      "id": "eio4Ow124ISNuI0VAUUjS",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1250.7639028142198,
      "y": 137.72914909405728,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 224.03197101896893,
      "height": 26.01198231582164,
      "seed": 789536888,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3856,
      "versionNonce": 1069784840,
      "isDeleted": false,
      "id": "_6fRtYyVCjDNRCEwdM5Cy",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1250.5409710428607,
      "y": 163.83262915775464,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 224.03197101896893,
      "height": 26.01198231582164,
      "seed": 342819592,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3800,
      "versionNonce": 1192211832,
      "isDeleted": false,
      "id": "ZqV86lTip0q28AN63sD4l",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1251.72784114917,
      "y": 86.97311738448298,
      "strokeColor": "#00e676",
      "backgroundColor": "#e6e6e7",
      "width": 63.93385920866094,
      "height": 24.3311416053828,
      "seed": 1883017592,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3844,
      "versionNonce": 990543368,
      "isDeleted": false,
      "id": "YChnGbrEsJ7DoNKmra0ci",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1251.9893564140477,
      "y": 112.83199916684202,
      "strokeColor": "#00e676",
      "backgroundColor": "#e6e6e7",
      "width": 63.93385920866094,
      "height": 24.3311416053828,
      "seed": 749298184,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 3953,
      "versionNonce": 795494008,
      "isDeleted": false,
      "id": "2izY3o3TkA3Cx67YfzF9f",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1251.8560783631362,
      "y": 138.66947661867334,
      "strokeColor": "#00e676",
      "backgroundColor": "#e6e6e7",
      "width": 63.93385920866094,
      "height": 24.3311416053828,
      "seed": 1453040248,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "rectangle",
      "version": 4044,
      "versionNonce": 136150280,
      "isDeleted": false,
      "id": "Va8ZaCNlht0FpfumBHJKw",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1251.633146591779,
      "y": 164.77295668237002,
      "strokeColor": "#00e676",
      "backgroundColor": "#e6e6e7",
      "width": 63.93385920866094,
      "height": 24.3311416053828,
      "seed": 742383880,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755945
    },
    {
      "type": "text",
      "version": 3621,
      "versionNonce": 810203000,
      "isDeleted": false,
      "id": "sVvye4L-cb_iVCic8Mm8d",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1254.7093435648121,
      "y": 98.52504819457317,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 32.130700199808395,
      "height": 12.657548563560873,
      "seed": 552637304,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.73657581812376,
      "fontFamily": 1,
      "text": "____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 3688,
      "versionNonce": 1484299272,
      "isDeleted": false,
      "id": "Kmc9TdtkzFXb1_8VdBftg",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1254.9708588296896,
      "y": 124.38392997693245,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 32.130700199808395,
      "height": 12.657548563560873,
      "seed": 313425928,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1641741755946,
      "fontSize": 9.73657581812376,
      "fontFamily": 1,
      "text": "____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 3782,
      "versionNonce": 2113210488,
      "isDeleted": false,
      "id": "4eUPUHqH2EOhDN59TKy-H",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1254.837580778779,
      "y": 150.22140742876354,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 32.130700199808395,
      "height": 12.657548563560873,
      "seed": 360670328,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1641741755946,
      "fontSize": 9.73657581812376,
      "fontFamily": 1,
      "text": "____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 3872,
      "versionNonce": 1310186248,
      "isDeleted": false,
      "id": "b-QV7ztzPBTYvkYvn2n8j",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1254.614649007421,
      "y": 176.32488749246102,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 32.130700199808395,
      "height": 12.657548563560873,
      "seed": 267468552,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.73657581812376,
      "fontFamily": 1,
      "text": "____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 3901,
      "versionNonce": 606943608,
      "isDeleted": false,
      "id": "4KBkw2hbsRceHQmpPy7CU",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1320.8795641602012,
      "y": 98.59324391505857,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 39.91996085430739,
      "height": 12.657548563560873,
      "seed": 302781816,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.736575818123757,
      "fontFamily": 1,
      "text": "_____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 3967,
      "versionNonce": 1802975752,
      "isDeleted": false,
      "id": "ypoyWHhBrPKGhPAUDIGbn",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1321.141079425078,
      "y": 124.4521256974175,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 39.91996085430739,
      "height": 12.657548563560873,
      "seed": 1954815496,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.736575818123757,
      "fontFamily": 1,
      "text": "_____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 4061,
      "versionNonce": 372722296,
      "isDeleted": false,
      "id": "1jfyF2ZuAzbvS4YBl80za",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1321.0078013741684,
      "y": 150.28960314924905,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 39.91996085430739,
      "height": 12.657548563560873,
      "seed": 467314296,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.736575818123757,
      "fontFamily": 1,
      "text": "_____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 4152,
      "versionNonce": 59947272,
      "isDeleted": false,
      "id": "7coLrbQfexZARdWa0iB51",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1320.7848696028097,
      "y": 176.39308321294527,
      "strokeColor": "#00e676",
      "backgroundColor": "#ffffff",
      "width": 39.91996085430739,
      "height": 12.657548563560873,
      "seed": 560143624,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 9.736575818123757,
      "fontFamily": 1,
      "text": "_____",
      "baseline": 8.657548563560873,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": ""
    },
    {
      "type": "text",
      "version": 282,
      "versionNonce": 1054292856,
      "isDeleted": false,
      "id": "uG1gxQCUX_W4xoK8WjzqS",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 762,
      "y": 202,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 117,
      "height": 26,
      "seed": 657172488,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Input Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Input Data"
    },
    {
      "type": "arrow",
      "version": 312,
      "versionNonce": 702961672,
      "isDeleted": false,
      "id": "3DJ_wB8Ty8UNX4KAZfwjT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 878.680938765565,
      "y": 132.68136619602433,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#ff7043",
      "width": 104.88363252862348,
      "height": 0.6395755770326446,
      "seed": 1663624312,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755946,
      "startBinding": {
        "elementId": "9itz4bqXi6IMeZhGhc42J",
        "focus": -0.09772881248040827,
        "gap": 14.870387259234633
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          104.88363252862348,
          0.6395755770326446
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 444,
      "versionNonce": 273747064,
      "isDeleted": false,
      "id": "IEnqZv0JAf73xE5G5LY1s",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1148.3291790735977,
      "y": 134.9159571720989,
      "strokeColor": "#00e676",
      "backgroundColor": "#ff7043",
      "width": 100.9588566647617,
      "height": 1.6503135268827123,
      "seed": 2055241224,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755946,
      "startBinding": {
        "elementId": "sxpRnxFiIOgK8uOIc81pk",
        "focus": 0.16194715672100898,
        "gap": 5.601483341858389
      },
      "endBinding": {
        "elementId": "Kmc9TdtkzFXb1_8VdBftg",
        "focus": -0.3333798833901454,
        "gap": 5.682823091330192
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          100.9588566647617,
          -1.6503135268827123
        ]
      ]
    },
    {
      "type": "text",
      "version": 247,
      "versionNonce": 577640200,
      "isDeleted": false,
      "id": "g9t3YfCk9S7S38pod_Itm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1007.5,
      "y": 71,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 107,
      "height": 36,
      "seed": 1032706680,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755946,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Process",
      "baseline": 25,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Process"
    },
    {
      "type": "line",
      "version": 67,
      "versionNonce": 342408568,
      "isDeleted": false,
      "id": "agZFvlVOC2laS2bFwE9Rc",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1079.5,
      "y": 90,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 0,
      "height": 0,
      "seed": 1617963896,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1641741755947,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          0
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/query.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 3038,
      "versionNonce": 1386987419,
      "isDeleted": false,
      "id": "pjs0X-Y0kuuXB-vpf_dvP",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 934.5,
      "y": -240.29250000000002,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 312,
      "height": 146,
      "seed": 1534167490,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "of5l7HpIH0kPvZ95rOMwM",
          "type": "text"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        },
        {
          "id": "of5l7HpIH0kPvZ95rOMwM",
          "type": "text"
        },
        {
          "type": "text",
          "id": "of5l7HpIH0kPvZ95rOMwM"
        }
      ],
      "updated": 1669033845173,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2554,
      "versionNonce": 1319492501,
      "isDeleted": false,
      "id": "of5l7HpIH0kPvZ95rOMwM",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 939.5,
      "y": -235.29250000000002,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 260,
      "height": 130,
      "seed": 1972792158,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1669033845173,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "SELECT\n  count(*), flag\nFROM txtai\nGROUP BY flag\nORDER BY count(*) DESC",
      "baseline": 122,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": "pjs0X-Y0kuuXB-vpf_dvP",
      "originalText": "SELECT\n  count(*), flag\nFROM txtai\nGROUP BY flag\nORDER BY count(*) DESC"
    },
    {
      "type": "rectangle",
      "version": 3354,
      "versionNonce": 1004999739,
      "isDeleted": false,
      "id": "xMKXZw8aL27lv21eVGy_C",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 581,
      "y": -238.29250000000002,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 302,
      "height": 140,
      "seed": 1704428866,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "tlu8eCXVp5ApnEG74CF38",
          "type": "text"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        },
        {
          "type": "text",
          "id": "tlu8eCXVp5ApnEG74CF38"
        }
      ],
      "updated": 1669033845173,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2750,
      "versionNonce": 686792949,
      "isDeleted": false,
      "id": "tlu8eCXVp5ApnEG74CF38",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 586,
      "y": -233.29250000000002,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 290,
      "height": 130,
      "seed": 92268510,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1669033845173,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "SELECT\n  text, flag, entry\nFROM txtai\nWHERE\n  similar('text') AND flag = 1",
      "baseline": 122,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": "xMKXZw8aL27lv21eVGy_C",
      "originalText": "SELECT\n  text, flag, entry\nFROM txtai\nWHERE\n  similar('text') AND flag = 1"
    },
    {
      "type": "rectangle",
      "version": 3169,
      "versionNonce": 1477367003,
      "isDeleted": false,
      "id": "Tsn_ukSxtC7CIwEMUMHxI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1301.5,
      "y": -236.79250000000002,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 304,
      "height": 140,
      "seed": 919682718,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "t3OC1akz9Oe2w6N8OCavg",
          "type": "text"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        },
        {
          "id": "t3OC1akz9Oe2w6N8OCavg",
          "type": "text"
        },
        {
          "id": "t3OC1akz9Oe2w6N8OCavg",
          "type": "text"
        },
        {
          "type": "text",
          "id": "t3OC1akz9Oe2w6N8OCavg"
        }
      ],
      "updated": 1669033845173,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 2769,
      "versionNonce": 345619029,
      "isDeleted": false,
      "id": "t3OC1akz9Oe2w6N8OCavg",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1306.5,
      "y": -231.79250000000002,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 259,
      "height": 130,
      "seed": 1222509122,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1669033845173,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "SELECT\n  translation(text, 'fr')\nFROM txtai\nWHERE\n  similar('feel good story')",
      "baseline": 122,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": "Tsn_ukSxtC7CIwEMUMHxI",
      "originalText": "SELECT\n  translation(text, 'fr')\nFROM txtai\nWHERE\n  similar('feel good story')"
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/rag.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "text",
      "version": 1836,
      "versionNonce": 917552614,
      "isDeleted": false,
      "id": "YCEwpQiKAjRS3zIG3_E8s",
      "fillStyle": "hachure",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2014.213462044321,
      "y": -342.9945000407677,
      "strokeColor": "#00e676",
      "backgroundColor": "transparent",
      "width": 54.983333587646484,
      "height": 36,
      "seed": 676249636,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "LLM",
      "textAlign": "left",
      "verticalAlign": "middle",
      "containerId": null,
      "originalText": "LLM",
      "lineHeight": 1.2857142857142858,
      "baseline": 25
    },
    {
      "type": "line",
      "version": 3253,
      "versionNonce": 1479474042,
      "isDeleted": false,
      "id": "aFBJSnHUVo8ZDTb0uAkQ2",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1746.3803586395986,
      "y": -463.34177128578517,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 116.42036295658872,
      "height": 103.65107323746608,
      "seed": 1963890941,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -62.44191743896485,
          19.19929080548739
        ],
        [
          -63.17668831316513,
          79.43840749607878
        ],
        [
          -7.618334228588694,
          103.65107323746608
        ],
        [
          51.963117173367294,
          79.15871076413049
        ],
        [
          53.24367464342358,
          21.28567723840068
        ],
        [
          0,
          0
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 2707,
      "versionNonce": 1122767142,
      "isDeleted": false,
      "id": "ZTKEryJuyeKfeG4nAI4_e",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1980.1453046789213,
      "y": -432.63793463623756,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 23.299119994671287,
      "height": 58.247799986678125,
      "seed": 1188509340,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "5xuE_JZVT_PTcKy4YrPE8",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2850,
      "versionNonce": 1047886906,
      "isDeleted": false,
      "id": "CckqOoWddQBHuvidbpk4P",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2028.4647375510242,
      "y": -487.74904809174427,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 23.29911999467125,
      "height": 116.4955999733563,
      "seed": 331107492,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2732,
      "versionNonce": 718994534,
      "isDeleted": false,
      "id": "J8ox4BBdzEg-04DGRL-jh",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2072.5083285134306,
      "y": -452.8003680997374,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 23.29911999467127,
      "height": 81.54691998134939,
      "seed": 1316437916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 2936,
      "versionNonce": 1101915386,
      "isDeleted": false,
      "id": "V54vdkZmQeI13AEahvNPV",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2114.182425101253,
      "y": -477.75421594683803,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 23.29911999467125,
      "height": 104.84603997602068,
      "seed": 1025829916,
      "groupIds": [
        "SFux9iuPQVutxOMe0wlW3",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "tEBpXPxm5iq2pVW3uI-zN",
          "type": "arrow"
        },
        {
          "id": "QwMjSCRfpekAf9are5X7i",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1533,
      "versionNonce": 1682603942,
      "isDeleted": false,
      "id": "iRbBQnNoV4xKl3KqmeMQk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1354.935979235434,
      "y": -344.1286480222933,
      "strokeColor": "#03a9f4",
      "backgroundColor": "transparent",
      "width": 172.6999969482422,
      "height": 70,
      "seed": 437964304,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Extract and\nIndex text",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": null,
      "originalText": "Extract and\nIndex text",
      "lineHeight": 1.25,
      "baseline": 60
    },
    {
      "type": "text",
      "version": 1816,
      "versionNonce": 995833274,
      "isDeleted": false,
      "id": "6MGKGOL-iHR1ALgkWdtgq",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1644.3134452350068,
      "y": -339.59711992736334,
      "strokeColor": "#fab005",
      "backgroundColor": "#00e676",
      "width": 192.14999389648438,
      "height": 36,
      "seed": 737085168,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Vector Search",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": null,
      "originalText": "Vector Search",
      "lineHeight": 1.2857142857142858,
      "baseline": 25
    },
    {
      "type": "arrow",
      "version": 1734,
      "versionNonce": 593992422,
      "isDeleted": false,
      "id": "TekVO-JfHFlOug8RyHvF6",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1495.4312181086514,
      "y": -410.21715689579304,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#f8f9fa",
      "width": 183.6089623410503,
      "height": 1.4014353332532892,
      "seed": 1668067473,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "ZX3LSm-VHgqQG71nXy8Jp",
        "focus": 0.09017987719185833,
        "gap": 1.2826405001093732
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          183.6089623410503,
          1.4014353332532892
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 1424,
      "versionNonce": 1554017914,
      "isDeleted": false,
      "id": "5xuE_JZVT_PTcKy4YrPE8",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1802.8350046840696,
      "y": -410.21116509773026,
      "strokeColor": "#00e676",
      "backgroundColor": "#f8f9fa",
      "width": 170.5772610888498,
      "height": 0.7934282297927382,
      "seed": 2139598001,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": {
        "elementId": "ZTKEryJuyeKfeG4nAI4_e",
        "focus": 0.19940295698482952,
        "gap": 6.73303890600198
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          170.5772610888498,
          0.7934282297927382
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 829,
      "versionNonce": 247418406,
      "isDeleted": false,
      "id": "ceYyi4FMiqSJFJi9xVvwk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2246.94133763324,
      "y": -461.9464695959758,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 151.86184901372147,
      "height": 45,
      "seed": 1675993499,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "TKBvCE2CYBCmPq45Bjqws"
        },
        {
          "id": "tEBpXPxm5iq2pVW3uI-zN",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 805,
      "versionNonce": 977309498,
      "isDeleted": false,
      "id": "TKBvCE2CYBCmPq45Bjqws",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2277.347260614222,
      "y": -456.9464695959758,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 91.05000305175781,
      "height": 35,
      "seed": 1955981979,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Answer",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "ceYyi4FMiqSJFJi9xVvwk",
      "originalText": "Answer",
      "lineHeight": 1.25,
      "baseline": 25
    },
    {
      "type": "rectangle",
      "version": 925,
      "versionNonce": 1933482342,
      "isDeleted": false,
      "id": "8kLHMUMRHF0HnBTmfmRbB",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2248.212626691299,
      "y": -408.2942759824447,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 150.58992984312886,
      "height": 45,
      "seed": 1950178933,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 3
      },
      "boundElements": [
        {
          "type": "text",
          "id": "Z9_eH0JKgQC5deMjnP4Nl"
        },
        {
          "id": "QwMjSCRfpekAf9are5X7i",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 909,
      "versionNonce": 1486788602,
      "isDeleted": false,
      "id": "Z9_eH0JKgQC5deMjnP4Nl",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 2268.8992580252175,
      "y": -403.2942759824447,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#ffeb3b",
      "width": 109.21666717529297,
      "height": 35,
      "seed": 2052310997,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Citation",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "8kLHMUMRHF0HnBTmfmRbB",
      "originalText": "Citation",
      "lineHeight": 1.25,
      "baseline": 25
    },
    {
      "type": "arrow",
      "version": 2914,
      "versionNonce": 2016574630,
      "isDeleted": false,
      "id": "tEBpXPxm5iq2pVW3uI-zN",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 2145.854477497225,
      "y": -412.39564497927813,
      "strokeColor": "#7950f2",
      "backgroundColor": "#f8f9fa",
      "width": 98.40776781852423,
      "height": 26.51909525787792,
      "seed": 67586843,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "V54vdkZmQeI13AEahvNPV",
        "focus": 0.3299220128455593,
        "gap": 8.372932401300432
      },
      "endBinding": {
        "elementId": "ceYyi4FMiqSJFJi9xVvwk",
        "focus": 0.48070935597063635,
        "gap": 2.679092317490813
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          98.40776781852423,
          -26.51909525787792
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 3333,
      "versionNonce": 935168186,
      "isDeleted": false,
      "id": "QwMjSCRfpekAf9are5X7i",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 2145.973109719745,
      "y": -412.4720176121603,
      "strokeColor": "#ff7043",
      "backgroundColor": "#f8f9fa",
      "width": 99.14068321372724,
      "height": 28.4745045092954,
      "seed": 1135076219,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "V54vdkZmQeI13AEahvNPV",
        "focus": 0.12685170125624176,
        "gap": 8.491564623820295
      },
      "endBinding": {
        "elementId": "8kLHMUMRHF0HnBTmfmRbB",
        "focus": -0.5509825166646869,
        "gap": 3.098833757826924
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          99.14068321372724,
          28.4745045092954
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 5987,
      "versionNonce": 1397225446,
      "isDeleted": false,
      "id": "Wm6KQXptnXme2EhsZEkeR",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1131.175427860874,
      "y": -462.1767369812002,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 55.277464194934204,
      "height": 83.88048030659263,
      "seed": 2083904347,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 6037,
      "versionNonce": 1598054778,
      "isDeleted": false,
      "id": "9qC6dzMnsEUYrUkMWcOVp",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1125.2726075818707,
      "y": -467.6671236066981,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 55.277464194934204,
      "height": 83.88048030659263,
      "seed": 758626299,
      "groupIds": [
        "LrzsLb2hfnEcKX352d6a4",
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 6141,
      "versionNonce": 2089552678,
      "isDeleted": false,
      "id": "NSEdoJ3xQn_cWFxLphwMQ",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1117.9247419835488,
      "y": -474.1224740500794,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 55.277464194934204,
      "height": 83.88048030659263,
      "seed": 1729429659,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 5257,
      "versionNonce": 1785340474,
      "isDeleted": false,
      "id": "YEmnwZhbbK23DEmyJvwCv",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1127.2992831812242,
      "y": -435.9974349769433,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 36.42992238421251,
      "height": 2.541777233017194,
      "seed": 1400911163,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          31.61584953581463,
          0.12309654138955267
        ],
        [
          36.42992238421251,
          -2.4186806916276415
        ]
      ]
    },
    {
      "type": "line",
      "version": 5281,
      "versionNonce": 1651226214,
      "isDeleted": false,
      "id": "MEri2dzaVIq9n6tGJD7WN",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1128.7502692954959,
      "y": -460.6063232007082,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 35.638112199057964,
      "height": 2.1924492101548854,
      "seed": 1104926171,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          13.164676061582496,
          -2.1924492101548854
        ],
        [
          35.638112199057964,
          -0.25617388343228015
        ]
      ]
    },
    {
      "type": "line",
      "version": 5306,
      "versionNonce": 337496826,
      "isDeleted": false,
      "id": "vgzKUuIqRDM9pNKM15LRR",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1128.6903610887653,
      "y": -407.89742505595746,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 37.803945940804155,
      "height": 3.34788687616908,
      "seed": 1899651707,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          20.65693041859134,
          -0.19961601306414317
        ],
        [
          29.422469585925523,
          1.8108435755749763
        ],
        [
          37.803945940804155,
          -1.5370433005941033
        ]
      ]
    },
    {
      "type": "line",
      "version": 5343,
      "versionNonce": 498334118,
      "isDeleted": false,
      "id": "Cp8KP5gHGupNwbMjKHtU1",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1126.200129220062,
      "y": -399.10419905313154,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 42.55148011818046,
      "height": 2.27562254893163,
      "seed": 442391323,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          7.9508648746316855,
          -0.9124268560932918
        ],
        [
          12.61573202565594,
          -0.658732843111803
        ],
        [
          36.180402367882216,
          0.47907843135401373
        ],
        [
          42.55148011818046,
          -1.7965441175776165
        ]
      ]
    },
    {
      "type": "line",
      "version": 5309,
      "versionNonce": 1121868730,
      "isDeleted": false,
      "id": "Z_ssChLTRG5S_-bNEkKSc",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1126.4229038259869,
      "y": -448.82748079933765,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 36.70273093540022,
      "height": 1.9362753267226067,
      "seed": 968507323,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          14.229294797924695,
          -0.4624437635986851
        ],
        [
          36.70273093540022,
          1.4738315631239216
        ]
      ]
    },
    {
      "type": "line",
      "version": 5323,
      "versionNonce": 630314214,
      "isDeleted": false,
      "id": "s_UIbn_04wQnRoID1xAww",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1126.5069699582718,
      "y": -421.8581929690114,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 36.70273093540022,
      "height": 1.9362753267226052,
      "seed": 426807387,
      "groupIds": [
        "Qj7F6yX0WhR-X_WVj6a3Q",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          6.330239931623038,
          1.1168104379056514
        ],
        [
          14.229294797924746,
          -0.4624437635986836
        ],
        [
          36.70273093540022,
          1.4738315631239216
        ]
      ]
    },
    {
      "type": "text",
      "version": 1665,
      "versionNonce": 349099130,
      "isDeleted": false,
      "id": "KQzTCckYJwmcri2ZUXolF",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1094.845592872532,
      "y": -343.81672500592333,
      "strokeColor": "#868e96",
      "backgroundColor": "transparent",
      "width": 144.43333435058594,
      "height": 35,
      "seed": 748785339,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Documents",
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": null,
      "originalText": "Documents",
      "lineHeight": 1.25,
      "baseline": 25
    },
    {
      "type": "ellipse",
      "version": 407,
      "versionNonce": 59918374,
      "isDeleted": false,
      "id": "ZX3LSm-VHgqQG71nXy8Jp",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1372.5849155794615,
      "y": -469.38832974419654,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 121.85485206598776,
      "height": 107.68568322110525,
      "seed": 54693813,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [
        {
          "id": "TekVO-JfHFlOug8RyHvF6",
          "type": "arrow"
        },
        {
          "id": "ocOrefpIzXSYwlrYeB5zZ",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 6067,
      "versionNonce": 1806841146,
      "isDeleted": false,
      "id": "896_9tzrmBEIv0wPMOAA2",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1159.008440355131,
      "y": -433.48589612410836,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 41.38251178619768,
      "height": 62.79566212875594,
      "seed": 866216059,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 6117,
      "versionNonce": 128078694,
      "isDeleted": false,
      "id": "FyfaSNb5FLCEHf0iOIi1n",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1154.5893966575723,
      "y": -437.5961785429837,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 41.38251178619768,
      "height": 62.79566212875594,
      "seed": 1173659931,
      "groupIds": [
        "ZcBCWYu7yQph5fcMtQ5UB",
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 6222,
      "versionNonce": 2028699130,
      "isDeleted": false,
      "id": "uhf9NUQGgUhU8COKxhwFD",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1149.0885449243297,
      "y": -442.4288643345245,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 41.38251178619768,
      "height": 62.79566212875594,
      "seed": 1721222587,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": null,
      "boundElements": [
        {
          "id": "ocOrefpIzXSYwlrYeB5zZ",
          "type": "arrow"
        }
      ],
      "updated": 1701521662211,
      "link": null,
      "locked": false
    },
    {
      "type": "line",
      "version": 5337,
      "versionNonce": 2081355430,
      "isDeleted": false,
      "id": "-9TDrRU-TUCFATZg1NUYi",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1156.1066320825057,
      "y": -413.8872171773836,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 27.272627541642883,
      "height": 1.902857300622384,
      "seed": 481121883,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          23.668655664678663,
          0.09215408393064761
        ],
        [
          27.272627541642883,
          -1.8107032166917363
        ]
      ]
    },
    {
      "type": "line",
      "version": 5361,
      "versionNonce": 1232973498,
      "isDeleted": false,
      "id": "XvJ-iIG8KhdoW9fCxaP1l",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1157.1928875909657,
      "y": -432.31023317127756,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 26.679852623386196,
      "height": 1.6413389543326597,
      "seed": 1728566011,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          9.855505678747075,
          -1.6413389543326597
        ],
        [
          26.679852623386196,
          -0.19178012061240474
        ]
      ]
    },
    {
      "type": "line",
      "version": 5386,
      "versionNonce": 1501901286,
      "isDeleted": false,
      "id": "VxXBrorj_TWBXByFNwrzO",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1157.1480383543515,
      "y": -392.8506336675699,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 28.30126637038239,
      "height": 2.5063372593097366,
      "seed": 1177140123,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          15.464451543932462,
          -0.14943905502267457
        ],
        [
          22.0266199282381,
          1.355656535634992
        ],
        [
          28.30126637038239,
          -1.1506807236747445
        ]
      ]
    },
    {
      "type": "line",
      "version": 5423,
      "versionNonce": 1376503674,
      "isDeleted": false,
      "id": "Y5UL5WFItbSvBNwQhfIs9",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1155.283769593875,
      "y": -386.26773801560205,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 31.855425229005572,
      "height": 1.7036052272587878,
      "seed": 105222203,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          5.952276649750224,
          -0.6830724903220912
        ],
        [
          9.444548277434599,
          -0.49314888157492376
        ],
        [
          27.08582872286446,
          0.3586537320544714
        ],
        [
          31.855425229005572,
          -1.3449514952043164
        ]
      ]
    },
    {
      "type": "line",
      "version": 5394,
      "versionNonce": 2064778534,
      "isDeleted": false,
      "id": "48QS3eGwek6u2meHCxeow",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1155.450545926502,
      "y": -423.49220774985906,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 30.39283293375309,
      "height": 1.5921546638326731,
      "seed": 1228787931,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -2.9159720169125194,
          1.5921546638326731
        ],
        [
          27.47686091684057,
          1.103358356250995
        ]
      ]
    },
    {
      "type": "line",
      "version": 5403,
      "versionNonce": 2061027386,
      "isDeleted": false,
      "id": "KCwYMxaGjkAMea5v0qmQT",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1155.513480573754,
      "y": -403.30211965956437,
      "strokeColor": "#868e96",
      "backgroundColor": "#fff",
      "width": 27.47686091684057,
      "height": 1.449558833720255,
      "seed": 1352990075,
      "groupIds": [
        "jC1bSinvdhon1lqLCwioC",
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          4.739023983735066,
          0.8360807027362631
        ],
        [
          10.652513972201453,
          -0.3462004774692599
        ],
        [
          27.47686091684057,
          1.103358356250995
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 2523,
      "versionNonce": 1377988710,
      "isDeleted": false,
      "id": "ocOrefpIzXSYwlrYeB5zZ",
      "fillStyle": "hachure",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1205.9627879525642,
      "y": -406.5621788129673,
      "strokeColor": "#868e96",
      "backgroundColor": "#f8f9fa",
      "width": 164.6348419304046,
      "height": 1.709635034590832,
      "seed": 1351597109,
      "groupIds": [
        "j11_AWYuFwJUdzbrHyyU4"
      ],
      "frameId": null,
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1701521662211,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "uhf9NUQGgUhU8COKxhwFD",
        "focus": 0.1532483360492926,
        "gap": 15.491731242036735
      },
      "endBinding": {
        "elementId": "ZX3LSm-VHgqQG71nXy8Jp",
        "focus": -0.12294829093000738,
        "gap": 2.5175197483459826
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          164.6348419304046,
          -1.709635034590832
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/schedule.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 5213,
      "versionNonce": 290163336,
      "isDeleted": false,
      "id": "9itz4bqXi6IMeZhGhc42J",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 793.1319708150991,
      "y": 84.01158837080237,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2026142776,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "sbyEF_hTthhrIaA-47du2",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1643333366010
    },
    {
      "type": "rectangle",
      "version": 5260,
      "versionNonce": 586039800,
      "isDeleted": false,
      "id": "7HUYNqBGn8JbkXw6qt_jr",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 785.5845377793848,
      "y": 76.99149908508815,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1090159432,
      "groupIds": [
        "6R5o6OnVZ7fFC_87zCo7G",
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1643333366010
    },
    {
      "type": "rectangle",
      "version": 5356,
      "versionNonce": 2072574344,
      "isDeleted": false,
      "id": "mI_Mfw3cNZf0AuQhK5KUO",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 776.1894484936695,
      "y": 68.73759283508838,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1146631480,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1643333366010
    },
    {
      "type": "line",
      "version": 4486,
      "versionNonce": 415449848,
      "isDeleted": false,
      "id": "y3oKPiP36axXzEF0c7OiC",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 788.1758750805805,
      "y": 117.48482988244103,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1952111176,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366010,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4512,
      "versionNonce": 977307784,
      "isDeleted": false,
      "id": "-52Udo1sfovtw-gxgUT91",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 790.0311273185948,
      "y": 86.01954258360246,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 923586104,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366010,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4537,
      "versionNonce": 1850745848,
      "isDeleted": false,
      "id": "7Tgba-Lgh4JYVwHG-MlYn",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 789.9545278022254,
      "y": 153.41391685395956,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 664293704,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366010,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4574,
      "versionNonce": 2033072008,
      "isDeleted": false,
      "id": "KsHbDZNNeZoS7zbhCYNon",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 786.770480620695,
      "y": 164.65706526223835,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 2043037496,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4536,
      "versionNonce": 563668216,
      "isDeleted": false,
      "id": "R5nE2rgdgOpe93Q8VvYhl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.0553235162608,
      "y": 101.08014413686368,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 571402312,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4550,
      "versionNonce": 210634376,
      "isDeleted": false,
      "id": "lfWLpL8lTqrf2GMoqJY8y",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.1628117124112,
      "y": 135.5634731727032,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 2028511288,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "text",
      "version": 600,
      "versionNonce": 1344374264,
      "isDeleted": false,
      "id": "uG1gxQCUX_W4xoK8WjzqS",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 762,
      "y": 202,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 117,
      "height": 26,
      "seed": 657172488,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Input Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Input Data"
    },
    {
      "type": "arrow",
      "version": 2896,
      "versionNonce": 1629222280,
      "isDeleted": false,
      "id": "3DJ_wB8Ty8UNX4KAZfwjT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 878.680938765565,
      "y": 131.68136619602433,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#ff7043",
      "width": 104.88363252862348,
      "height": 0.6395755770326446,
      "seed": 1663624312,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "startBinding": {
        "elementId": "9itz4bqXi6IMeZhGhc42J",
        "focus": -0.11630205093491493,
        "gap": 14.870387259234633
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          104.88363252862348,
          0.6395755770326446
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 2644,
      "versionNonce": 1190004472,
      "isDeleted": false,
      "id": "IEnqZv0JAf73xE5G5LY1s",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1132.8223412617328,
      "y": 135.16943809537315,
      "strokeColor": "#00e676",
      "backgroundColor": "#ff7043",
      "width": 116.4656944766266,
      "height": 1.9037944501569655,
      "seed": 2055241224,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "startBinding": null,
      "endBinding": {
        "elementId": "lIucERyh1MMe7Vzo8_q7d",
        "focus": -0.22308504197782658,
        "gap": 15.365988697358375
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          116.4656944766266,
          -1.9037944501569655
        ]
      ]
    },
    {
      "type": "text",
      "version": 859,
      "versionNonce": 1813251208,
      "isDeleted": false,
      "id": "g9t3YfCk9S7S38pod_Itm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1023.5,
      "y": 66,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 75,
      "height": 36,
      "seed": 1032706680,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Timer",
      "baseline": 25,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Timer"
    },
    {
      "type": "rectangle",
      "version": 5052,
      "versionNonce": 310536184,
      "isDeleted": false,
      "id": "qfqvZwfY8g2maDsv8tzi-",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1281.5965467571473,
      "y": 81.99348491888458,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 320002220,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "sbyEF_hTthhrIaA-47du2",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1643333366011
    },
    {
      "type": "rectangle",
      "version": 5100,
      "versionNonce": 456274824,
      "isDeleted": false,
      "id": "Cydk-vrvGXEEAXU4XHWk7",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1274.049113721433,
      "y": 74.97339563317036,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2124765204,
      "groupIds": [
        "m2CJZBWcxZ3sXLBcib5Jt",
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1643333366011
    },
    {
      "type": "rectangle",
      "version": 5197,
      "versionNonce": 249735416,
      "isDeleted": false,
      "id": "lIucERyh1MMe7Vzo8_q7d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1264.6540244357177,
      "y": 66.71948938317058,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 722133804,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1643333366011
    },
    {
      "type": "line",
      "version": 4318,
      "versionNonce": 1152549512,
      "isDeleted": false,
      "id": "3UaWROnZ6NNaCTSa51ntD",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1276.6404510226287,
      "y": 115.46672643052324,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 166278548,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4344,
      "versionNonce": 2006108664,
      "isDeleted": false,
      "id": "H3c0943V86dq4MeskNY-E",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1278.495703260643,
      "y": 84.00143913168466,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 1285397932,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4369,
      "versionNonce": 391180680,
      "isDeleted": false,
      "id": "g62fLo1BCU_TuN8OEk_ie",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1278.4191037442736,
      "y": 151.39581340204182,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 198533908,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4406,
      "versionNonce": 2006371064,
      "isDeleted": false,
      "id": "jMbD-eUPMlCihQh7GK--c",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.2350565627432,
      "y": 162.6389618103206,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 1358235692,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4371,
      "versionNonce": 172589192,
      "isDeleted": false,
      "id": "lmzq_v_7Zzb2T4WxQPEqG",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.519899458309,
      "y": 99.06204068494588,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 719315092,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4386,
      "versionNonce": 963816440,
      "isDeleted": false,
      "id": "YoZdONxrJotpnXjxFe748",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.6273876544594,
      "y": 133.54536972078546,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1543185068,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "text",
      "version": 596,
      "versionNonce": 1297624968,
      "isDeleted": false,
      "id": "fAIYPmdTgnjEqc8PGu-SI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1255.9645759420482,
      "y": 203.9818965480822,
      "strokeColor": "#00e676",
      "backgroundColor": "#03a9f4",
      "width": 134,
      "height": 26,
      "seed": 1891202964,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1643333366011,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Output Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Output Data"
    },
    {
      "type": "ellipse",
      "version": 901,
      "versionNonce": 1487149704,
      "isDeleted": false,
      "id": "j-wM9an_6t3rpXVK34zli",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1022.0933939958717,
      "y": 100.46179899047428,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 81.77778795030434,
      "height": 81.77778795030434,
      "seed": 1020498168,
      "groupIds": [
        "bSaKYXsjdi0TEAthptqF8",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        },
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1643333383394
    },
    {
      "type": "ellipse",
      "version": 1081,
      "versionNonce": 1766820232,
      "isDeleted": false,
      "id": "9vh3f3W1oSyVisk-dvz1f",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1025.1015457438984,
      "y": 102.95003035938365,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 76.42992266951718,
      "height": 76.42992266951718,
      "seed": 119976440,
      "groupIds": [
        "bSaKYXsjdi0TEAthptqF8",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1643333383394
    },
    {
      "type": "line",
      "version": 892,
      "versionNonce": 1354045576,
      "isDeleted": false,
      "id": "iqDqmrD3b6Ygz-GFOrXwa",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1061.8311628994297,
      "y": 120.0482909120675,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.0883491007133227,
      "height": 25.95185095385813,
      "seed": 1013822200,
      "groupIds": [
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.0883491007133227,
          25.95185095385813
        ]
      ]
    },
    {
      "type": "line",
      "version": 970,
      "versionNonce": 1922090888,
      "isDeleted": false,
      "id": "ZwJWJndwUCnxKlrLJvMPl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 1.9734847452380198,
      "x": 1070.7802432970152,
      "y": 142.30181631205494,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.8395797827843811,
      "height": 15.147722701630906,
      "seed": 321688568,
      "groupIds": [
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          0.8395797827843811,
          15.147722701630906
        ]
      ]
    },
    {
      "type": "line",
      "version": 845,
      "versionNonce": 1528731272,
      "isDeleted": false,
      "id": "t0x1SEUOcZRg50TGtYP-w",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.280886784478146,
      "x": 1064.6162574402501,
      "y": 173.36067968426403,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1365726456,
      "groupIds": [
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1529,
      "versionNonce": 790267272,
      "isDeleted": false,
      "id": "Yw89MpxBojMQgM6zgnIuk",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.1241889472809277,
      "x": 1062.1357257347163,
      "y": 102.4866803252352,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1554118136,
      "groupIds": [
        "YELVY3p7Zx7E_9iDQmVra",
        "NVEUOovoAoQURmU2RDepy",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1166,
      "versionNonce": 465605768,
      "isDeleted": false,
      "id": "cvYavS_2xzzp8a7_MTILV",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.81125135713188,
      "x": 1082.8189605518937,
      "y": 166.68716660884547,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1239447288,
      "groupIds": [
        "p-wnEbrxq-_9Ag8WjVI7N",
        "SHbRHlotuJGYNAc1emjPb",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 682,
      "versionNonce": 1187824520,
      "isDeleted": false,
      "id": "yixQP0fZUpIySDIZ4MLuw",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1068.9460036459864,
      "y": 178.0991106962765,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1839650808,
      "groupIds": [
        "Mnb3dkXLcGQCoV-PGx3Al",
        "SHbRHlotuJGYNAc1emjPb",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383394,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 754,
      "versionNonce": 1048253064,
      "isDeleted": false,
      "id": "MAdztZi4TKHJd1ANMjVVe",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.108887538323692,
      "x": 1073.4119741916074,
      "y": 177.75916345378153,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1811486968,
      "groupIds": [
        "Mnb3dkXLcGQCoV-PGx3Al",
        "SHbRHlotuJGYNAc1emjPb",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 759,
      "versionNonce": 1770488200,
      "isDeleted": false,
      "id": "4KXJlSh8ncnoULQOmGcEa",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.108887538323692,
      "x": 1076.38361498242,
      "y": 176.04686252433794,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 1213938168,
      "groupIds": [
        "Mnb3dkXLcGQCoV-PGx3Al",
        "SHbRHlotuJGYNAc1emjPb",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 809,
      "versionNonce": 1999507592,
      "isDeleted": false,
      "id": "wdrgkWEU57wvnk67HRB3Z",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.031023708299764,
      "x": 1079.4047722054033,
      "y": 174.21268269656684,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 1976646392,
      "groupIds": [
        "Mnb3dkXLcGQCoV-PGx3Al",
        "SHbRHlotuJGYNAc1emjPb",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1268,
      "versionNonce": 1328201608,
      "isDeleted": false,
      "id": "LFYbBqV93gdPJr7Cums4g",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.181114071523705,
      "x": 1095.014529821029,
      "y": 152.77748807307205,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1725897720,
      "groupIds": [
        "P0muzq2rC_ILu5MzTfeLX",
        "995t9r7bEj_eb_vHEY1Ks",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 775,
      "versionNonce": 28937864,
      "isDeleted": false,
      "id": "Rx0uNkINQcMII9UMJRJ4k",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.806701628990837,
      "x": 1086.0116981168749,
      "y": 169.7448604067505,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 495508728,
      "groupIds": [
        "HS-2qloHB3pdPonai7TyU",
        "995t9r7bEj_eb_vHEY1Ks",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 838,
      "versionNonce": 932609416,
      "isDeleted": false,
      "id": "VLCl-pe-WQcf1BfPXBLt0",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.632403860134943,
      "x": 1088.9868069295608,
      "y": 166.505670147792,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 995261944,
      "groupIds": [
        "HS-2qloHB3pdPonai7TyU",
        "995t9r7bEj_eb_vHEY1Ks",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 834,
      "versionNonce": 570264712,
      "isDeleted": false,
      "id": "mUOYjWx-N-OWdJoscwtE-",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.632403860134943,
      "x": 1092.1465529032048,
      "y": 163.23329625583096,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 289406712,
      "groupIds": [
        "HS-2qloHB3pdPonai7TyU",
        "995t9r7bEj_eb_vHEY1Ks",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 886,
      "versionNonce": 1789574024,
      "isDeleted": false,
      "id": "qVO9k0FHbZuyI5LKP9GmF",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.5545400301110135,
      "x": 1094.2769300914624,
      "y": 160.64155267647118,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 802512888,
      "groupIds": [
        "HS-2qloHB3pdPonai7TyU",
        "995t9r7bEj_eb_vHEY1Ks",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1308,
      "versionNonce": 601492104,
      "isDeleted": false,
      "id": "NadcFz4xk5R9NqW8X-MU3",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.703634473789069,
      "x": 1098.3775913645477,
      "y": 133.59478791519757,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1997971704,
      "groupIds": [
        "zA8Ync2bFFM2m53cjUMBg",
        "B0S01CmiwscoHuDIbl3Gl",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 809,
      "versionNonce": 944208264,
      "isDeleted": false,
      "id": "diAnwk9JCDVD9lfXnCqgW",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.236849790887245,
      "x": 1097.7186508669784,
      "y": 153.30649098297056,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 646431224,
      "groupIds": [
        "DWLrPYo50Rd-VJ5VCi5f7",
        "B0S01CmiwscoHuDIbl3Gl",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383395,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 883,
      "versionNonce": 1529809032,
      "isDeleted": false,
      "id": "mtlzm_62GJjdDi5RAw4o1",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.062552022031351,
      "x": 1098.749454343585,
      "y": 149.1780894880784,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1831650040,
      "groupIds": [
        "DWLrPYo50Rd-VJ5VCi5f7",
        "B0S01CmiwscoHuDIbl3Gl",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 894,
      "versionNonce": 2018293640,
      "isDeleted": false,
      "id": "w7cJevDtNweeN4giVd7kF",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 5.062552022031351,
      "x": 1099.6846444314046,
      "y": 145.2886034713148,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 2039063544,
      "groupIds": [
        "DWLrPYo50Rd-VJ5VCi5f7",
        "B0S01CmiwscoHuDIbl3Gl",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 942,
      "versionNonce": 601654920,
      "isDeleted": false,
      "id": "f56a_kgrxmRNa3Z53Q60z",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.984688192007422,
      "x": 1099.6193083343842,
      "y": 141.54632901313482,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 161637624,
      "groupIds": [
        "DWLrPYo50Rd-VJ5VCi5f7",
        "B0S01CmiwscoHuDIbl3Gl",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1389,
      "versionNonce": 177894792,
      "isDeleted": false,
      "id": "vR3QmWptomkwPkB3sxLie",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.175269496869788,
      "x": 1091.7346739152727,
      "y": 117.50269095596911,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 2063271416,
      "groupIds": [
        "J0ilB_WV5wNeHV0loD0PU",
        "picpNQI8seSh60PT9pkMm",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 894,
      "versionNonce": 1640137864,
      "isDeleted": false,
      "id": "OotDa77gvcGT8eSVzCVHe",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.652271214951025,
      "x": 1099.0076812735788,
      "y": 133.53986124540666,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1519879928,
      "groupIds": [
        "hyo7dIZ-atzeosWf4KGV-",
        "picpNQI8seSh60PT9pkMm",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 970,
      "versionNonce": 1460712328,
      "isDeleted": false,
      "id": "KYXdIOqSNI3wL2_91JoSI",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.477973446095131,
      "x": 1097.9247650219345,
      "y": 129.89327965571465,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1962665976,
      "groupIds": [
        "hyo7dIZ-atzeosWf4KGV-",
        "picpNQI8seSh60PT9pkMm",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 984,
      "versionNonce": 12649096,
      "isDeleted": false,
      "id": "1amz5KsYAaVgqHJnfpUZX",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.477973446095131,
      "x": 1096.4518739202772,
      "y": 125.37009602466333,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 1714170104,
      "groupIds": [
        "hyo7dIZ-atzeosWf4KGV-",
        "picpNQI8seSh60PT9pkMm",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1030,
      "versionNonce": 1301246344,
      "isDeleted": false,
      "id": "M8DZXa5ygyRUlbzzJWUwL",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.400109616071202,
      "x": 1094.1448873155746,
      "y": 122.38879744583448,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 699156984,
      "groupIds": [
        "hyo7dIZ-atzeosWf4KGV-",
        "picpNQI8seSh60PT9pkMm",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1445,
      "versionNonce": 1958215816,
      "isDeleted": false,
      "id": "JH6nmvMEUuLCuyxh7Y-Yl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.866572733087537,
      "x": 1078.0076399536329,
      "y": 106.44508741487394,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 713050872,
      "groupIds": [
        "Xj8brNstIhDD7KOQvspnH",
        "B8ckCsKKOGs9N9f2IAKBw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 965,
      "versionNonce": 1710665608,
      "isDeleted": false,
      "id": "GDNvbfgT_WZFh6Gi2xvwP",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.210232827779823,
      "x": 1090.4068310713433,
      "y": 117.91497341317267,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1395403768,
      "groupIds": [
        "hogKkVF1m8maBYwOG6eeL",
        "B8ckCsKKOGs9N9f2IAKBw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1030,
      "versionNonce": 965181064,
      "isDeleted": false,
      "id": "PvojRAHs1S-Tx4T69Sj14",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.035935058923929,
      "x": 1088.4824681935847,
      "y": 115.14139125873646,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 498330872,
      "groupIds": [
        "hogKkVF1m8maBYwOG6eeL",
        "B8ckCsKKOGs9N9f2IAKBw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1042,
      "versionNonce": 539389320,
      "isDeleted": false,
      "id": "kLpgd0Yu3CP-rHZvMfTmB",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 4.035935058923929,
      "x": 1085.3441396608778,
      "y": 112.34041881492135,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 597396984,
      "groupIds": [
        "hogKkVF1m8maBYwOG6eeL",
        "B8ckCsKKOGs9N9f2IAKBw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1093,
      "versionNonce": 1393523848,
      "isDeleted": false,
      "id": "XW0QvYMk_SlAi17ooDA2B",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.9580712288999997,
      "x": 1082.318504219098,
      "y": 110.56683796369228,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 770117368,
      "groupIds": [
        "hogKkVF1m8maBYwOG6eeL",
        "B8ckCsKKOGs9N9f2IAKBw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1023,
      "versionNonce": 67021704,
      "isDeleted": false,
      "id": "F30qFOuZ0dhNLow90Cpan",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.7190910390046312,
      "x": 1076.2565286147315,
      "y": 107.99017766168507,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1159360504,
      "groupIds": [
        "oCizKDxwJR0_xadyu1U-s",
        "XOmrDYeKZbtBad03NUVlw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1095,
      "versionNonce": 1294989960,
      "isDeleted": false,
      "id": "LHYAA9-Xbe2aHOuAq8GMZ",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.5447932701487375,
      "x": 1073.786092965824,
      "y": 106.12279110057776,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1094299896,
      "groupIds": [
        "oCizKDxwJR0_xadyu1U-s",
        "XOmrDYeKZbtBad03NUVlw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1106,
      "versionNonce": 559901064,
      "isDeleted": false,
      "id": "bYj77Rzmjzf8NSjoCwzxt",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.5447932701487375,
      "x": 1069.089741413119,
      "y": 105.47928063454734,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 1506333176,
      "groupIds": [
        "oCizKDxwJR0_xadyu1U-s",
        "XOmrDYeKZbtBad03NUVlw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1147,
      "versionNonce": 1003026568,
      "isDeleted": false,
      "id": "EHmP4t_Ln-k9nf0WDuFxp",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.466929440124808,
      "x": 1065.1678246220067,
      "y": 104.30035945599846,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 902550264,
      "groupIds": [
        "oCizKDxwJR0_xadyu1U-s",
        "XOmrDYeKZbtBad03NUVlw",
        "gOQmYoFEh-8jFI92IdwYc",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1476,
      "versionNonce": 271099784,
      "isDeleted": false,
      "id": "mG1ykrgdbCjwTTLG076UZ",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.970693917422981,
      "x": 1043.6956781873973,
      "y": 109.05152880383471,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 2076619768,
      "groupIds": [
        "aWz-oPOUVw7blecn5ozPM",
        "htHHyoSrCWjL5m6ei2Yqp",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 980,
      "versionNonce": 985298568,
      "isDeleted": false,
      "id": "Gg1-a5DZ1iqKankEc7NZZ",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 3.159442560291101,
      "x": 1058.2415775748786,
      "y": 105.58456047024029,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1639806200,
      "groupIds": [
        "aWJ4c9SQuu3MMphEuQ-ho",
        "htHHyoSrCWjL5m6ei2Yqp",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383396,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1047,
      "versionNonce": 1882875272,
      "isDeleted": false,
      "id": "WwKJCewx6wFvIxRehBPm7",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 9.268330098614793,
      "x": 1054.6420144085232,
      "y": 105.72185538275599,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1026823672,
      "groupIds": [
        "aWJ4c9SQuu3MMphEuQ-ho",
        "htHHyoSrCWjL5m6ei2Yqp",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1066,
      "versionNonce": 1843688584,
      "isDeleted": false,
      "id": "LHWQbDIvQxDN7glIRnmaX",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 9.268330098614793,
      "x": 1049.9323826134132,
      "y": 107.30966487248091,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 321820408,
      "groupIds": [
        "aWJ4c9SQuu3MMphEuQ-ho",
        "htHHyoSrCWjL5m6ei2Yqp",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1109,
      "versionNonce": 1769251720,
      "isDeleted": false,
      "id": "WrjcrvJkD_rOZMXa3vD3o",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 9.190466268590864,
      "x": 1046.5890933046576,
      "y": 108.06753249633519,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 674297848,
      "groupIds": [
        "aWJ4c9SQuu3MMphEuQ-ho",
        "htHHyoSrCWjL5m6ei2Yqp",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1562,
      "versionNonce": 19329672,
      "isDeleted": false,
      "id": "0xUXIFFhdKATefNgdnCTU",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.340556631814806,
      "x": 1030.9425520228833,
      "y": 123.07231991978585,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 272578808,
      "groupIds": [
        "qYk4rrM4sZlnENZKO3UnW",
        "sntyvqr_NecJhARNGx2SL",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1055,
      "versionNonce": 733438344,
      "isDeleted": false,
      "id": "ZfGjqQgXwQywcm8hhtTbN",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.966144189281938,
      "x": 1040.3818202212965,
      "y": 113.2999529930529,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1054693880,
      "groupIds": [
        "TjeeBqtMlR-uheSJHDiPh",
        "sntyvqr_NecJhARNGx2SL",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1127,
      "versionNonce": 677507208,
      "isDeleted": false,
      "id": "aZlViJDSlHn7tGAruxEJU",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.791846420426044,
      "x": 1037.2111805862442,
      "y": 115.53583148972575,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 386882296,
      "groupIds": [
        "TjeeBqtMlR-uheSJHDiPh",
        "sntyvqr_NecJhARNGx2SL",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1136,
      "versionNonce": 1860345736,
      "isDeleted": false,
      "id": "5d6J2AYoIsJCqodlKVvFF",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.791846420426044,
      "x": 1034.5956147314077,
      "y": 118.39218538985745,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 2069445624,
      "groupIds": [
        "TjeeBqtMlR-uheSJHDiPh",
        "sntyvqr_NecJhARNGx2SL",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1187,
      "versionNonce": 527789704,
      "isDeleted": false,
      "id": "Id2K7BQkmsbEgwiaNHvnl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.713982590402114,
      "x": 1031.5298366450852,
      "y": 121.68478825218517,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 1572797688,
      "groupIds": [
        "TjeeBqtMlR-uheSJHDiPh",
        "sntyvqr_NecJhARNGx2SL",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1640,
      "versionNonce": 1926651272,
      "isDeleted": false,
      "id": "WQpwsmcJYAvTLHlb5qqMO",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.850435269712758,
      "x": 1027.322055413258,
      "y": 141.29685506859505,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1338285560,
      "groupIds": [
        "mdak74oZmgKI-RKor4lH3",
        "Lcydq1Ofxyd1uRFHNQht1",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1132,
      "versionNonce": 708930696,
      "isDeleted": false,
      "id": "kY4qf7bBSoSDFsDAonEva",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.383650586810933,
      "x": 1027.767632811453,
      "y": 129.6196997470763,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 309206776,
      "groupIds": [
        "rXzQvZA_jRDvfnnUX32Os",
        "Lcydq1Ofxyd1uRFHNQht1",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1206,
      "versionNonce": 2138217352,
      "isDeleted": false,
      "id": "iYo2CtOtKYQEDM57HD40c",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.20935281795504,
      "x": 1025.9097311088917,
      "y": 132.79754112674334,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 641695736,
      "groupIds": [
        "rXzQvZA_jRDvfnnUX32Os",
        "Lcydq1Ofxyd1uRFHNQht1",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1221,
      "versionNonce": 346417800,
      "isDeleted": false,
      "id": "BDJwLwzCNzpBXbwJLDZ2G",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.20935281795504,
      "x": 1025.7465834795503,
      "y": 136.198675323957,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 1530708216,
      "groupIds": [
        "rXzQvZA_jRDvfnnUX32Os",
        "Lcydq1Ofxyd1uRFHNQht1",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1268,
      "versionNonce": 1687640456,
      "isDeleted": false,
      "id": "B1tuUNRwrNKyWw6chx8J7",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 8.13148898793111,
      "x": 1025.7189251669538,
      "y": 140.34816595989207,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 1625940472,
      "groupIds": [
        "rXzQvZA_jRDvfnnUX32Os",
        "Lcydq1Ofxyd1uRFHNQht1",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1689,
      "versionNonce": 847440008,
      "isDeleted": false,
      "id": "1X5bM69TBInx5Cc39a4MP",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.334712057160889,
      "x": 1034.5138068116148,
      "y": 158.13530394464817,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1447748344,
      "groupIds": [
        "zN689KS9Kxbl5l-WJ8L2b",
        "9XDR2v2yWGCJb07_IfL46",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1190,
      "versionNonce": 2049218440,
      "isDeleted": false,
      "id": "smte7vi0Hb6SsmIRNQvAU",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.811713775242126,
      "x": 1027.0233729407373,
      "y": 149.3365526837065,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 1508099064,
      "groupIds": [
        "Jioetyts4aSni0Wc5m4to",
        "9XDR2v2yWGCJb07_IfL46",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1269,
      "versionNonce": 1605920392,
      "isDeleted": false,
      "id": "7A1rcZxIgibmrNb7OsBBA",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.637416006386232,
      "x": 1027.450721482312,
      "y": 153.30036268261665,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 431523064,
      "groupIds": [
        "Jioetyts4aSni0Wc5m4to",
        "9XDR2v2yWGCJb07_IfL46",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1281,
      "versionNonce": 536499592,
      "isDeleted": false,
      "id": "U7GuWg-FQFdG1o39ef9rx",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.637416006386232,
      "x": 1029.1754665346089,
      "y": 155.6877986065798,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 1666000376,
      "groupIds": [
        "Jioetyts4aSni0Wc5m4to",
        "9XDR2v2yWGCJb07_IfL46",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383397,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1327,
      "versionNonce": 2516104,
      "isDeleted": false,
      "id": "h8jL7BgZwCiKbJiccyKI1",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.559552176362303,
      "x": 1031.2590192142006,
      "y": 158.803031253158,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 866494200,
      "groupIds": [
        "Jioetyts4aSni0Wc5m4to",
        "9XDR2v2yWGCJb07_IfL46",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1734,
      "versionNonce": 1039700872,
      "isDeleted": false,
      "id": "HfpI3EBqLOZIUw8CopY49",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.026015293378638,
      "x": 1047.1706930973965,
      "y": 169.5461324783206,
      "strokeColor": "#ff7043",
      "backgroundColor": "#fff",
      "width": 0.002850147906090324,
      "height": 5.074594937944382,
      "seed": 1195771896,
      "groupIds": [
        "gCe_KP01zh55vsj8TEB6c",
        "7iwovNHTIOdyQSVLGokMy",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.002850147906090324,
          5.074594937944382
        ]
      ]
    },
    {
      "type": "line",
      "version": 1257,
      "versionNonce": 2142036616,
      "isDeleted": false,
      "id": "xsKRboS9ZYcxtioyXFXXx",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.369675388070924,
      "x": 1035.0746865041747,
      "y": 164.3384099264308,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 707938552,
      "groupIds": [
        "z9Y9AYl6CQNhQWcmpyr7H",
        "7iwovNHTIOdyQSVLGokMy",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1337,
      "versionNonce": 1140806024,
      "isDeleted": false,
      "id": "5s0mIhhlw3ALQwNvHssE3",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.19537761921503,
      "x": 1037.6241316546934,
      "y": 167.32634419511794,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 1206807032,
      "groupIds": [
        "z9Y9AYl6CQNhQWcmpyr7H",
        "7iwovNHTIOdyQSVLGokMy",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1360,
      "versionNonce": 1378607240,
      "isDeleted": false,
      "id": "iQY4LZHj3mjwoiMUfCtHA",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.19537761921503,
      "x": 1040.6724429007581,
      "y": 170.40154204034366,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 2071882488,
      "groupIds": [
        "z9Y9AYl6CQNhQWcmpyr7H",
        "7iwovNHTIOdyQSVLGokMy",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1394,
      "versionNonce": 703500168,
      "isDeleted": false,
      "id": "vsDVSWZjnA9E_gvwthaHk",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 7.117513789191101,
      "x": 1043.7775848433466,
      "y": 171.6359211404589,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 526797816,
      "groupIds": [
        "z9Y9AYl6CQNhQWcmpyr7H",
        "7iwovNHTIOdyQSVLGokMy",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    },
    {
      "type": "line",
      "version": 1319,
      "versionNonce": 1937989256,
      "isDeleted": false,
      "id": "olJn68aW_Z7IJUSsLvQkM",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.878533599295732,
      "x": 1048.7652459830783,
      "y": 175.01029570474202,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.13385788646434224,
      "height": 1.824909818276623,
      "seed": 497683704,
      "groupIds": [
        "yPIdiHVxLiy_ePyCR-30f",
        "S_idRPhbaHho_kpbqXhbB",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.13385788646434224,
          -1.824909818276623
        ]
      ]
    },
    {
      "type": "line",
      "version": 1387,
      "versionNonce": 1873169800,
      "isDeleted": false,
      "id": "Edcd597v0NtE3ZGvSieOB",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.704235830439838,
      "x": 1052.236174741296,
      "y": 176.6542187430698,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.10961722295540602,
      "height": 1.9312918250918398,
      "seed": 2144185848,
      "groupIds": [
        "yPIdiHVxLiy_ePyCR-30f",
        "S_idRPhbaHho_kpbqXhbB",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.10961722295540602,
          -1.9312918250918398
        ]
      ]
    },
    {
      "type": "line",
      "version": 1401,
      "versionNonce": 1737710728,
      "isDeleted": false,
      "id": "U24QSlSEfvlCb4V6bZh8e",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.704235830439838,
      "x": 1056.1418818003908,
      "y": 177.28050978386295,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15393862916480724,
      "height": 1.4493087289890376,
      "seed": 2127900408,
      "groupIds": [
        "yPIdiHVxLiy_ePyCR-30f",
        "S_idRPhbaHho_kpbqXhbB",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15393862916480724,
          -1.4493087289890376
        ]
      ]
    },
    {
      "type": "line",
      "version": 1445,
      "versionNonce": 486968200,
      "isDeleted": false,
      "id": "dLghPO0y4N7KB8GxBCKSw",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 6.626372000415909,
      "x": 1059.7637637793173,
      "y": 177.55113577418354,
      "strokeColor": "#ff7043",
      "backgroundColor": "transparent",
      "width": 0.15495851926454807,
      "height": 1.1604068145567383,
      "seed": 323260408,
      "groupIds": [
        "yPIdiHVxLiy_ePyCR-30f",
        "S_idRPhbaHho_kpbqXhbB",
        "cSYzw6LXU8ej7pfT_uMdR",
        "-IHBW5QNCpMiroffvx8Q_"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1643333383398,
      "startBinding": null,
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          -0.15495851926454807,
          -1.1604068145567383
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/search.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "text",
      "version": 354,
      "versionNonce": 1260634512,
      "isDeleted": false,
      "id": "Buic2Lx427wuSIW8P_Rw5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 829,
      "y": 184,
      "strokeColor": "#000000",
      "backgroundColor": "#228be6",
      "width": 452,
      "height": 46,
      "seed": 373648901,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676789638,
      "link": null,
      "locked": false,
      "fontSize": 36,
      "fontFamily": 1,
      "text": "What is semantic search?",
      "baseline": 32,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "What is semantic search?"
    },
    {
      "type": "rectangle",
      "version": 786,
      "versionNonce": 1756557200,
      "isDeleted": false,
      "id": "U2NgEIEiFpAlwmv5Xnyzr",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 50,
      "angle": 0,
      "x": 555.5,
      "y": 425.30499999999995,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 995.3286713286714,
      "height": 339.0000000000001,
      "seed": 1946478225,
      "groupIds": [
        "C_65R9XVeMQED2nMfIW2D"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676804492,
      "link": "",
      "locked": false
    },
    {
      "type": "text",
      "version": 569,
      "versionNonce": 1675727216,
      "isDeleted": false,
      "id": "fbSO8bnZmAdvIXZxBAtHY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 570.7237762237763,
      "y": 439.50080419580416,
      "strokeColor": "#000",
      "backgroundColor": "#03a9f4",
      "width": 967,
      "height": 312,
      "seed": 1586673137,
      "groupIds": [
        "C_65R9XVeMQED2nMfIW2D"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676804492,
      "link": null,
      "locked": false,
      "fontSize": 20.27972027972028,
      "fontFamily": 1,
      "text": "Query                  Best Match\n----------------------------------------------------\nfeel good story        Maine man wins $1M from $25 lottery ticket\nclimate change         Canada's last fully intact ice shelf has suddenly collapsed, forming a \n                       Manhattan-sized iceberg\npublic health story     US tops 5 million confirmed virus cases\nwar                    Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nwildlife                 The National Park Service warns against sacrificing slower friends in a\n                        bear attack\nasia                   Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nlucky                   Maine man wins $1M from $25 lottery ticket\ndishonest junk          Make huge profits without work, earn up to $100,000 a day",
      "baseline": 304,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Query                  Best Match\n----------------------------------------------------\nfeel good story        Maine man wins $1M from $25 lottery ticket\nclimate change         Canada's last fully intact ice shelf has suddenly collapsed, forming a \n                       Manhattan-sized iceberg\npublic health story     US tops 5 million confirmed virus cases\nwar                    Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nwildlife                 The National Park Service warns against sacrificing slower friends in a\n                        bear attack\nasia                   Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nlucky                   Maine man wins $1M from $25 lottery ticket\ndishonest junk          Make huge profits without work, earn up to $100,000 a day"
    },
    {
      "type": "rectangle",
      "version": 952,
      "versionNonce": 581259632,
      "isDeleted": false,
      "id": "UO6MS3wSDu7yg2421__LI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 933,
      "y": 267,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 214,
      "height": 49,
      "seed": 1629565989,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "text",
          "id": "8sp7H8ijWBlh6aMgZ0XTP"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        },
        {
          "id": "SJ0F0Y81z9hir5qQWAJjk",
          "type": "arrow"
        }
      ],
      "updated": 1658676831306,
      "link": null,
      "locked": false
    },
    {
      "type": "rectangle",
      "version": 1611,
      "versionNonce": 1109835152,
      "isDeleted": false,
      "id": "qYd3q0Vjks7VOHUC9RR51",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 549,
      "y": 267.5,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 219,
      "height": 52,
      "seed": 1441952427,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "text",
          "id": "WPeWn6N4rCHf0jY16N9Ge"
        },
        {
          "id": "Qzp41i_jzQIBlAB_qFKFH",
          "type": "arrow"
        }
      ],
      "updated": 1658676831306,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1297,
      "versionNonce": 1725539184,
      "isDeleted": false,
      "id": "WPeWn6N4rCHf0jY16N9Ge",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 554,
      "y": 275.5,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 209,
      "height": 36,
      "seed": 870516459,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831306,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Vectorize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "qYd3q0Vjks7VOHUC9RR51",
      "originalText": "Vectorize"
    },
    {
      "type": "rectangle",
      "version": 1198,
      "versionNonce": 1201179536,
      "isDeleted": false,
      "id": "5VuUdI_BsJ5pyE1nTqJUI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1332,
      "y": 268,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 218,
      "height": 49,
      "seed": 1044404613,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "bJJ9SGsJsvT071qBBH0w5",
          "type": "text"
        },
        {
          "id": "bJJ9SGsJsvT071qBBH0w5",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        },
        {
          "type": "text",
          "id": "bJJ9SGsJsvT071qBBH0w5"
        },
        {
          "id": "SJ0F0Y81z9hir5qQWAJjk",
          "type": "arrow"
        }
      ],
      "updated": 1658676831306,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1381,
      "versionNonce": 78886256,
      "isDeleted": false,
      "id": "bJJ9SGsJsvT071qBBH0w5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1337,
      "y": 274.5,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 208,
      "height": 36,
      "seed": 128953675,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831306,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Search",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "5VuUdI_BsJ5pyE1nTqJUI",
      "originalText": "Search"
    },
    {
      "type": "text",
      "version": 942,
      "versionNonce": 429955472,
      "isDeleted": false,
      "id": "8sp7H8ijWBlh6aMgZ0XTP",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 938,
      "y": 273.5,
      "strokeColor": "#000",
      "backgroundColor": "transparent",
      "width": 204,
      "height": 36,
      "seed": 1854823263,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831306,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Index",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "UO6MS3wSDu7yg2421__LI",
      "originalText": "Index"
    },
    {
      "type": "text",
      "version": 382,
      "versionNonce": 1098614640,
      "isDeleted": false,
      "id": "jWJpSXHkTCzRTCA4tbAgv",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 548.5,
      "y": 347.30499999999995,
      "strokeColor": "#000",
      "backgroundColor": "#03a9f4",
      "width": 296,
      "height": 42,
      "seed": 1241563487,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831307,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Transform input into numbers\n- Similar concepts have similar values",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Transform input into numbers\n- Similar concepts have similar values"
    },
    {
      "type": "text",
      "version": 381,
      "versionNonce": 1266083728,
      "isDeleted": false,
      "id": "qEnmXs0P_MQE8r4c4OWGh",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 931.5,
      "y": 346.2074999999999,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 230,
      "height": 42,
      "seed": 1038536465,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831307,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Save vectors\n- Store content with vectors",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Save vectors\n- Store content with vectors"
    },
    {
      "type": "text",
      "version": 491,
      "versionNonce": 2037979504,
      "isDeleted": false,
      "id": "1q8bzjK8lnKUZj8_A9v7D",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1243,
      "y": 349.2074999999999,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 333,
      "height": 42,
      "seed": 304472945,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1658676831307,
      "link": null,
      "locked": false,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "- Find similar vectors with cosine similarity\n- Add rule-based filters using content",
      "baseline": 36,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Find similar vectors with cosine similarity\n- Add rule-based filters using content"
    },
    {
      "type": "arrow",
      "version": 1813,
      "versionNonce": 1230206352,
      "isDeleted": false,
      "id": "Qzp41i_jzQIBlAB_qFKFH",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 770.5,
      "y": 289.8470411964629,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 158.1310513485223,
      "height": 0.5692601572380909,
      "seed": 660786897,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1658676831307,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "qYd3q0Vjks7VOHUC9RR51",
        "focus": -0.15367587596362536,
        "gap": 2.5
      },
      "endBinding": {
        "elementId": "UO6MS3wSDu7yg2421__LI",
        "focus": 0.027437144815141,
        "gap": 4.3689486514776945
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          158.1310513485223,
          0.5692601572380909
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 1962,
      "versionNonce": 1072982896,
      "isDeleted": false,
      "id": "SJ0F0Y81z9hir5qQWAJjk",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1149.5,
      "y": 292.6790761701911,
      "strokeColor": "#000",
      "backgroundColor": "#f44336",
      "width": 181.5,
      "height": 1.5898915058209013,
      "seed": 899541905,
      "groupIds": [
        "3sURMvhuRfR0M-Q3VRPbg"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1658676831307,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "UO6MS3wSDu7yg2421__LI",
        "focus": 0.08406032225724415,
        "gap": 2.5
      },
      "endBinding": {
        "elementId": "5VuUdI_BsJ5pyE1nTqJUI",
        "focus": 0.09327847520504394,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          181.5,
          -1.5898915058209013
        ]
      ]
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/task.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 4906,
      "versionNonce": 976633260,
      "isDeleted": false,
      "id": "9itz4bqXi6IMeZhGhc42J",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 793.1319708150991,
      "y": 84.01158837080237,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2026142776,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "sbyEF_hTthhrIaA-47du2",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1642714818536
    },
    {
      "type": "rectangle",
      "version": 4954,
      "versionNonce": 1503052564,
      "isDeleted": false,
      "id": "7HUYNqBGn8JbkXw6qt_jr",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 785.5845377793848,
      "y": 76.99149908508815,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1090159432,
      "groupIds": [
        "6R5o6OnVZ7fFC_87zCo7G",
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1642714818536
    },
    {
      "type": "rectangle",
      "version": 5050,
      "versionNonce": 1115269164,
      "isDeleted": false,
      "id": "mI_Mfw3cNZf0AuQhK5KUO",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 776.1894484936695,
      "y": 68.73759283508838,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 1146631480,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1642714818536
    },
    {
      "type": "line",
      "version": 4173,
      "versionNonce": 2037727380,
      "isDeleted": false,
      "id": "y3oKPiP36axXzEF0c7OiC",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 788.1758750805805,
      "y": 117.48482988244103,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 1952111176,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4199,
      "versionNonce": 1944830636,
      "isDeleted": false,
      "id": "-52Udo1sfovtw-gxgUT91",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 790.0311273185948,
      "y": 86.01954258360246,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 923586104,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4224,
      "versionNonce": 1671958036,
      "isDeleted": false,
      "id": "7Tgba-Lgh4JYVwHG-MlYn",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 789.9545278022254,
      "y": 153.41391685395956,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 664293704,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4261,
      "versionNonce": 355896620,
      "isDeleted": false,
      "id": "KsHbDZNNeZoS7zbhCYNon",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 786.770480620695,
      "y": 164.65706526223835,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 2043037496,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4226,
      "versionNonce": 1764043668,
      "isDeleted": false,
      "id": "R5nE2rgdgOpe93Q8VvYhl",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.0553235162608,
      "y": 101.08014413686368,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 571402312,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4241,
      "versionNonce": 130287532,
      "isDeleted": false,
      "id": "lfWLpL8lTqrf2GMoqJY8y",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 787.1628117124112,
      "y": 135.5634731727032,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 2028511288,
      "groupIds": [
        "zRT55AcPrMo-vI2M6Q7JX"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818536,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "text",
      "version": 378,
      "versionNonce": 1294951852,
      "isDeleted": false,
      "id": "uG1gxQCUX_W4xoK8WjzqS",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 762,
      "y": 202,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 117,
      "height": 26,
      "seed": 657172488,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Input Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Input Data"
    },
    {
      "type": "arrow",
      "version": 657,
      "versionNonce": 1814305556,
      "isDeleted": false,
      "id": "3DJ_wB8Ty8UNX4KAZfwjT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 878.680938765565,
      "y": 132.68136619602433,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#ff7043",
      "width": 104.88363252862348,
      "height": 0.6395755770326446,
      "seed": 1663624312,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "startBinding": {
        "elementId": "9itz4bqXi6IMeZhGhc42J",
        "focus": -0.09772881248040827,
        "gap": 14.870387259234633
      },
      "endBinding": {
        "elementId": "FEMcxzUOmwHX8lGW-KR6K",
        "focus": 0.36409382861238426,
        "gap": 13.827470007461386
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          104.88363252862348,
          0.6395755770326446
        ]
      ]
    },
    {
      "type": "arrow",
      "version": 778,
      "versionNonce": 1812844588,
      "isDeleted": false,
      "id": "IEnqZv0JAf73xE5G5LY1s",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1132.8223412617328,
      "y": 135.16943809537315,
      "strokeColor": "#00e676",
      "backgroundColor": "#ff7043",
      "width": 116.4656944766266,
      "height": 1.9037944501569655,
      "seed": 2055241224,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "startBinding": {
        "elementId": "SitnQfRbjMmvvsNhTxQRZ",
        "focus": 0.4543845673944436,
        "gap": 15.775328240798956
      },
      "endBinding": {
        "elementId": "lIucERyh1MMe7Vzo8_q7d",
        "focus": -0.22308504197782658,
        "gap": 15.365988697358375
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          116.4656944766266,
          -1.9037944501569655
        ]
      ]
    },
    {
      "type": "text",
      "version": 354,
      "versionNonce": 9934996,
      "isDeleted": false,
      "id": "g9t3YfCk9S7S38pod_Itm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1019.5,
      "y": 70,
      "strokeColor": "#ff7043",
      "backgroundColor": "#7950f2",
      "width": 84,
      "height": 36,
      "seed": 1032706680,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Action",
      "baseline": 25,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Action"
    },
    {
      "type": "diamond",
      "version": 775,
      "versionNonce": 79652372,
      "isDeleted": false,
      "id": "jfnfQYlAh24YsuMMDHNx7",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1006.140893315531,
      "y": 143.61217895662338,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 112.64736525303451,
      "height": 36.77344700318558,
      "seed": 2110406188,
      "groupIds": [
        "HaVHECspFkRC2Qpa0lGum"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537
    },
    {
      "type": "diamond",
      "version": 816,
      "versionNonce": 903918892,
      "isDeleted": false,
      "id": "6xQj2yQQKSeIivJj9W1ju",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1006.140893315531,
      "y": 134.74039902836483,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 112.64736525303451,
      "height": 36.77344700318558,
      "seed": 1235290772,
      "groupIds": [
        "HaVHECspFkRC2Qpa0lGum"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537
    },
    {
      "type": "diamond",
      "version": 898,
      "versionNonce": 496075668,
      "isDeleted": false,
      "id": "FEMcxzUOmwHX8lGW-KR6K",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1006.140893315531,
      "y": 122.10983943365278,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 112.64736525303451,
      "height": 36.77344700318558,
      "seed": 1689884844,
      "groupIds": [
        "HaVHECspFkRC2Qpa0lGum"
      ],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        },
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1642714818537
    },
    {
      "type": "diamond",
      "version": 940,
      "versionNonce": 1625894828,
      "isDeleted": false,
      "id": "SitnQfRbjMmvvsNhTxQRZ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1006.140893315531,
      "y": 109.57816713635543,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 112.64736525303451,
      "height": 36.77344700318558,
      "seed": 264935444,
      "groupIds": [
        "HaVHECspFkRC2Qpa0lGum"
      ],
      "strokeSharpness": "round",
      "boundElements": [
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1642714818537
    },
    {
      "type": "rectangle",
      "version": 4955,
      "versionNonce": 1631842580,
      "isDeleted": false,
      "id": "qfqvZwfY8g2maDsv8tzi-",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1281.5965467571473,
      "y": 81.99348491888458,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 320002220,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "sbyEF_hTthhrIaA-47du2",
          "type": "arrow"
        },
        {
          "id": "3DJ_wB8Ty8UNX4KAZfwjT",
          "type": "arrow"
        }
      ],
      "updated": 1642714818537
    },
    {
      "type": "rectangle",
      "version": 5003,
      "versionNonce": 972970540,
      "isDeleted": false,
      "id": "Cydk-vrvGXEEAXU4XHWk7",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1274.049113721433,
      "y": 74.97339563317036,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 2124765204,
      "groupIds": [
        "m2CJZBWcxZ3sXLBcib5Jt",
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        }
      ],
      "updated": 1642714818537
    },
    {
      "type": "rectangle",
      "version": 5100,
      "versionNonce": 793554580,
      "isDeleted": false,
      "id": "lIucERyh1MMe7Vzo8_q7d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1264.6540244357177,
      "y": 66.71948938317058,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 70.67858069123133,
      "height": 107.25081879410921,
      "seed": 722133804,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "arrow",
          "id": "CFu0B4Mw_1wC1Hbgx8Fs0"
        },
        {
          "type": "arrow",
          "id": "XIl_NhaFtRO00pX5Pq6VU"
        },
        {
          "type": "arrow",
          "id": "EndiSTFlx1AT7vcBVjgve"
        },
        {
          "id": "IEnqZv0JAf73xE5G5LY1s",
          "type": "arrow"
        }
      ],
      "updated": 1642714818537
    },
    {
      "type": "line",
      "version": 4221,
      "versionNonce": 1007301804,
      "isDeleted": false,
      "id": "3UaWROnZ6NNaCTSa51ntD",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1276.6404510226287,
      "y": 115.46672643052324,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.57983585730082,
      "height": 3.249953844290203,
      "seed": 166278548,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          40.42449133807562,
          0.1573930526684746
        ],
        [
          46.57983585730082,
          -3.0925607916217284
        ]
      ]
    },
    {
      "type": "line",
      "version": 4247,
      "versionNonce": 140622868,
      "isDeleted": false,
      "id": "H3c0943V86dq4MeskNY-E",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1278.495703260643,
      "y": 84.00143913168466,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 45.567415680676426,
      "height": 2.8032978840147194,
      "seed": 1285397932,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          16.832548902953302,
          -2.8032978840147194
        ],
        [
          45.567415680676426,
          -0.3275477042019195
        ]
      ]
    },
    {
      "type": "line",
      "version": 4272,
      "versionNonce": 1958133548,
      "isDeleted": false,
      "id": "g62fLo1BCU_TuN8OEk_ie",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1278.4191037442736,
      "y": 151.39581340204182,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 48.33668263438425,
      "height": 4.280657518731036,
      "seed": 198533908,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          26.41225578429045,
          -0.2552319773002338
        ],
        [
          37.62000339651456,
          2.3153712935189787
        ],
        [
          48.33668263438425,
          -1.9652862252120569
        ]
      ]
    },
    {
      "type": "line",
      "version": 4309,
      "versionNonce": 1719212436,
      "isDeleted": false,
      "id": "jMbD-eUPMlCihQh7GK--c",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.2350565627432,
      "y": 162.6389618103206,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 54.40694982784246,
      "height": 2.9096445412231735,
      "seed": 1358235692,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          10.166093050596771,
          -1.166642430373031
        ],
        [
          16.130660965377448,
          -0.8422655250909383
        ],
        [
          46.26079588567538,
          0.6125567455206506
        ],
        [
          54.40694982784246,
          -2.297087795702523
        ]
      ]
    },
    {
      "type": "line",
      "version": 4274,
      "versionNonce": 745404844,
      "isDeleted": false,
      "id": "lmzq_v_7Zzb2T4WxQPEqG",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.519899458309,
      "y": 99.06204068494588,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 719315092,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "line",
      "version": 4289,
      "versionNonce": 348702484,
      "isDeleted": false,
      "id": "YoZdONxrJotpnXjxFe748",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1275.6273876544594,
      "y": 133.54536972078546,
      "strokeColor": "#00e676",
      "backgroundColor": "#fff",
      "width": 46.92865289294453,
      "height": 2.4757501798128,
      "seed": 1543185068,
      "groupIds": [
        "33LizGdDAL7lJNoiSiJZy"
      ],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": null,
      "points": [
        [
          0,
          0
        ],
        [
          8.093938105125233,
          1.4279702913643746
        ],
        [
          18.193786115221407,
          -0.5912874140789839
        ],
        [
          46.92865289294453,
          1.884462765733816
        ]
      ]
    },
    {
      "type": "text",
      "version": 450,
      "versionNonce": 240772140,
      "isDeleted": false,
      "id": "fAIYPmdTgnjEqc8PGu-SI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 0,
      "opacity": 100,
      "angle": 0,
      "x": 1255.9645759420482,
      "y": 203.9818965480822,
      "strokeColor": "#00e676",
      "backgroundColor": "#03a9f4",
      "width": 134,
      "height": 26,
      "seed": 1891202964,
      "groupIds": [],
      "strokeSharpness": "round",
      "boundElements": [],
      "updated": 1642714818537,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "Output Data",
      "baseline": 18,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Output Data"
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/why.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 639,
      "versionNonce": 1876686474,
      "isDeleted": false,
      "id": "mG5J3YEl3JUm8tWD2gLax",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 840.0920889678455,
      "y": 323.8650942251963,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 412.0000000000001,
      "height": 54.999999999999964,
      "seed": 322487816,
      "groupIds": [
        "VYua_NGvuS1eX_KPHCgOc"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731084295790,
      "index": "a4",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 210,
      "versionNonce": 82579222,
      "isDeleted": false,
      "id": "N9ytCRthOA395FeHYtiWS",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1019.5920889678455,
      "y": 335.8362480713501,
      "strokeColor": "#343a40",
      "backgroundColor": "#03a9f4",
      "width": 53,
      "height": 36,
      "seed": 1396884744,
      "groupIds": [
        "VYua_NGvuS1eX_KPHCgOc"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731084295790,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "API",
      "baseline": 25,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "API",
      "index": "a5",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false,
      "autoResize": true,
      "lineHeight": 1.2857142857142858
    },
    {
      "type": "rectangle",
      "version": 460,
      "versionNonce": 1551291798,
      "isDeleted": false,
      "id": "QKBksUpSSix31GSKFO1uV",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 837.5920889678456,
      "y": 456.8747096098117,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 417,
      "height": 60.846153846153825,
      "seed": 910262792,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "4o-d8ZzuqEpB7ROBAyReL",
          "type": "text"
        }
      ],
      "updated": 1731084298697,
      "index": "a6",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 263,
      "versionNonce": 1011157002,
      "isDeleted": false,
      "id": "4o-d8ZzuqEpB7ROBAyReL",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 842.4670889678456,
      "y": 469.2977865328886,
      "strokeColor": "#343a40",
      "backgroundColor": "#03a9f4",
      "width": 407,
      "height": 36,
      "seed": 1820749832,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731084298697,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Embeddings",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "QKBksUpSSix31GSKFO1uV",
      "originalText": "Embeddings",
      "index": "a7",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false,
      "autoResize": true,
      "lineHeight": 1.2857142857142858
    },
    {
      "type": "rectangle",
      "version": 847,
      "versionNonce": 266827407,
      "isDeleted": false,
      "id": "uFbzRXgEesWgll3llORrW",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 969.4670889678455,
      "y": 387.5069211482732,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 135,
      "height": 60.846153846153825,
      "seed": 531948552,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "text",
          "id": "Ctud-Ap642NinH0-64fKT"
        }
      ],
      "updated": 1731149308913,
      "index": "a8",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 617,
      "versionNonce": 1566326241,
      "isDeleted": false,
      "id": "Ctud-Ap642NinH0-64fKT",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 988.8004223011789,
      "y": 399.9299980713501,
      "strokeColor": "#000000",
      "backgroundColor": "#03a9f4",
      "width": 96.33333333333333,
      "height": 36,
      "seed": 720254072,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731149308913,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Pipeline",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "uFbzRXgEesWgll3llORrW",
      "originalText": "Pipeline",
      "index": "a9",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false,
      "autoResize": true,
      "lineHeight": 1.2857142857142858
    },
    {
      "type": "rectangle",
      "version": 827,
      "versionNonce": 1640467681,
      "isDeleted": false,
      "id": "_zQO_kyS5Pvq1zDhEn9hv",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1111.4863197370764,
      "y": 388.1766044061918,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 141,
      "height": 59.76470588235287,
      "seed": 1988829960,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "id": "tL_gw7MpliV1bDFJp8Cql",
          "type": "text"
        }
      ],
      "updated": 1731149323000,
      "index": "aA",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 617,
      "versionNonce": 1776634049,
      "isDeleted": false,
      "id": "tL_gw7MpliV1bDFJp8Cql",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1122.7113197370763,
      "y": 400.0589573473683,
      "strokeColor": "#000000",
      "backgroundColor": "#03a9f4",
      "width": 118.55,
      "height": 36,
      "seed": 274132856,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731149323001,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Workflow",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "_zQO_kyS5Pvq1zDhEn9hv",
      "originalText": "Workflow",
      "index": "aB",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false,
      "autoResize": true,
      "lineHeight": 1.2857142857142858
    },
    {
      "type": "rectangle",
      "version": 910,
      "versionNonce": 391148577,
      "isDeleted": false,
      "id": "STAkXjIBdAZgXMIKZfGnx",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 839.6978581986145,
      "y": 387.38672884058093,
      "strokeColor": "#9775fa",
      "backgroundColor": "#9775fa",
      "width": 121,
      "height": 60.846153846153825,
      "seed": 362222090,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [
        {
          "type": "text",
          "id": "1iyFs-1PG5afe6QKq1s34"
        }
      ],
      "updated": 1731149289560,
      "index": "aC",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 690,
      "versionNonce": 456727151,
      "isDeleted": false,
      "id": "1iyFs-1PG5afe6QKq1s34",
      "fillStyle": "cross-hatch",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 861.8895238480286,
      "y": 399.80980576365783,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "#03a9f4",
      "width": 76.61666870117188,
      "height": 36,
      "seed": 565985482,
      "groupIds": [
        "DlS9eiqyrIHp45FmS35GN"
      ],
      "strokeSharpness": "sharp",
      "boundElements": [],
      "updated": 1731149289560,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Agent",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "STAkXjIBdAZgXMIKZfGnx",
      "originalText": "Agent",
      "index": "aD",
      "frameId": null,
      "roundness": null,
      "link": null,
      "locked": false,
      "autoResize": true,
      "lineHeight": 1.2857142857142858
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}


================================================
FILE: docs/images/workflow.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 1659,
      "versionNonce": 1307774681,
      "isDeleted": false,
      "id": "qYd3q0Vjks7VOHUC9RR51",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 425,
      "y": 339.5,
      "strokeColor": "#00e676",
      "backgroundColor": "#00e676",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1441952427,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "WPeWn6N4rCHf0jY16N9Ge"
        }
      ],
      "updated": 1673789051585,
      "link": null,
      "locked": false
    },
    {
      "type": "arrow",
      "version": 311,
      "versionNonce": 924248664,
      "isDeleted": false,
      "id": "tgnQzXC9s8RY4oImBOXuB",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 562.5285999651802,
      "y": 272.4578674923631,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.4714000348197942,
      "height": 72.5421325076369,
      "seed": 650463755,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1658678202111,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "B435ajoI5vAQBDvzkd8aY",
        "focus": 0.05963733900881362,
        "gap": 5.457867492363107
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          0.4714000348197942,
          72.5421325076369
        ]
      ]
    },
    {
      "type": "text",
      "version": 1347,
      "versionNonce": 162309943,
      "isDeleted": false,
      "id": "WPeWn6N4rCHf0jY16N9Ge",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 430,
      "y": 344.5,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 870516459,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673789051585,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Translate",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "qYd3q0Vjks7VOHUC9RR51",
      "originalText": "Translate"
    },
    {
      "type": "rectangle",
      "version": 168,
      "versionNonce": 2070244952,
      "isDeleted": false,
      "id": "IxMxusRKX2PpnJT1uY0cC",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 819,
      "y": 217.5,
      "strokeColor": "#ffeb3b",
      "backgroundColor": "#ffeb3b",
      "width": 290.0000000000001,
      "height": 49,
      "seed": 1364021803,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "type": "text",
          "id": "SSXjX_URKvcVC8h-Qgz4j"
        },
        {
          "id": "_YNRYTAnVzdhhKxFtkVyg",
          "type": "arrow"
        }
      ],
      "updated": 1658678121633,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 73,
      "versionNonce": 1790001960,
      "isDeleted": false,
      "id": "SSXjX_URKvcVC8h-Qgz4j",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 824,
      "y": 224,
      "strokeColor": "#000000",
      "backgroundColor": "#82c91e",
      "width": 280,
      "height": 36,
      "seed": 948915563,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678121633,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Extract Text",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "IxMxusRKX2PpnJT1uY0cC",
      "originalText": "Extract Text"
    },
    {
      "type": "rectangle",
      "version": 205,
      "versionNonce": 1180033880,
      "isDeleted": false,
      "id": "lATIKISgJPOGnUHleuFRH",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 821,
      "y": 338,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1256311595,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "21WiUuyDtpQ9FnJ74REpJ",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1658678121633,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 84,
      "versionNonce": 1559141928,
      "isDeleted": false,
      "id": "21WiUuyDtpQ9FnJ74REpJ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 826,
      "y": 343,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 626266373,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678121633,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Summarize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "lATIKISgJPOGnUHleuFRH",
      "originalText": "Summarize"
    },
    {
      "type": "arrow",
      "version": 150,
      "versionNonce": 108519256,
      "isDeleted": false,
      "id": "_YNRYTAnVzdhhKxFtkVyg",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 962.2340210300499,
      "y": 270,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.2659789699500834,
      "height": 75,
      "seed": 101249451,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1658678205029,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "IxMxusRKX2PpnJT1uY0cC",
        "focus": 0.012856281024868153,
        "gap": 3.5
      },
      "endBinding": null,
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          0.2659789699500834,
          75
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 213,
      "versionNonce": 1959303464,
      "isDeleted": false,
      "id": "6B3J0wJfi561c9gMsgtXG",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 824,
      "y": 455,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1164190923,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "qJDqmDEZF9Ewh2UrjXq5Z",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1658678121634,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 135,
      "versionNonce": 210169176,
      "isDeleted": false,
      "id": "qJDqmDEZF9Ewh2UrjXq5Z",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 829,
      "y": 460,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 1307943269,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678121634,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Build Vector Index",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "6B3J0wJfi561c9gMsgtXG",
      "originalText": "Build Vector Index"
    },
    {
      "type": "arrow",
      "version": 203,
      "versionNonce": 133313624,
      "isDeleted": false,
      "id": "Q51e0Hav-kYfjX3b8tt-r",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 963.600775377322,
      "y": 387.5,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 0.29392358842710564,
      "height": 66.5,
      "seed": 59173285,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1658678208037,
      "link": null,
      "locked": false,
      "startBinding": {
        "elementId": "lATIKISgJPOGnUHleuFRH",
        "focus": 0.015253485349599789,
        "gap": 3.5
      },
      "endBinding": {
        "elementId": "6B3J0wJfi561c9gMsgtXG",
        "focus": -0.039966641220930875,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          -0.29392358842710564,
          66.5
        ]
      ]
    },
    {
      "type": "rectangle",
      "version": 1246,
      "versionNonce": 842910649,
      "isDeleted": false,
      "id": "5VuUdI_BsJ5pyE1nTqJUI",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1195,
      "y": 221,
      "strokeColor": "#7950f2",
      "backgroundColor": "#7950f2",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 1044404613,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "bJJ9SGsJsvT071qBBH0w5",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        }
      ],
      "updated": 1673789051585,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 1431,
      "versionNonce": 1914742871,
      "isDeleted": false,
      "id": "bJJ9SGsJsvT071qBBH0w5",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1200,
      "y": 226,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 128953675,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673789051585,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Run similarity query",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "5VuUdI_BsJ5pyE1nTqJUI",
      "originalText": "Run similarity query"
    },
    {
      "type": "rectangle",
      "version": 334,
      "versionNonce": 7828646,
      "isDeleted": false,
      "id": "pSbgtf1qAB7tWl-pTld7e",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1197,
      "y": 343,
      "strokeColor": "#ff7043",
      "backgroundColor": "#ff7043",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 210689733,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "pQzKSM3audka1kiQm_Sku",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        },
        {
          "id": "k67YzXNtt1GLh4i8Es5zJ",
          "type": "arrow"
        }
      ],
      "updated": 1673791639562,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 293,
      "versionNonce": 271822438,
      "isDeleted": false,
      "id": "pQzKSM3audka1kiQm_Sku",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1219.5,
      "y": 348,
      "strokeColor": "#000",
      "backgroundColor": "#fa5252",
      "width": 245,
      "height": 36,
      "seed": 1869028363,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1673791644235,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Send notifications",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "pSbgtf1qAB7tWl-pTld7e",
      "originalText": "Send notifications"
    },
    {
      "type": "arrow",
      "version": 310,
      "versionNonce": 1574300026,
      "isDeleted": false,
      "id": "k67YzXNtt1GLh4i8Es5zJ",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1339.1171359117898,
      "y": 265.5,
      "strokeColor": "#000",
      "backgroundColor": "#228be6",
      "width": 1.0119492860442278,
      "height": 76.5,
      "seed": 1656816747,
      "groupIds": [],
      "roundness": {
        "type": 2
      },
      "boundElements": [],
      "updated": 1673791641098,
      "link": null,
      "locked": false,
      "startBinding": null,
      "endBinding": {
        "elementId": "pSbgtf1qAB7tWl-pTld7e",
        "focus": -0.010690950588675632,
        "gap": 1
      },
      "lastCommittedPoint": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "points": [
        [
          0,
          0
        ],
        [
          1.0119492860442278,
          76.5
        ]
      ]
    },
    {
      "type": "text",
      "version": 308,
      "versionNonce": 641647912,
      "isDeleted": false,
      "id": "kKPPLMCj8QQIJLbW-B5hm",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 1180,
      "y": 451,
      "strokeColor": "#000",
      "backgroundColor": "#fab005",
      "width": 296,
      "height": 52,
      "seed": 1079990731,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678121635,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "- API bindings for JavaScript,\n  Rust, Go and Java",
      "baseline": 44,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- API bindings for JavaScript,\n  Rust, Go and Java"
    },
    {
      "type": "text",
      "version": 560,
      "versionNonce": 1977188696,
      "isDeleted": false,
      "id": "1AQ3rj-V4weRtPA8a5-z-",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 437.5,
      "y": 449.5,
      "strokeColor": "#000",
      "backgroundColor": "#fab005",
      "width": 278,
      "height": 52,
      "seed": 836512075,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678121635,
      "link": null,
      "locked": false,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "- Build with Python or YAML\n- Run local or via API",
      "baseline": 44,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "- Build with Python or YAML\n- Run local or via API"
    },
    {
      "type": "rectangle",
      "version": 292,
      "versionNonce": 259776552,
      "isDeleted": false,
      "id": "B435ajoI5vAQBDvzkd8aY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 426,
      "y": 221,
      "strokeColor": "#03a9f4",
      "backgroundColor": "#03a9f4",
      "width": 290.0000000000001,
      "height": 46,
      "seed": 942981672,
      "groupIds": [],
      "roundness": null,
      "boundElements": [
        {
          "id": "AoGnxEHn4x-zq2-0C0VrT",
          "type": "text"
        },
        {
          "id": "Q51e0Hav-kYfjX3b8tt-r",
          "type": "arrow"
        },
        {
          "id": "tgnQzXC9s8RY4oImBOXuB",
          "type": "arrow"
        }
      ],
      "updated": 1658678121635,
      "link": null,
      "locked": false
    },
    {
      "type": "text",
      "version": 171,
      "versionNonce": 2093287976,
      "isDeleted": false,
      "id": "AoGnxEHn4x-zq2-0C0VrT",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 431,
      "y": 226,
      "strokeColor": "#000000",
      "backgroundColor": "#fa5252",
      "width": 280,
      "height": 36,
      "seed": 604886104,
      "groupIds": [],
      "roundness": null,
      "boundElements": [],
      "updated": 1658678188284,
      "link": null,
      "locked": false,
      "fontSize": 28,
      "fontFamily": 1,
      "text": "Summarize",
      "baseline": 25,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "B435ajoI5vAQBDvzkd8aY",
      "originalText": "Summarize"
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#3030"
  },
  "files": {}
}


================================================
FILE: docs/overrides/main.html
================================================
{% extends "base.html" %}

{% block extrahead %}
    {% set title = config.site_name %}
    {% if page and page.meta and page.meta.title %}
        {% set title = title ~ " - " ~ page.meta.title %}
    {% elif page and page.title and not page.is_homepage %}
        {% set title = title ~ " - " ~ page.title %}
    {% endif %}

    <meta property="og:type" content="website" />
    <meta property="og:title" content="{{ title }}" />
    <meta property="og:description" content="{{ config.site_description }}" />
    <meta property="og:url" content="{{ page.canonical_url }}" />
    <meta property="og:image" content="https://repository-images.githubusercontent.com/286301447/a5a3cc1d-0e3f-4fee-b1a4-a30a0d11e794" />
    <meta property="og:image:type" content="image/png" />
    <meta property="og:image:width" content="1920" />
    <meta property="og:image:height" content="960" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="{{ title }}" />
    <meta name="twitter:description" content="{{ config.site_description }}" />
    <meta name="twitter:image" content="https://repository-images.githubusercontent.com/286301447/a5a3cc1d-0e3f-4fee-b1a4-a30a0d11e794" />
{% endblock %}



================================================
FILE: docs/pipeline/index.md
================================================
# Pipeline

![pipeline](../images/pipeline.png#only-light)
![pipeline](../images/pipeline-dark.png#only-dark)

txtai provides a generic pipeline processing framework with the only interface requirement being a `__call__` method. Pipelines are flexible and process various types of data. Pipelines can wrap machine learning models as well as other processes.

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../workflow/#configuration-driven-example) or the [API](../api#local-instance).

## List of pipelines

The following is a list of the current pipelines available in txtai. All pipelines use default models when otherwise not specified. See the [model guide](../models) for the current model recommendations. All pipelines are designed to work with local models via the [Transformers library](https://github.com/huggingface/transformers).

The `LLM` and `RAG` pipelines also have integrations for [llama.cpp](https://github.com/abetlen/llama-cpp-python) and [hosted API models via LiteLLM](https://github.com/BerriAI/litellm). The `LLM` pipeline can be prompted to accomplish many of the same tasks (i.e. summarization, translation, classification).

- Audio
    - [AudioMixer](audio/audiomixer)
    - [AudioStream](audio/audiostream)
    - [Microphone](audio/microphone)
    - [TextToAudio](audio/texttoaudio)
    - [TextToSpeech](audio/texttospeech)
    - [Transcription](audio/transcription)
- Data Processing
    - [FileToHTML](data/filetohtml)
    - [HTMLToMarkdown](data/htmltomd)
    - [Segmentation](data/segmentation)
    - [Tabular](data/tabular)
    - [Text extraction](data/textractor)
- Image
    - [Caption](image/caption)
    - [Image Hash](image/imagehash)
    - [Objects](image/objects)
- Text
    - [Entity](text/entity)
    - [Labeling](text/labels)
    - [LLM](text/llm)
    - [RAG](text/rag)
    - [Reranker](text/reranker)
    - [Similarity](text/similarity)
    - [Summary](text/summary)
    - [Translation](text/translation)
- Training
    - [HF ONNX](train/hfonnx)
    - [ML ONNX](train/mlonnx)
    - [Trainer](train/trainer)



================================================
FILE: docs/pipeline/audio/audiomixer.md
================================================
# Audio Mixer

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Audio Mixer pipeline mixes multiple audio streams into a single stream.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import AudioMixer

# Create and run pipeline
mixer = AudioMixer()
mixer(((audio1, rate1), (audio2, rate2)))
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
audiomixer:

# Run pipeline with workflow
workflow:
  audiomixer:
    tasks:
      - action: audiomixer
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("audiomixer", [[[audio1, rate1], [audio2, rate2]]]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"audiomixer", "elements":[[[audio1, rate1], [audio2, rate2]]]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.AudioMixer.__init__
### ::: txtai.pipeline.AudioMixer.__call__



================================================
FILE: docs/pipeline/audio/audiostream.md
================================================
# Audio Stream

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Audio Stream pipeline is a threaded pipeline that plays audio segments. This pipeline is designed to run on local machines given that it requires access to write to an output device.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import AudioStream

# Create and run pipeline
audio = AudioStream()
audio(data)
```

This pipeline may require additional system dependencies. See [this section](../../../install#environment-specific-prerequisites) for more.

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
audiostream:

# Run pipeline with workflow
workflow:
  audiostream:
    tasks:
      - action: audiostream
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("audiostream", [["numpy data", "sample rate"]]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"audiostream", "elements":[["numpy data", "sample rate"]]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.AudioStream.__init__
### ::: txtai.pipeline.AudioStream.__call__



================================================
FILE: docs/pipeline/audio/microphone.md
================================================
# Microphone

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Microphone pipeline reads input speech from a microphone device. This pipeline is designed to run on local machines given that it requires access to read from an input device.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Microphone

# Create and run pipeline
microphone = Microphone()
microphone()
```

This pipeline may require additional system dependencies. See [this section](../../../install#environment-specific-prerequisites) for more.

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
microphone:

# Run pipeline with workflow
workflow:
  microphone:
    tasks:
      - action: microphone
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("microphone", ["1"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"microphone", "elements":["1"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Microphone.__init__
### ::: txtai.pipeline.Microphone.__call__



================================================
FILE: docs/pipeline/audio/texttoaudio.md
================================================
# Text To Audio

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Text To Audio pipeline generates audio from text.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import TextToAudio

# Create and run pipeline
tta = TextToAudio()
tta("Describe the audio to generate here")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
texttoaudio:

# Run pipeline with workflow
workflow:
  tta:
    tasks:
      - action: texttoaudio
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("tta", ["Describe the audio to generate here"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"tta", "elements":["Describe the audio to generate here"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.TextToAudio.__init__
### ::: txtai.pipeline.TextToAudio.__call__



================================================
FILE: docs/pipeline/audio/texttospeech.md
================================================
# Text To Speech

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Text To Speech pipeline generates speech from text.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import TextToSpeech

# Create and run pipeline with default model
tts = TextToSpeech()
tts("Say something here")

# Stream audio - incrementally generates snippets of audio
yield from tts(
  "Say something here. And say something else.".split(),
  stream=True
)

# Generate audio using a speaker id
tts = TextToSpeech("neuml/vctk-vits-onnx")
tts("Say something here", speaker=15)

# Generate audio using speaker embeddings
tts = TextToSpeech("neuml/txtai-speecht5-onnx")
tts("Say something here", speaker=np.array(...))
```

See the links below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Text to speech generation](https://github.com/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) | Generate speech from text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |

This pipeline is backed by ONNX models from the Hugging Face Hub. The following models are currently available.

- [kokoro-base-onnx](https://huggingface.co/NeuML/kokoro-base-onnx) | [fp16](https://huggingface.co/NeuML/kokoro-fp16-onnx) | [int8](https://huggingface.co/NeuML/kokoro-int8-onnx)
- [ljspeech-jets-onnx](https://huggingface.co/NeuML/ljspeech-jets-onnx)
- [ljspeech-vits-onnx](https://huggingface.co/NeuML/ljspeech-vits-onnx)
- [vctk-vits-onnx](https://huggingface.co/NeuML/vctk-vits-onnx)
- [txtai-speecht5-onnx](https://huggingface.co/NeuML/txtai-speecht5-onnx)

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
texttospeech:

# Run pipeline with workflow
workflow:
  tts:
    tasks:
      - action: texttospeech
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("tts", ["Say something here"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"tts", "elements":["Say something here"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.TextToSpeech.__init__
### ::: txtai.pipeline.TextToSpeech.__call__



================================================
FILE: docs/pipeline/audio/transcription.md
================================================
# Transcription

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Transcription pipeline converts speech in audio files to text.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Transcription

# Create and run pipeline
transcribe = Transcription()
transcribe("path to wav file")
```

This pipeline may require additional system dependencies. See [this section](../../../install#environment-specific-prerequisites) for more.

See the links below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
transcription:

# Run pipeline with workflow
workflow:
  transcribe:
    tasks:
      - action: transcription
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("transcribe", ["path to wav file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"transcribe", "elements":["path to wav file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Transcription.__init__
### ::: txtai.pipeline.Transcription.__call__



================================================
FILE: docs/pipeline/data/filetohtml.md
================================================
# File To HTML

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The File To HTML pipeline transforms files to HTML. It supports the following text extraction backends.

## Apache Tika

[Apache Tika](https://tika.apache.org/) detects and extracts metadata and text from over a thousand different file types. See [this link](https://tika.apache.org/2.9.2/formats.html) for a list of supported document formats.

Apache Tika requires [Java](https://en.wikipedia.org/wiki/Java_(programming_language)) to be installed. An alternative to that is starting a separate Apache Tika service via [this Docker Image](https://hub.docker.com/r/apache/tika) and setting these [environment variables](https://github.com/chrismattmann/tika-python?tab=readme-ov-file#environment-variables).

## Docling

[Docling](https://github.com/DS4SD/docling) parses documents and exports them to the desired format with ease and speed. This is a library that has rapidly gained popularity starting in late 2024. Docling excels in parsing formatting elements from PDFs (tables, sections etc).

See [this link](https://github.com/DS4SD/docling?tab=readme-ov-file#features) for a list of supported document formats.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import FileToHTML

# Create and run pipeline
html = FileToHTML()
html("/path/to/file")
```

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
filetohtml:

# Run pipeline with workflow
workflow:
  html:
    tasks:
      - action: filetohtml
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("html", ["/path/to/file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"html", "elements":["/path/to/file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.FileToHTML.__init__
### ::: txtai.pipeline.FileToHTML.__call__



================================================
FILE: docs/pipeline/data/htmltomd.md
================================================
# HTML To Markdown 

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The HTML To Markdown pipeline transforms HTML to Markdown.

Markdown formatting is applied for headings, blockquotes, lists, code, tables and text. Visual formatting is also included (bold, italic etc).

This pipeline searches for the best node that has relevant text, often found with an `article`, `main` or `body` tag.

The HTML to Markdown pipeline requires the [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/) library to be installed.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import HTMLToMarkdown

# Create and run pipeline
md = HTMLToMarkdown()
md("<html><body>This is a test</body></html>")
```

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
htmltomarkdown:

# Run pipeline with workflow
workflow:
  markdown:
    tasks:
      - action: htmltomarkdown
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("markdown", ["<html><body>This is a test</body></html>"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"markdown", "elements":["<html><body>This is a test</body></html>"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.HTMLToMarkdown.__init__
### ::: txtai.pipeline.HTMLToMarkdown.__call__



================================================
FILE: docs/pipeline/data/segmentation.md
================================================
# Segmentation

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Segmentation pipeline segments text into semantic units.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Segmentation

# Create and run pipeline
segment = Segmentation(sentences=True)
segment("This is a test. And another test.")

# Load third-party chunkers
segment = Segmentation(chunker="semantic")
segment("This is a test. And another test.")
```

The Segmentation pipeline supports segmenting `sentences`, `lines`, `paragraphs` and `sections` using a rules-based approach. Each of these modes can be set when creating the pipeline. Third-party chunkers are also supported via the `chunker` parameter.

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
segmentation:
  sentences: true

# Run pipeline with workflow
workflow:
  segment:
    tasks:
      - action: segmentation
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("segment", ["This is a test. And another test."]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"segment", "elements":["This is a test. And another test."]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Segmentation.__init__
### ::: txtai.pipeline.Segmentation.__call__



================================================
FILE: docs/pipeline/data/tabular.md
================================================
# Tabular

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Tabular pipeline splits tabular data into rows and columns. The tabular pipeline is most useful in creating (id, text, tag) tuples to load into Embedding indexes. 

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Tabular

# Create and run pipeline
tabular = Tabular("id", ["text"])
tabular("path to csv file")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Transform tabular data with composable workflows](https://github.com/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) | Transform, index and search tabular data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
tabular:
    idcolumn: id
    textcolumns:
      - text

# Run pipeline with workflow
workflow:
  tabular:
    tasks:
      - action: tabular
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("tabular", ["path to csv file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"tabular", "elements":["path to csv file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Tabular.__init__
### ::: txtai.pipeline.Tabular.__call__



================================================
FILE: docs/pipeline/data/textractor.md
================================================
# Textractor

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Textractor pipeline extracts and splits text from documents. This pipeline extends the [Segmentation](../segmentation) pipeline.

Each document goes through the following process.

- Content is retrieved if it's not local
- If the document `mime-type` isn't plain text or HTML, it's converted to HTML via the [FiletoHTML](../filetohtml) pipeline
- HTML is converted to Markdown via the [HTMLToMarkdown](../htmltomd) pipeline
- Content is split/chunked based on the [segmentation parameters](../segmentation/#txtai.pipeline.Segmentation.__init__) and returned

The [backend](../filetohtml/#txtai.pipeline.FileToHTML.__init__) parameter sets the FileToHTML pipeline backend. If a backend isn't available, this pipeline assumes input is HTML content and only converts it to Markdown.

See the [FiletoHTML](../filetohtml) and [HTMLToMarkdown](../htmltomd) pipelines to learn more on the dependencies necessary for each of those pipelines.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Textractor

# Create and run pipeline
textract = Textractor()
textract("https://github.com/neuml/txtai")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Extract text from documents](https://github.com/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) | Extract text from PDF, Office, HTML and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
textractor:

# Run pipeline with workflow
workflow:
  textract:
    tasks:
      - action: textractor
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("textract", ["https://github.com/neuml/txtai"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"textract", "elements":["https://github.com/neuml/txtai"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Textractor.__init__
### ::: txtai.pipeline.Textractor.__call__



================================================
FILE: docs/pipeline/image/caption.md
================================================
# Caption

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The caption pipeline reads a list of images and returns a list of captions for those images.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Caption

# Create and run pipeline
caption = Caption()
caption("path to image file")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Generate image captions and detect objects](https://github.com/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) | Captions and object detection for images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
caption:

# Run pipeline with workflow
workflow:
  caption:
    tasks:
      - action: caption
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("caption", ["path to image file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"caption", "elements":["path to image file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Caption.__init__
### ::: txtai.pipeline.Caption.__call__



================================================
FILE: docs/pipeline/image/imagehash.md
================================================
# ImageHash

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The image hash pipeline generates perceptual image hashes. These hashes can be used to detect near-duplicate images. This method is not backed by machine learning models and not intended to find conceptually similar images.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import ImageHash

# Create and run pipeline
ihash = ImageHash()
ihash("path to image file")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Near duplicate image detection](https://github.com/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) | Identify duplicate and near-duplicate images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
imagehash:

# Run pipeline with workflow
workflow:
  imagehash:
    tasks:
      - action: imagehash
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("imagehash", ["path to image file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"imagehash", "elements":["path to image file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.ImageHash.__init__
### ::: txtai.pipeline.ImageHash.__call__



================================================
FILE: docs/pipeline/image/objects.md
================================================
# Objects

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Objects pipeline reads a list of images and returns a list of detected objects.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Objects

# Create and run pipeline
objects = Objects()
objects("path to image file")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Generate image captions and detect objects](https://github.com/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) | Captions and object detection for images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
objects:

# Run pipeline with workflow
workflow:
  objects:
    tasks:
      - action: objects
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("objects", ["path to image file"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"objects", "elements":["path to image file"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Objects.__init__
### ::: txtai.pipeline.Objects.__call__



================================================
FILE: docs/pipeline/text/entity.md
================================================
# Entity

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Entity pipeline applies a token classifier to text and extracts entity/label combinations.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Entity

# Create and run pipeline
entity = Entity()
entity("Canada's last fully intact ice shelf has suddenly collapsed, " \
       "forming a Manhattan-sized iceberg")

# Extract entities using a GLiNER model which supports dynamic labels
entity = Entity("gliner-community/gliner_medium-v2.5")
entity("Canada's last fully intact ice shelf has suddenly collapsed, " \
       "forming a Manhattan-sized iceberg", labels=["country", "city"])
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Entity extraction workflows](https://github.com/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) | Identify entity/label combinations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
entity:

# Run pipeline with workflow
workflow:
  entity:
    tasks:
      - action: entity
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("entity", ["Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"entity", "elements": ["Canadas last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Entity.__init__
### ::: txtai.pipeline.Entity.__call__



================================================
FILE: docs/pipeline/text/labels.md
================================================
# Labels

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling).

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Labels

# Create and run pipeline
labels = Labels()
labels(
    ["Great news", "That's rough"],
    ["positive", "negative"]
)
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Apply labels with zero shot classification](https://github.com/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) | Use zero shot learning for labeling, classification and topic modeling | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
labels:

# Run pipeline with workflow
workflow:
  labels:
    tasks:
      - action: labels
        args: [["positive", "negative"]]
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("labels", ["Great news", "That's rough"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"labels", "elements": ["Great news", "Thats rough"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Labels.__init__
### ::: txtai.pipeline.Labels.__call__



================================================
FILE: docs/pipeline/text/llm.md
================================================
# LLM

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The LLM pipeline runs prompts through a large language model (LLM). This pipeline autodetects the LLM framework based on the model path.

## Example

The following shows a simple example using this pipeline.

```python
from txtai import LLM

# Create LLM pipeline
llm = LLM()

# Run prompt
llm(
  """
  Answer the following question using the provided context.

  Question:
  What are the applications of txtai?

  Context:
  txtai is an open-source platform for semantic search and
  workflows powered by language models.
  """
)

# Instruction tuned models typically require string prompts to
# follow a specific chat template set by the model
llm(
  """
  <|im_start|>system
  You are a friendly assistant.<|im_end|>
  <|im_start|>user
  Answer the following question...<|im_end|>
  <|im_start|>assistant
  """
)

# Chat messages automatically handle templating
llm([
  {"role": "system", "content": "You are a friendly assistant."},
  {"role": "user", "content": "Answer the following question..."}
])

# Set the default role to user and string inputs are converted to chat messages
llm("Answer the following question...", defaultrole="user")
```

The LLM pipeline automatically detects the underlying LLM framework. This can also be manually set.

[Hugging Face Transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/abetlen/llama-cpp-python) and [hosted API models via LiteLLM](https://github.com/BerriAI/litellm) are all supported by this pipeline.

See the [LiteLLM documentation](https://litellm.vercel.app/docs/providers) for the options available with LiteLLM models. llama.cpp models support both local and remote GGUF paths on the HF Hub.

```python
from txtai import LLM

# Transformers
llm = LLM("meta-llama/Meta-Llama-3.1-8B-Instruct")
llm = LLM("meta-llama/Meta-Llama-3.1-8B-Instruct", method="transformers")

# llama.cpp
llm = LLM("microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf")
llm = LLM("microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf",
           method="llama.cpp")

# LiteLLM
llm = LLM("ollama/llama3.1")
llm = LLM("ollama/llama3.1", method="litellm")

# Custom Ollama endpoint
llm = LLM("ollama/llama3.1", api_base="http://localhost:11434")

# Custom OpenAI-compatible endpoint
llm = LLM("openai/llama3.1", api_base="http://localhost:4000")

# LLM APIs - must also set API key via environment variable
llm = LLM("gpt-4o")
llm = LLM("claude-3-5-sonnet-20240620")
```

Models can be externally loaded and passed to pipelines. This is useful for models that are not yet supported by Transformers and/or need special initialization.

```python
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer
from txtai import LLM

# Load Phi 3.5-mini
path = "microsoft/Phi-3.5-mini-instruct"
model = AutoModelForCausalLM.from_pretrained(
  path,
  torch_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(path)

llm = LLM((model, tokenizer))
```

See the links below for more detailed examples.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Prompt-driven search with LLMs](https://github.com/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) | Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) |
| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |
| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |
| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |
| [Generate knowledge with Semantic Graphs and RAG](https://github.com/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) | Knowledge exploration and discovery with Semantic Graphs and RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) |
| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |
| [Advanced RAG with graph path traversal](https://github.com/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) | Graph path traversal to collect complex sets of data for advanced RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) |
| [Advanced RAG with guided generation](https://github.com/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) | Retrieval Augmented and Guided Generation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) |
| [RAG with llama.cpp and external API services](https://github.com/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) | RAG with additional vector and LLM frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) |
| [How RAG with txtai works](https://github.com/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) | Create RAG processes, API services and Docker instances | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Getting started with LLM APIs](https://github.com/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) | Generate embeddings and run LLMs with OpenAI, Claude, Gemini, Bedrock and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |
| [Medical RAG Research with txtai](https://github.com/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) | Analyze PubMed article metadata with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) |
| [GraphRAG with Wikipedia and GPT OSS](https://github.com/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) | Deep graph search powered RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
llm:

# Run pipeline with workflow
workflow:
  llm:
    tasks:
      - action: llm
```

Similar to the Python example above, the underlying [Hugging Face pipeline parameters](https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.pipeline.model) and [model parameters](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel.from_pretrained) can be set in pipeline configuration.

```yaml
llm:
  path: microsoft/Phi-3.5-mini-instruct
  torch_dtype: torch.bfloat16
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("llm", [
  """
  Answer the following question using the provided context.
 
  Question:
  What are the applications of txtai? 

  Context:
  txtai is an open-source platform for semantic search and
  workflows powered by language models.
  """
]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"llm", "elements": ["Answer the following question..."]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.LLM.__init__
### ::: txtai.pipeline.LLM.__call__



================================================
FILE: docs/pipeline/text/rag.md
================================================
# RAG

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Retrieval Augmented Generation (RAG) pipeline joins a prompt, context data store and generative model together to extract knowledge.

The data store can be an embeddings database or a similarity instance with associated input text. The generative model can be a prompt-driven large language model (LLM), an extractive question-answering model or a custom pipeline.

## Example

The following shows a simple example using this pipeline.

```python
from txtai import Embeddings, RAG

# Input data
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, " +
  "forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends " +
  "in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Build embeddings index
embeddings = Embeddings(content=True)
embeddings.index(data)

# Create and run pipeline
rag = RAG(embeddings, "google/flan-t5-base", template="""
  Answer the following question using the provided context.

  Question:
  {question}

  Context:
  {context}
""")

rag("What was won?")

# Instruction tuned models typically require string prompts to
# follow a specific chat template set by the model
rag = RAG(embeddings, "meta-llama/Meta-Llama-3.1-8B-Instruct", template="""
  <|im_start|>system
  You are a friendly assistant.<|im_end|>
  <|im_start|>user
  Answer the following question using the provided context.

  Question:
  {question}

  Context:
  {context}
  <|im_start|>assistant
  """
)
rag("What was won?")

# Inputs are automatically converted to chat messages when a
# system prompt is provided
rag = RAG(
  embeddings,
  "meta-llama/Meta-Llama-3.1-8B-Instruct",
  system="You are a friendly assistant",
  template="""
  Answer the following question using the provided context.

  Question:
  {question}

  Context:
  {context}
""")
rag("What was won?")

# LLM options can be passed as additional arguments
rag = RAG(embeddings, "meta-llama/Meta-Llama-3.1-8B-Instruct", template="""
  Answer the following question using the provided context.

  Question:
  {question}

  Context:
  {context}
""")

# Set the default role to user and string inputs are converted to chat messages
rag("What was won?", defaultrole="user")
```

See the [Embeddings](../../../embeddings) and [LLM](../llm) pages for additional configuration options.

See the links below for more detailed examples.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Prompt-driven search with LLMs](https://github.com/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) | Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) |
| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |
| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |
| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |
| [Generate knowledge with Semantic Graphs and RAG](https://github.com/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) | Knowledge exploration and discovery with Semantic Graphs and RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb) |
| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |
| [Advanced RAG with graph path traversal](https://github.com/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) | Graph path traversal to collect complex sets of data for advanced RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) |
| [Advanced RAG with guided generation](https://github.com/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) | Retrieval Augmented and Guided Generation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/60_Advanced_RAG_with_guided_generation.ipynb) |
| [RAG with llama.cpp and external API services](https://github.com/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) | RAG with additional vector and LLM frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb) |
| [How RAG with txtai works](https://github.com/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) | Create RAG processes, API services and Docker instances | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) |
| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [â–¶ï¸](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |
| [Generative Audio](https://github.com/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) | Storytelling with generative audio workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/66_Generative_Audio.ipynb) |
| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |
| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |
| [Getting started with LLM APIs](https://github.com/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) | Generate embeddings and run LLMs with OpenAI, Claude, Gemini, Bedrock and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/70_Getting_started_with_LLM_APIs.ipynb) |
| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |
| [Extractive QA with txtai](https://github.com/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) | Introduction to extractive question-answering with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) |
| [Extractive QA with Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) | Run extractive question-answering queries with Elasticsearch | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) |
| [Extractive QA to build structured data](https://github.com/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) | Build structured datasets using extractive question-answering | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) |
| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |
| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |
| [Medical RAG Research with txtai](https://github.com/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) | Analyze PubMed article metadata with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/75_Medical_RAG_Research_with_txtai.ipynb) |
| [GraphRAG with Wikipedia and GPT OSS](https://github.com/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) | Deep graph search powered RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Allow documents to be indexed
writable: True

# Content is required for extractor pipeline
embeddings:
  content: True

rag:
  path: google/flan-t5-base
  template: |
    Answer the following question using the provided context.

    Question:
    {question}

    Context:
    {context}

workflow:
  search:
    tasks:
      - action: rag
```

### Run with Workflows

Built in tasks make using the extractor pipeline easier.

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
app.add([
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, " +
  "forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends " +
  "in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
])
app.index()

list(app.workflow("search", ["What was won?"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name": "search", "elements": ["What was won"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.RAG.__init__
### ::: txtai.pipeline.RAG.__call__



================================================
FILE: docs/pipeline/text/reranker.md
================================================
# Reranker

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Reranker pipeline runs embeddings queries and re-ranks them using a similarity pipeline. 

## Example

The following shows a simple example using this pipeline.

```python
from txtai import Embeddings
from txtai.pipeline import Reranker, Similarity

# Embeddings instance
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

# Similarity instance
similarity = Similarity(path="colbert-ir/colbertv2.0", lateencode=True)

# Reranking pipeline
reranker = Reranker(embeddings, similarity)
reranker("Tell me about AI")
```

_Note: Content must be enabled with the embeddings instance for this to work properly._

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [What's new in txtai 9.0](https://github.com/neuml/txtai/blob/master/examples/76_Whats_new_in_txtai_9_0.ipynb) | Learned sparse vectors, late interaction models and rerankers | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/76_Whats_new_in_txtai_9_0.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
embeddings:

similarity:

# Create pipeline using lower case class name
reranker:

# Run pipeline with workflow
workflow:
  translate:
    tasks:
      - reranker
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("reranker", ["Tell me about AI"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"rerank", "elements":["Tell me about AI"]}'
```

## Methods 

Python documentation for the pipeline.

### ::: txtai.pipeline.Reranker.__init__
### ::: txtai.pipeline.Reranker.__call__



================================================
FILE: docs/pipeline/text/similarity.md
================================================
# Similarity

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Similarity pipeline computes similarity between queries and list of text using a text classifier.

This pipeline supports both standard text classification models and zero-shot classification models. The pipeline uses the queries as labels for the input text. The results are transposed to get scores per query/label vs scores per input text. 

Cross-encoder models are supported via the `crossencode=True` constructor parameter. Late interaction (i.e. ColBERT) models are also supported via the `lateencode=True` constructor parameter. CrossEncoder and LateEncoder pipelines back each of these models and can be instantiated directly as well.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Similarity

# Create and run pipeline
similarity = Similarity()
similarity("feel good story", [
    "Maine man wins $1M from $25 lottery ticket", 
    "Don't sacrifice slower friends in a bear attack"
])
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Add semantic search to Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb)  | Add semantic search to existing search systems | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
similarity:
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
app.similarity("feel good story", [
    "Maine man wins $1M from $25 lottery ticket", 
    "Don't sacrifice slower friends in a bear attack"
])
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/similarity" \
  -H "Content-Type: application/json" \
  -d '{"query": "feel good story", "texts": ["Maine man wins $1M from $25 lottery ticket", "Dont sacrifice slower friends in a bear attack"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Similarity.__init__
### ::: txtai.pipeline.Similarity.__call__



================================================
FILE: docs/pipeline/text/summary.md
================================================
# Summary

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Summary pipeline summarizes text. This pipeline runs a text2text model that abstractively creates a summary of the input text.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Summary

# Create and run pipeline
summary = Summary()
summary("Enter long, detailed text to summarize here")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
summary:

# Run pipeline with workflow
workflow:
  summary:
    tasks:
      - action: summary
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("summary", ["Enter long, detailed text to summarize here"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"summary", "elements":["Enter long, detailed text to summarize here"]}'
```

## Methods

Python documentation for the pipeline.

### ::: txtai.pipeline.Summary.__init__
### ::: txtai.pipeline.Summary.__call__



================================================
FILE: docs/pipeline/text/translation.md
================================================
# Translation

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

The Translation pipeline translates text between languages. It supports over 100+ languages. Automatic source language detection is built-in. This pipeline detects the language of each input text row, loads a model for the source-target combination and translates text to the target language.

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import Translation

# Create and run pipeline
translate = Translation()
translate("This is a test translation into Spanish", "es")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |

## Configuration-driven example

Pipelines are run with Python or configuration. Pipelines can be instantiated in [configuration](../../../api/configuration/#pipeline) using the lower case name of the pipeline. Configuration-driven pipelines are run with [workflows](../../../workflow/#configuration-driven-example) or the [API](../../../api#local-instance).

### config.yml
```yaml
# Create pipeline using lower case class name
translation:

# Run pipeline with workflow
workflow:
  translate:
    tasks:
      - action: translation
        args: ["es"]
```

### Run with Workflows

```python
from txtai import Application

# Create and run pipeline with workflow
app = Application("config.yml")
list(app.workflow("translate", ["This is a test translation into Spanish"]))
```

### Run with API

```bash
CONFIG=config.yml uvicorn "txtai.api:app" &

curl \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"translate", "elements":["This is a test translation into Spanish"]}'
```

## Methods 

Python documentation for the pipeline.

### ::: txtai.pipeline.Translation.__init__
### ::: txtai.pipeline.Translation.__call__



================================================
FILE: docs/pipeline/train/hfonnx.md
================================================
# HFOnnx

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to
sequence models (summarization, transcription, translation).

## Example

The following shows a simple example using this pipeline.

```python
from txtai.pipeline import HFOnnx, Labels

# Model path
path = "distilbert-base-uncased-finetuned-sst-2-english"

# Export model to ONNX
onnx = HFOnnx()
model = onnx(path, "text-classification", "model.onnx", True)

# Run inference and validate
labels = Labels((model, path), dynamic=False)
labels("I am happy")
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Export and run models with ONNX](https://github.com/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) | Export models with ONNX, run natively in JavaScript, Java and Rust | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) |

## Methods 

Python documentation for the pipeline.

### ::: txtai.pipeline.HFOnnx.__call__



================================================
FILE: docs/pipeline/train/mlonnx.md
================================================
# MLOnnx

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

Exports a traditional machine learning model (i.e. scikit-learn) to ONNX.

## Example

See the link below for a detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Export and run other machine learning models](https://github.com/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) | Export and run models from scikit-learn, PyTorch and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) |

## Methods 

Python documentation for the pipeline.

### ::: txtai.pipeline.MLOnnx.__call__



================================================
FILE: docs/pipeline/train/trainer.md
================================================
# HFTrainer

![pipeline](../../images/pipeline.png#only-light)
![pipeline](../../images/pipeline-dark.png#only-dark)

Trains a new Hugging Face Transformer model using the Trainer framework.

## Example

The following shows a simple example using this pipeline.

```python
import pandas as pd

from datasets import load_dataset

from txtai.pipeline import HFTrainer

trainer = HFTrainer()

# Pandas DataFrame
df = pd.read_csv("training.csv")
model, tokenizer = trainer("bert-base-uncased", df)

# Hugging Face dataset
ds = load_dataset("glue", "sst2")
model, tokenizer = trainer("bert-base-uncased", ds["train"], columns=("sentence", "label"))

# List of dicts
dt = [{"text": "sentence 1", "label": 0}, {"text": "sentence 2", "label": 1}]]
model, tokenizer = trainer("bert-base-uncased", dt)

# Support additional TrainingArguments
model, tokenizer = trainer("bert-base-uncased", dt, 
                            learning_rate=3e-5, num_train_epochs=5)
```

All [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) are supported as function arguments to the trainer call.

See the links below for more detailed examples.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Train a text labeler](https://github.com/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) | Build text sequence classification models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) |
| [Train without labels](https://github.com/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) | Use zero-shot classifiers to train new models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) |
| [Train a QA model](https://github.com/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) | Build and fine-tune question-answering models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) |
| [Train a language model from scratch](https://github.com/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) | Build new language models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) |

## Training tasks

The HFTrainer pipeline builds and/or fine-tunes models for following training tasks.

| Task | Description |
|:-----|:------------|
| language-generation | Causal language model for text generation (e.g. GPT) |
| language-modeling | Masked language model for general tasks (e.g. BERT) |
| question-answering | Extractive question-answering model, typically with the SQuAD dataset |
| sequence-sequence  | Sequence-Sequence model (e.g. T5) |
| text-classification | Classify text with a set of labels |
| token-detection | ELECTRA-style pre-training with replaced token detection |

## PEFT

Parameter-Efficient Fine-Tuning (PEFT) is supported through [Hugging Face's PEFT library](https://github.com/huggingface/peft). Quantization is provided through [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). See the examples below.

```python
from txtai.pipeline import HFTrainer

trainer = HFTrainer()
trainer(..., quantize=True, lora=True)
```

When these parameters are set to True, they use default configuration. This can also be customized.

```python
quantize = {
    "load_in_4bit": True,
    "bnb_4bit_use_double_quant": True,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_compute_dtype": "bfloat16"
}

lora = {
    "r": 16,
    "lora_alpha": 8,
    "target_modules": "all-linear",
    "lora_dropout": 0.05,
    "bias": "none"
}

trainer(..., quantize=quantize, lora=lora)
```

The parameters also accept `transformers.BitsAndBytesConfig` and `peft.LoraConfig` instances.

See the following PEFT documentation links for more information.

- [Quantization](https://huggingface.co/docs/peft/developer_guides/quantization)
- [LoRA](https://huggingface.co/docs/peft/developer_guides/lora)

## Methods 

Python documentation for the pipeline.

### ::: txtai.pipeline.HFTrainer.__call__



================================================
FILE: docs/workflow/index.md
================================================
# Workflow

![workflow](../images/workflow.png#only-light)
![workflow](../images/workflow-dark.png#only-dark)

Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming and work on data in batches, allowing large volumes of data to be processed efficiently.

Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Large language models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. 

An example of the most basic workflow:

```python
workflow = Workflow([Task(lambda x: [y * 2 for y in x])])
list(workflow([1, 2, 3]))
```

This example multiplies each input value by 2 and returns transformed elements via a generator.

Since workflows run as generators, output must be consumed for execution to occur. The following snippets show how output can be consumed.

```python
# Small dataset where output fits in memory
list(workflow(elements))

# Large dataset
for output in workflow(elements):
    function(output)

# Large dataset where output is discarded
for _ in workflow(elements):
    pass
```

Workflows are run with Python or configuration. Examples of both methods are shown below.

## Example

A full-featured example is shown below in Python. This workflow transcribes a set of audio files, translates the text into French and indexes the data.

```python
from txtai import Embeddings
from txtai.pipeline import Transcription, Translation
from txtai.workflow import FileTask, Task, Workflow

# Embeddings instance
embeddings = Embeddings({
    "path": "sentence-transformers/paraphrase-MiniLM-L3-v2",
    "content": True
})

# Transcription instance
transcribe = Transcription()

# Translation instance
translate = Translation()

tasks = [
    FileTask(transcribe, r"\.wav$"),
    Task(lambda x: translate(x, "fr"))
]

# List of files to process
data = [
  "US_tops_5_million.wav",
  "Canadas_last_fully.wav",
  "Beijing_mobilises.wav",
  "The_National_Park.wav",
  "Maine_man_wins_1_mil.wav",
  "Make_huge_profits.wav"
]

# Workflow that translate text to French
workflow = Workflow(tasks)

# Index data
embeddings.index((uid, text, None) for uid, text in enumerate(workflow(data)))

# Search
embeddings.search("wildlife", 1)
```

## Configuration-driven example

Workflows can also be defined with YAML configuration.

```yaml
writable: true
embeddings:
  path: sentence-transformers/paraphrase-MiniLM-L3-v2
  content: true

# Transcribe audio to text
transcription:

# Translate text between languages
translation:

workflow:
  index:
    tasks:
      - action: transcription
        select: "\\.wav$"
        task: file
      - action: translation
        args: ["fr"]
      - action: index
```

```python
# Create and run the workflow
from txtai import Application

# Create and run the workflow
app = Application("workflow.yml")
list(app.workflow("index", [
  "US_tops_5_million.wav",
  "Canadas_last_fully.wav",
  "Beijing_mobilises.wav",
  "The_National_Park.wav",
  "Maine_man_wins_1_mil.wav",
  "Make_huge_profits.wav"
]))

# Search
app.search("wildlife")
```

The code above executes a workflow defined in the file `workflow.yml.

## LLM workflow example

Workflows can connect multiple LLM prompting tasks together.

```yaml
llm:
  path: google/flan-t5-xl

workflow:
  llm:
    tasks:
      - task: template
        template: |
          Extract keywords for the following text.

          {text}
        action: llm
      - task: template
        template: |
          Translate the following text into French.

          {text}
        action: llm
```

```python
from txtai import Application

app = Application("workflow.yml")
list(app.workflow("llm", [
  """
  txtai is an open-source platform for semantic search
  and workflows powered by language models.
  """
]))
```

Any txtai pipeline/workflow task can be connected in workflows with LLMs.

```yaml
llm:
  path: google/flan-t5-xl

translation:

workflow:
  llm:
    tasks:
      - task: template
        template: |
          Extract keywords for the following text.

          {text}
        action: llm
      - action: translation
        args:
          - fr
```

See the following links for more information.

- [Workflow Demo](https://huggingface.co/spaces/NeuML/txtai)
- [Workflow YAML Examples](https://huggingface.co/spaces/NeuML/txtai/tree/main/workflows)
- [Workflow YAML Guide](../api/configuration/#workflow)

## Methods

Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements. 

### ::: txtai.workflow.Workflow.__init__
### ::: txtai.workflow.Workflow.__call__
### ::: txtai.workflow.Workflow.schedule

## More examples

See [this link](../examples/#workflows) for a full list of workflow examples.



================================================
FILE: docs/workflow/schedule.md
================================================
# Schedule

![schedule](../images/schedule.png#only-light)
![schedule](../images/schedule-dark.png#only-dark)

Workflows can run on a repeating basis with schedules. This is suitable in cases where a workflow is run against a dynamically expanding input, like an API service or directory of files. 

The schedule method takes a cron expression, list of static elements (which dynamically expand i.e. API service, directory listing) and an optional maximum number of iterations.

Below are a couple example cron expressions.

```bash
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)
# | â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)
# | | â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of the month (1 - 31)
# | | | â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)
# | | | | â”Œâ”€â”€â”€â”€â”€â”€â”€ day of the week (0 - 6)
# | | | | | â”Œâ”€â”€â”€â”€â”€ second (0 - 59)
# | | | | | |
  * * * * * *      # Run every second
0/5 * * * *        # Run every 5 minutes
  0 0 1 * *        # Run monthly on 1st
  0 0 1 1 *        # Run on Jan 1 at 12am
  0 0 * * mon,wed  # Run Monday and Wednesday
```

## Python
Simple workflow [scheduled](../#txtai.workflow.base.Workflow.schedule) with Python.

```python
workflow = Workflow(tasks)
workflow.schedule("0/5 * * * *", elements)
```

See the link below for a more detailed example.

| Notebook  | Description  |       |
|:----------|:-------------|------:|
| [Workflow Scheduling](https://github.com/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) | Schedule workflows with cron expressions | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) |

## Configuration 
Simple workflow scheduled with configuration.

```yaml
workflow:
  index:
    schedule:
      cron: 0/5 * * * *
      elements: [...]
    tasks: [...]
```

```python
# Create and run the workflow
from txtai import Application

# Create and run the workflow
app = Application("workflow.yml")

# Wait for scheduled workflows
app.wait()
```

See the links below for more information on cron expressions.

- [cron overview](https://en.wikipedia.org/wiki/Cron)
- [croniter - library used by txtai](https://github.com/kiorky/croniter)



================================================
FILE: docs/workflow/task/console.md
================================================
# Console Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Console Task prints task inputs and outputs to standard output. This task is mainly used for debugging and can be added at any point in a workflow.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import FileTask, Workflow

workflow = Workflow([ConsoleTask()])
workflow(["Input 1", "Input2"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: console
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.ConsoleTask.__init__



================================================
FILE: docs/workflow/task/export.md
================================================
# Export Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Export Task exports task outputs to CSV or Excel.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import FileTask, Workflow

workflow = Workflow([ExportTask()])
workflow(["Input 1", "Input2"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: export
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.ExportTask.__init__
### ::: txtai.workflow.ExportTask.register



================================================
FILE: docs/workflow/task/file.md
================================================
# File Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The File Task validates a file exists. It handles both file paths and local file urls. Note that this task _only_ works with local files.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import FileTask, Workflow

workflow = Workflow([FileTask()])
workflow(["/path/to/file", "file:///path/to/file"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: file
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.FileTask.__init__



================================================
FILE: docs/workflow/task/image.md
================================================
# Image Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Image Task reads file paths, check the file is an image and opens it as an Image object. Note that this task _only_ works with local files.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import ImageTask, Workflow

workflow = Workflow([ImageTask()])
workflow(["image.jpg", "image.gif"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: image
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.ImageTask.__init__



================================================
FILE: docs/workflow/task/index.md
================================================
# Tasks

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. While similar to pipelines, tasks encapsulate processing and don't perform signficant transformations on their own. Tasks perform logic to prepare content for the underlying action(s).

A simple task is shown below.

```python
Task(lambda x: [y * 2 for y in x])
```

The task above executes the function above for all input elements.

Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element.

```python
summary = Summary()
Task(summary)
```

Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing.

```python
summary = Summary()
task = Task(summary)
task(["Very long text here"])

workflow = Workflow([task])
list(workflow(["Very long text here"]))
```

Tasks can also be created with configuration as part of a workflow.

```yaml
workflow:
  tasks:
    - action: summary 
```

::: txtai.workflow.Task.__init__

## Multi-action task concurrency

The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks.

In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below.

- *multithreading* - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions.

- *multiprocessing* - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions.

More information on multiprocessing can be found in the [Python documentation](https://docs.python.org/3/library/multiprocessing.html).

## Multi-action task merges

Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways.

### ::: txtai.workflow.Task.hstack
### ::: txtai.workflow.Task.vstack
### ::: txtai.workflow.Task.concat

## Extract task output columns

With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element.

A simple example:

```python
workflow = Workflow([Task(lambda x: [y * 3 for y in x], unpack=False, column=0)])
list(workflow([(2, 8)]))
```

For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. 

```python
workflow = Workflow([Task([lambda x: [y * 3 for y in x], 
                           lambda x: [y - 1 for y in x]],
                           unpack=False, column={0:0, 1:1})])
list(workflow([(2, 8)]))
```

The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!



================================================
FILE: docs/workflow/task/retrieve.md
================================================
# Retrieve Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Retrieve Task connects to a url and downloads the content locally. This task is helpful when working with actions that require data to be available locally.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import RetrieveTask, Workflow

workflow = Workflow([RetrieveTask(directory="/tmp")])
workflow(["https://file.to.download", "/local/file/to/copy"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: retrieve
      directory: /tmp
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.RetrieveTask.__init__
### ::: txtai.workflow.RetrieveTask.register



================================================
FILE: docs/workflow/task/service.md
================================================
# Service Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Service Task extracts content from a http service.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import ServiceTask, Workflow

workflow = Workflow([ServiceTask(url="https://service.url/action)])
workflow(["parameter"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: service
      url: https://service.url/action
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.ServiceTask.__init__
### ::: txtai.workflow.ServiceTask.register



================================================
FILE: docs/workflow/task/storage.md
================================================
# Storage Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Storage Task expands a local directory or cloud storage bucket into a list of URLs to process.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import StorageTask, Workflow

workflow = Workflow([StorageTask()])
workflow(["s3://path/to/bucket", "local://local/directory"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: storage
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.StorageTask.__init__



================================================
FILE: docs/workflow/task/template.md
================================================
# Template Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Template Task generates text from a template and task inputs. Templates can be used to prepare data for a number of tasks including generating large
language model (LLM) prompts.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import TemplateTask, Workflow

workflow = Workflow([TemplateTask(template="This is a {text} task")])
workflow([{"text": "template"}])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: template
      template: This is a {text} task
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.TemplateTask.__init__



================================================
FILE: docs/workflow/task/url.md
================================================
# Url Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Url Task validates that inputs start with a url prefix.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import UrlTask, Workflow

workflow = Workflow([UrlTask()])
workflow(["https://file.to.download", "file:////local/file/to/copy"])
```

## Configuration-driven example

This task can also be created with workflow configuration.

```yaml
workflow:
  tasks:
    - task: url
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.UrlTask.__init__



================================================
FILE: docs/workflow/task/workflow.md
================================================
# Workflow Task

![task](../../images/task.png#only-light)
![task](../../images/task-dark.png#only-dark)

The Workflow Task runs a workflow. Allows creating workflows of workflows.

## Example

The following shows a simple example using this task as part of a workflow.

```python
from txtai.workflow import WorkflowTask, Workflow

workflow = Workflow([WorkflowTask(otherworkflow)])
workflow(["input data"])
```

## Methods

Python documentation for the task.

### ::: txtai.workflow.WorkflowTask.__init__



================================================
FILE: examples/01_Introducing_txtai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Introducing txtai

[txtai](https://github.com/neuml/txtai) is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

The key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.

This foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.

Build autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.

The following is a summary of key features:

- ğŸ” Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing
- ğŸ“„ Create embeddings for text, documents, audio, images and video
- ğŸ’¡ Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more
- â†ªï¸ï¸ Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.
- ğŸ¤– Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems
- âš™ï¸ Web and Model Context Protocol (MCP) APIs. Bindings available for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).
- ğŸ”‹ Batteries included with defaults to get up and running fast
- â˜ï¸ Run local or scale out with container orchestration

txtai is built with Python 3.10+, [Hugging Face Transformers](https://github.com/huggingface/transformers), [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi). txtai is open-source under an Apache 2.0 license.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph]

# Install translation pipeline dependencies for later examples
!pip install sentencepiece sacremoses staticvectors

"""
# Semantic search

Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.

The basic use case for an embeddings database is building an approximate nearest neighbor (ANN) index for semantic search. The following example indexes a small number of text entries to demonstrate the value of semantic search.

"""

%%capture

from txtai import Embeddings

# Works with a list, dataset or generator
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create an embeddings
embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2")

# Create an index for the list of text
embeddings.index(data)

print("%-20s %s" % ("Query", "Best Match"))
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"):
  # Extract uid of first result
  # search result format: (uid, score)
  uid = embeddings.search(query, 1)[0][0]

  # Print text
  print("%-20s %s" % (query, data[uid]))
# Output:
#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
The example above shows that for all of the queries, the query text isnâ€™t in the data. This is the true power of transformers models over token based search. What you get out of the box is ğŸ”¥ğŸ”¥ğŸ”¥!
"""

"""
# Updates and deletes

Updates and deletes are supported for embeddings. The upsert operation will insert new data and update existing data

The following section runs a query, then updates a value changing the top result and finally deletes the updated value to revert back to the original query results.
"""

# Run initial query
uid = embeddings.search("feel good story", 1)[0][0]
print("Initial: ", data[uid])

# Create a copy of data to modify
udata = data.copy()

# Update data
udata[0] = "See it: baby panda born"
embeddings.upsert([(0, udata[0], None)])

uid = embeddings.search("feel good story", 1)[0][0]
print("After update: ", udata[uid])

# Remove record just added from index
embeddings.delete([0])

# Ensure value matches previous value
uid = embeddings.search("feel good story", 1)[0][0]
print("After delete: ", udata[uid])
# Output:
#   Initial:  Maine man wins $1M from $25 lottery ticket

#   After update:  See it: baby panda born

#   After delete:  Maine man wins $1M from $25 lottery ticket


"""
# Persistence

Embeddings can be saved to storage and reloaded.
"""

embeddings.save("index")

embeddings = Embeddings()
embeddings.load("index")

uid = embeddings.search("climate change", 1)[0][0]
print(data[uid])
# Output:
#   Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg


"""
# Hybrid search

While dense vector indexes are by far the best option for semantic search systems, sparse keyword indexes can still add value. There may be cases where finding an exact match is important.

Hybrid search combines the results from sparse and dense vector indexes for the best of both worlds.
"""

# Create an embeddings
embeddings = Embeddings(hybrid=True, path="sentence-transformers/nli-mpnet-base-v2")

# Create an index for the list of text
embeddings.index(data)

print("%-20s %s" % ("Query", "Best Match"))
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"):
  # Extract uid of first result
  # search result format: (uid, score)
  uid = embeddings.search(query, 1)[0][0]

  # Print text
  print("%-20s %s" % (query, data[uid]))
# Output:
#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
Same results as with semantic search. Let's run the same example with just a keyword index to view those results.
"""

# Create an embeddings
embeddings = Embeddings(keyword=True)

# Create an index for the list of text
embeddings.index(data)

print(embeddings.search("feel good story"))
print(embeddings.search("lottery"))
# Output:
#   []

#   [(4, 0.5234998733628726)]


"""
See that when the embeddings instance only uses a keyword index, it can't find semantic matches, only keyword matches.
"""

"""
# Content storage

Up to this point, all the examples are referencing the original data array to retrieve the input text. This works fine for a demo but what if you have millions of documents? In this case, the text needs to be retrieved from an external datastore using the id.

Content storage adds an associated database (i.e. SQLite, DuckDB) that stores associated metadata with the vector index. The document text, additional metadata and additional objects can be stored and retrieved right alongside the indexed vectors.
"""

# Create embeddings with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2", content=True, objects=True)

# Create an index for the list of text
embeddings.index(data)

print(embeddings.search("feel good story", 1)[0]["text"])
# Output:
#   Maine man wins $1M from $25 lottery ticket


"""
The only change above is setting the *content* flag to True. This enables storing text and metadata content (if provided) alongside the index. Note how the text is pulled right from the query result!

Let's add some metadata.
"""

"""
# Query with SQL

When content is enabled, the entire dictionary is stored and can be queried. In addition to vector queries, txtai accepts SQL queries. This enables combined queries using both a vector index and content stored in a database backend.
"""

# Create an index for the list of text
embeddings.index([{"text": text, "length": len(text)} for text in data])

# Filter by score
print(embeddings.search("select text, score from txtai where similar('hiking danger') and score >= 0.15"))

# Filter by metadata field 'length'
print(embeddings.search("select text, length, score from txtai where similar('feel good story') and score >= 0.05 and length >= 40"))

# Run aggregate queries
print(embeddings.search("select count(*), min(length), max(length), sum(length) from txtai"))
# Output:
#   [{'text': 'The National Park Service warns against sacrificing slower friends in a bear attack', 'score': 0.3151373863220215}]

#   [{'text': 'Maine man wins $1M from $25 lottery ticket', 'length': 42, 'score': 0.08329027891159058}]

#   [{'count(*)': 6, 'min(length)': 39, 'max(length)': 94, 'sum(length)': 387}]


"""
This example above adds a simple additional field, text length.

Note the second query is filtering on the metadata field length along with a `similar` query clause. This gives a great blend of vector search with traditional filtering to help identify the best results.
"""

"""
# Object storage

In addition to metadata, binary content can also be associated with documents. The example below downloads an image, upserts it along with associated text into the embeddings index.
"""

import urllib

from IPython.display import Image

# Get an image
request = urllib.request.urlopen("https://raw.githubusercontent.com/neuml/txtai/master/demo.gif")

# Upsert new record having both text and an object
embeddings.upsert([("txtai", {"text": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.", "object": request.read()}, None)])

# Query txtai for the most similar result to "machine learning" and get associated object
result = embeddings.search("select object from txtai where similar('machine learning') limit 1")[0]["object"]

# Display image
Image(result.getvalue(), width=600)
# Output:
#   <IPython.core.display.Image object>

"""
# Topic modeling

Topic modeling is enabled via semantic graphs. Semantic graphs, also known as knowledge graphs or semantic networks, build a graph network with semantic relationships connecting the nodes. In txtai, they can take advantage of the relationships inherently learned within an embeddings index.
"""

# Create embeddings with a graph index
embeddings = Embeddings(
  path="sentence-transformers/nli-mpnet-base-v2",
  content=True,
  functions=[
    {"name": "graph", "function": "graph.attribute"},
  ],
  expressions=[
    {"name": "category", "expression": "graph(indexid, 'category')"},
    {"name": "topic", "expression": "graph(indexid, 'topic')"},
  ],
  graph={
    "topics": {
      "categories": ["health", "climate", "finance", "world politics"]
    }
  }
)

embeddings.index(data)
embeddings.search("select topic, category, text from txtai")

# Output:
#   [{'topic': 'confirmed_cases_us_5',

#     'category': 'health',

#     'text': 'US tops 5 million confirmed virus cases'},

#    {'topic': 'collapsed_iceberg_ice_intact',

#     'category': 'climate',

#     'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg"},

#    {'topic': 'beijing_along_craft_tensions',

#     'category': 'world politics',

#     'text': 'Beijing mobilises invasion craft along coast as Taiwan tensions escalate'}]

"""
When a graph index is enabled, topics are assigned to each of the entries in the embeddings instance. Topics are dynamically created using a sparse index over graph nodes grouped by [community detection algorithms](https://en.wikipedia.org/wiki/Community_structure).

Topic categories are also be derived as shown above.
"""

"""
# Subindexes

Subindexes can be configured for an embeddings. A single embeddings instance can have multiple subindexes each with different configurations.

We'll build an embeddings index having both a keyword and dense index to demonstrate.
"""

# Create embeddings with subindexes
embeddings = Embeddings(
  content=True,
  defaults=False,
  indexes={
    "keyword": {
      "keyword": True
    },
    "dense": {
      "path": "sentence-transformers/nli-mpnet-base-v2"
    }
  }
)
embeddings.index(data)

embeddings.search("feel good story", limit=1, index="keyword")
# Output:
#   []

embeddings.search("feel good story", limit=1, index="dense")
# Output:
#   [{'id': '4',

#     'text': 'Maine man wins $1M from $25 lottery ticket',

#     'score': 0.08329027891159058}]

"""
Once again, this example demonstrates the difference between keyword and semantic search. The first search call uses the defined keyword index, the second uses the dense vector index.
"""

"""
# LLM orchestration

txtai is an all-in-one AI framework. txtai supports building autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).

The [RAG pipeline](https://neuml.github.io/txtai/pipeline/text/rag/) is txtai's spin on retrieval augmented generation (RAG). This pipeline extracts knowledge from content by joining a prompt, context data store and generative model together.

The following example shows how a large language model (LLM) can use an embeddings database for context.
"""

import torch
from txtai import RAG

def prompt(question):
  return [{
    "query": question,
    "question": f"""
Answer the following question using the context below.
Question: {question}
Context:
"""
}]

# Create embeddings
embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2", content=True, autoid="uuid5")

# Create an index for the list of text
embeddings.index(data)

# Create and run RAG instance
rag = RAG(embeddings, "google/flan-t5-large", torch_dtype=torch.bfloat16, output="reference")
rag(prompt("What country is having issues with climate change?"))[0]
# Output:
#   {'answer': 'Canada', 'reference': 'da633124-33ff-58d6-8ecb-14f7a44c042a'}

"""
The logic above first builds an embeddings index. It then loads a LLM and uses the embeddings index to drive a LLM prompt.

The RAG pipeline can optionally return a reference to the id of the best matching record with the answer. That id can be used to resolve the full answer reference. Note that the embeddings above used an [uuid autosequence](https://neuml.github.io/txtai/embeddings/configuration/general/#autoid).
"""

uid = rag(prompt("What country is having issues with climate change?"))[0]["reference"]
embeddings.search(f"select id, text from txtai where id = '{uid}'")
# Output:
#   [{'id': 'da633124-33ff-58d6-8ecb-14f7a44c042a',

#     'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg"}]

"""
LLM inference can also be run standalone.
"""

from txtai import LLM

llm = LLM("google/flan-t5-large", torch_dtype=torch.bfloat16)
llm("Where is one place you'd go in Washington, DC?")
# Output:
#   'national museum of american history'

"""
# Language model workflows

Language model workflows, also known as semantic workflows, connect language models together to build intelligent applications.

Workflows can run right alongside an embeddings instance, similar to a stored procedure in a relational database. Workflows can be written in either Python or YAML. We'll demonstrate how to write a workflow with YAML.
"""

%%writefile embeddings.yml

# Embeddings instance
writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
  functions:
    - {name: translation, argcount: 2, function: translation}

# Translation pipeline
translation:

# Workflow definitions
workflow:
  search:
    tasks:
      - search
      - action: translation
        args:
          target: fr
        task: template
        template: "{text}"
# Output:
#   Overwriting embeddings.yml


"""
The workflow above loads an embeddings index and defines a search workflow. The search workflow runs a search and then passes the results to a translation pipeline. The translation pipeline translates results to French.
"""

from txtai import Application

# Build index
app = Application("embeddings.yml")
app.add(data)
app.index()

# Run workflow
list(app.workflow("search", ["select text from txtai where similar('feel good story') limit 1"]))

# Output:
#   ['Maine homme gagne $1M Ã  partir de $25 billet de loterie']

"""
SQL functions, in some cases, can accomplish the same thing as a workflow. The function below runs the translation pipeline as a function.
"""

app.search("select translation(text, 'fr') text from txtai where similar('feel good story') limit 1")
# Output:
#   [{'text': 'Maine homme gagne $1M Ã  partir de $25 billet de loterie'}]

"""
LLM chains with templates are also possible with workflows. Workflows are self-contained, they operate both with and without an associated embeddings instance. The following workflow uses a LLM to conditionally translate text to French and then detect the language of the text.
"""

%%writefile workflow.yml

sequences:
  path: google/flan-t5-large
  torch_dtype: torch.bfloat16

workflow:
  chain:
    tasks:
      - task: template
        template: Translate '{statement}' to {language} if it's English
        action: sequences
      - task: template
        template: What language is the following text? {text}
        action: sequences
# Output:
#   Overwriting workflow.yml


inputs = [
  {"statement": "Hello, how are you", "language": "French"},
  {"statement": "Hallo, wie geht's dir", "language": "French"}
]

app = Application("workflow.yml")
list(app.workflow("chain", inputs))
# Output:
#   ['French', 'German']

"""
# Wrapping up

AI is advancing at a rapid pace. Things not possible even a year ago are now possible. This notebook introduced txtai, an all-in-one AI framework. The possibilities are limitless and weâ€™re excited to see what can be built on top of txtai!

Visit the links below for more.

[GitHub](https://github.com/neuml/txtai) | [Documentation](https://neuml.github.io/txtai) | [Examples](https://neuml.github.io/txtai/examples/)
"""



================================================
FILE: examples/02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build an Embeddings index with Hugging Face Datasets

This notebook shows how txtai can index and search with Hugging Face's [Datasets](https://github.com/huggingface/datasets) library. Datasets opens access to a large and growing list of publicly available datasets. Datasets has functionality to select, transform and filter data stored in each dataset.

In this example, txtai will be used to index and query a dataset.

**Make sure to select a GPU runtime when running this notebook**
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Also install `datasets`.
"""

%%capture
!pip install git+https://github.com/neuml/txtai
!pip install datasets

"""
# Load dataset and build a txtai index

In this example, we'll load the `ag_news` dataset, which is a collection of news article headlines. This only takes a single line of code!

Next, txtai will index the first 10,000 rows of the dataset. A sentence similarity model is used to compute sentence embeddings. sentence-transformers has a number of [pre-trained models](https://huggingface.co/models?pipeline_tag=sentence-similarity) that can be swapped in.

In addition to the embeddings index, we'll also create a Similarity instance to re-rank search hits for relevancy. 
"""

%%capture
from datasets import load_dataset

from txtai.embeddings import Embeddings
from txtai.pipeline import Similarity

def stream(dataset, field, limit):
  index = 0
  for row in dataset:
    yield (index, row[field], None)
    index += 1

    if index >= limit:
      break

def search(query):
  return [(result["score"], result["text"]) for result in embeddings.search(query, limit=50)]

def ranksearch(query):
  results = [text for _, text in search(query)]
  return [(score, results[x]) for x, score in similarity(query, results)]

# Load HF dataset
dataset = load_dataset("ag_news", split="train")

# Create embeddings model, backed by sentence-transformers & transformers, enable content storage
embeddings = Embeddings({"path": "sentence-transformers/paraphrase-MiniLM-L3-v2", "content": True})
embeddings.index(stream(dataset, "text", 10000))

# Create similarity instance for re-ranking
similarity = Similarity("valhalla/distilbart-mnli-12-3")

"""
# Search the dataset

Now that an index is ready, let's search the data! The following section runs a series of queries and show the results. Like basic search engines, txtai finds token matches. But the real power of txtai is finding semantically similar results.

sentence-transformers has a great overview on [information retrieval](https://www.sbert.net/examples/applications/information-retrieval/README.html) that is well worth a read. 
"""

from IPython.core.display import display, HTML

def table(query, rows):
    html = """
    <style type='text/css'>
    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');
    table {
      border-collapse: collapse;
      width: 900px;
    }
    th, td {
        border: 1px solid #9e9e9e;
        padding: 10px;
        font: 15px Oswald;
    }
    </style>
    """

    html += "<h3>%s</h3><table><thead><tr><th>Score</th><th>Text</th></tr></thead>" % (query)
    for score, text in rows:
        html += "<tr><td>%.4f</td><td>%s</td></tr>" % (score, text)
    html += "</table>"

    display(HTML(html))

for query in ["Positive Apple reports", "Negative Apple reports", "Best planets to explore for life", "LA Dodgers good news", "LA Dodgers bad news"]:
  table(query, ranksearch(query)[:2])

# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>



================================================
FILE: examples/03_Build_an_Embeddings_index_from_a_data_source.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build an Embeddings index from a data source

In Part 1, we gave a general overview of txtai, the backing technology and examples of how to use it for similarity searches. Part 2 covered an embedding index with a larger dataset.

For real world large-scale use cases, data is often stored in a database (Elasticsearch, SQL, MongoDB, files, etc). Here we'll show how to read from SQLite, build an Embedding index and run queries against the generated Embeddings index.

This example covers functionality found in the [paperai](https://github.com/neuml/paperai) library. See that library for a full solution that can be used with the dataset discussed below.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Download data

This example is going to work off a subset of the [CORD-19](https://www.semanticscholar.org/cord19) dataset. COVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, covering COVID-19 and the coronavirus family of viruses.

The following download is a SQLite database generated from a [Kaggle notebook](https://www.kaggle.com/davidmezzetti/cord-19-slim/output). More information on this data format, can be found in the [CORD-19 Analysis](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings) notebook.
"""

%%capture
!wget https://github.com/neuml/txtai/releases/download/v1.1.0/tests.gz
!gunzip tests.gz
!mv tests articles.sqlite

"""
# Build an embeddings index

The following steps build an embeddings index using a vector model designed for medical papers, [PubMedBERT Embeddings](https://huggingface.co/NeuML/pubmedbert-base-embeddings).
"""

import sqlite3

import regex as re

from txtai import Embeddings

def stream():
  # Connection to database file
  db = sqlite3.connect("articles.sqlite")
  cur = db.cursor()

  # Select tagged sentences without a NLP label. NLP labels are set for non-informative sentences.
  cur.execute("SELECT Id, Name, Text FROM sections WHERE (labels is null or labels NOT IN ('FRAGMENT', 'QUESTION')) AND tags is not null")

  count = 0
  for row in cur:
    # Unpack row
    uid, name, text = row

    # Only process certain document sections
    if not name or not re.search(r"background|(?<!.*?results.*?)discussion|introduction|reference", name.lower()):
      document = (uid, text, None)

      count += 1
      if count % 1000 == 0:
        print("Streamed %d documents" % (count), end="\r")

      yield document

  print("Iterated over %d total rows" % (count))

  # Free database resources
  db.close()

# Create embeddings index 
embeddings = Embeddings(path="neuml/pubmedbert-base-embeddings")

# Build embeddings index
embeddings.index(stream())

# Output:
#   Iterated over 21499 total rows


"""
# Query data

The following runs a query against the embeddings index for the terms "risk factors". It finds the top 5 matches and returns the corresponding documents associated with each match.
"""

import pandas as pd

from IPython.display import display, HTML

pd.set_option("display.max_colwidth", None)

db = sqlite3.connect("articles.sqlite")
cur = db.cursor()

results = []
for uid, score in embeddings.search("risk factors", 5):
  cur.execute("SELECT article, text FROM sections WHERE id = ?", [uid])
  uid, text = cur.fetchone()

  cur.execute("SELECT Title, Published, Reference from articles where id = ?", [uid])
  results.append(cur.fetchone() + (text,))

# Free database resources
db.close()

df = pd.DataFrame(results, columns=["Title", "Published", "Reference", "Match"])

# It has been reported that displaying HTML within VSCode doesn't work.
# When using VSCode, the data can be exported to an external HTML file to view.
# See example below.

# htmlData = df.to_html(index=False)
# with open("data.html", "w") as file:
#     file.write(htmlData)

display(HTML(df.to_html(index=False)))
# Output:
#   <IPython.core.display.HTML object>

"""
# Extracting additional columns from query results

The example above uses the Embeddings index to find the top 5 best matches. In addition to this, an Extractor instance (this will be explained further in part 5) is used to ask additional questions over the search results, creating a richer query response.
"""

%%capture
from txtai.pipeline import Extractor

# Create extractor instance using qa model designed for the CORD-19 dataset
# Note: That extractive QA was a predecessor to Large Language Models (LLMs). LLMs likely will get better results.
extractor = Extractor(embeddings, "NeuML/bert-small-cord19qa")

db = sqlite3.connect("articles.sqlite")
cur = db.cursor()

results = []
for uid, score in embeddings.search("risk factors", 5):
  cur.execute("SELECT article, text FROM sections WHERE id = ?", [uid])
  uid, text = cur.fetchone()

  # Get list of document text sections to use for the context
  cur.execute("SELECT Name, Text FROM sections WHERE (labels is null or labels NOT IN ('FRAGMENT', 'QUESTION')) AND article = ? ORDER BY Id", [uid])
  texts = []
  for name, txt in cur.fetchall():
    if not name or not re.search(r"background|(?<!.*?results.*?)discussion|introduction|reference", name.lower()):
      texts.append(txt)

  cur.execute("SELECT Title, Published, Reference from articles where id = ?", [uid])
  article = cur.fetchone()

  # Use QA extractor to derive additional columns
  answers = extractor([("Risk Factors", "risk factors", "What risk factors?", False),
                       ("Locations", "hospital country", "What locations?", False)], texts)

  results.append(article + (text,) + tuple([answer[1] for answer in answers]))

# Free database resources
db.close()

df = pd.DataFrame(results, columns=["Title", "Published", "Reference", "Match", "Risk Factors", "Locations"])
display(HTML(df.to_html(index=False)))
# Output:
#   <IPython.core.display.HTML object>

"""
In the example above, the Embeddings index is used to find the top N results for a given query. On top of that, a question-answer extractor is used to derive additional columns based on a list of questions. In this case, the "Risk Factors" and "Location" columns were pulled from the document text.
"""



================================================
FILE: examples/04_Add_semantic_search_to_Elasticsearch.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Add semantic search to Elasticsearch

Part 2 and Part 3 of this series showed how to index and search data in txtai. Part 2 indexed and searched a Hugging Face Dataset, Part 3 indexed and searched an external data source. 

txtai is modular in design, it's components can be individually used. txtai has a similarity function that works on lists of text. This method can be integrated with any external search service, such as a REST API, a SQL query or anything else that returns text search results. 

In this notebook, we'll take the same Hugging Face Dataset used in Part 2, index it in Elasticsearch and rank the search results using a semantic similarity function from txtai.

**Make sure to select a GPU runtime when running this notebook**
"""

"""
# Install dependencies

Install `txtai`, `datasets` and `Elasticsearch`.
"""

%%capture

# Install txtai, datasets and elasticsearch python client
!pip install git+https://github.com/neuml/txtai datasets elasticsearch

# Download and extract elasticsearch
!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz
!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz
!chown -R daemon:daemon elasticsearch-7.10.1

"""
Start an instance of Elasticsearch directly within this notebook.
"""

import os
from subprocess import Popen, PIPE, STDOUT

# If issues are encountered with this section, ES can be manually started as follows:
# ./elasticsearch-7.10.1/bin/elasticsearch

# Start and wait for server
server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))
!sleep 30

"""
# Load data into Elasticsearch

The following block loads the dataset into Elasticsearch.
"""

from datasets import load_dataset

from elasticsearch import Elasticsearch, helpers

# Connect to ES instance
es = Elasticsearch(hosts=["http://localhost:9200"], timeout=60, retry_on_timeout=True)

# Load HF dataset
dataset = load_dataset("ag_news", split="train")["text"][:50000]

# Elasticsearch bulk buffer
buffer = []
rows = 0

for x, text in enumerate(dataset):
  # Article record
  article = {"_id": x, "_index": "articles", "title": text}

  # Buffer article
  buffer.append(article)

  # Increment number of articles processed
  rows += 1

  # Bulk load every 1000 records
  if rows % 1000 == 0:
    helpers.bulk(es, buffer)
    buffer = []

    print("Inserted {} articles".format(rows), end="\r")

if buffer:
  helpers.bulk(es, buffer)

print("Total articles inserted: {}".format(rows))

"""
# Query data with Elasticsearch

Elasticsearch is a token-based search system. Queries and documents are parsed into tokens and the most relevant query-document matches are calculated using a scoring algorithm. The default scoring algorithm is [BM25](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables). Powerful queries can be built using a [rich query syntax](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax) and [Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl.html). 

The following section runs a query against Elasticsearch, finds the top 5 matches and returns the corresponding documents associated with each match.

"""

from IPython.display import display, HTML

def table(category, query, rows):
    html = """
    <style type='text/css'>
    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');
    table {
      border-collapse: collapse;
      width: 900px;
    }
    th, td {
        border: 1px solid #9e9e9e;
        padding: 10px;
        font: 15px Oswald;
    }
    </style>
    """

    html += "<h3>[%s] %s</h3><table><thead><tr><th>Score</th><th>Text</th></tr></thead>" % (category, query)
    for score, text in rows:
        html += "<tr><td>%.4f</td><td>%s</td></tr>" % (score, text)
    html += "</table>"

    display(HTML(html))

def search(query, limit):
  query = {
      "size": limit,
      "query": {
          "query_string": {"query": query}
      }
  }

  results = []
  for result in es.search(index="articles", body=query)["hits"]["hits"]:
    source = result["_source"]
    results.append((min(result["_score"], 18) / 18, source["title"]))

  return results

limit = 3
query= "+yankees lose"
table("Elasticsearch", query, search(query, limit))
# Output:
#   <IPython.core.display.HTML object>

"""
The table above shows the results for the query `+yankees lose`. This query requires the token `yankees`. The search doesn't understand the semantic meaning of the query. It returns the most relevant results with those two tokens.

We can see in this case, the results aren't capturing the meaning of the search. Let's try adding semantic similarity to the search!
"""

"""
# Ranking search results with txtai

txtai has a similarity module that computes the similarity between a query and a list of strings. Of course, txtai can also build a full index as shown in the previous notebooks but in this case we'll just use the ad-hoc similarity function.

The code below creates a Similarity instance and defines a ranking function to order search results based on the computed similarity.

`ranksearch` queries Elasticsearch for a larger set of results, ranks the results using the similarity instance and returns the top n results. 
"""

%%capture
from txtai.pipeline import Similarity

def ranksearch(query, limit):
  results = [text for _, text in search(query, limit * 10)]
  return [(score, results[x]) for x, score in similarity(query, results)][:limit]

# Create similarity instance for re-ranking
similarity = Similarity("valhalla/distilbart-mnli-12-3")

"""
Now let's re-run the previous search.
"""

# Run the search
table("Elasticsearch + txtai", query, ranksearch(query, limit))
# Output:
#   <IPython.core.display.HTML object>

"""
The results above do a much better job of finding results semantically similar in meaning to the query. Instead of just finding matches with `yankees` and `lose`, it finds matches where the `yankees lose`. 

This combination is effective and powerful. It takes advantage of the high performance of Elasticsearch while adding a semantic search capability. We may already have a large Elasticsearch cluster with TBs (or PBs)+ of data and years of engineering investment that solves most use cases. Semantically ranking search results is a practical approach.
"""

"""
# More examples

Now for some more examples comparing the results from Elasticsearch vs Elasticsearch + txtai.
"""

for query in ["good news +economy", "bad news +economy"]:
  table("Elasticsearch", query, search(query, limit))
  table("Elasticsearch + txtai", query, ranksearch(query, limit))
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
Once again while Elasticsearch usually returns quality results, occasionally it will match results that aren't semantically relevant. The power of semantic search is that not only will it find direct matches but matches with the same meaning.  
"""



================================================
FILE: examples/05_Extractive_QA_with_txtai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Extractive QA with txtai

In Parts 1 through 4, we gave a general overview of txtai, the backing technology and examples of how to use it for similarity searches. This notebook builds on that and extends to building extractive question-answering systems.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Create an Embeddings and Extractor instances

The Embeddings instance is the main entrypoint for txtai. An Embeddings instance defines the method used to tokenize and convert a segment of text into an embeddings vector.

The Extractor instance is the entrypoint for extractive question-answering.

Both the Embeddings and Extractor instances take a path to a transformer model. Any model on the [Hugging Face model hub](https://huggingface.co/models) can be used in place of the models below.
"""

%%capture

from txtai.embeddings import Embeddings
from txtai.pipeline import Extractor

# Create embeddings model, backed by sentence-transformers & transformers
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

# Create extractor instance
extractor = Extractor(embeddings, "distilbert-base-cased-distilled-squad")

data = ["Giants hit 3 HRs to down Dodgers",
        "Giants 5 Dodgers 4 final",
        "Dodgers drop Game 2 against the Giants, 5-4",
        "Blue Jays beat Red Sox final score 2-1",
        "Red Sox lost to the Blue Jays, 2-1",
        "Blue Jays at Red Sox is over. Score: 2-1",
        "Phillies win over the Braves, 5-0",
        "Phillies 5 Braves 0 final",
        "Final: Braves lose to the Phillies in the series opener, 5-0",
        "Lightning goaltender pulled, lose to Flyers 4-1",
        "Flyers 4 Lightning 1 final",
        "Flyers win 4-1"]

questions = ["What team won the game?", "What was score?"]

execute = lambda query: extractor([(question, query, question, False) for question in questions], data)

for query in ["Red Sox - Blue Jays", "Phillies - Braves", "Dodgers - Giants", "Flyers - Lightning"]:
    print("----", query, "----")
    for answer in execute(query):
        print(answer)
    print()

# Ad-hoc questions
question = "What hockey team won?"

print("----", question, "----")
print(extractor([(question, question, question, False)], data))
# Output:
#   ---- Red Sox - Blue Jays ----

#   ('What team won the game?', 'Blue Jays')

#   ('What was score?', '2-1')

#   

#   ---- Phillies - Braves ----

#   ('What team won the game?', 'Phillies')

#   ('What was score?', '5-0')

#   

#   ---- Dodgers - Giants ----

#   ('What team won the game?', 'Giants')

#   ('What was score?', '5-4')

#   

#   ---- Flyers - Lightning ----

#   ('What team won the game?', 'Flyers')

#   ('What was score?', '4-1')

#   

#   ---- What hockey team won? ----

#   [('What hockey team won?', 'Flyers')]




================================================
FILE: examples/06_Extractive_QA_with_Elasticsearch.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Extractive QA with Elasticsearch

txtai is datastore agnostic, the library analyzes sets of text. The following example shows how extractive question-answering can be added on top of an Elasticsearch system.
"""

"""
# Install dependencies

Install `txtai` and `Elasticsearch`.
"""

%%capture

# Install txtai and elasticsearch python client
!pip install git+https://github.com/neuml/txtai elasticsearch

# Download and extract elasticsearch
!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz
!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz
!chown -R daemon:daemon elasticsearch-7.10.1

"""
Start an instance of Elasticsearch directly within this notebook. 
"""

import os
from subprocess import Popen, PIPE, STDOUT

# If issues are encountered with this section, ES can be manually started as follows:
# ./elasticsearch-7.10.1/bin/elasticsearch

# Start and wait for server
server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))
!sleep 30

"""
# Download data

This example is going to work off a subset of the [CORD-19](https://www.semanticscholar.org/cord19) dataset. COVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, covering COVID-19 and the coronavirus family of viruses.

The following download is a SQLite database generated from a [Kaggle notebook](https://www.kaggle.com/davidmezzetti/cord-19-slim/output). More information on this data format, can be found in the [CORD-19 Analysis](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings) notebook.
"""

%%capture
!wget https://github.com/neuml/txtai/releases/download/v1.1.0/tests.gz
!gunzip tests.gz
!mv tests articles.sqlite

"""
# Load data into Elasticsearch

The following block copies rows from SQLite to Elasticsearch.
"""

import sqlite3

import regex as re

from elasticsearch import Elasticsearch, helpers

# Connect to ES instance
es = Elasticsearch(hosts=["http://localhost:9200"], timeout=60, retry_on_timeout=True)

# Connection to database file
db = sqlite3.connect("articles.sqlite")
cur = db.cursor()

# Elasticsearch bulk buffer
buffer = []
rows = 0

# Select tagged sentences without a NLP label. NLP labels are set for non-informative sentences.
cur.execute("SELECT s.Id, Article, Title, Published, Reference, Name, Text FROM sections s JOIN articles a on s.article=a.id WHERE (s.labels is null or s.labels NOT IN ('FRAGMENT', 'QUESTION')) AND s.tags is not null")
for row in cur:
  # Build dict of name-value pairs for fields
  article = dict(zip(("id", "article", "title", "published", "reference", "name", "text"), row))
  name = article["name"]

  # Only process certain document sections
  if not name or not re.search(r"background|(?<!.*?results.*?)discussion|introduction|reference", name.lower()):
    # Bulk action fields
    article["_id"] = article["id"]
    article["_index"] = "articles"

    # Buffer article
    buffer.append(article)

    # Increment number of articles processed
    rows += 1

    # Bulk load every 1000 records
    if rows % 1000 == 0:
      helpers.bulk(es, buffer)
      buffer = []

      print("Inserted {} articles".format(rows), end="\r")

if buffer:
  helpers.bulk(es, buffer)

print("Total articles inserted: {}".format(rows))

# Output:
#   Total articles inserted: 21499


"""
# Query data

The following runs a query against Elasticsearch for the terms "risk factors". It finds the top 5 matches and returns the corresponding documents associated with each match.


"""

import pandas as pd

from IPython.display import display, HTML

pd.set_option("display.max_colwidth", None)

query = {
    "_source": ["article", "title", "published", "reference", "text"],
    "size": 5,
    "query": {
        "query_string": {"query": "risk factors"}
    }
}

results = []
for result in es.search(index="articles", body=query)["hits"]["hits"]:
  source = result["_source"]
  results.append((source["title"], source["published"], source["reference"], source["text"]))

df = pd.DataFrame(results, columns=["Title", "Published", "Reference", "Match"])

display(HTML(df.to_html(index=False)))
# Output:
#   <IPython.core.display.HTML object>

"""
# Derive columns with Extractive QA

The next section uses Extractive QA to derive additional columns. For each article, the full text is retrieved and a series of questions are asked of the document. The answers are added as a derived column per article.
"""

%%capture
from txtai.embeddings import Embeddings
from txtai.pipeline import Extractor

# Create embeddings model, backed by sentence-transformers & transformers
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

# Create extractor instance using qa model designed for the CORD-19 dataset
extractor = Extractor(embeddings, "NeuML/bert-small-cord19qa")

document = {
    "_source": ["id", "name", "text"],
    "size": 1000,
    "query": {
        "term": {"article": None}
    },
    "sort" : ["id"]
}

def sections(article):
  rows = []

  search = document.copy()
  search["query"]["term"]["article"] = article

  for result in es.search(index="articles", body=search)["hits"]["hits"]:
    source = result["_source"]
    name, text = source["name"], source["text"]

    if not name or not re.search(r"background|(?<!.*?results.*?)discussion|introduction|reference", name.lower()):
      rows.append(text)
  
  return rows

results = []
for result in es.search(index="articles", body=query)["hits"]["hits"]:
  source = result["_source"]

  # Use QA extractor to derive additional columns
  answers = extractor([("Risk factors", "risk factor", "What are names of risk factors?", False),
                       ("Locations", "city country state", "What are names of locations?", False)], sections(source["article"]))

  results.append((source["title"], source["published"], source["reference"], source["text"]) + tuple([answer[1] for answer in answers]))

df = pd.DataFrame(results, columns=["Title", "Published", "Reference", "Match", "Risk Factors", "Locations"])

display(HTML(df.to_html(index=False)))
# Output:
#   <IPython.core.display.HTML object>



================================================
FILE: examples/07_Apply_labels_with_zero_shot_classification.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Apply labels with zero-shot classification

This notebook shows how zero-shot classification can be used to perform text classification, labeling and topic modeling. txtai provides a light-weight wrapper around the zero-shot-classification pipeline in Hugging Face Transformers. This method works impressively well out of the box. Kudos to the Hugging Face team for the phenomenal work on zero-shot classification!

The examples in this notebook pick the best matching label using a list of labels for a snippet of text.

[tldrstory](https://github.com/neuml/tldrstory) has full-stack implementation of a zero-shot classification system using Streamlit, FastAPI and Hugging Face Transformers. There is also a [Medium article describing tldrstory](https://towardsdatascience.com/tldrstory-ai-powered-understanding-of-headlines-and-story-text-fc86abd702fc) and zero-shot classification. 

"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Create a Labels instance

The Labels instance is the main entrypoint for zero-shot classification. This is a light-weight wrapper around the zero-shot-classification pipeline in Hugging Face Transformers.

In addition to the default model, additional models can be found on the [Hugging Face model hub](https://huggingface.co/models?search=mnli).

"""

%%capture

from txtai.pipeline import Labels

# Create labels model
labels = Labels()

# Alternate models can be used via passing the model path as shown below
# labels = Labels("roberta-large-mnli")

"""
# Applying labels to text

The example below shows how a zero-shot classifier can be applied to arbitary text. The default model for the zero-shot classification pipeline is *bart-large-mnli*. 

Look at the results below. It's nothing short of amazingâœ¨ how well it performs. These aren't all simple even for a human. For example, intercepted was purposely picked as that is more common in football than basketball. The amount of knowledge stored in larger Transformer models continues to impress me. 
"""

data = ["Dodgers lose again, give up 3 HRs in a loss to the Giants",
        "Giants 5 Cardinals 4 final in extra innings",
        "Dodgers drop Game 2 against the Giants, 5-4",
        "Flyers 4 Lightning 1 final. 45 saves for the Lightning.",
        "Slashing, penalty, 2 minute power play coming up",
        "What a stick save!",
        "Leads the NFL in sacks with 9.5",
        "UCF 38 Temple 13",
        "With the 30 yard completion, down to the 10 yard line",
        "Drains the 3pt shot!!, 0:15 remaining in the game",
        "Intercepted! Drives down the court and shoots for the win",
        "Massive dunk!!! they are now up by 15 with 2 minutes to go"]

# List of labels
tags = ["Baseball", "Football", "Hockey", "Basketball"]

print("%-75s %s" % ("Text", "Label"))
print("-" * 100)

for text in data:
    print("%-75s %s" % (text, tags[labels(text, tags)[0][0]]))
# Output:
#   Text                                                                        Label

#   ----------------------------------------------------------------------------------------------------

#   Dodgers lose again, give up 3 HRs in a loss to the Giants                   Baseball

#   Giants 5 Cardinals 4 final in extra innings                                 Baseball

#   Dodgers drop Game 2 against the Giants, 5-4                                 Baseball

#   Flyers 4 Lightning 1 final. 45 saves for the Lightning.                     Hockey

#   Slashing, penalty, 2 minute power play coming up                            Hockey

#   What a stick save!                                                          Hockey

#   Leads the NFL in sacks with 9.5                                             Football

#   UCF 38 Temple 13                                                            Football

#   With the 30 yard completion, down to the 10 yard line                       Football

#   Drains the 3pt shot!!, 0:15 remaining in the game                           Basketball

#   Intercepted! Drives down the court and shoots for the win                   Basketball

#   Massive dunk!!! they are now up by 15 with 2 minutes to go                  Basketball


"""
# Let's try emoji ğŸ˜€

Does the model have knowledge of emoji? Check out the run below, sure looks like it does! Notice the labels are applied based on the perspective from which the information is presented. 
"""

tags = ["ğŸ˜€", "ğŸ˜¡"]

print("%-75s %s" % ("Text", "Label"))
print("-" * 100)

for text in data:
    print("%-75s %s" % (text, tags[labels(text, tags)[0][0]]))
# Output:
#   Text                                                                        Label

#   ----------------------------------------------------------------------------------------------------

#   Dodgers lose again, give up 3 HRs in a loss to the Giants                   ğŸ˜¡

#   Giants 5 Cardinals 4 final in extra innings                                 ğŸ˜€

#   Dodgers drop Game 2 against the Giants, 5-4                                 ğŸ˜¡

#   Flyers 4 Lightning 1 final. 45 saves for the Lightning.                     ğŸ˜€

#   Slashing, penalty, 2 minute power play coming up                            ğŸ˜¡

#   What a stick save!                                                          ğŸ˜€

#   Leads the NFL in sacks with 9.5                                             ğŸ˜€

#   UCF 38 Temple 13                                                            ğŸ˜€

#   With the 30 yard completion, down to the 10 yard line                       ğŸ˜€

#   Drains the 3pt shot!!, 0:15 remaining in the game                           ğŸ˜€

#   Intercepted! Drives down the court and shoots for the win                   ğŸ˜€

#   Massive dunk!!! they are now up by 15 with 2 minutes to go                  ğŸ˜€




================================================
FILE: examples/08_API_Gallery.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# API Gallery

The txtai API is a web-based service backed by [FastAPI](https://fastapi.tiangolo.com/). All txtai functionality including similarity search, extractive QA and zero-shot labeling is available via the API.

This notebook installs the txtai API and shows an example using each of the supported language bindings for txtai.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook uses the API, we need to install the api extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api]

"""
# Python

The first method we'll try is direct access via Python. We'll use zero-shot labeling for all the examples here. See [this notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) for more details on zero-shot classification. 
"""

"""
## Configure Labels instance
"""

%%capture
import os
from IPython.core.display import display, HTML
from txtai.pipeline import Labels

def table(rows):
    html = """
    <style type='text/css'>
    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');
    table {
      border-collapse: collapse;
      width: 900px;
    }
    th, td {
        border: 1px solid #9e9e9e;
        padding: 10px;
        font: 20px Oswald;
    }
    </style>
    """

    html += "<table><thead><tr><th>Text</th><th>Label</th></tr></thead>"
    for text, label in rows:
        html += "<tr><td>%s</td><td>%s</td></tr>" % (text, label)
    html += "</table>"

    display(HTML(html))

# Create labels model
labels = Labels()

"""
## Apply labels to text
"""

data = ["Wears a red suit and says ho ho",
        "Pulls a flying sleigh",
        "This is cut down and decorated",
        "Santa puts these under the tree",
        "Best way to spend the holidays"]

# List of labels
tags = ["ğŸ… Santa Clause", "ğŸ¦Œ Reindeer", "ğŸª Cookies", "ğŸ„ Christmas Tree", "ğŸ Gifts", "ğŸ‘ª Family"]

# Render output to table
table([(text, tags[labels(text, tags)[0][0]]) for text in data])
# Output:
#   <IPython.core.display.HTML object>

"""
Once again we see the power of zero-shot labeling. The model wasn't trained on any data specific to this example. Still amazed with how much knowledge is stored in large NLP models.
"""

"""
# Start an API instance

Now we'll start an API instance to run the remaining examples. The API needs a configuration file to run. The example below is simplified to only include labeling. See [this link](https://github.com/neuml/txtai#api) for a more detailed configuration example.

The API instance is started in the background.

"""

%%writefile index.yml

# Labels settings
labels:
# Output:
#   Writing index.yml


!CONFIG=index.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 90

"""
# JavaScript

txtai.js is available via NPM and can be installed as follows.

```bash
npm install txtai
```

For this example, we'll clone the txtai.js project to import the example build configuration.
"""

%%capture
!git clone https://github.com/neuml/txtai.js

"""
## Create labels.js

The following file is a JavaScript version of the labels example.
"""

%%writefile txtai.js/examples/node/src/labels.js
import {Labels} from "txtai";
import {sprintf} from "sprintf-js";

const run = async () => {
    try {
        let labels = new Labels("http://localhost:8000");

        let data = ["Wears a red suit and says ho ho",
                    "Pulls a flying sleigh",
                    "This is cut down and decorated",
                    "Santa puts these under the tree",
                    "Best way to spend the holidays"];

        // List of labels
        let tags = ["ğŸ… Santa Clause", "ğŸ¦Œ Reindeer", "ğŸª Cookies", "ğŸ„ Christmas Tree", "ğŸ Gifts", "ğŸ‘ª Family"];

        console.log(sprintf("%-40s %s", "Text", "Label"));
        console.log("-".repeat(75))

        for (let text of data) {
            let label = await labels.label(text, tags);
            label = tags[label[0].id];

            console.log(sprintf("%-40s %s", text, label));
        }
    }
    catch (e) {
        console.trace(e);
    }
};

run();

# Output:
#   Overwriting txtai.js/examples/node/src/labels.js


"""
## Build and run labels example




"""

%%capture
os.chdir("txtai.js/examples/node")
!npm install
!npm run build

!node dist/labels.js
# Output:
#   Text                                     Label

#   ---------------------------------------------------------------------------

#   Wears a red suit and says ho ho          ğŸ… Santa Clause

#   Pulls a flying sleigh                    ğŸ¦Œ Reindeer

#   This is cut down and decorated           ğŸ„ Christmas Tree

#   Santa puts these under the tree          ğŸ Gifts

#   Best way to spend the holidays           ğŸ‘ª Family


"""
The JavaScript program is showing the same results as when natively running through Python!
"""

"""
# Java

txtai.java integrates with standard Java build tools (Gradle, Maven, SBT). The following shows how to add txtai as a dependency to Gradle.

```gradle
implementation 'com.github.neuml:txtai.java:v4.0.0'
```

For this example, we'll clone the txtai.java project to import the example build configuration.
"""

%%capture
os.chdir("/content")
!git clone https://github.com/neuml/txtai.java

"""
## Create LabelsDemo.java

The following file is a Java version of the labels example.
"""

%%writefile txtai.java/examples/src/main/java/LabelsDemo.java
import java.util.Arrays;
import java.util.ArrayList;
import java.util.List;

import txtai.API.IndexResult;
import txtai.Labels;

public class LabelsDemo {
    public static void main(String[] args) {
        try {
            Labels labels = new Labels("http://localhost:8000");

            List <String> data = 
                Arrays.asList("Wears a red suit and says ho ho",
                              "Pulls a flying sleigh",
                              "This is cut down and decorated",
                              "Santa puts these under the tree",
                              "Best way to spend the holidays");

            // List of labels
            List<String> tags = Arrays.asList("ğŸ… Santa Clause", "ğŸ¦Œ Reindeer", "ğŸª Cookies", "ğŸ„ Christmas Tree", "ğŸ Gifts", "ğŸ‘ª Family");

            System.out.printf("%-40s %s%n", "Text", "Label");
            System.out.println(new String(new char[75]).replace("\0", "-"));

            for (String text: data) {
                List<IndexResult> label = labels.label(text, tags);
                System.out.printf("%-40s %s%n", text, tags.get(label.get(0).id));
            }
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}

# Output:
#   Overwriting txtai.java/examples/src/main/java/LabelsDemo.java


"""
## Build and run labels example
"""

os.chdir("txtai.java/examples")
!../gradlew -q --console=plain labels 2> /dev/null
# Output:
#   Text                                     Label

#   ---------------------------------------------------------------------------

#   Wears a red suit and says ho ho          ğŸ… Santa Clause

#   Pulls a flying sleigh                    ğŸ¦Œ Reindeer

#   This is cut down and decorated           ğŸ„ Christmas Tree

#   Santa puts these under the tree          ğŸ Gifts

#   Best way to spend the holidays           ğŸ‘ª Family

#   [m

"""
The Java program is showing the same results as when natively running through Python!
"""

"""
# Rust

txtai.rs is available via crates.io and can be installed by adding the following to your cargo.toml file.

```toml
[dependencies]
txtai = { version = "4.0" }
tokio = { version = "0.2", features = ["full"] }
```

For this example, we'll clone the txtai.rs project to import the example build configuration. First we need to install Rust.
"""

%%capture
os.chdir("/content")
!apt-get install rustc
!git clone https://github.com/neuml/txtai.rs

"""
## Create labels.rs

The following file is a Rust version of the labels example.
"""

%%writefile txtai.rs/examples/demo/src/labels.rs
use std::error::Error;

use txtai::labels::Labels;

pub async fn labels() -> Result<(), Box<dyn Error>> {
    let labels = Labels::new("http://localhost:8000");

    let data = ["Wears a red suit and says ho ho",
                "Pulls a flying sleigh",
                "This is cut down and decorated",
                "Santa puts these under the tree",
                "Best way to spend the holidays"];

    println!("{:<40} {}", "Text", "Label");
    println!("{}", "-".repeat(75));

    for text in data.iter() {
        let tags = vec!["ğŸ… Santa Clause", "ğŸ¦Œ Reindeer", "ğŸª Cookies", "ğŸ„ Christmas Tree", "ğŸ Gifts", "ğŸ‘ª Family"];
        let label = labels.label(text, &tags).await?[0].id;

        println!("{:<40} {}", text, tags[label]);
    }

    Ok(())
}
# Output:
#   Overwriting txtai.rs/examples/demo/src/labels.rs


"""
## Build and run labels example




"""

%%capture
os.chdir("txtai.rs/examples/demo")
!cargo build

!cargo run labels
# Output:
#   [0m[0m[1m[32m    Finished[0m dev [unoptimized + debuginfo] target(s) in 0.07s

#   [0m[0m[1m[32m     Running[0m `target/debug/demo labels`

#   Text                                     Label

#   ---------------------------------------------------------------------------

#   Wears a red suit and says ho ho          ğŸ… Santa Clause

#   Pulls a flying sleigh                    ğŸ¦Œ Reindeer

#   This is cut down and decorated           ğŸ„ Christmas Tree

#   Santa puts these under the tree          ğŸ Gifts

#   Best way to spend the holidays           ğŸ‘ª Family


"""
The Rust program is showing the same results as when natively running through Python!
"""

"""
# Go

txtai.go can be installed by adding the following import statement. When using modules, txtai.go will automatically be installed. Otherwise use `go get`.

```golang
import "github.com/neuml/txtai.go"
```

For this example, we'll create a standalone process for labeling. First we need to install Go.
"""

%%capture
os.chdir("/content")
!apt install golang-go
!go get "github.com/neuml/txtai.go"

"""
## Create labels.go

The following file is a Go version of the labels example.
"""

%%writefile labels.go
package main

import (
	"fmt"
	"strings"
	"github.com/neuml/txtai.go"
)

func main() {
	labels := txtai.Labels("http://localhost:8000")

	data := []string{"Wears a red suit and says ho ho",
                   "Pulls a flying sleigh",
                   "This is cut down and decorated",
                   "Santa puts these under the tree",
                   "Best way to spend the holidays"}

	// List of labels
	tags := []string{"ğŸ… Santa Clause", "ğŸ¦Œ Reindeer", "ğŸª Cookies", "ğŸ„ Christmas Tree", "ğŸ Gifts", "ğŸ‘ª Family"}

	fmt.Printf("%-40s %s\n", "Text", "Label")
	fmt.Println(strings.Repeat("-", 75))

	for _, text := range data {
		label := labels.Label(text, tags)
		fmt.Printf("%-40s %s\n", text, tags[label[0].Id])
	}
}
# Output:
#   Writing labels.go


"""
## Build and run labels example

"""

!go run labels.go
# Output:
#   Text                                     Label

#   ---------------------------------------------------------------------------

#   Wears a red suit and says ho ho          ğŸ… Santa Clause

#   Pulls a flying sleigh                    ğŸ¦Œ Reindeer

#   This is cut down and decorated           ğŸ„ Christmas Tree

#   Santa puts these under the tree          ğŸ Gifts

#   Best way to spend the holidays           ğŸ‘ª Family


"""
The Go program is showing the same results as when natively running through Python!
"""



================================================
FILE: examples/09_Building_abstractive_text_summaries.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Building abstractive text summaries

In the field of text summarization, there are two primary categories of summarization, extractive and abstractive summarization.

Extractive summarization takes subsections of the text and joins them together to form a summary. This is commonly backed by graph algorithms like TextRank to find the sections/sentences with the most commonality. These summaries can be highly effective but they are unable to transform text and don't have a contextual understanding.

Abstractive summarization uses Natural Language Processing (NLP) models to build transformative summaries of text. This is similar to having a human read an article and asking what was it about. A human wouldn't just give a verbose reading of the text. This notebook shows how blocks of text can be summarized using an abstractive summarization pipeline. 
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

"""
# Create a Summary instance

The Summary instance is the main entrypoint for text summarization. This is a light-weight wrapper around the summarization pipeline in Hugging Face Transformers.

In addition to the default model, additional models can be found on the [Hugging Face model hub](https://huggingface.co/models?pipeline_tag=summarization).

"""

%%capture

from txtai.pipeline import Summary

# Create summary model
summary = Summary()

"""
# Summarize text

The example below shows how a large block of text can be distilled down into a smaller summary.
"""

text = ("Search is the base of many applications. Once data starts to pile up, users want to be able to find it. Itâ€™s the foundation "
       "of the internet and an ever-growing challenge that is never solved or done. The field of Natural Language Processing (NLP) is "
       "rapidly evolving with a number of new developments. Large-scale general language models are an exciting new capability "
       "allowing us to add amazing functionality quickly with limited compute and people. Innovation continues with new models "
       "and advancements coming in at what seems a weekly basis. This article introduces txtai, an AI-powered search engine "
       "that enables Natural Language Understanding (NLU) based search in any application."
)

summary(text, maxlength=10)
# Output:
#   'Search is the foundation of the internet'

"""
Notice how the summarizer built a sentence using parts of the document above. It takes a basic understanding of language in order to understand the first two sentences and how to combine them into a single transformative sentence.
"""

"""
# Summarize a document

The next section retrieves an article, extracts text from it (more to come on this topic) and summarizes that text.
"""

!wget -q "https://medium.com/neuml/time-lapse-video-for-the-web-a7d8874ff397"

from txtai.pipeline import Textractor

textractor = Textractor()
text = textractor("time-lapse-video-for-the-web-a7d8874ff397")

summary(text)
# Output:
#   'Time-lapse video is a popular way to show an area or event over a long period of time. The same concept can be applied to a dynamic real-time website with frequently updated data. webelapse is an open source project developed to provide this functionality. It can be used as is or modified for different use cases.'

"""
Click through the link to see the full article. This summary does a pretty good job of covering what the article is about!
"""



================================================
FILE: examples/10_Extract_text_from_documents.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Extract text from documents

Up to this point, all the examples have been working with sections of text, which have already been split through some other means. What happens if we're working with documents? First we need to get the text out of these documents, then figure out how to index to best support vector search.

This notebook shows how documents can have text extracted and split to support vector search and retrieval augmented generation (RAG).
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v6.2.0/tests.tar.gz
!tar -xvzf tests.tar.gz

# Install NLTK
import nltk
nltk.download(['punkt', 'punkt_tab'])

"""
# Create a Textractor instance

The Textractor instance is the main entrypoint for extracting text. This method is backed by Apache Tika, a robust text extraction library written in Java. [Apache Tika](https://tika.apache.org/0.9/formats.html) has support for a large number of file formats: PDF, Word, Excel, HTML and others. The [Python Tika package](https://github.com/chrismattmann/tika-python) automatically installs Tika and starts a local REST API instance used to read extracted data.

*Note: This requires Java to be installed locally.*
"""

%%capture

from txtai.pipeline import Textractor

# Create textractor model
textractor = Textractor()

"""
# Extract text

The example below shows how to extract text from a file.
"""

textractor("txtai/article.pdf")
# Output:
#   'Introducing txtai, an AI-powered search engine \nbuilt on Transformers\n\nAdd Natural Language Understanding to any application\n\nSearch is the base of many applications. Once data starts to pile up, users want to be able to find it. Itâ€™s \nthe foundation of the internet and an ever-growing challenge that is never solved or done.\n\nThe field of Natural Language Processing (NLP) is rapidly evolving with a number of new \ndevelopments. Large-scale general language models are an exciting new capability allowing us to add \namazing functionality quickly with limited compute and people. Innovation continues with new models\nand advancements coming in at what seems a weekly basis.\n\nThis article introduces txtai, an AI-powered search engine that enables Natural Language \nUnderstanding (NLU) based search in any application.\n\nIntroducing txtai\ntxtai builds an AI-powered index over sections of text. txtai supports building text indices to perform \nsimilarity searches and create extractive question-answering based systems. txtai also has functionality \nfor zero-shot classification. txtai is open source and available on GitHub.\n\ntxtai and/or the concepts behind it has already been used to power the Natural Language Processing \n(NLP) applications listed below:\n\nâ€¢ paperai â€” AI-powered literature discovery and review engine for medical/scientific papers\nâ€¢ tldrstory â€” AI-powered understanding of headlines and story text\nâ€¢ neuspo â€” Fact-driven, real-time sports event and news site\nâ€¢ codequestion â€” Ask coding questions directly from the terminal\n\nBuild an Embeddings index\nFor small lists of texts, the method above works. But for larger repositories of documents, it doesnâ€™t \nmake sense to tokenize and convert all embeddings for each query. txtai supports building pre-\ncomputed indices which significantly improves performance.\n\nBuilding on the previous example, the following example runs an index method to build and store the \ntext embeddings. In this case, only the query is converted to an embeddings vector each search.\n\nhttps://github.com/neuml/codequestion\nhttps://neuspo.com/\nhttps://github.com/neuml/tldrstory\nhttps://github.com/neuml/paperai\n - Introducing txtai, an AI-powered search engine built on Transformers\n - Add Natural Language Understanding to any application\n - Introducing txtai\n - Build an Embeddings index'

"""
Note that the text from the article was extracted into a single string. Depending on the articles, this may be acceptable. For long articles, often you'll want to split the content into logical sections to build better downstream vectors.
"""

"""
# Extract sentences

Sentence extraction uses a model that specializes in sentence detection. This call returns a list of sentences.
"""

textractor = Textractor(sentences=True)
textractor("txtai/article.pdf")
# Output:
#   ['Introducing txtai, an AI-powered search engine \nbuilt on Transformers\n\nAdd Natural Language Understanding to any application\n\nSearch is the base of many applications.',

#    'Once data starts to pile up, users want to be able to find it.',

#    'Itâ€™s \nthe foundation of the internet and an ever-growing challenge that is never solved or done.',

#    'The field of Natural Language Processing (NLP) is rapidly evolving with a number of new \ndevelopments.',

#    'Large-scale general language models are an exciting new capability allowing us to add \namazing functionality quickly with limited compute and people.',

#    'Innovation continues with new models\nand advancements coming in at what seems a weekly basis.',

#    'This article introduces txtai, an AI-powered search engine that enables Natural Language \nUnderstanding (NLU) based search in any application.',

#    'Introducing txtai\ntxtai builds an AI-powered index over sections of text.',

#    'txtai supports building text indices to perform \nsimilarity searches and create extractive question-answering based systems.',

#    'txtai also has functionality \nfor zero-shot classification.',

#    'txtai is open source and available on GitHub.',

#    'txtai and/or the concepts behind it has already been used to power the Natural Language Processing \n(NLP) applications listed below:\n\nâ€¢ paperai â€” AI-powered literature discovery and review engine for medical/scientific papers\nâ€¢ tldrstory â€” AI-powered understanding of headlines and story text\nâ€¢ neuspo â€” Fact-driven, real-time sports event and news site\nâ€¢ codequestion â€” Ask coding questions directly from the terminal\n\nBuild an Embeddings index\nFor small lists of texts, the method above works.',

#    'But for larger repositories of documents, it doesnâ€™t \nmake sense to tokenize and convert all embeddings for each query.',

#    'txtai supports building pre-\ncomputed indices which significantly improves performance.',

#    'Building on the previous example, the following example runs an index method to build and store the \ntext embeddings.',

#    'In this case, only the query is converted to an embeddings vector each search.',

#    'https://github.com/neuml/codequestion\nhttps://neuspo.com/\nhttps://github.com/neuml/tldrstory\nhttps://github.com/neuml/paperai\n - Introducing txtai, an AI-powered search engine built on Transformers\n - Add Natural Language Understanding to any application\n - Introducing txtai\n - Build an Embeddings index']

"""
Now the document is split up at the sentence level. These sentences can be feed to a workflow that adds each sentence to an embeddings index. Depending on the task, this may work well. Alternatively, it may be even better to split at the paragraph level.
"""

"""
# Extract paragraphs

Paragraph detection looks for consecutive newlines. This call returns a list of paragraphs.
"""

textractor = Textractor(paragraphs=True)
for paragraph in textractor("txtai/article.pdf"):
  print(paragraph, "\n----")
# Output:
#   Introducing txtai, an AI-powered search engine 

#   built on Transformers 

#   ----

#   Add Natural Language Understanding to any application 

#   ----

#   Search is the base of many applications. Once data starts to pile up, users want to be able to find it. Itâ€™s 

#   the foundation of the internet and an ever-growing challenge that is never solved or done. 

#   ----

#   The field of Natural Language Processing (NLP) is rapidly evolving with a number of new 

#   developments. Large-scale general language models are an exciting new capability allowing us to add 

#   amazing functionality quickly with limited compute and people. Innovation continues with new models

#   and advancements coming in at what seems a weekly basis. 

#   ----

#   This article introduces txtai, an AI-powered search engine that enables Natural Language 

#   Understanding (NLU) based search in any application. 

#   ----

#   Introducing txtai

#   txtai builds an AI-powered index over sections of text. txtai supports building text indices to perform 

#   similarity searches and create extractive question-answering based systems. txtai also has functionality 

#   for zero-shot classification. txtai is open source and available on GitHub. 

#   ----

#   txtai and/or the concepts behind it has already been used to power the Natural Language Processing 

#   (NLP) applications listed below: 

#   ----

#   â€¢ paperai â€” AI-powered literature discovery and review engine for medical/scientific papers

#   â€¢ tldrstory â€” AI-powered understanding of headlines and story text

#   â€¢ neuspo â€” Fact-driven, real-time sports event and news site

#   â€¢ codequestion â€” Ask coding questions directly from the terminal 

#   ----

#   Build an Embeddings index

#   For small lists of texts, the method above works. But for larger repositories of documents, it doesnâ€™t 

#   make sense to tokenize and convert all embeddings for each query. txtai supports building pre-

#   computed indices which significantly improves performance. 

#   ----

#   Building on the previous example, the following example runs an index method to build and store the 

#   text embeddings. In this case, only the query is converted to an embeddings vector each search. 

#   ----

#   https://github.com/neuml/codequestion

#   https://neuspo.com/

#   https://github.com/neuml/tldrstory

#   https://github.com/neuml/paperai

#    - Introducing txtai, an AI-powered search engine built on Transformers

#    - Add Natural Language Understanding to any application

#    - Introducing txtai

#    - Build an Embeddings index 

#   ----


"""
# Extract sections

Section extraction is format dependent. If page breaks are available, each section is a page. Otherwise, this call returns logical sections such by headings.
"""

textractor = Textractor(sections=True)
print("\n[PAGE BREAK]\n".join(section for section in textractor("txtai/article.pdf")))
# Output:
#   Introducing txtai, an AI-powered search engine 

#   built on Transformers

#   

#   Add Natural Language Understanding to any application

#   

#   Search is the base of many applications. Once data starts to pile up, users want to be able to find it. Itâ€™s 

#   the foundation of the internet and an ever-growing challenge that is never solved or done.

#   

#   The field of Natural Language Processing (NLP) is rapidly evolving with a number of new 

#   developments. Large-scale general language models are an exciting new capability allowing us to add 

#   amazing functionality quickly with limited compute and people. Innovation continues with new models

#   and advancements coming in at what seems a weekly basis.

#   

#   This article introduces txtai, an AI-powered search engine that enables Natural Language 

#   Understanding (NLU) based search in any application.

#   

#   Introducing txtai

#   txtai builds an AI-powered index over sections of text. txtai supports building text indices to perform 

#   similarity searches and create extractive question-answering based systems. txtai also has functionality 

#   for zero-shot classification. txtai is open source and available on GitHub.

#   

#   txtai and/or the concepts behind it has already been used to power the Natural Language Processing 

#   (NLP) applications listed below:

#   

#   â€¢ paperai â€” AI-powered literature discovery and review engine for medical/scientific papers

#   â€¢ tldrstory â€” AI-powered understanding of headlines and story text

#   â€¢ neuspo â€” Fact-driven, real-time sports event and news site

#   â€¢ codequestion â€” Ask coding questions directly from the terminal

#   

#   Build an Embeddings index

#   For small lists of texts, the method above works. But for larger repositories of documents, it doesnâ€™t 

#   make sense to tokenize and convert all embeddings for each query. txtai supports building pre-

#   computed indices which significantly improves performance.

#   

#   Building on the previous example, the following example runs an index method to build and store the 

#   text embeddings. In this case, only the query is converted to an embeddings vector each search.

#   

#   https://github.com/neuml/codequestion

#   https://neuspo.com/

#   https://github.com/neuml/tldrstory

#   https://github.com/neuml/paperai

#   [PAGE BREAK]

#   - Introducing txtai, an AI-powered search engine built on Transformers

#    - Add Natural Language Understanding to any application

#    - Introducing txtai

#    - Build an Embeddings index




================================================
FILE: examples/11_Transcribe_audio_to_text.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Transcribe audio to text

This notebook covers the transcription of audio files to text using models provided by Hugging Face.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package. We'll also demonstrate running this pipeline through the API.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline]

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz
!tar -xvzf tests.tar.gz

"""
# Create a Transcription instance

The Transcription instance is the main entrypoint for transcribing audio to text. The pipeline abstracts transcribing audio into a one line call! 

The pipeline executes logic to read audio files into memory, run the data through a machine learning model and output the results to text.


"""

%%capture

from txtai.pipeline import Transcription

# Create transcription model
transcribe = Transcription()

"""
# Transcribe audio to text

The example below shows how to transcribe a list of audio files to text. Let's transcribe audio to text and look at each result.
"""

from IPython.display import Audio, display

files = ["Beijing_mobilises.wav", "Canadas_last_fully.wav", "Maine_man_wins_1_mil.wav", "Make_huge_profits.wav", "The_National_Park.wav", "US_tops_5_million.wav"]
files = ["txtai/%s" % x for x in files]

for x, text in enumerate(transcribe(files)):
  display(Audio(files[x]))
  print(text)
  print()

# Output:
#   <IPython.lib.display.Audio object>
#   Baging mobilizes invasion kraft along coast as tie one tensions escalates

#   

#   <IPython.lib.display.Audio object>
#   Canodas last fully intact ice shelf has suddenly collapsed forming a manhattan sized iceberge

#   

#   <IPython.lib.display.Audio object>
#   Main man wins from lottery ticket

#   

#   <IPython.lib.display.Audio object>
#   Make huge profits without working make up to one hundred thousand dollars a day

#   

#   <IPython.lib.display.Audio object>
#   National park service warns against sacrificing slower friends in a bare attack

#   

#   <IPython.lib.display.Audio object>
#   Ues virus cases top a million

#   


"""
Overall, the results are solid. Each result sounds phonetically like the audio.
"""

"""
# OpenAI Whisper

In September 2022, [OpenAI Whisper](https://github.com/openai/whisper) was released. This model brings a dramatic improvement in transcription quality. Whisper support was added to Hugging Face Transformers in v4.23.0. Let's give it a try.
"""

# Transcribe files
transcribe = Transcription("openai/whisper-base")
for text in transcribe(files):
  print(text)
# Output:
#   Beijing mobilizes invasion craft along coast as Taiwan tensions escalate.

#   Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan sized iceberg.

#   Maine Man wins from lottery ticket.

#   make huge profits without working. Make up to $100,000 a day.

#   National Park Service warns against sacrificing slower friends in a bear attack.

#   U.S. virus cases top of million.


"""
Results were transcribed with near perfect accuracy, amazing!

This can also be run as a txtai application or API instance. Let's try a full indexing workflow with a txtai application.
"""

%%writefile workflow.yml
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

transcription:
  path: openai/whisper-base

workflow:
  index:
    tasks:
      - transcription
      - index
# Output:
#   Overwriting workflow.yml


from txtai.app import Application

app = Application("workflow.yml")

list(app.workflow("index", files))
app.search("feel good story", 1)
# Output:
#   [{'id': '2',

#     'text': 'Maine Man wins from lottery ticket.',

#     'score': 0.1285860687494278}]

"""
This workflow transcribed the input files, loaded the transcriptions into an embeddings index and finally ran a search. Last thing we'll do is run the workflow as an API instance.
"""

!CONFIG=workflow.yml uvicorn "txtai.api:app" &> api.log &
!sleep 30

# Run indexing workflow
!curl -s -o /dev/null \
  -X POST "http://localhost:8000/workflow" \
  -H "Content-Type: application/json" \
  -d '{"name":"index", "elements":["txtai/Beijing_mobilises.wav", "txtai/Canadas_last_fully.wav", "txtai/Maine_man_wins_1_mil.wav", "txtai/Make_huge_profits.wav", "txtai/The_National_Park.wav", "txtai/US_tops_5_million.wav"]}'

# Test API search
!curl "http://localhost:8000/search?query=feel+good+story&limit=1"
# Output:
#   [{"id":"2","text":"Maine Man wins from lottery ticket.","score":0.1285860687494278}]

"""
Once again, the same results as in Python and with an application.
"""

"""
# Wrapping up

There is a lot of development in the audio transcription space. In only a couple of lines of code, high-quality transcription models are now readily available!
"""



================================================
FILE: examples/12_Translate_text_between_languages.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Translate text between languages

This notebook covers machine translation backed by Hugging Face models. The quality of machine translation via cloud services has come a very long way and produces high quality results. This notebook shows how the models from Hugging Face give developers a reasonable alternative for local machine translation.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

"""
# Create a Translation instance

The Translation instance is the main entrypoint for translating text between languages. The pipeline abstracts translating text into a one line call! 

The pipeline has logic to detect the input language, load the relevant model that handles translating from source to target language and return results. The translation pipeline also has built-in logic to handle splitting large text blocks into smaller sections the models can handle.


"""

%%capture

from txtai.pipeline import Translation

# Create translation model
translate = Translation()

"""
# Translate text

The example below shows how to translate text from English to Spanish. This text is then translated back to English.
"""

translation = translate("This is a test translation into Spanish", "es")
translation
# Output:
#   'Esta es una traducciÃ³n de prueba al espaÃ±ol'

translate(translation, "en")
# Output:
#   'This is a test translation into Spanish'

"""
# Translating multiple languages in a single call

The section below translates a single English sentence into 5 different languages. The results are then passed to a single translation call to translate back into English. The pipeline detects each input language and is able to load the relevant translation models.
"""

def run():
  languages = ["fr", "es", "de", "hi", "ja"]
  translations = [translate("The sky is blue, the stars are far", language) for language in languages]
  english = translate(translations, "en")

  for x, text in enumerate(translations):
    print("Original Language: %s" % languages[x])
    print("Translation: %s" % text)
    print("Back to English: %s" % english[x])
    print()

# Run multiple translations
run()
# Output:
#   Original Language: fr

#   Translation: Le ciel est bleu, les Ã©toiles sont loin

#   Back to English: The sky is blue, the stars are far away

#   

#   Original Language: es

#   Translation: El cielo es azul, las estrellas estÃ¡n lejos.

#   Back to English: The sky is blue, the stars are far away.

#   

#   Original Language: de

#   Translation: Der Himmel ist blau, die Sterne sind weit

#   Back to English: The sky is blue, the stars are wide

#   

#   Original Language: hi

#   Translation: à¤†à¤•à¤¾à¤¶ à¤¨à¥€à¤²à¤¾ à¤¹à¥ˆ, à¤¤à¤¾à¤°à¥‡ à¤¦à¥‚à¤° à¤¹à¥ˆà¤‚

#   Back to English: Sky is blue, stars are away

#   

#   Original Language: ja

#   Translation: å¤©ã¯é’ã„ã€æ˜Ÿã¯é ã„ã€‚

#   Back to English: The heavens are blue and the stars are far away.

#   


"""
The translation quality overall is very high!
"""

"""
# Additional model types

The translation pipeline is flexible and supports multiple model types. The default mode for the pipeline is to scan the Hugging Face Hub for models that best match the source-target translation pair. This often produces the best quality and is usually a smaller model than a large multi-language mode.

There is a parameter that can override this and always use the base model.
"""

translate = Translation("t5-small", findmodels=False)
translate("translate English to French: The sky is blue, the stars are far", None)
# Output:
#   'Le ciel est bleu, les Ã©toiles sont loin'

"""
Translation isn't limited to spoken languages. txtai provides a text-to-sql model that converts English text into a txtai-compatible SQL statement. 
"""

translate = Translation("NeuML/t5-small-txtsql", findmodels=False)
translate("translate English to SQL: feel good story since yesterday", None)
# Output:
#   "select id, text, score from txtai where similar('feel good story') and entry >= date('now', '-1 day')"

"""
Last thing we'll do is run the multiple language example only using a single large language model.
"""

translate = Translation("facebook/mbart-large-50-many-to-many-mmt", findmodels=False)
run()
# Output:
#   Original Language: fr

#   Translation: Le ciel est bleu, les Ã©toiles sont loin

#   Back to English: The sky is blue, the stars are far away

#   

#   Original Language: es

#   Translation: El cielo es azul, las estrellas estÃ¡n lejos.

#   Back to English: The sky is blue, the stars are far away.

#   

#   Original Language: de

#   Translation: Der Himmel ist blau, die Sterne sind weit

#   Back to English: The sky is blue, the stars are far.

#   

#   Original Language: hi

#   Translation: à¤†à¤•à¤¾à¤¶ à¤¨à¥€à¤²à¥€ à¤¹à¥ˆ, à¤¤à¤¾à¤°à¥‡ à¤¦à¥‚à¤° à¤¹à¥ˆà¤‚à¥¤

#   Back to English: The sky is blue, and the stars are far away.

#   

#   Original Language: ja

#   Translation: ç©ºã¯é’ã„ã€æ˜Ÿã¯é ã„

#   Back to English: the sky is blue, the stars are far away.

#   


"""
# Wrapping up

Machine translation has made giant leaps and strides the last couple of years. These models give developers a solid, locally-hosted alternative to cloud translation services. Additionally, there are models built for low resource languages that cloud translation services don't support.

A number of different models and configurations are supported, give it a try!
"""



================================================
FILE: examples/13_Similarity_search_with_images.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Similarity search with images

txtai as the name implies works with text and ai, pretty straightforward. But that doesn't mean it can't work with different types of content. For example, an image can be described with words. We can use that description to compare an image to a query or other documents. This notebook shows how images and text can be embedded into the same space to support similarity search.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook uses sentence-transformers directly, we need to install the similarity extras package.
"""

%%capture
!pip install torchvision ipyplot git+https://github.com/neuml/txtai#egg=txtai[similarity]

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz
!tar -xvzf tests.tar.gz

"""
# Create an Embeddings model

[sentence-transformers](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/image-search) has support for the [OpenAI CLIP model](https://github.com/openai/CLIP). This model embeds text and images into the same space, enabling image similarity search. txtai can directly utilize these models through sentence-transformers. Check out the sentence-transformers link above for additional examples on how to use this model.

This section builds an embeddings index over a series of images.


"""

%%capture

import glob

from PIL import Image

from txtai.embeddings import Embeddings
from txtai.pipeline import Caption

def images():
  # Create image caption pipeline
  caption = Caption()

  for path in glob.glob('txtai/*jpg'):
    # Add image object along with image metadata
    image = Image.open(path)

    yield (path, {"object": image, "format": image.format, "width": image.width, "height": image.height, "caption": caption(image)}, None)

# Index with content and objects
embeddings = Embeddings({"method": "sentence-transformers", "path": "sentence-transformers/clip-ViT-B-32", "content": True, "objects": "image"})
embeddings.index(images())

"""
Next let's query and see what's available in the index.
"""

embeddings.search("select id, object, format, width, height, caption from txtai")
# Output:
#   [{'id': 'txtai/books.jpg',

#     'object': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x682>,

#     'format': 'JPEG',

#     'width': 1024,

#     'height': 682,

#     'caption': 'a book shelf filled with books and a stack of books'},

#    {'id': 'txtai/buildings.jpg',

#     'object': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=700x466>,

#     'format': 'JPEG',

#     'width': 700,

#     'height': 466,

#     'caption': 'a city skyline with buildings and a sky background'},

#    {'id': 'txtai/chop.jpg',

#     'object': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=700x466>,

#     'format': 'JPEG',

#     'width': 700,

#     'height': 466,

#     'caption': 'a tree branch with a person holding a stick'}]

"""
The query above shows the metadata that was added in addition to the image object. These fields can be retrieved on search and/or used to filter results.
"""

"""
# Search the index

Now that we have an index, let's search it! This section runs a list of queries against the index and shows the top result for each query. Have to say this is pretty ğŸ”¥ğŸ”¥ğŸ”¥
"""

import ipyplot
from PIL import Image

def resize(images):
  results = []
  for image in images:
    results.append(image.resize((350, int(image.height * (350 / image.width))), Image.Resampling.LANCZOS))

  return results

images, labels = [], []
for query in ["Walking into the office", "Saturday cleaning the yard", "Working on the latest analysis", "Working on my homework", "Watching an exciting race",
              "The universe is massive", "Time lapse video of traffic", "Relaxing Thanksgiving day"]:
  result = embeddings.search(f"select object from txtai where similar(\"{query}\")", 1)[0]
  images.append(result["object"])
  labels.append(query)

ipyplot.plot_images(resize(images), labels, img_width=350, force_b64=True)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
# Search with SQL

txtai has support for SQL bind parameters, which enables similarity search with binary content.
"""

result = embeddings.search(f"select object from txtai where similar(:x)", 1, parameters={"x": Image.open("txtai/books.jpg")})[0]

ipyplot.plot_images(resize([result["object"]]), ["Result"], img_width=350, force_b64=True)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
# Multilingual Support

sentence-transformers also has a [model](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1) that supports over 50+ languages. This enables running queries using those languages with an image index.

Note this model only supports text, so images must first be indexed with the model used above.
"""

import ipyplot

from txtai.pipeline import Translation

# Update model at query time to support multilingual queries
embeddings.config["path"] = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
embeddings.model = embeddings.loadvectors()

# Translate queries to German
queries = ["Walking into the office", "Saturday cleaning the yard", "Working on the latest analysis", "Working on my homework", "Watching an exciting race",
           "The universe is massive", "Time lapse video of traffic", "Relaxing Thanksgiving day"]
translate = Translation()
translated = translate(queries, "de")

images, labels = [], []
for x, query in enumerate(translated):
  result = embeddings.search(f"select object from txtai where similar(:x)", 1, parameters={"x": query})[0]

  images.append(result["object"])
  labels.append("%s<br/>(%s)" % (query, queries[x]))

ipyplot.plot_images(resize(images), labels, img_width=350, force_b64=True)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>



================================================
FILE: examples/14_Run_pipeline_workflows.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Run pipeline workflows

txtai has a growing list of models available through it's pipeline framework. Pipelines wrap a machine learning model and transform data. Currently, pipelines can wrap Hugging Face models, Hugging Face pipelines or PyTorch models (support for TensorFlow is in the backlog).

The following is a list of the currently implemented pipelines.

* **Questions** - Answer questions using a text context
* **Labels** - Apply labels to text using a zero-shot classification model. Also supports similarity comparisions.
* **Summary** - Abstractive text summarization
* **Textractor** - Extract text from documents
* **Transcription** - Transcribe audio to text
* **Translation** - Machine translation

Pipelines are great and make using a variety of machine learning models easier. But what if we want to glue the results of different pipelines together? For example, extract text, summarize it, translate it to English and load it into an Embedding index. That would require code to join those operations together in an efficient manner.

Enter workflows. Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows don't know they are working with pipelines but enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines/workflows, we need to install the pipeline and workflow extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline,workflow] sacremoses

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v2.0.0/tests.tar.gz
!tar -xvzf tests.tar.gz

"""
# Create a series of pipelines to use in this notebook
"""

%%capture
from txtai.pipeline import Summary, Textractor, Transcription, Translation

# Summary instance
summary = Summary()

# Text extraction
textractor = Textractor()

# Transcription instance
transcribe = Transcription("facebook/wav2vec2-large-960h")

# Create a translation instance
translate = Translation()

"""
# Basic workflow

The following shows a basic workflow in action!
"""

from txtai.workflow import Workflow, Task

# Workflow that translate text to French
workflow = Workflow([Task(lambda x: translate(x, "fr"))])

# Data to run through the pipeline
data = ["The sky is blue", "Forest through the trees"]

# Workflows are generators for efficiency, read results to list for display
list(workflow(data))
# Output:
#   ['Le ciel est bleu', 'ForÃªt Ã  travers les arbres']

"""
This isn't too different from previous pipeline examples. The only difference is data is feed through the workflow. In this example, the workflow calls the translation pipeline and translates text to French. Let's look at a more complex example.
"""

"""
# Multistep workflow

The following workflow reads a series of audio files, transcribes them to text and translates the text to French. This is based on the classic txtai example from [Introducing txtai](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb).

Workflows take two main parameters. The action to execute which is a callable and a pattern to filter data with. Data that is accepted by the filter will be processed, otherwise it will be passed through to the next task.
"""

from txtai.workflow import FileTask

tasks = [
    FileTask(transcribe, r"\.wav$"),
    Task(lambda x: translate(x, "fr"))
]

# List of files to process
data = [
  "txtai/US_tops_5_million.wav",
  "txtai/Canadas_last_fully.wav",
  "txtai/Beijing_mobilises.wav",
  "txtai/The_National_Park.wav",
  "txtai/Maine_man_wins_1_mil.wav",
  "txtai/Make_huge_profits.wav"
]

# Workflow that translate text to French
workflow = Workflow(tasks)

# Run workflow
list(workflow(data))
# Output:
#   ["Les cas de virus U sont en tÃªte d'un million",

#    "La derniÃ¨re plate-forme de glace entiÃ¨rement intacte du Canada s'est soudainement effondrÃ©e en formant un berge de glace de taille manhatten",

#    "Bagage mobilise les embarcations d'invasion le long des cÃ´tes Ã  mesure que les tensions tiwaniennes s'intensifient",

#    "Le service des parcs nationaux met en garde contre le sacrifice d'amis plus lents dans une attaque nue",

#    "L'homme principal gagne du billet de loterie",

#    "Faire d'Ã©normes profits sans travailler faire jusqu'Ã  cent mille dollars par jour"]

"""
# Complex workflow

Let's put this all together into a full-fledged workflow to build an embeddings index. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index.
"""

from txtai.embeddings import Embeddings, Documents
from txtai.workflow import FileTask, WorkflowTask

# Embeddings index
embeddings = Embeddings({"path": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "content": True})
documents = Documents()

# List of files to process
files = [
  "txtai/article.pdf",
  "txtai/US_tops_5_million.wav",
  "txtai/Canadas_last_fully.wav",
  "txtai/Beijing_mobilises.wav",
  "txtai/The_National_Park.wav",
  "txtai/Maine_man_wins_1_mil.wav",
  "txtai/Make_huge_profits.wav"
]

data = [(x, element, None) for x, element in enumerate(files)]

# Workflow that extracts text and builds a summary
articles = Workflow([
    FileTask(textractor),
    Task(summary)
])

# Define workflow tasks. Workflows can also be tasks!
tasks = [
    WorkflowTask(articles, r".\.pdf$"),
    FileTask(transcribe, r"\.wav$"),
    Task(lambda x: translate(x, "fr")),
    Task(documents.add, unpack=False)
]

# Workflow that translate text to French
workflow = Workflow(tasks)

# Run workflow and show results to be indexed
for x in workflow(data):
  print(x)

# Build the embeddings index
embeddings.index(documents)

# Cleanup temporary storage
documents.close()
# Output:
#   (0, "Txtai, un moteur de recherche alimentÃ© par l'IA construit sur Transformers, permet la recherche basÃ©e sur la comprÃ©hension du langage naturel (NLU) dans n'importe quelle application. Le champ de traitement du langage naturel (NLP) Ã©volue rapidement avec un certain nombre de nouveaux dÃ©veloppements. Le moteur de recherche open-source est open source et disponible sur GitHub.", None)

#   (1, "Les cas de virus U sont en tÃªte d'un million", None)

#   (2, "La derniÃ¨re plate-forme de glace entiÃ¨rement intacte du Canada s'est soudainement effondrÃ©e en formant un berge de glace de taille manhatten", None)

#   (3, "Bagage mobilise les embarcations d'invasion le long des cÃ´tes Ã  mesure que les tensions tiwaniennes s'intensifient", None)

#   (4, "Le service des parcs nationaux met en garde contre le sacrifice d'amis plus lents dans une attaque nue", None)

#   (5, "L'homme principal gagne du billet de loterie", None)

#   (6, "Faire d'Ã©normes profits sans travailler faire jusqu'Ã  cent mille dollars par jour", None)


"""
# Query for results in French
"""

# Run a search query and show the result.
embeddings.search("changement climatique", 1)[0]
# Output:
#   {'id': '2',

#    'score': 0.2982647716999054,

#    'text': "La derniÃ¨re plate-forme de glace entiÃ¨rement intacte du Canada s'est soudainement effondrÃ©e en formant un berge de glace de taille manhatten"}

# Run a search query and show the result.
embeddings.search("traitement du langage naturel", 1)[0]
# Output:
#   {'id': '0',

#    'score': 0.47031939029693604,

#    'text': "Txtai, un moteur de recherche alimentÃ© par l'IA construit sur Transformers, permet la recherche basÃ©e sur la comprÃ©hension du langage naturel (NLU) dans n'importe quelle application. Le champ de traitement du langage naturel (NLP) Ã©volue rapidement avec un certain nombre de nouveaux dÃ©veloppements. Le moteur de recherche open-source est open source et disponible sur GitHub."}

"""
# Configuration-driven workflow

Workflows can also be defined with YAML and run as an application. Applications can run standalone or as a FastAPI instance. More information can be [found here](https://neuml.github.io/txtai/api/). 
"""

workflow = """
writable: true
embeddings:
  path: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
  content: True

# Summarize text
summary:

# Extract text from documents
textractor:

# Transcribe audio to text
transcription:
  path: facebook/wav2vec2-large-960h

# Translate text between languages
translation:

workflow:
  summarize:
    tasks:
      - action: textractor
        task: file
      - summary
  index:
    tasks:
      - action: summarize
        select: '\\.pdf$'
      - action: transcription
        select: '\\.wav$'
        task: file
      - action: translation
        args: ['fr']
      - action: index
"""

# Create and run the workflow
from txtai.app import Application

# Create and run the workflow
app = Application(workflow)
list(app.workflow("index", files))
# Output:
#   ["Txtai, un moteur de recherche alimentÃ© par l'IA construit sur Transformers, permet la recherche basÃ©e sur la comprÃ©hension du langage naturel (NLU) dans n'importe quelle application. Le champ de traitement du langage naturel (NLP) Ã©volue rapidement avec un certain nombre de nouveaux dÃ©veloppements. Le moteur de recherche open-source est open source et disponible sur GitHub.",

#    "Les cas de virus U sont en tÃªte d'un million",

#    "La derniÃ¨re plate-forme de glace entiÃ¨rement intacte du Canada s'est soudainement effondrÃ©e en formant un berge de glace de taille manhatten",

#    "Bagage mobilise les embarcations d'invasion le long des cÃ´tes Ã  mesure que les tensions tiwaniennes s'intensifient",

#    "Le service des parcs nationaux met en garde contre le sacrifice d'amis plus lents dans une attaque nue",

#    "L'homme principal gagne du billet de loterie",

#    "Faire d'Ã©normes profits sans travailler faire jusqu'Ã  cent mille dollars par jour"]

# Run a search query and show the result.
app.search("changement climatique", 1)[0]
# Output:
#   {'id': '2',

#    'score': 0.2982647716999054,

#    'text': "La derniÃ¨re plate-forme de glace entiÃ¨rement intacte du Canada s'est soudainement effondrÃ©e en formant un berge de glace de taille manhatten"}

# Run a search query and show the result.
app.search("traitement du langage naturel", 1)[0]
# Output:
#   {'id': '0',

#    'score': 0.47031939029693604,

#    'text': "Txtai, un moteur de recherche alimentÃ© par l'IA construit sur Transformers, permet la recherche basÃ©e sur la comprÃ©hension du langage naturel (NLU) dans n'importe quelle application. Le champ de traitement du langage naturel (NLP) Ã©volue rapidement avec un certain nombre de nouveaux dÃ©veloppements. Le moteur de recherche open-source est open source et disponible sur GitHub."}

"""
# Wrapping up

Results are good! We can see the power of workflows and how they can join a series of pipelines together in an efficient manner. Workflows can work with any callable, not just pipelines, workflows transform data from one format to another. Workflows are an exciting and promising development for txtai.
"""



================================================
FILE: examples/15_Distributed_embeddings_cluster.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Distributed embeddings cluster

The txtai API is a web-based service backed by [FastAPI](https://fastapi.tiangolo.com/). All txtai functionality is available via the API. The API can also cluster multiple embeddings indices into a single logical index to horizontally scale over multiple nodes. 

This notebook installs the txtai API and shows an example of building an embeddings cluster.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook uses the API, we need to install the api extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api]

"""
# Start distributed embeddings cluster

First we'll start multiple API instances that will serve as embeddings index shards. Each shard stores a subset of the indexed data and these shards work in tandem to form a single logical index.

Then we'll start the main API instance that clusters the shards together into a logical instance.

The API instances are all started in the background.

"""

import os
os.chdir("/content")

%%writefile index.yml
writable: true

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2
    content: true
# Output:
#   Writing index.yml


%%writefile cluster.yml
# Embeddings cluster
cluster:
    shards:
        - http://127.0.0.1:8001
        - http://127.0.0.1:8002
# Output:
#   Writing cluster.yml


# Start embeddings shards
!CONFIG=index.yml nohup uvicorn --port 8001 "txtai.api:app" &> shard-1.log &
!CONFIG=index.yml nohup uvicorn --port 8002 "txtai.api:app" &> shard-2.log &

# Start main instance
!CONFIG=cluster.yml nohup uvicorn --port 8000 "txtai.api:app" &> main.log &

# Wait for startup
!sleep 90

"""
# Python

Let's first try the cluster out directly in Python. The code below aggregates the two shards into a single cluster and executes actions against the cluster.
"""

%%writefile run.py
from txtai.api import Cluster

cluster = Cluster({"shards": ["http://127.0.0.1:8001", "http://127.0.0.1:8002"]})

data = [
    "US tops 5 million confirmed virus cases",
    "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
    "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
    "The National Park Service warns against sacrificing slower friends in a bear attack",
    "Maine man wins $1M from $25 lottery ticket",
    "Make huge profits without work, earn up to $100,000 a day",
]

# Index data
cluster.add([{"id": x, "text": row} for x, row in enumerate(data)])
cluster.index()

# Test search
result = cluster.search("feel good story", 1)[0]
print("Query: feel good story\nResult:", result["text"])
# Output:
#   Writing run.py


!python run.py
# Output:
#   Query: feel good story

#   Result: Maine man wins $1M from $25 lottery ticket


"""
# JavaScript

Next let's try to run the same code above via the API using JavaScript.

```bash
npm install txtai
```

For this example, we'll clone the txtai.js project to import the example build configuration.
"""

%%capture
!git clone https://github.com/neuml/txtai.js

"""
## Run cluster.js

The following script is a JavaScript version of the logic above
"""

%%writefile txtai.js/examples/node/src/cluster.js
import {Embeddings} from "txtai";
import {sprintf} from "sprintf-js";

const run = async () => {
    try {
        let embeddings = new Embeddings(process.argv[2]);

        let data  = ["US tops 5 million confirmed virus cases",
                     "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
                     "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
                     "The National Park Service warns against sacrificing slower friends in a bear attack",
                     "Maine man wins $1M from $25 lottery ticket",
                     "Make huge profits without work, earn up to $100,000 a day"];

        console.log();
        console.log("Querying an Embeddings cluster");
        console.log(sprintf("%-20s %s", "Query", "Best Match"));
        console.log("-".repeat(50));

        for (let query of ["feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"]) {
            let results = await embeddings.search(query, 1);
            if (results && results.length > 0) {
              let result = results[0].text;
              console.log(sprintf("%-20s %s", query, result));
            }
        }
    }
    catch (e) {
        console.trace(e);
    }
};

run();
# Output:
#   Writing txtai.js/examples/node/src/cluster.js


"""
## Build and run cluster.js




"""

%%capture
os.chdir("txtai.js/examples/node")
!npm install
!npm run build

"""
Next lets run the code against the main cluster URL
"""

!node dist/cluster.js http://127.0.0.1:8000
# Output:
#   

#   Querying an Embeddings cluster

#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
The JavaScript program is showing the same results as the Python code above. This is running a clustered query against both nodes in the cluster and aggregating the results together.

Queries can be run against each individual shard to see what the queries independently return.
"""

!node dist/cluster.js http://127.0.0.1:8001
# Output:
#   

#   Querying an Embeddings cluster

#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket


!node dist/cluster.js http://127.0.0.1:8002
# Output:
#   

#   Querying an Embeddings cluster

#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Make huge profits without work, earn up to $100,000 a day

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  The National Park Service warns against sacrificing slower friends in a bear attack

#   war                  The National Park Service warns against sacrificing slower friends in a bear attack

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 The National Park Service warns against sacrificing slower friends in a bear attack

#   lucky                The National Park Service warns against sacrificing slower friends in a bear attack

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
Note the differences. The section below runs a count against the full cluster and each shard to show the count of records in each.
"""

!curl http://127.0.0.1:8000/count
!printf "\n"
!curl http://127.0.0.1:8001/count
!printf "\n"
!curl http://127.0.0.1:8002/count
# Output:
#   6

#   3

#   3

"""
This notebook showed how a distributed embeddings cluster can be created with txtai. This example can be further scaled out on Kubernetes with StatefulSets, which will be covered in a future tutorial.
"""



================================================
FILE: examples/16_Train_a_text_labeler.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Train a text labeler

The [Hugging Face Model Hub](https://huggingface.co/models) has a wide range of models that can handle many tasks. While these models perform well, the best performance often is found when fine-tuning a model with task-specific data. 

Hugging Face provides a [number of full-featured examples](https://github.com/huggingface/transformers/tree/master/examples) available to assist with training task-specific models. When building models from the command line, these scripts are a great way to get started.

txtai provides a training pipeline that can be used to train new models programatically using the Transformers Trainer framework. The training pipeline supports the following:

- Building transient models without requiring an output directory
- Load training data from Hugging Face datasets, pandas DataFrames and list of dicts
- Text sequence classification tasks (single/multi label classification and regression) including all GLUE tasks
- All training arguments

This notebook shows examples of how to use txtai to train/fine-tune new models.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-train] datasets pandas

"""
# Train a model

Let's get right to it! The following example fine-tunes a tiny Bert model with the sst2 dataset.

The trainer pipeline is basically a one-liner that fine-tunes any text classification/regression model available (locally and/or from the HF Hub). 

"""

from datasets import load_dataset

from txtai.pipeline import HFTrainer

trainer = HFTrainer()

# Hugging Face dataset
ds = load_dataset("glue", "sst2")
model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", ds["train"], columns=("sentence", "label"))

"""
The default trainer pipeline functionality will not store any logs, checkpoints or models to disk. The trainer can take any of the standard TrainingArguments to enable persistent models.

The next section creates a Labels pipeline using the newly built model and runs the model against the sst2 validation set. 
"""

from txtai.pipeline import Labels

labels = Labels((model, tokenizer), dynamic=False)

# Determine accuracy on validation set
results = [row["label"] == labels(row["sentence"])[0][0] for row in ds["validation"]]
sum(results) / len(ds["validation"])
# Output:
#   0.8268348623853211

"""
82.68% accuracy - not bad for a tiny Bert model. 


"""

"""
# Train a model with Lists

As mentioned earlier, the trainer pipeline supports Hugging Face datasets, pandas DataFrames and lists of dicts. The example below trains a model using lists.
"""

data = [{"text": "This is a test sentence", "label": 0}, {"text": "This is not a test", "label": 1}]

model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", data)
# Output:
#   Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']

#   - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).

#   - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

#   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']

#   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

#   <IPython.core.display.HTML object>

"""
# Train a model with DataFrames

The next section builds a new model using data stored in a pandas DataFrame.
"""

import pandas as pd

df = pd.DataFrame(data)

model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", data)
# Output:
#   Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']

#   - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).

#   - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

#   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']

#   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

#   <IPython.core.display.HTML object>

"""
# Train a regression model

The previous models were classification tasks. The following model trains a sentence similarity model with a regression output per sentence pair between 0 (dissimilar) and 1 (similar).
"""

ds = load_dataset("glue", "stsb")
model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", ds["train"], columns=("sentence1", "sentence2", "label"))

labels = Labels((model, tokenizer), dynamic=False)
labels([[("Sailing to the arctic", "Dogs and cats don't get along")], 
        [("Walking down the road", "Walking down the street")]])
# Output:
#   [[(0, 0.5648878216743469)], [(0, 0.97544926404953)]]



================================================
FILE: examples/17_Train_without_labels.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Train without labels

Almost all data available is unlabeled. Labeled data takes effort to manually review and/or takes time to collect. Zero-shot classification takes existing large language models and runs a similarity comparison between candidate text and a list of labels. This has been shown to perform surprisingly well.

The problem with zero-shot classifiers is that they need to have a large number of parameters (400M+) to perform well against general tasks, which comes with sizable hardware requirements.

This notebook explores using zero-shot classifiers to build training data for smaller models. A simple form of [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation). 
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-train] datasets pandas

"""
# Apply zero-shot classifier to unlabeled text

The following section takes a small 1000 record random sample of the sst2 dataset and applies a zero-shot classifer to the text. The labels are ignored. This dataset was chosen only to be able to evaluate the accuracy at then end. 
"""

import random

from datasets import load_dataset

from txtai.pipeline import Labels

def batch(texts, size):
    return [texts[x : x + size] for x in range(0, len(texts), size)]

# Set random seed for repeatable sampling
random.seed(42)

ds = load_dataset("glue", "sst2")

sentences = random.sample(ds["train"]["sentence"], 1000)

# Load a zero shot classifier - txtai provides this through the Labels pipeline
labels = Labels("microsoft/deberta-large-mnli")

train = []

# Zero-shot prediction using ["negative", "positive"] labels
for chunk in batch(sentences, 32):
    train.extend([{"text": chunk[x], "label": label[0][0]} for x, label in enumerate(labels(chunk, ["negative", "positive"]))])

"""
Next, we'll use the training set we just built to train a smaller Electra model.
"""

from txtai.pipeline import HFTrainer

trainer = HFTrainer()
model, tokenizer = trainer("google/electra-base-discriminator", train, num_train_epochs=5)
# Output:
#   Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']

#   - This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).

#   - This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

#   Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']

#   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

#   <IPython.core.display.HTML object>

"""
# Evaluating accuracy

Recall the training set is only 1000 records. To be clear, training an Electra model against the full sst2 dataset would perform better than below. But for this exercise, we're are not using the training labels and simulating labeled data not being available.

First, lets see what the baseline accuracy for the zero-shot model would be against the sst2 evaluation set. Reminder that this has not seen any of the sst2 training data. 

"""

labels = Labels("microsoft/deberta-large-mnli")
# Output:
#   Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']

#   - This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).

#   - This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).


results = [row["label"] == labels(row["sentence"], ["negative", "positive"])[0][0] for row in ds["validation"]]
sum(results) / len(ds["validation"])
# Output:
#   0.8818807339449541

"""
88.19% accuracy, not bad for a model that has not been trained on the dataset at all! Shows the power of zero-shot classification.

Next, let's test our model trained on the 1000 zero-shot labeled records.
"""

labels = Labels((model, tokenizer), dynamic=False)

results = [row["label"] == labels(row["sentence"])[0][0] for row in ds["validation"]]
sum(results) / len(ds["validation"])
# Output:
#   0.8738532110091743

"""
87.39% accuracy! Wouldn't get too carried away with the percentages but this at least nearly meets the accuracy of the zero-shot classifier.

Now this model will be highly tuned for a specific task but it had the opportunity to learn from the combined 1000 records whereas the zero-shot classifier views each record independently. It's also much more performant. 
"""

"""
# Conclusion

This notebook explored a method of building trained text classifiers without training data being available. Given the amount of resources needed to run large-scale zero-shot classifiers, this method is a simple way to build smaller models tuned for specific tasks. In this example, the zero-shot classifier has 400M parameters and the trained text classifier has 110M. 
"""



================================================
FILE: examples/18_Export_and_run_models_with_ONNX.ipynb
================================================
Error processing notebook: 'text/plain'


================================================
FILE: examples/19_Train_a_QA_model.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Train a QA model

The [Hugging Face Model Hub](https://huggingface.co/models) has a wide range of models that can handle many tasks. While these models perform well, the best performance is often found when fine-tuning a model with task-specific data. 

Hugging Face provides a [number of full-featured examples](https://github.com/huggingface/transformers/tree/master/examples) to assist with training task-specific models. When building models from the command line, these scripts are a great way to get started.

txtai provides a training pipeline that can be used to train new models programatically using the Transformers Trainer framework.

This example trains a small QA model and then further fine-tunes it with a couple new examples (few-shot learning).
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-train] datasets pandas

"""
# Train a SQuAD 2.0 Model

The first step is training a SQuAD 2.0 model. SQuAD is a question-answer dataset that poses a question with a context along with the identified answer. It's also possible to not have an answer. See the [SQuAD dataset website](https://rajpurkar.github.io/SQuAD-explorer/) for more information.

We'll use a tiny Bert model with a portion of SQuAD 2.0 for efficiency purposes.
"""

from datasets import load_dataset
from txtai.pipeline import HFTrainer

ds = load_dataset("squad_v2")

trainer = HFTrainer()
trainer("google/bert_uncased_L-2_H-128_A-2", ds["train"].select(range(3000)), task="question-answering", output_dir="bert-tiny-squadv2")
print("Training complete")
# Output:
#   Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)

#   Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-73bbe029cf3366fc.arrow

#   Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']

#   - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).

#   - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

#   Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']

#   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

#   <IPython.core.display.HTML object>
#   Training complete


"""
# Fine-tune with new data

Next we'll add a few additional examples. Fine-tuning a QA model will help with framing a certain type of question or improve performance for a specific use-case. 

For smaller models with a narrow use case, this helps the model zero in on the types of questions that are to be asked. In this case, we want to tell the model exactly the types of information we're looking for when asking for ingredients. This will help improve confidence in the answers the model is generating.

"""

# Training data
data = [
    {"question": "What ingredient?", "context": "Pour 1 can whole tomatoes", "answers": "tomatoes"},
    {"question": "What ingredient?", "context": "Dice 1 yellow onion", "answers": "onion"},
    {"question": "What ingredient?", "context": "Cut 1 red pepper", "answers": "pepper"},
    {"question": "What ingredient?", "context": "Peel and dice 1 clove garlic", "answers": "garlic"},
    {"question": "What ingredient?", "context": "Put 1/2 lb beef", "answers": "beef"},
]

model, tokenizer = trainer("bert-tiny-squadv2", data, task="question-answering", num_train_epochs=10)
# Output:
#   <IPython.core.display.HTML object>

"""
# Test the model

Now we're ready to test the results! The following sections run a question against the original model only trained with SQuAD 2.0 and the further fine-tuned model.
"""

from transformers import pipeline

questions = pipeline("question-answering", model="bert-tiny-squadv2")
questions("What ingredient?", "Peel and dice 1 shallot")
# Output:
#   {'answer': 'dice 1 shallot',

#    'end': 23,

#    'score': 0.05128436163067818,

#    'start': 9}

from transformers import pipeline

questions = pipeline("question-answering", model=model.to("cpu"), tokenizer=tokenizer)
questions("What ingredient?", "Peel and dice 1 shallot")
# Output:
#   {'answer': 'shallot', 'end': 23, 'score': 0.13187439739704132, 'start': 16}

"""
See how the results are more confident and have a better answer. This method allows using a smaller model with a narrow set of functionality with the upside of increased speed. Give it a try with your own data!
"""



================================================
FILE: examples/20_Extractive_QA_to_build_structured_data.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Extractive QA to build structured data

Traditional ETL/data parsing systems establish rules to extract information of interest. Regular expressions, string parsing and similar methods define fixed rules. This works in many cases but what if you are working with unstructured data containing numerous variations? The rules can be cumbersome and hard to maintain over time.

This notebook uses machine learning and extractive question-answering (QA) to utilize the vast knowledge built into large language models. These models have been trained on extremely large datasets, learning the many variations of natural language. 
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-train]

"""
# Train a QA model with few-shot learning

The code below trains a new QA model using a few examples. These examples gives the model hints on the type of questions that will be asked and the type of answers to look for. It doesn't take a lot of examples to do this as shown below.
"""

import pandas as pd
from txtai.pipeline import HFTrainer, Questions, Labels

# Training data for few-shot learning
data = [
    {"question": "What is the url?",
     "context": "Faiss (https://github.com/facebookresearch/faiss) is a library for efficient similarity search.",
     "answers": "https://github.com/facebookresearch/faiss"},
    {"question": "What is the url", "context": "The last release was Wed Sept 25 2021", "answers": None},
    {"question": "What is the date?", "context": "The last release was Wed Sept 25 2021", "answers": "Wed Sept 25 2021"},
    {"question": "What is the date?", "context": "The order total comes to $44.33", "answers": None},
    {"question": "What is the amount?", "context": "The order total comes to $44.33", "answers": "$44.33"},
    {"question": "What is the amount?", "context": "The last release was Wed Sept 25 2021", "answers": None},
]

# Fine-tune QA model
trainer = HFTrainer()
model, tokenizer = trainer("distilbert-base-cased-distilled-squad", data, task="question-answering")
# Output:
#   <IPython.core.display.HTML object>

"""
# Parse data into a structured table

The next section takes a series of rows of text and runs a set of questions against each row. The answers are then used to build a pandas DataFrame.
"""

# Input data
context = ["Released on 6/03/2021",
           "Release delayed until the 11th of August",
           "Documentation can be found here: neuml.github.io/txtai",
           "The stock price fell to three dollars",
           "Great day: closing price for March 23rd is $33.11, for details - https://finance.google.com"]

# Define column queries
queries = ["What is the url?", "What is the date?", "What is the amount?"]

# Extract fields
questions = Questions(path=(model, tokenizer), gpu=True)
results = [questions([question] * len(context), context) for question in queries]
results.append(context)

# Load into DataFrame
pd.DataFrame(list(zip(*results)), columns=["URL", "Date", "Amount", "Text"])
# Output:
#                             URL  ...                                               Text

#   0                        None  ...                              Released on 6/03/2021

#   1                        None  ...           Release delayed until the 11th of August

#   2       neuml.github.io/txtai  ...  Documentation can be found here: neuml.github....

#   3                        None  ...              The stock price fell to three dollars

#   4  https://finance.google.com  ...  Great day: closing price for March 23rd is $33...

#   

#   [5 rows x 4 columns]

"""
# Add additional columns

This method can be combined with other models to categorize, group or otherwise derive additional columns. The code below derives an additional sentiment column.
"""

# Add sentiment
labels = Labels(path="distilbert-base-uncased-finetuned-sst-2-english", dynamic=False)
labels = ["POSITIVE" if x[0][0] == 1 else "NEGATIVE" for x in labels(context)]
results.insert(len(results) - 1, labels)

# Load into DataFrame
pd.DataFrame(list(zip(*results)), columns=["URL", "Date", "Amount", "Sentiment", "Text"])
# Output:
#                             URL  ...                                               Text

#   0                        None  ...                              Released on 6/03/2021

#   1                        None  ...           Release delayed until the 11th of August

#   2       neuml.github.io/txtai  ...  Documentation can be found here: neuml.github....

#   3                        None  ...              The stock price fell to three dollars

#   4  https://finance.google.com  ...  Great day: closing price for March 23rd is $33...

#   

#   [5 rows x 5 columns]



================================================
FILE: examples/21_Export_and_run_other_machine_learning_models.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Export and run other machine learning models

txtai primarily has support for [Hugging Face Transformers](https://github.com/huggingface/transformers) and [ONNX](https://github.com/microsoft/onnxruntime) models. This enables txtai to hook into the rich model framework available in Python, export this functionality via the API to other languages (JavaScript, Java, Go, Rust) and even export and natively load models with ONNX.

What about other machine learning frameworks? Say we have an existing TF-IDF + Logistic Regression model that has been well tuned. Can this model be exported to ONNX and used in txtai for labeling and similarity queries? Or what about a simple PyTorch text classifier? Yes, both of these can be done!

With the [onnxmltools](https://github.com/onnx/onnxmltools) library, traditional models from [scikit-learn](https://scikit-learn.org/stable/), [XGBoost](https://xgboost.readthedocs.io/en/latest/) and others can be exported to ONNX and loaded with txtai. Additionally, Hugging Face's trainer module can train generic PyTorch modules. This notebook will walk through all these examples.


"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline,similarity] datasets

"""
# Train a TF-IDF + Logistic Regression model

For this example, we'll load the emotion dataset from Hugging Face datasets and build a TF-IDF + Logistic Regression model with scikit-learn.

The emotion dataset has the following labels:

- sadness (0)
- joy (1)
- love (2)
- anger (3)
- fear (4)
- surprise (5)

"""

from datasets import load_dataset

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

ds = load_dataset("emotion")

# Train the model
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('lr', LogisticRegression(max_iter=250))
])

pipeline.fit(ds["train"]["text"], ds["train"]["label"])

# Determine accuracy on validation set
results = pipeline.predict(ds["validation"]["text"])
labels = ds["validation"]["label"]

results = [results[x] == label for x, label in enumerate(labels)]
print("Accuracy =", sum(results) / len(ds["validation"]))
# Output:
#   Using custom data configuration default

#   Reusing dataset emotion (/root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)

#   Accuracy = 0.8595


"""
86% accuracy - not too bad! While we all get caught up in deep learning and advanced methods, good ole TF-IDF + Logistic Regression is still a solid performer and runs much faster. If that level of accuracy works, no reason to overcomplicate things.
"""

"""
# Export and load with txtai

The next section exports this model to ONNX and shows how the model can be used for similarity queries. 
"""

from txtai.pipeline import Labels, MLOnnx, Similarity

def tokenize(inputs, **kwargs):
    if isinstance(inputs, str):
        inputs = [inputs]

    return {"input_ids": [[x] for x in inputs]}

def query(model, tokenizer, multilabel=False):
    # Load models into similarity pipeline
    similarity = Similarity((model, tokenizer), dynamic=False)

    # Add labels to model
    similarity.pipeline.model.config.id2label = {0: "sadness", 1: "joy", 2: "love", 3: "anger", 4: "fear", 5: "surprise"}
    similarity.pipeline.model.config.label2id = dict((v, k) for k, v in similarity.pipeline.model.config.id2label.items())

    inputs = ["that caught me off guard", "I didn t see that coming", "i feel bad", "What a wonderful goal!"]
    scores = similarity("joy", inputs, multilabel)
    for uid, score in scores[:5]:
        print(inputs[uid], score)

# Export to ONNX
onnx = MLOnnx()
model = onnx(pipeline)

# Create labels pipeline using scikit-learn ONNX model
sklabels = Labels((model, tokenize), dynamic=False)

# Add labels to model
sklabels.pipeline.model.config.id2label = {0: "sadness", 1: "joy", 2: "love", 3: "anger", 4: "fear", 5: "surprise"}
sklabels.pipeline.model.config.label2id = dict((v, k) for k, v in sklabels.pipeline.model.config.id2label.items())

# Run test query using model
query(model, tokenize, None)
# Output:
#   What a wonderful goal! 0.909473717212677

#   I didn t see that coming 0.47113093733787537

#   that caught me off guard 0.42067453265190125

#   i feel bad 0.019547615200281143


"""
txtai can use a standard text classification model for similarity queries, where the label(s) are a list of fixed queries. The output above shows the best results for the query "joy".
"""

"""
# Train a PyTorch model

The next section defines a simple PyTorch text classifier. The transformers library has a trainer package that supports training PyTorch models, assuming some standard conventions/naming is used. 
"""

# Set predictable seeds
import os
import random
import torch

import numpy as np

from torch import nn
from torch.nn import CrossEntropyLoss
from transformers import AutoConfig, AutoTokenizer

from txtai.models import Registry
from txtai.pipeline import HFTrainer

from transformers.modeling_outputs import SequenceClassifierOutput

def seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

class Simple(nn.Module):
    def __init__(self, vocab, dimensions, labels):
        super().__init__()

        self.config = AutoConfig.from_pretrained("bert-base-uncased")
        self.labels = labels

        self.embedding = nn.EmbeddingBag(vocab, dimensions)
        self.classifier = nn.Linear(dimensions, labels)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.classifier.weight.data.uniform_(-initrange, initrange)
        self.classifier.bias.data.zero_()

    def forward(self, input_ids=None, labels=None, **kwargs):
        embeddings = self.embedding(input_ids)
        logits = self.classifier(embeddings)

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.labels), labels.view(-1))

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
        )

# Set seed for reproducibility
seed()

# Define model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = Simple(tokenizer.vocab_size, 128, len(ds["train"].unique("label")))

# Train model
train = HFTrainer()
model, tokenizer = train((model, tokenizer), ds["train"], per_device_train_batch_size=8, learning_rate=1e-3, num_train_epochs=15, logging_steps=10000)

# Register custom model to fully support pipelines
Registry.register(model)

# Create labels pipeline using PyTorch model
thlabels = Labels((model, tokenizer), dynamic=False)

# Determine accuracy on validation set
results = [row["label"] == thlabels(row["text"])[0][0] for row in ds["validation"]]
print("Accuracy = ", sum(results) / len(ds["validation"]))
# Output:
#   Loading cached processed dataset at /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-a983327c4471f5aa.arrow

#   <IPython.core.display.HTML object>
#   Accuracy =  0.883


"""
88% accuracy this time. Pretty good for such a simple network and something that could definitely be improved upon. 

Once again let's run similarity queries using this model.
"""

query(model, tokenizer)
# Output:
#   What a wonderful goal! 1.0

#   that caught me off guard 0.9998751878738403

#   I didn t see that coming 0.7328283190727234

#   i feel bad 5.2972134609891875e-19


"""
Same result order as with the scikit-learn model with scoring variations which is expected given this is a completely different model.
"""

"""
# Pooled embeddings

The PyTorch model above consists of an embeddings layer with a linear classifier on top of it. What if we take that embeddings layer and use it for similarity queries? Let's give it a try.
"""

from txtai.embeddings import Embeddings

class SimpleEmbeddings(nn.Module):
    def __init__(self, embeddings):
        super().__init__()

        self.embeddings = embeddings

    def forward(self, input_ids=None, **kwargs):
        return (self.embeddings(input_ids),)

embeddings = Embeddings({"method": "pooling", "path": SimpleEmbeddings(model.embedding), "tokenizer": "bert-base-uncased"})
print(embeddings.similarity("mad", ["Glad you found it", "Happy to see you", "I'm angry"]))
# Output:
#   [(2, 0.8323876857757568), (1, -0.11010512709617615), (0, -0.16152513027191162)]


"""
Definitely looks like the embeddings have stored knowledge. Could these embeddings be good enough to build a semantic search index, especially for sentiment based data, given the training dataset? Possibly. It certainly would run faster than a standard transformer model (see below). 
"""

"""
# Train a transformer model and compare accuracy/speed

Let's train a standard transformer sequence classifier and compare the accuracy/speed between the two. 
"""

train = HFTrainer()
model, tokenizer = train("microsoft/xtremedistil-l6-h384-uncased", ds["train"], logging_steps=2000)

tflabels = Labels((model, tokenizer), dynamic=False)

# Determine accuracy on validation set
results = [row["label"] == tflabels(row["text"])[0][0] for row in ds["validation"]]
print("Accuracy = ", sum(results) / len(ds["validation"]))
# Output:
#   Loading cached processed dataset at /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-98b7ef31bf6ca944.arrow

#   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']

#   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

#   <IPython.core.display.HTML object>
#   Accuracy =  0.926


"""
As expected, the accuracy is better. The model above is a distilled model and even better accuracy can be obtained with a model like "roberta-base" with the tradeoff being increased training/inference time. 

Speaking of speed, let's compare the speed of these models.
"""

import time

# Test inputs
inputs = ds["test"]["text"]
print("Testing speed of %d items" % len(inputs))

start = time.time()
r1 = sklabels(inputs, multilabel=None)
print("TF-IDF + Logistic Regression time =", time.time() - start)

start = time.time()
r2 = thlabels(inputs)
print("PyTorch time =", time.time() - start)

start = time.time()
r3 = tflabels(inputs)
print("Transformers time =", time.time() - start, "\n")

# Compare model results
for x in range(5):
  print("index: %d" % x)
  print(r1[x][0])
  print(r2[x][0])
  print(r3[x][0], "\n")
# Output:
#   Testing speed of 2000 items

#   TF-IDF + Logistic Regression time = 1.0483319759368896

#   PyTorch time = 2.0001697540283203

#   Transformers time = 13.71584439277649 

#   

#   index: 0

#   (0, 0.7258279323577881)

#   (0, 1.0)

#   (0, 0.998375654220581) 

#   

#   index: 1

#   (0, 0.854256272315979)

#   (0, 1.0)

#   (0, 0.9983494281768799) 

#   

#   index: 2

#   (0, 0.6306578516960144)

#   (0, 0.9999700784683228)

#   (0, 0.9982945322990417) 

#   

#   index: 3

#   (1, 0.554378092288971)

#   (1, 0.9998960494995117)

#   (1, 0.99846351146698) 

#   

#   index: 4

#   (0, 0.8961835503578186)

#   (0, 1.0)

#   (0, 0.9984095692634583) 

#   


"""
# Wrapping up

This notebook showed how frameworks outside of Transformers and ONNX can be used as models in txtai.

As seen in the section above, TF-IDF + Logistic Regression is 16 times faster than a distilled Transformers model. A simple PyTorch network is 8 times faster. Depending on your accuracy requirements, it may make sense to use a simpler model to get better runtime performance.
"""



================================================
FILE: examples/22_Transform_tabular_data_with_composable_workflows.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Transform tabular data with composable workflows

txtai has support for processing both unstructured and structured data. Structured or tabular data is grouped into rows and columns. This can be a spreadsheet, an API call that returns JSON or XML or even list of key-value pairs.

This notebook will walk through examples on how to use workflows with the tabular pipeline to transform and index structured data.

"""

"""
# Install dependencies

Install `txtai` and all dependencies. We will install the api, pipeline and workflow optional extras packages. 
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline,workflow] sacremoses

"""
# CSV Workflow

The first example will transform and index a CSV file. The [COVID-19 Open Research Dataset](https://allenai.org/data/cord-19) (CORD-19) is a repository of medical articles covering COVID-19. This workflow reads the input CSV and builds a semantic search index.

The first step is downloading the dataset locally.
"""

%%capture
# Get CORD-19 metadata file
!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-11-01/metadata.csv
!head -1 metadata.csv > input.csv
!tail -10000 metadata.csv >> input.csv

"""
The next section creates a simple workflow consisting of a tabular pipeline. The tabular pipeline builds a list of (id, text, tag) tuples that can be easily loaded into an Embeddings index. For this example, we'll use the `url` column as the id and the `title` column as the text column. The textcolumns parameter takes a list of columns to support indexing text content from multiple columns. 

The file input.csv is processed and the first 5 rows are shown.
"""

from txtai.pipeline import Tabular
from txtai.workflow import Task, Workflow

# Create tabular instance mapping input.csv fields
tabular = Tabular("url", ["title"])

# Create workflow
workflow = Workflow([Task(tabular)])

# Print 5 rows of input.csv via workflow
list(workflow(["input.csv"]))[:5]
# Output:
#   [('https://doi.org/10.1016/j.cmpb.2021.106469; https://www.ncbi.nlm.nih.gov/pubmed/34715516/',

#     'Computer simulation of the dynamics of a spatial susceptible-infected-recovered epidemic model with time delays in transmission and treatment.',

#     None),

#    ('https://www.ncbi.nlm.nih.gov/pubmed/34232002/; https://doi.org/10.36849/jdd.5544',

#     'Understanding the Potential Role of Abrocitinib in the Time of SARS-CoV-2',

#     None),

#    ('https://doi.org/10.1186/1471-2458-8-42; https://www.ncbi.nlm.nih.gov/pubmed/18234083/',

#     "Can the concept of Health Promoting Schools help to improve students' health knowledge and practices to combat the challenge of communicable diseases: Case study in Hong Kong?",

#     None),

#    ('https://www.ncbi.nlm.nih.gov/pubmed/32983582/; https://www.sciencedirect.com/science/article/pii/S2095809920302514?v=s5; https://api.elsevier.com/content/article/pii/S2095809920302514; https://doi.org/10.1016/j.eng.2020.07.018',

#     'Buying time for an effective epidemic response: The impact of a public holiday for outbreak control on COVID-19 epidemic spread',

#     None),

#    ('https://doi.org/10.1093/pcmedi/pbab016',

#     'The SARS-CoV-2 spike L452R-E484Q variant in the Indian B.1.617 strain showed significant reduction in the neutralization activity of immune sera',

#     None)]

"""
Next, we take the workflow output, build an Embeddings index and run a search query.
"""

from txtai.embeddings import Embeddings

# Embeddings with sentence-transformers backend
embeddings = Embeddings({"method": "transformers", "path": "sentence-transformers/paraphrase-mpnet-base-v2"})

# Index subset of CORD-19 data
data = list(workflow(["input.csv"]))
embeddings.index(data)

for uid, _ in embeddings.search("insulin"):
  title = [text for url, text, _ in data if url == uid][0]
  print(title, uid)
# Output:
#   Importance of diabetes management during the COVID-19 pandemic. https://doi.org/10.1080/00325481.2021.1978704; https://www.ncbi.nlm.nih.gov/pubmed/34602003/

#   Position Statement on How to Manage Patients with Diabetes and COVID-19 https://www.ncbi.nlm.nih.gov/pubmed/33442169/; https://doi.org/10.15605/jafes.035.01.03

#   Successful blood glucose management of a severe COVID-19 patient with diabetes: A case report https://www.ncbi.nlm.nih.gov/pubmed/32590779/; https://doi.org/10.1097/md.0000000000020844


"""
The example searched for the term `insulin`. The top results mention diabetes and blood glucose which are a closely associated terms for diabetes.
"""

"""
# Workflow with stored content

Next we'll re-run the same example adding in full content storage. Full content storage enables SQL queries.
"""

import json

# Create tabular instance mapping input.csv fields
tabular = Tabular("url", ["title"], True)

# Create workflow
workflow = Workflow([Task(tabular)])

# Embeddings with sentence-transformers backend
embeddings = Embeddings({"method": "transformers", "path": "sentence-transformers/paraphrase-mpnet-base-v2", "content": True})

# Index subset of CORD-19 data
data = list(workflow(["input.csv"]))
embeddings.index(data)

for result in embeddings.search("select title, abstract, authors, doi from txtai where similar('insulin')"):
  print(json.dumps(result, default=str, indent=2))
# Output:
#   {

#     "title": "Importance of diabetes management during the COVID-19 pandemic.",

#     "abstract": "Uncontrolled diabetes and/or hyperglycemia is associated with severe COVID-19 disease and increased mortality. It is now known that poor glucose control before hospital admission can be associated with a high risk of in-hospital death. By achieving and maintaining glycemic control, primary care physicians (PCPs) play a critical role in limiting this potentially devastating outcome. Further, despite the hope that mass vaccination will help control the pandemic, genetic variants of the virus are causing surges in some countries. As such, PCPs will treat an increasing number of patients with diabetes who have symptoms of post-COVID-19 infection, or even have new-onset type 2 diabetes as a result of COVID-19 infection. However, much of the literature published focuses on the effects of COVID-19 in hospitalized patients, with few publications providing information and advice to those caring for people with diabetes in the primary care setting. This manuscript reviews the current knowledge of the risk and outcomes of individuals with diabetes who are infected with COVID-19 and provides information for PCPs on the importance of glucose control, appropriate treatment, and use of telemedicine and online prescription delivery systems to limit the potentially devastating effects of COVID-19 in people with hyperglycemia.",

#     "authors": "Pettus, Jeremy; Skolnik, Neil",

#     "doi": "10.1080/00325481.2021.1978704"

#   }

#   {

#     "title": "Position Statement on How to Manage Patients with Diabetes and COVID-19",

#     "abstract": null,

#     "authors": null,

#     "doi": "10.15605/jafes.035.01.03"

#   }

#   {

#     "title": "Successful blood glucose management of a severe COVID-19 patient with diabetes: A case report",

#     "abstract": "RATIONALE: Coronavirus disease 2019 (COVID-19) has emerged as a rapidly spreading communicable disease affecting individuals worldwide. Patients with diabetes are more vulnerable to the disease, and the mortality is higher than in those without diabetes. We reported a severe COVID-19 patient with diabetes and shared our experience with blood glucose management. PATIENT CONCERNS: A 64-year-old female diabetes patient was admitted to the intensive care unit due to productive coughing for 8 days without any obvious cause. The results of blood gas analysis indicated that the partial pressure of oxygen was 84 mm Hg with oxygen 8 L/min, and the oxygenation index was less than 200 mm Hg. In addition, postprandial blood glucose levels were abnormal (29.9 mmol/L). DIAGNOSES: The patient was diagnosed with COVID-19 (severe type) and type 2 diabetes. INTERVENTIONS: Comprehensive interventions including establishing a multidisciplinary team, closely monitoring her blood glucose level, an individualized diabetes diet, early activities, psychological care, etc, were performed to control blood glucose while actively treating COVID-19 infection. OUTCOMES: After the comprehensive measures, the patient's blood glucose level gradually became stable, and the patient was discharged after 20 days of hospitalization. LESSONS: This case indicated that the comprehensive measures performed by a multidisciplinary team achieved good treatment effects on a COVID-19 patient with diabetes. Targeted treatment and nursing methods should be performed based on patients\u2019 actual situations in clinical practice.",

#     "authors": "Hu, Rujun; Gao, Huiming; Huang, Di; Jiang, Deyu; Chen, Fang; Fu, Bao; Yuan, Xiaoli; Li, Jin; Jiang, Zhixia",

#     "doi": "10.1097/md.0000000000020844"

#   }


"""
Note how the same results are returned with additional content fields.
"""

"""
# JSON Service Workflow

The next example builds a workflow that runs a query against a remote URL, retrieves the results, then transforms and indexes the tabular data. This example gets the top results from the [Hacker News front page](https://news.ycombinator.com/). 

Below shows how to build the ServiceTask and prints the first JSON result. Details on how to configure the ServiceTask can be found in [txtai's documentation](https://neuml.github.io/txtai/workflows/).
"""

from txtai.workflow import ServiceTask

service = ServiceTask(url="https://hn.algolia.com/api/v1/search", method="get", params={"tags": None}, batch=False, extract="hits")
workflow = Workflow([service])

list(workflow(["front_page"]))[0][2]
# Output:
#   {'_highlightResult': {'author': {'matchLevel': 'none',

#      'matchedWords': [],

#      'value': 'cheesestain'},

#     'title': {'matchLevel': 'none',

#      'matchedWords': [],

#      'value': 'Ante: A low-level functional language'},

#     'url': {'matchLevel': 'none',

#      'matchedWords': [],

#      'value': 'https://antelang.org/'}},

#    '_tags': ['story', 'author_cheesestain', 'story_31775216', 'front_page'],

#    'author': 'cheesestain',

#    'comment_text': None,

#    'created_at': '2022-06-17T07:39:40.000Z',

#    'created_at_i': 1655451580,

#    'num_comments': 109,

#    'objectID': '31775216',

#    'parent_id': None,

#    'points': 207,

#    'story_id': None,

#    'story_text': None,

#    'story_title': None,

#    'story_url': None,

#    'title': 'Ante: A low-level functional language',

#    'url': 'https://antelang.org/'}

"""
Next we'll map the JSON data using the tabular pipeline. `url` will be used as the id column and `title` as the text to index.
"""

from txtai.workflow import Task

# Create tabular instance mapping input.csv fields
tabular = Tabular("url", ["title"])

# Recreate service applying the tabular pipeline to each result
service = ServiceTask(action=tabular, url="https://hn.algolia.com/api/v1/search", method="get", params={"tags": None}, batch=False, extract="hits")
workflow = Workflow([service])

list(workflow(["front_page"]))[2]
# Output:
#   ('https://antelang.org/', 'Ante: A low-level functional language', None)

"""
As we did previously, let's build an Embeddings index and run a search query.
"""

# Embeddings with sentence-transformers backend
embeddings = Embeddings({"method": "transformers", "path": "sentence-transformers/paraphrase-mpnet-base-v2"})

# Index Hacker News front page
data = list(workflow(["front_page"]))
embeddings.index(data)

for uid, _ in embeddings.search("programming"):
  title = [text for url, text, _ in data if url == uid][0]
  print(title, uid)
# Output:
#   Bundling binary tools in Python wheels https://simonwillison.net/2022/May/23/bundling-binary-tools-in-python-wheels/

#   Ante: A low-level functional language https://antelang.org/

#   Adding a Rust compiler front end to GCC [video] https://www.youtube.com/watch?v=R8Pr21nlhig


"""
# XML Service workflow

txtai's ServiceTask can consume both JSON and XML. This example runs a query against the [arXiv API](https://arxiv.org/), transforms the results and indexes them for search.

Below shows how to build the ServiceTask and prints the first XML result.
"""

service = ServiceTask(url="http://export.arxiv.org/api/query", method="get", params={"search_query": None, "max_results": 25}, batch=False, extract=["feed", "entry"])
workflow = Workflow([service])

list(workflow(["all:aliens"]))[0][:1]
# Output:
#   [{'arxiv:comment': {'#text': 'To appear in Astrophysical Journal',

#      '@xmlns:arxiv': 'http://arxiv.org/schemas/atom'},

#     'arxiv:doi': {'#text': '10.3847/1538-4357/ac2369',

#      '@xmlns:arxiv': 'http://arxiv.org/schemas/atom'},

#     'arxiv:primary_category': {'@scheme': 'http://arxiv.org/schemas/atom',

#      '@term': 'q-bio.OT',

#      '@xmlns:arxiv': 'http://arxiv.org/schemas/atom'},

#     'author': [{'name': 'Robin Hanson'},

#      {'name': 'Daniel Martin'},

#      {'name': 'Calvin McCarter'},

#      {'name': 'Jonathan Paulson'}],

#     'category': [{'@scheme': 'http://arxiv.org/schemas/atom',

#       '@term': 'q-bio.OT'},

#      {'@scheme': 'http://arxiv.org/schemas/atom', '@term': 'physics.pop-ph'}],

#     'id': 'http://arxiv.org/abs/2102.01522v3',

#     'link': [{'@href': 'http://dx.doi.org/10.3847/1538-4357/ac2369',

#       '@rel': 'related',

#       '@title': 'doi'},

#      {'@href': 'http://arxiv.org/abs/2102.01522v3',

#       '@rel': 'alternate',

#       '@type': 'text/html'},

#      {'@href': 'http://arxiv.org/pdf/2102.01522v3',

#       '@rel': 'related',

#       '@title': 'pdf',

#       '@type': 'application/pdf'}],

#     'published': '2021-02-01T18:27:12Z',

#     'summary': "If life on Earth had to achieve n 'hard steps' to reach humanity's level,\nthen the chance of this event rose as time to the n-th power. Integrating this\nover habitable star formation and planet lifetime distributions predicts >99%\nof advanced life appears after today, unless n<3 and max planet duration\n<50Gyr. That is, we seem early. We offer this explanation: a deadline is set by\n'loud' aliens who are born according to a hard steps power law, expand at a\ncommon rate, change their volumes' appearances, and prevent advanced life like\nus from appearing in their volumes. 'Quiet' aliens, in contrast, are much\nharder to see. We fit this three-parameter model of loud aliens to data: 1)\nbirth power from the number of hard steps seen in Earth history, 2) birth\nconstant by assuming a inform distribution over our rank among loud alien birth\ndates, and 3) expansion speed from our not seeing alien volumes in our sky. We\nestimate that loud alien civilizations now control 40-50% of universe volume,\neach will later control ~10^5 - 3x10^7 galaxies, and we could meet them in\n~200Myr - 2Gyr. If loud aliens arise from quiet ones, a depressingly low\ntransition chance (~10^-4) is required to expect that even one other quiet\nalien civilization has ever been active in our galaxy. Which seems bad news for\nSETI. But perhaps alien volume appearances are subtle, and their expansion\nspeed lower, in which case we predict many long circular arcs to find in our\nsky.",

#     'title': 'If Loud Aliens Explain Human Earliness, Quiet Aliens Are Also Rare',

#     'updated': '2021-09-06T14:18:23Z'}]

"""
Next we'll map the XML data using the tabular pipeline. `id` will be used as the id column and `title` as the text to index.
"""

from txtai.workflow import Task

# Create tablular pipeline with new mapping
tabular = Tabular("id", ["title"])

# Recreate service applying the tabular pipeline to each result
service = ServiceTask(action=tabular, url="http://export.arxiv.org/api/query", method="get", params={"search_query": None, "max_results": 25}, batch=False, extract=["feed", "entry"])
workflow = Workflow([service])

list(workflow(["all:aliens"]))[:1]
# Output:
#   [('http://arxiv.org/abs/2102.01522v3',

#     'If Loud Aliens Explain Human Earliness, Quiet Aliens Are Also Rare',

#     None)]

"""
As we did previously, let's build an Embeddings index and run a search query.
"""

# Embeddings with sentence-transformers backend
embeddings = Embeddings({"method": "transformers", "path": "sentence-transformers/paraphrase-mpnet-base-v2"})

# Index Hacker News front page
data = list(workflow(["all:aliens"]))
embeddings.index(data)

for uid, _ in embeddings.search("alien radio signals"):
  title = [text for url, text, _ in data if url == uid][0]
  print(title, uid)
# Output:
#   Calculating the probability of detecting radio signals from alien

#     civilizations http://arxiv.org/abs/0707.0011v2

#   Field Trial of Alien Wavelengths on GARR Optical Network http://arxiv.org/abs/1805.04278v1

#   Aliens on Earth. Are reports of close encounters correct? http://arxiv.org/abs/1203.6805v2


"""
# Build a workflow with no code!

The next example shows how one of the same workflows above can be constructed via API configuration. This is a no-code way to build a txtai indexing workflow!
"""

%%writefile workflow.yml
# Index settings
writable: true
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2

# Tabular pipeline
tabular:
  idcolumn: id
  textcolumns: 
    - title

# Workflow definitions
workflow:
  index:
    tasks:
      - task: service
        action: tabular
        url: http://export.arxiv.org/api/query?max_results=25
        method: get
        params:
          search_query: null
        batch: false
        extract: [feed, entry]
      - action: upsert
# Output:
#   Writing workflow.yml


"""
This workflow once again runs an arXiv query and indexes article titles. The workflow configures the same actions that were configured in Python previously. 

Let's start an API instance 
"""

!killall -9 uvicorn
!CONFIG=workflow.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 30
!cat api.log
# Output:
#   INFO:     Started server process [754]

#   2022-06-17 15:05:58,554 [INFO] serve: Started server process [754]

#   INFO:     Waiting for application startup.

#   2022-06-17 15:05:58,554 [INFO] startup: Waiting for application startup.

#   INFO:     Application startup complete.

#   2022-06-17 15:06:07,707 [INFO] startup: Application startup complete.

#   INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)

#   2022-06-17 15:06:07,707 [INFO] _log_started_message: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)


"""
Next we'll execute the workflow. txtai has API bindings for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Golang](https://github.com/neuml/txtai.go). But to keep things simple, we'll just run the commands via cURL. 
"""

# Execute workflow via API call
!curl -X POST "http://localhost:8000/workflow" -H  "accept: application/json" -H  "Content-Type: application/json" -d "{\"name\":\"index\",\"elements\":[\"all:aliens\"]}"
# Output:
#   [["http://arxiv.org/abs/2102.01522v3","If Loud Aliens Explain Human Earliness, Quiet Aliens Are Also Rare",null],["http://arxiv.org/abs/cs/0306071v1","AliEnFS - a Linux File System for the AliEn Grid Services",null],["http://arxiv.org/abs/physics/0306103v1","AliEn - EDG Interoperability in ALICE",null],["http://arxiv.org/abs/2103.05559v1","Oumuamua Is Not a Probe Sent to our Solar System by an Alien\n  Civilization",null],["http://arxiv.org/abs/1403.3979v1","Robust transitivity and density of periodic points of partially\n  hyperbolic diffeomorphisms",null],["http://arxiv.org/abs/1712.09210v1","Sampling alien species inside and outside protected areas: does it\n  matter?",null],["http://arxiv.org/abs/cs/0306067v1","The AliEn system, status and perspectives",null],["http://arxiv.org/abs/0707.0011v2","Calculating the probability of detecting radio signals from alien\n  civilizations",null],["http://arxiv.org/abs/1805.04278v1","Field Trial of Alien Wavelengths on GARR Optical Network",null],["http://arxiv.org/abs/1808.00529v1","Open Category Detection with PAC Guarantees",null],["http://arxiv.org/abs/1206.3640v1","The Study of Climate on Alien Worlds",null],["http://arxiv.org/abs/1203.6805v2","Aliens on Earth. Are reports of close encounters correct?",null],["http://arxiv.org/abs/1604.05078v1","The Imprecise Search for Habitability",null],["http://arxiv.org/abs/1006.2613v1","Resurgence, Stokes phenomenon and alien derivatives for level-one linear\n  differential systems",null],["http://arxiv.org/abs/1307.0653v1","General and alien solutions of a functional equation and of a functional\n  inequality",null],["http://arxiv.org/abs/1705.03394v1","That is not dead which can eternal lie: the aestivation hypothesis for\n  resolving Fermi's paradox",null],["http://arxiv.org/abs/1701.02294v1","Alien Calculus and non perturbative effects in Quantum Field Theory",null],["http://arxiv.org/abs/1801.06180v1","Are Alien Civilizations Technologically Advanced?",null],["http://arxiv.org/abs/1902.05387v1","Simultaneous x, y Pixel Estimation and Feature Extraction for Multiple\n  Small Objects in a Scene: A Description of the ALIEN Network",null],["http://arxiv.org/abs/0711.4034v1","The q-analogue of the wild fundamental group (II)",null],["http://arxiv.org/abs/2111.07895v1","Research Programs Arising from 'Oumuamua Considered as an Alien Craft",null],["http://arxiv.org/abs/2112.15226v1","Variations on the Resurgence of the Gamma Function",null],["http://arxiv.org/abs/astro-ph/0501119v1","Expanding advanced civilizations in the universe",null],["http://arxiv.org/abs/cs/0306068v1","AliEn Resource Brokers",null],["http://arxiv.org/abs/hep-ph/9403231v2","The Renormalization of Composite Operators in Yang-Mills Theories Using\n  General Covariant Gauge",null]]

"""
The data is now indexed. Note that the index configuration has an `upsert` action. Each workflow call will insert new rows or update existing rows. This call could be scheduled with a system cron to execute periodically and build an index of arXiv article titles. 

Now that the index is ready, let's run a search.
"""

# Run a search
!curl -X GET "http://localhost:8000/search?query=radio&limit=3" -H  "accept: application/json"
# Output:
#   [{"id":"http://arxiv.org/abs/0707.0011v2","score":0.40350058674812317},{"id":"http://arxiv.org/abs/1805.04278v1","score":0.3406212031841278},{"id":"http://arxiv.org/abs/1902.05387v1","score":0.22262491285800934}]

"""
# Add a translation step to workflow

Next we'll recreate the workflow, adding one additional step, translating the text into French before indexing. This workflow runs an arXiv query, translates the results and builds an semantic index of titles in French. 
"""

%%writefile workflow.yml
# Index settings
writable: true
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2

# Tabular pipeline
tabular:
  idcolumn: id
  textcolumns: 
    - title

# Translation pipeline
translation:

# Workflow definitions
workflow:
  index:
    tasks:
      - task: service
        action: tabular
        url: http://export.arxiv.org/api/query?max_results=25
        method: get
        params:
          search_query: null
        batch: false
        extract: [feed, entry]
      - action: translation
        args: [fr]
      - action: upsert
# Output:
#   Overwriting workflow.yml


!killall -9 uvicorn
!CONFIG=workflow.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 30
!cat api.log
# Output:
#   INFO:     Started server process [775]

#   2022-06-17 15:06:29,397 [INFO] serve: Started server process [775]

#   INFO:     Waiting for application startup.

#   2022-06-17 15:06:29,397 [INFO] startup: Waiting for application startup.

#   INFO:     Application startup complete.

#   2022-06-17 15:06:40,198 [INFO] startup: Application startup complete.

#   INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)

#   2022-06-17 15:06:40,199 [INFO] _log_started_message: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)


"""
Same as before, we'll run the index workflow and a search
"""

# Execute workflow via API call
!curl -s -X POST "http://localhost:8000/workflow" -H  "accept: application/json" -H  "Content-Type: application/json" -d "{\"name\":\"index\",\"elements\":[\"all:aliens\"]}" > /dev/null

# Run a search
!curl -X GET "http://localhost:8000/search?query=radio&limit=3" -H  "accept: application/json"
# Output:
#   [{"id":"http://arxiv.org/abs/0707.0011v2","score":0.532800555229187},{"id":"http://arxiv.org/abs/0711.4034v1","score":0.24413327872753143},{"id":"http://arxiv.org/abs/2102.01522v3","score":0.22881504893302917}]

"""
# Run YAML workflow in Python

Workflow YAML files can also be directly executed in Python. In this case, all input data is passed locally in Python and not through network interfaces. The following section shows how to do this!
"""

import yaml

from txtai.app import Application

with open("workflow.yml") as config:
  workflow = yaml.safe_load(config)

app = Application(workflow)

# Run the workflow
data = list(app.workflow("index", ["all:aliens"]))

# Run a search
for result in app.search("radio", None):
  text = [row[1] for row in data if row[0] == result["id"]][0]
  print(result["id"], result["score"], text)
# Output:
#   http://arxiv.org/abs/0707.0011v2 0.532800555229187 Calcul de la probabilitÃ© de dÃ©tection des signaux radio de l'Ã©trangercivilisations

#   http://arxiv.org/abs/0711.4034v1 0.24413327872753143 Le q-analogue du groupe fondamental sauvage (II)

#   http://arxiv.org/abs/2102.01522v3 0.22881504893302917 Si les Ã©trangers louds expliquent le dÃ©but de l'humanitÃ©, les Ã©trangers tranquilles sont aussi rares


"""
# Wrapping up

This notework demonstrated how to transform, index and search tabular data from a variety of sources. txtai offers maximum flexibility in building composable workflows to maximize the number of ways data can be indexed for semantic search. 
"""



================================================
FILE: examples/23_Tensor_workflows.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Tensor workflows

Many of the examples and use cases for txtai focus on transforming text. Makes sense as txt is even in the name! But that doesn't mean txtai only works with text.

This notebook will cover examples of how to efficiently process tensors using txtai workflows.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. We will install the api, pipeline and workflow optional extras packages, along with the datasets package. 
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline,workflow] datasets

"""
# Transform large tensor arrays

The first section attempts to apply a simple transform to a very large memory-mapped array (2,000,000 x 1024).
"""

import numpy as np
import torch

# Generate large memory-mapped array
rows, cols = 2000000, 1024
data = np.memmap("data.npy", dtype=np.float32, mode="w+", shape=(rows, cols))
del data

# Open memory-mapped array
data = np.memmap("data.npy", dtype=np.float32, shape=(rows, cols))

# Create tensor
tensor = torch.from_numpy(data).to("cuda:0")

# Apply tanh transform to tensor
torch.tanh(tensor).shape
# Output:
#   Error: RuntimeError: ignored

!ls -l --block-size=MB data.npy
# Output:
#   -rw-r--r-- 1 root root 8192MB Dec  6 23:24 data.npy


"""
Not surprisingly this runs out of CUDA memory. The array needs `2,000,000 * 1024 * 4 = 8GB` which exceeds the amount of GPU memory available.

One of the great things about NumPy and PyTorch arrays is that they can be sliced without having to copy data. Additionally, PyTorch has methods to work directly on NumPy arrays without copying data, in other words both NumPy arrays and PyTorch arrays can share the same memory. This opens the door to efficient processing of tensor data in place. 

Let's try applying a simple tanh transform in batches over the array.
"""

def process(x):
  print(x.shape)
  return torch.tanh(torch.from_numpy(x).to("cuda:0")).cpu().numpy()

# Split into 250,000 rows per call
batch = 250000
count = 0
for x in range(0, len(data), batch):
  for row in process(data[x : x + batch]):
    count += 1

print(count)
# Output:
#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   2000000


"""
Iterating over the data array and selecting slices to operate on allows the transform to complete successfully! Each `torch.from_numpy` call is building a view of a portion the existing large NumPy data array. 
"""

"""
# Enter workflows

The next section takes the same array and shows how workflows can apply transformations to tensors. 
"""

from txtai.workflow import Task, Workflow

# Create workflow with a single task calling process for each batch
task = Task(process)
workflow = Workflow([task], batch)

# Run workflow
count = 0
for row in workflow(data):
  count += 1

print(count)
# Output:
#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   (250000, 1024)

#   2000000


"""
Workflows process the data in the same fashion as the code in the previous section. On top of that, workflows can handle text, images, video, audio, document, tensors and more. Workflow graphs can also be connected together to handle complex use cases.
"""

"""
# Workflows with PyTorch models

The next example applies a PyTorch model to the same data. The model applies a series of transforms and outputs a single float per row.
"""

from torch import nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()

        self.gelu = nn.ReLU()
        self.linear1 = nn.Linear(1024, 512)
        self.dropout = nn.Dropout(0.5)
        self.norm = nn.LayerNorm(512)
        self.linear2 = nn.Linear(512, 1)

    def forward(self, inputs):
        outputs = self.gelu(inputs)
        outputs = self.linear1(outputs)
        outputs = self.dropout(outputs)
        outputs = self.norm(outputs)
        outputs = self.linear2(outputs)

        return outputs

model = Model().to("cuda:0")

def process(x):
  with torch.no_grad():
    outputs = model(torch.from_numpy(x).to("cuda:0")).cpu().numpy()
    print(outputs.shape)
    return outputs

# Create workflow with a single task calling model for each batch
task = Task(process)
workflow = Workflow([task], batch)

# Run workflow
count = 0
for row in workflow(data):
  count += 1

print(count)
# Output:
#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   (250000, 1)

#   2000000


"""
Once again the data can be processed in batches using workflows, even with a more complex model. Let's try a more interesting example.
"""

"""
# Workflows in parallel

Workflows consist of a series of tasks. Each task can output one to many outputs per input element. Multi-output tasks have options available to [merge the data](https://neuml.github.io/txtai/workflow/task/#multi-action-task-merges) for downstream tasks.

The following example builds a workflow with a task having three separate actions. Each action takes text as an input an applies a sentiment classifier. This is followed by a task that merges the three outputs for each row using a mean transform. Essentially, this workflow builds a weighted sentiment classifier using the outputs of three models. 
"""

import time

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class Tokens:
    def __init__(self, texts):
        tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        tokens = tokenizer(texts, padding=True, return_tensors="pt").to("cuda:0")

        self.inputs, self.attention = tokens["input_ids"], tokens["attention_mask"]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, value):
        return (self.inputs[value], self.attention[value])

class Classify:
    def __init__(self, model):
        self.model = model

    def __call__(self, tokens):
        with torch.no_grad():
            inputs, attention = tokens
            outputs = self.model(input_ids=inputs, attention_mask=attention)
            outputs = outputs["logits"]

        return outputs

# Load reviews from the rotten tomatoes dataset
ds = load_dataset("rotten_tomatoes")
texts = ds["train"]["text"]

tokens = Tokens(texts)

model1 = AutoModelForSequenceClassification.from_pretrained("M-FAC/bert-tiny-finetuned-sst2")
model1 = model1.to("cuda:0")

model2 = AutoModelForSequenceClassification.from_pretrained("howey/electra-base-sst2")
model2 = model2.to("cuda:0")

model3 = AutoModelForSequenceClassification.from_pretrained("philschmid/MiniLM-L6-H384-uncased-sst2")
model3 = model3.to("cuda:0")

task1 = Task([Classify(model1), Classify(model2), Classify(model3)])
task2 = Task([lambda x: torch.sigmoid(x).mean(axis=1).cpu().numpy()])

workflow = Workflow([task1, task2], 250)

start = time.time()
for x in workflow(tokens):
  pass

print(f"Took {time.time() - start} seconds")
# Output:
#   Using custom data configuration default

#   Reusing dataset rotten_tomatoes_movie_review (/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)

#   Took 84.73194456100464 seconds


"""
Note that while the task actions are parallel, that doesn't necessarily mean the operations are concurrent. In the case above, the actions are are executed sequentially.

Workflows have an additional option to run task actions concurrently. The two supported modes are "thread" and "process". I/O bound actions will do better with multithreading and CPU bound actions will do better with multiprocessing. More can be read in the [txtai documentation](https://neuml.github.io/txtai/workflow/task/#multi-action-task-concurrency). 
"""

task1 = Task([Classify(model1), Classify(model2), Classify(model3)], concurrency="thread")
task2 = Task([lambda x: torch.sigmoid(x).mean(axis=1).cpu().numpy()])

workflow = Workflow([task1, task2], 250)

start = time.time()
for x in workflow(tokens):
  pass

print(f"Took {time.time() - start} seconds")
# Output:
#   Took 85.21102929115295 seconds


"""
In this case, concurrency doesn't improve performance. While the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) is a factor, a bigger factor is that the GPU is already fully loaded. This method would be more beneficial if the system had a second GPU or the primary GPU had idle cycles. 
"""

"""
# Wrapping up

This notebook introduced a number of different ways to work with large-scale tensor data and process it efficiently. This notebook purposely didn't cover embeddings and pipelines to demonstrate how workflows can stand on their own. In addition to workflows, this notebook covered efficient methods to work with large tensor arrays in PyTorch and NumPy.
"""



================================================
FILE: examples/24_Whats_new_in_txtai_4_0.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ğŸ’¡ What's new in txtai 4.0

txtai 4.0 brings a number of major feature enhancements, most importantly the capability to store full document content and text right in txtai. This notebook will cover all the changes with examples.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Content storage
Up to now with txtai, once text was vectorized, it was no longer possible to trace back to the input text. Only document ids and vectors were stored. Results consisted of ids and scores. It was the responsibility of the developer to resolve matches with an external data store. 

txtai 4.0 brings a major paradigm shift. Content can now be stored alongside embeddings vectors. This opens up a number of exciting possibilities with txtai!

Let's see with the classic txtai example below.
"""

from txtai.embeddings import Embeddings

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "objects": True})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

print("%-20s %s" % ("Query", "Best Match"))
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"):
    # Extract text field from result
    text = embeddings.search(query, 1)[0]["text"]

    # Print text
    print("%-20s %s" % (query, text))
# Output:
#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
The only change above is setting the *content* flag to True. This enables storing text and metadata content (if provided) alongside the index. Note how the text is pulled right from the query result!
"""

"""
# Query with SQL

When content is enabled, the entire dictionary will be stored and can be queried. In addition to similarity queries, txtai accepts SQL queries. This enables combined queries using both a similarity index and content stored in a database backend.
"""

# Create an index for the list of text
embeddings.index([(uid, {"text": text, "length": len(text)}, None) for uid, text in enumerate(data)])

# Filter by score
print(embeddings.search("select text, score from txtai where similar('hiking danger') and score >= 0.15"))

# Filter by metadata field 'length'
print(embeddings.search("select text, length, score from txtai where similar('feel good story') and score >= 0.05 and length >= 40"))

# Run aggregate queries
print(embeddings.search("select count(*), min(length), max(length), sum(length) from txtai"))
print()
for x in embeddings.search("select count(*), min(length), max(length), sum(length), text, score from txtai group by text limit 10"):
  print(x)
# Output:
#   [{'text': 'The National Park Service warns against sacrificing slower friends in a bear attack', 'score': 0.3151373267173767}]

#   [{'text': 'Maine man wins $1M from $25 lottery ticket', 'length': 42, 'score': 0.08329004049301147}]

#   [{'count(*)': 6, 'min(length)': 39, 'max(length)': 94, 'sum(length)': 387}]

#   

#   {'count(*)': 1, 'min(length)': 72, 'max(length)': 72, 'sum(length)': 72, 'text': 'Beijing mobilises invasion craft along coast as Taiwan tensions escalate', 'score': None}

#   {'count(*)': 1, 'min(length)': 94, 'max(length)': 94, 'sum(length)': 94, 'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg", 'score': None}

#   {'count(*)': 1, 'min(length)': 42, 'max(length)': 42, 'sum(length)': 42, 'text': 'Maine man wins $1M from $25 lottery ticket', 'score': None}

#   {'count(*)': 1, 'min(length)': 57, 'max(length)': 57, 'sum(length)': 57, 'text': 'Make huge profits without work, earn up to $100,000 a day', 'score': None}

#   {'count(*)': 1, 'min(length)': 83, 'max(length)': 83, 'sum(length)': 83, 'text': 'The National Park Service warns against sacrificing slower friends in a bear attack', 'score': None}

#   {'count(*)': 1, 'min(length)': 39, 'max(length)': 39, 'sum(length)': 39, 'text': 'US tops 5 million confirmed virus cases', 'score': None}


"""
This example above adds a simple additional field, text length. Starting with txtai 4.0, the index method accepts dictionaries in the data field. 

Note the second query is filtering on the metadata field length along with a similarity query clause. This gives a great blend of similarity search with traditional filtering to help identify the best results.
"""

"""
# Object storage

In addition to metadata, binary content can also be associated with documents. The example below downloads an image, upserts it along with associated text into the embeddings index.
"""

import urllib

from IPython.display import Image

# Get an image
request = urllib.request.urlopen("https://raw.githubusercontent.com/neuml/txtai/master/demo.gif")

# Upsert new record having both text and an object
embeddings.upsert([("txtai", {"text": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.", "object": request.read()}, None)])

# Query txtai for the most similar result to "machine learning" and get associated object
result = embeddings.search("select object from txtai where similar('machine learning') limit 1")[0]["object"]

# Display image
Image(result.getvalue(), width=600)
# Output:
#   <IPython.core.display.Image object>

"""
# Reindex

Now that content is stored, embedding indexes can be rebuilt with different configuration settings.
"""

# Print index info before (info() is also new!)
embeddings.info()

# Reindex
embeddings.reindex({"path": "sentence-transformers/paraphrase-MiniLM-L3-v2"})

print("------")

# Print index info after
embeddings.info()
# Output:
#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2022-05-10T20:32:24Z",

#       "python": "3.7.13",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "4.5.0"

#     },

#     "content": "sqlite",

#     "dimensions": 768,

#     "objects": true,

#     "offset": 7,

#     "path": "sentence-transformers/nli-mpnet-base-v2",

#     "update": "2022-05-10T20:32:25Z"

#   }

#   ------

#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2022-05-10T20:32:26Z",

#       "python": "3.7.13",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "4.5.0"

#     },

#     "content": "sqlite",

#     "dimensions": 384,

#     "objects": true,

#     "offset": 7,

#     "path": "sentence-transformers/paraphrase-MiniLM-L3-v2",

#     "update": "2022-05-10T20:32:26Z"

#   }


"""
# Index compression

txtai normally saves index files to a directory. With 4.0, it is now possible to save compressed indexes. Indexes can be compressed to tar.gz, tar.bz2, tar.xz and zip. txtai can load compressed files and treats them as directories.

Compressed indexes can be used as a backup strategy and/or as the primary storage mechanism.
"""

# Save index as tar.xz
embeddings.save("index.tar.xz")
!tar -tvJf index.tar.xz
!echo
!xz -l index.tar.xz
!echo

# Reload index
embeddings.load("index.tar.xz")

# Test search
embeddings.search("lucky guy", 1)
# Output:
#   drwx------ root/root         0 2022-05-10 20:32 ./

#   -rw-r--r-- root/root       301 2022-05-10 20:32 ./config

#   -rw-r--r-- root/root     77824 2022-05-10 20:32 ./documents

#   -rw-r--r-- root/root     10898 2022-05-10 20:32 ./embeddings

#   

#   Strms  Blocks   Compressed Uncompressed  Ratio  Check   Filename

#       1       1     45.8 KiB    100.0 KiB  0.458  CRC64   index.tar.xz

#   

#   [{'id': '4',

#     'score': 0.3691234290599823,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
Note the compression ratio. Depending on the type of data stored, this could be quite substantial (text will compress much better than objects). 
"""

"""
# External vector models

txtai supports generating vectors with [Hugging Face Transformers](https://github.com/huggingface/transformers), [PyTorch](https://github.com/pytorch/pytorch), [ONNX](https://github.com/microsoft/onnxruntime) and [Word Vector](https://github.com/neuml/staticvectors) models.

This release adds support for pre-computed vectors using external models. External models may be an API, custom library and/or another way to vectorize data. This adds flexibility given the high computation cost in building embeddings vectors. Embeddings generation could be outsourced or consolidated to a group of servers with GPUs, leaving index servers to run on lower resourced machines. 

The example below uses the [Hugging Face Inference API](https://huggingface.co/inference-api) to build embeddings vectors. We'll load the [exact model as in the first example](#scrollTo=0p3WCDniUths) and produce the same results.
"""

import numpy as np
import requests

def transform(inputs):
  response = requests.post("https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/nli-mpnet-base-v2",
                           json={"inputs": inputs})

  return np.array(response.json(), dtype=np.float32)

# Index data using vectors from Inference API
embeddings = Embeddings({"method": "external", "transform": transform, "content": True})
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

print("%-20s %s" % ("Query", "Best Match"))
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"):
    # Extract text field from result
    text = embeddings.search(f"select id, text, score from txtai where similar('{query}')", 1)[0]["text"]

    # Print text
    print("%-20s %s" % (query, text))
# Output:
#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
The next example uses [spaCy](https://github.com/explosion/spaCy) to build vectors and then loads them into txtai. The vectors with this model are much faster to generate at the expense of accuracy.
"""

%%capture
!pip install spacy --upgrade
!python -m spacy download en_core_web_md

import spacy

# Load spacy
nlp = spacy.load("en_core_web_md")

def transform(inputs):
    return [result.vector for result in nlp.pipe(inputs)]

# Index data with spacy pipeline
embeddings = Embeddings({"method": "external", "transform": transform, "content": True})
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run search
print(embeddings.search("select id, text, score from txtai where similar('nature')", 1))
# Output:
#   [{'id': '3', 'text': 'The National Park Service warns against sacrificing slower friends in a bear attack', 'score': 0.44850602746009827}]


"""
# Wrapping up

This notebook gave a quick overview of txtai. txtai 4.0 is now out!

See the following links for more information.

- [4.0 Release on GitHub](https://github.com/neuml/txtai/releases/tag/v4.0.0)
- [Documentation site](https://neuml.github.io/txtai)
- [Full list of examples](https://neuml.github.io/txtai/examples/)
"""



================================================
FILE: examples/25_Generate_image_captions_and_detect_objects.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Generate image captions and detect objects

txtai as the name implies works with text and ai, pretty straightforward. But that doesn't mean it can't work with different types of content. For example, an image can be described with words. We can use that description to compare an image to a query or other documents. This notebook shows how images and text can be embedded into the same space to generate image captions and detect objects.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package.
"""

%%capture
!pip install ipyplot git+https://github.com/neuml/txtai#egg=txtai[pipeline]

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz
!tar -xvzf tests.tar.gz

"""
# Create a captions instance

The captions pipeline takes an image or list of images and generates captions. This pipelines works using a combination of an image encoder model and a text model. 
"""

%%capture

from txtai.pipeline import Caption

# Create caption pipeline
caption = Caption()

"""
# Generate captions

The example below shows how to generate captions. A list of images are read from a directory, passed to a caption model and text descriptions are returned.
"""

import glob
import ipyplot

from PIL import Image

# Get list of images
images = glob.glob('txtai/*jpg')

# Generate captions
captions = caption(images)

# Show image/caption pairs
ipyplot.plot_images([Image.open(image) for image in images], captions, img_width=425, force_b64=True)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
Reviewing the captions, they are all generally in the right ballpark but far from perfect. The default model does a decent job but more robust models are necessary to fully deploy an image captioning model. 
"""

"""
# Create an objects instance

The objects pipeline takes an image or list of images and generates a list of detected objects. This pipeline works using an object detection model.
"""

%%capture

from txtai.pipeline import Objects

# Create objects pipeline
objects = Objects()

"""
# Detect objects

The example below shows how to detect objects. A list of images are read from a directory, passed to an object detection model and detected objects are returned.
"""

import glob
import ipyplot

from PIL import Image

# Get list of images
images = glob.glob('txtai/*jpg')

# Detect objects
detected = objects(images)

# Show image/objects pairs
ipyplot.plot_images([Image.open(image) for image in images], detected, img_width=425, force_b64=True)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
Reviewing the detected objects, once again they are all generally in the right ballpark but far from perfect.

This model or larger models may do well for a specific use cases in which the model has a high accuracy. For example, the results could be filtered on only accept certain types of objects, which have shown to have high accuracy.
"""

"""
# Wrapping up

This notebook introduced image captions and object detection. While the default models for both tasks aren't where we'd like them to be, they provide a good baseline to build on. For certain, targeted use cases where the models excel, they can be used now. This is a fast-evolving area and it is fully expected these models will improve!
"""



================================================
FILE: examples/26_Entity_extraction_workflows.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Entity extraction workflows

Entity extraction is the process of identifying names, locations, organizations and other entity-like tokens in unstructured text. Entity extraction can organize data into topics and/or feed downstream machine learning pipelines.

This notebook will show how to use the entity extraction pipeline in txtai with workflows.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Extract entities

Let's get right to it! The following example creates an entity pipeline and extracts entities from text. 

"""

from txtai.pipeline import Entity

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

entity = Entity()

for x, e in enumerate(entity(data)):
  print(data[x])
  print(f"  {e}", "\n")
# Output:
#   US tops 5 million confirmed virus cases

#     [('US', 'LOC', 0.999273955821991)] 

#   

#   Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#     [('Canada', 'LOC', 0.999609649181366), ('Manhattan', 'MISC', 0.651396632194519)] 

#   

#   Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#     [('Beijing', 'LOC', 0.9996659755706787), ('Taiwan', 'LOC', 0.9996755123138428)] 

#   

#   The National Park Service warns against sacrificing slower friends in a bear attack

#     [('National Park Service', 'ORG', 0.9993489384651184)] 

#   

#   Maine man wins $1M from $25 lottery ticket

#     [('Maine', 'LOC', 0.9987521171569824)] 

#   

#   Make huge profits without work, earn up to $100,000 a day

#     [] 

#   


"""
The section above is running an entity extraction pipeline for each row in data. The outputs are the token(s) identified as part of an entity, the type of entity and score or confidence in the prediction.
"""

"""
# Feed entities to a workflow

The next section demonstrates how the entity extraction pipeline can be used as part of a workflow. This workflow uses the output entities and builds an embeddings index for each row. This effectively computes entity embeddings to compare the row similarity with a focus on mentioned entities.
"""

from txtai.embeddings import Embeddings, Documents
from txtai.workflow import Workflow, Task

# Create workflow with an entity pipeline output into a documents collection
documents = Documents()
workflow = Workflow([Task(lambda x: entity(x, flatten=True, join=True)), Task(documents.add, unpack=False)])

# Run workflow
for _ in workflow([(x, row, None) for x, row in enumerate(data)]):
  pass

embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})
embeddings.index(documents)

for query in ["North America", "Asia Pacific"]:
  index = embeddings.search(query, 1)[0][0]
  print(query, "\t", data[index])
# Output:
#   North America 	 Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   Asia Pacific 	 Beijing mobilises invasion craft along coast as Taiwan tensions escalate


"""
# Run workflow YAML

Below is the same example using workflow YAML.
"""

workflow = """
writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2

entity:

workflow:
  index:
    tasks:
      - action: entity
        args: [null, "simple", true, true]
      - action: index
"""

from txtai.app import Application

# Create and run workflow
app = Application(workflow)
for _ in app.workflow("index", [(x, row, None) for x, row in enumerate(data)]):
  pass

# Run queries
for query in ["North America", "Asia Pacific"]:
  index = app.search(query)[0]["id"]
  print(query, "\t", data[index])
# Output:
#   North America 	 Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   Asia Pacific 	 Beijing mobilises invasion craft along coast as Taiwan tensions escalate


"""
# Wrapping up

This notebook introduced entity extraction pipelines with txtai. This pipeline supports a number of different configurations to help feed downstream systems and/or directly use the entities.

As with other pipelines, the entity extraction pipeline can be used standalone in Python, as an API service or as part of a workflow!
"""



================================================
FILE: examples/27_Workflow_scheduling.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Workflow scheduling

Workflows are a simple yet powerful construct that takes a callable and returns elements. They are streaming and work on data in batches, allowing large volumes of data to be processed efficiently. When working with streaming data, workflows continually run until the data stream is exhausted. 

Workflows can also be scheduled to run. In this case, a static set of elements, dynamically expands. For example, an API service endpoint that returns items, or polling a directory with files coming in and out. 

This notebook will show how to use workflow scheduling in txtai.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install datasets git+https://github.com/neuml/txtai#egg=txtai[workflow]

"""
# Create workflow action

Workflows run a series of tasks to transform and process data. This section creates a callable object that can be used as a workflow action. The object iterates over a dataset, returning a batch of data.
"""

from datasets import load_dataset

class Stream:
  def __init__(self):
    self.dataset = load_dataset("ag_news", split="train")
    self.index, self.size = 0, 2500

  def __call__(self, fields):
    outputs = []
    for field in fields:
      output = []
      for row in self.dataset.select(range(self.index, self.index+self.size)):
        output.append((self.index, row[field], None))
        self.index += 1

      outputs.append(output)

    return outputs

"""
# Build workflow

Next we'll create the workflow. The workflow reads batches of data from a stream and loads it into an Embeddings index. We'll run this workflow four times on a scheduled interval to demonstrate a scheduled workflow.
"""

from txtai.app import Application

# Run up to every 5 seconds 4 times
workflow = """
writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

workflow:
  index:
    schedule:
      cron: '* * * * * 0/5'
      elements:
        - text
      iterations: 4
    tasks:
      - __main__.Stream
      - upsert
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-03 02:12:06,720 [WARNING] _create_builder_config: Using custom data configuration default

#   2022-02-03 02:12:06,727 [WARNING] download_and_prepare: Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)

#   2022-02-03 02:12:06,751 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-03 02:12:06,757 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:12:10+00:00

#   2022-02-03 02:12:34,937 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:12:35+00:00

#   2022-02-03 02:12:59,967 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:13:00+00:00

#   2022-02-03 02:13:23,349 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:13:25+00:00

#   2022-02-03 02:13:49,621 [INFO] schedule: 'index' max iterations (4) reached


"""
Reviewing the log above, we see the `index` job ran four times. Now let's query the index and see what was loaded.
"""

"""
# Run an embeddings search

Let's run a search against the newly created index.
"""

import json

# Show total number of records
print(f"Total records: {app.count()}")

# Run a search
print("Search:")
print(json.dumps(app.search("life on mars", limit=1), indent=2))
# Output:
#   Total records: 10000

#   Search:

#   [

#     {

#       "id": "119",

#       "text": "Life on Mars Likely, Scientist Claims (SPACE.com) SPACE.com - DENVER, COLORADO -- Those twin robots hard at work on Mars have transmitted teasing views that reinforce the prospect that microbial life may exist on the red planet.",

#       "score": 0.7236138582229614

#     }

#   ]


"""
The index has 10,000 records. We also see the top result for the query on `life on mars`.
"""

"""
# Run a scheduled embeddings search

Now let's incrementally load the dataset with a scheduled workflow and run a scheduled search after each batch is loaded.
"""

from txtai.app import Application

# Run every 5 seconds up to 4 times
workflow = """
writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

workflow:
  index:
    schedule:
      cron: '* * * * * 0/5'
      elements:
        - text
      iterations: 4
    tasks:
      - __main__.Stream
      - upsert
  search:
    schedule:
      cron: '* * * * * 0/5'
      elements:
        - life on mars
      iterations: 4
    tasks:
      - action: search
        args: [3]
        task: console
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-03 02:13:55,789 [WARNING] _create_builder_config: Using custom data configuration default

#   2022-02-03 02:13:55,797 [WARNING] download_and_prepare: Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)

#   2022-02-03 02:13:55,808 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-03 02:13:55,808 [INFO] schedule: 'search' scheduler started with schedule * * * * * 0/5

#   2022-02-03 02:13:55,810 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:14:00+00:00

#   2022-02-03 02:13:55,814 [INFO] schedule: 'search' next run scheduled for 2022-02-03T02:14:00+00:00

#   2022-02-03 02:14:00,001 [INFO] schedule: 'search' next run scheduled for 2022-02-03T02:14:05+00:00

#   Inputs: [

#     "life on mars"

#   ]

#   Outputs: [

#     null

#   ]

#   2022-02-03 02:14:24,500 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:14:25+00:00

#   2022-02-03 02:14:24,522 [INFO] schedule: 'search' next run scheduled for 2022-02-03T02:14:25+00:00

#   Inputs: [

#     "life on mars"

#   ]

#   Outputs: [

#     {

#       "id": "119",

#       "text": "Life on Mars Likely, Scientist Claims (SPACE.com) SPACE.com - DENVER, COLORADO -- Those twin robots hard at work on Mars have transmitted teasing views that reinforce the prospect that microbial life may exist on the red planet.",

#       "score": 0.7236138582229614

#     },

#     {

#       "id": "271",

#       "text": "Saturn's Moon Titan: Prebiotic Laboratory by Harry Bortman    In this second and final part of the interview, Lunine explains how Huygens may help scientists understand the origin of life on Earth, even if it doesn't detect life on Titan.    Astrobiology Magazine -- Titan is the only moon in our solar system with an atmosphere, and it is the organic chemistry that has been detected in that atmosphere that has sparked the imagination of planetary scientists like Lunine...",

#       "score": 0.4750666916370392

#     },

#     {

#       "id": "1132",

#       "text": "Is Mercury the Incredible Shrinking Planet? MESSENGER Spacecraft May Find Out (SPACE.com) SPACE.com - With a new spacecraft bound for Mercury, that tiny planet nbsp;near the heart of the solar system, researchers are hoping to solve a slew of riddles about the small world.",

#       "score": 0.47124743461608887

#     }

#   ]

#   2022-02-03 02:14:25,496 [INFO] schedule: 'search' next run scheduled for 2022-02-03T02:14:30+00:00

#   Inputs: [

#     "life on mars"

#   ]

#   Outputs: [

#     {

#       "id": "119",

#       "text": "Life on Mars Likely, Scientist Claims (SPACE.com) SPACE.com - DENVER, COLORADO -- Those twin robots hard at work on Mars have transmitted teasing views that reinforce the prospect that microbial life may exist on the red planet.",

#       "score": 0.7236138582229614

#     },

#     {

#       "id": "271",

#       "text": "Saturn's Moon Titan: Prebiotic Laboratory by Harry Bortman    In this second and final part of the interview, Lunine explains how Huygens may help scientists understand the origin of life on Earth, even if it doesn't detect life on Titan.    Astrobiology Magazine -- Titan is the only moon in our solar system with an atmosphere, and it is the organic chemistry that has been detected in that atmosphere that has sparked the imagination of planetary scientists like Lunine...",

#       "score": 0.4750666916370392

#     },

#     {

#       "id": "1132",

#       "text": "Is Mercury the Incredible Shrinking Planet? MESSENGER Spacecraft May Find Out (SPACE.com) SPACE.com - With a new spacecraft bound for Mercury, that tiny planet nbsp;near the heart of the solar system, researchers are hoping to solve a slew of riddles about the small world.",

#       "score": 0.47124743461608887

#     }

#   ]

#   2022-02-03 02:14:50,112 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:14:55+00:00

#   2022-02-03 02:14:50,138 [INFO] schedule: 'search' max iterations (4) reached

#   Inputs: [

#     "life on mars"

#   ]

#   Outputs: [

#     {

#       "id": "119",

#       "text": "Life on Mars Likely, Scientist Claims (SPACE.com) SPACE.com - DENVER, COLORADO -- Those twin robots hard at work on Mars have transmitted teasing views that reinforce the prospect that microbial life may exist on the red planet.",

#       "score": 0.7236138582229614

#     },

#     {

#       "id": "3300",

#       "text": "Mars Hills, Crater Yield Evidence of Flowing Water LOS ANGELES (Reuters) - The hills of Mars yielded more tantalizing clues about how water shaped the Red Planet in tests by NASA #39;s robotic geologist, Spirit, while its twin, Opportunity, observed the deep crater it climbed into two months ...",

#       "score": 0.6666488647460938

#     },

#     {

#       "id": "4201",

#       "text": "Martian hill shows signs of ancient water LOS ANGELES - NASA #39;s Spirit rover has found more evidence of past water on the hills of Mars, while its twin, Opportunity, has observed a field of dunes inside a crater. ",

#       "score": 0.6453495621681213

#     }

#   ]

#   2022-02-03 02:15:18,333 [INFO] schedule: 'index' next run scheduled for 2022-02-03T02:15:20+00:00

#   2022-02-03 02:15:44,592 [INFO] schedule: 'index' max iterations (4) reached


"""
The workflow above runs up to every 5 seconds. Note that since the index job takes longer than 5 seconds, the time difference between jobs is longer.

The index job loads the next batch of data and the search job runs a recurring search. 

See how the search results change over time as more relevant results are found.
"""

"""
# Wrapping up

This notebook covered how to use workflow scheduling with txtai. While there are existing ways to schedule jobs (system cron, serverless, and so on), this is another easy and quick way to do it. 
"""



================================================
FILE: examples/28_Push_notifications_with_workflows.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Push notifications with workflows

Workflows are a simple yet powerful construct that takes a callable and returns elements. They are streaming and work on data in batches, allowing large volumes of data to be processed efficiently.

This notebook will show how workflows can be used to push notifications upon certain event triggers. Using this method, an activity feed of content can be created.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install datasets git+https://github.com/neuml/txtai#egg=txtai[pipeline,workflow]

"""
# Create workflow notification action

Workflows run a series of tasks to transform and process data. This section creates a callable object that can be used as a workflow action. 

The action below pushes events to [Slack](https://slack.com). While Slack is used here, any notification service can easily be substituted in ([Zapier](https://zapier.com/), [IFTT](https://ifttt.com/) etc).

It is assumed there is a Slack workspace and application installed and ready to use. [See this comprehensive example](https://api.slack.com/tutorials/tracks/posting-messages-with-curl) for more information on setting up a new Slack app and posting messages via the API.

The channel id can be found from the Slack web interface. Log into Slack and click on the channel where messages will be posted. The `channel id` is the last part of the URL.

```
https://app.slack.com/client/<team id>/<channel id>
```
"""

import logging
import requests

# Uncomment and set. The following are dummy parameters. Your parameters should not be publicly shared!
# AUTH = "xoxb-not-a-real-token-this-will-not-work"
# CHANNEL = "C0XXXXXXXXX"
# URL = "https://slack.com/api/chat.postMessage"

# Logging configuration
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class Slack:
    def __init__(self):
        self.alerts = set()

    def __call__(self, elements):
        for alert in elements:
            uid, text, _ = self.extract(alert)
            if uid not in self.alerts:
                logger.info("Sending alert: %s", alert)
                self.alerts.add(uid)

                headers = {
                    "Content-type": "application/json",
                    "Authorization": f"Bearer {AUTH}"
                }

                response = requests.post(URL, headers=headers, json={
                    "channel": CHANNEL,
                    "text": f"{text} {uid}"
                }).json()

                if not response["ok"]:
                    logger.error(response)

        return elements

    def extract(self, alert):
        if isinstance(alert, dict):
            return (alert["id"], alert["text"], None)

        return alert


"""
# Build a semantic notification workflow

Next we'll create a notification workflow. The example below indexes the top trending Hacker News articles and pushes an alert when an article matches an embeddings query for `software development library`.
"""

from txtai.app import Application

workflow = """
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

tabular:
  idcolumn: url
  textcolumns:
    - title

__main__.Slack:

workflow:
  index:
    schedule:
      cron: "* * * * * 0/5"
      elements:
        - front_page
      iterations: 1
    tasks:
      - batch: false
        extract:
          - hits
        method: get
        params:
          tags: null
        task: service
        url: https://hn.algolia.com/api/v1/search?hitsPerPage=50
      - action: tabular
      - action: upsert
  alert:
    schedule:
      cron: 0/1 * * * *
      elements:
        - select id, text, score from txtai where similar('software development library') and score >= 0.4 and id like 'http%'
      iterations: 1
    tasks:
      - action: search
      - action: __main__.Slack
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-10 15:12:42,838 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-10 15:12:42,839 [INFO] schedule: 'alert' scheduler started with schedule 0/1 * * * *

#   2022-02-10 15:12:42,843 [INFO] schedule: 'index' next run scheduled for 2022-02-10T15:12:45+00:00

#   2022-02-10 15:12:42,851 [INFO] schedule: 'alert' next run scheduled for 2022-02-10T15:13:00+00:00

#   2022-02-10 15:12:45,884 [INFO] schedule: 'index' max iterations (1) reached

#   2022-02-10 15:13:00,042 [INFO] __call__: Sending alert: {'id': 'https://datastation.multiprocess.io/blog/2022-02-08-the-world-of-postgresql-wire-compatibility.html', 'text': 'The world of PostgreSQL wire compatibility', 'score': 0.40123000741004944}

#   2022-02-10 15:13:00,254 [INFO] schedule: 'alert' max iterations (1) reached


"""
The log above shows the indexing and alerting jobs each ran once. There was a single match and it was sent to Slack. The score threshold of 0.4 is relatively low, it can be raised if more strict matches are desired.

![3.png](data:image/png;base64,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)
"""

"""
# Build a notification workflow using SQL

The next example is similar but it instead runs a SQL search using another column.

This workflow indexes the top trending sports events as identified by [neuspo](https://neuspo.com). An alert is generated when an excitement factor of 40 or above is met (the field for excitement is called `weight`). 
"""

from txtai.app import Application

workflow = """
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

tabular:
  idcolumn: url
  textcolumns:
    - summary
  content: true

__main__.Slack:

workflow:
  index:
    schedule:
      cron: "* * * * * 0/5"
      elements:
        - 10
      iterations: 1
    tasks:
      - batch: false
        extract:
          - rows
        method: get
        params:
          size: null
        task: service
        url: https://neuspo.com/data/articles/list?category=Top&from=0
      - action: tabular
      - action: upsert
  alert:
    schedule:
      cron: 0/1 * * * *
      elements:
        - select 'https://neuspo.com' || id id, text from txtai where weight >= 40
      iterations: 1
    tasks:
      - action: search
      - action: __main__.Slack
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-10 15:16:49,702 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-10 15:16:49,704 [INFO] schedule: 'index' next run scheduled for 2022-02-10T15:16:50+00:00

#   2022-02-10 15:16:49,704 [INFO] schedule: 'alert' scheduler started with schedule 0/1 * * * *

#   2022-02-10 15:16:49,714 [INFO] schedule: 'alert' next run scheduled for 2022-02-10T15:17:00+00:00

#   2022-02-10 15:16:50,474 [INFO] schedule: 'index' max iterations (1) reached

#   2022-02-10 15:17:00,010 [INFO] __call__: Sending alert: {'id': 'https://neuspo.com/stream/be15c852925b53639b63feb7169a2842', 'text': 'Islanders 6, Canucks 3: Five-goal first period keys Islanders win in first game post-'}

#   2022-02-10 15:17:00,215 [INFO] schedule: 'alert' max iterations (1) reached


"""
And the notification in Slack for this job.

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnIAAADJCAMAAACQYi84AAAC/VBMVEUzFCIyGAwfHSAYHT4fHigeHjIlHx4tHiY4HR0zIx8hJFhMHyIQLGgpKi4yJGAKMVouKik0KSIrLCo4JFojKW5EJUI/Jk0uLE5GKCciL14kMUJNJTpTKDAsNS8mN1JVKFpqJyNcKyhGMyczOD07NzEzNWpqKi9hMCEkQjMgP29TNicnPIMLR4Z4Mh94MycfR480SGYeUzsoS3deQTQgVm6FPiNXTCxHUFckVaF7RitMUWNYUUguV545V4EUY4hvTjeSRyJaWCwQZakxXZ0JZ7oVZa9CXH8dZaMVcEYuZaUzZZsVbMBBY58VcLIhbbg4aLZMZok+bqZAbbKDXnE1c6hdbV2fXTAgeqwWe85Wa6ZkbHOqXyVza2RJc6N4by8vecUlfb6KZmGNaEphb6SvYx9sbqmBbZlsc6QUjNpZe6dneZxafMi8by0xjdGHgDBCiNF6erQ2jswLpltWh7o5kMSAfapqhq57gqomnuSzf1WOhaavgWC7gEhhlbyPi5CDj5TMgTtGn9Weki91lq2rh5yEkq+djotUnuSujHwwqu5pm9GBlryhkaj3gkKqk6M7su7Zk0perPBQsubnkFD4jFNqr9OeqHjQm3Coo7a4n62bqcOFsra7rS/UoWV0teiDstfDpZOSr9VsuPeprKzOpnVRwvaMtMvooliftJX4nWvPqIRnwO2issEU5352wOTMq7C6tMJozvyGyPqLz67durh+0/KH0fn1u3H2uJN41/y+xs2S0+XNxMib0PLnwpDtwnyozvPnwpy1zeO/y9mn0uiV2Pjsx4zM2knhzKLk0lXqyMKK4f36zIPW1LbQ2Mq43t33zcLF36Xt0b7d1NfR2uHo1c6a7f782I+l6v6t6P3b3dr83J773rO/6/zh4+D63c786jnS6PXe5+y49eGv9v/x5du+9P/X8en86dbC+v7P9v777tXj9tL/8a3+87j+7+rT/Pz+9sD99sng/P/n/PD9+NX69/z/+eLp/v3w/f//++z/+/n5/v/+/vT///9lhc9kAAAAAWJLR0QAiAUdSAAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB+YCChEkFZhlLTEAAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAgAElEQVR42u2dC1xVxbrAdbdBMekapSamJ8Ue5DHNV3nOIW0n7lBB8JBvhMQOjyNyyWcoJRlHRUKkvAKVqKlclUS9+EK05NHxAhVHUEGBfeAK8UhBtrDZj7V+95vHWnttXgICks5nAXutNTPfzPzX930za82eXjwTJt0qvVgTMGHIMWHIMWHCkGPCkGPChCHHhCHHhAlDjglDjglDjgkThhyTRwW5G0usLEWxshxgNWCAlZXVAEv0C/2/5EaHi9EXNS961gOPM3I3ACxAzApoA86siBDuLPsMQOhZiszdHWO2B35xK2TuBtMMa6cMvsDrdr8q3yI9UdSSsB54nJFbYmklNzOTyy3lcjM52DtMG2KvV68+w/r0GQYkLjEiJxuS3hi5gvdDDXxWX9kW/oyF/E8neYYck9aRA+Imf/31ypUrP/30U7ncihq8AVa9Xv/ww3c/fKUX+luCnOyPhY2QizOHD5oNvoXcCvM5Wr4xcj8t88wtKtrw5zcWbS85AD9Dchlyjz1yZl8vffvptydPtlr66RMohANXCsQBcK8Adu/2sjJFTjZXjZHjLk21sHZO08x7WoblifOL4af8PQuwg3XTiINFyF1ctCQWkFt74/JfYw94/vLT+0FS5HRHvGp4XrN3yaJ9auMBzYZFixYtQWfgwAZPdGa3j5qY1Aus937vyFkNMFv59oC3Vy4d9fbTX8v7UAHYXnm9z+vD4FcfqwGSWO69vkATIMef6St/Y6T5kPzdr8qGL1kyue8Tpy9NkQ1f9NUkmb8hq2//fGrlSjfE7l5bAsgV/fQ+Qq7oI58SI3JVWz8BsLjd27WaDXsM4gEsB0KxJS0I/AeibPeiWPip2bqEIff7R87K7NPJkz9dOmrpqMkfGuW7d98d9vrr777+LjJzEuSOrjB/4hggVztJ5q5FP3SLkZe9PuKJ02D8wLFGmPev2ATnKHKX/5Z2+a9pxModamLlylMK/lbD1/4VAL25pmZ3qJYcwAOSzwrRL273notrGwC5rwLh88XtGxhyjwBy8pVg4+QrJ789+Z+vvI7klVdeGfZun1cQca+/+6EpcvvrpsmGTJa53x4h389zm8zH3muE3J3XzL4a80S6EMvtXrRkyZ+DWo7lEGEFf6sAxP5WeCWFHiCoYSNX+9dCTOTukwdCtXWf/cKQezSs3NKlZkvfXvr2258+1wtJH/j37jCwcMDe642R42++DDEbRW5FU+QAw2EW9g0UOfCmRUUHvH4BxwqCHGujEasEuTLxAP5ViT9eDNVyu8HF7j5Z937agX0cQ+6RQG7lSvnbS5dOXir/1AwNWIcNsxrW5/XvhiEjh8YPw0yR479/TiZxrNximZMUOYQknr3DyB3xAsguv3+oVeQExypBTrebZFG3YdHHHy/xKgTk+IuLAisZco/GiHXlylFLIZaTf/r1E6PQvByamUMjVhg/fPih6YgVIcdF9DEOH9LBqlk750uQ04Hpq2zzvBwijA4feIljvbmGGjkvGKiic4Ccbu8PPEPukUDOUr7y60+/hv/+dxSaCyZPH/r0QdNy333Yy7IJcrxmo7k4SQKMTOljHSpBTrNYvoVvI3K1W5cs+nifQXMETZKAA9UKB7aSKWXdhj0UvN3kAEPuUXCslgMs5fLe4+b3sUJPH8jTLswcDB3e7YWeg1m2K++svv3L+DZbOSaPpZWzGmDZd+L83vgZq2Dl4B8MInpZ4bPtyRqMnJ/waII91mfSPHIQv/UG5IAtxB9EchhDq2FW+G/4xBqMSWc6VvSOEkYOwTVAfJMEaMOP94FBS9ZgTDp1kgS5UmLlrATkMHUIOksMHWswJp3pWImVe+st7FjpG3MD8FwJeasEfrMGY9K5jtVSQM7Uyg0YQKI6hhyTTkZugFVzyEmFNRiTzh8+MOSYPBQrR/4x5Jh0KXJ4Brj3WxN7W1kOYFaOSbdYOUsLC+vV8216W9AnDy0ix60YS1/Z5eOerWg25zPP5LVW8PdD90iWgFVtmGCOn9O2LtxyJ0PjY5emmluvUosfr4/Ab8P7Q/7taojaSVsEVUbK/BuVUhfV0ErSu681uv72Cztb0tZU4voJbXR94Olmr6BZ3U8iXsyXftTM+0uN6QUFy57GT8VNju0zdApDl37oOHKWFlZvBc+fH7zaxsJyQLPMdRpyN1/2lyzIKZgyfPO3e5f1y28/ctef90k68pyxz68/uSo5OTmpOKuDyOlW2KbUNy4ka3RNz0Yurgly6ka1m1ncpFYR7p2CnGbxzg4jZzXg6fkTJ1oN6D0/ADPXhVZOM+8dSZvUzbNF75pzRYZ2I8eteBMM0CYjEtef3N+hhhOQuztmSzM92ipyLXHSjcjdT64PSm/amCs6B7naSR1GboBl74kTJ1pPtJhoPX9ib8sB97Ny3IFXZeNiMXLg3IaHavm4mQdGyu2AngL4/B4gp9uNLjFwi1d/ZOFXu8zC2oe8/MZFvFgoNSK9BKW5IxNk4y7w3OKgjyzMwF1ibs88lYfcnXzGBdyJ3wy+oPkIPhE3rCpG9+tY9c3n95sgd33o6etDT2JcKnFiwWtDWnOZ7NkK3YEJsuGxWqFIihx+0xmdvvvSV1PMTmKd1ZssZDIMB1Rkw0gzN6jERVrjdw6MfPO3SVsMon+v2mBh/d6TRuRAW3pxHDLiuhVzJMoj5KCtrEMbEHLI+bmpja0oZkXSk1YUU9dO8jPwt0fsxDzFPVsmthm9EVEFzIeT6OWMBa6VvTbrqX9N7Z+Hi/xtEjTDWHQ5dCT8+cRp2hIoodzlt2UWODGtqEnlaV0JADdfhsTtgdcUubcmWk7sC4ZuYkBvq2aHrFLkrg/dVXRpH0LuzktBud8MOs3Hyd1SL7/srr07ZmbxT/OgOSNeTOO+GZTOLbcZ90nKineKq8IJclnPvTHqT9tFOxfxlHCzZw069PO8/mXccrOg3APP7RSR0yy2TSvduw914veooCFp6DVNKvqLz+/hbz6/R0BOr9cbEHKaxU5avm6aH5jU4qqNoyuFsKdQt3FImiFucGzRgee2GGiRgpW789oWbLnvvmbj/Ekx1TmCWjnQaxXo5WfIGhirvTxiPx9nOfyTfZDUcP35VTcuT5nbgBXd2EdEDmlLL77zAuR88/mTEuUBudpJq0quxCDkNPPcci+O2CK2opgVTY9b8Qdj6k3AS5w5GPmIsTUIOaHNBOTGDI/9eWO/dNqu6XAQkBtq4xvzGylSs5iCkjXoGPf9wFAtbYm7Y2zTaifZhBoinkkXyjapvFBXAoChvYbYFLn5lsDcW9aWbwWgF5jug9yZZ9IFx1pPHFLc6Ars5LC/AFruvgbK6Bb7cctHF4LLn6sVvf+QQz9LArBNzwrv1aGcoPW55XO0PGoSATlisFAn/jjwGLJqlVLfI3dpPHx4Uw3I8XGActagTOxV7rx00uhOrj9zq3YSKj5idAUtUnSsAnJj5jSIOhuRw2s6Rv+KYOYj3myIQ8uJICn0MahwZlD6dWRsjY4VaaujF9/bBMEIuGiJ8tBQt1/aaaCOFW4UbvEcrdCKQlZietSKkqpnPZOnW/zJH/I08/x5jBxtMwE5VBEBBxE5uMd4WqSIXBzodfe1/ULj44SoYW6/tF8ou0FS+RqhrhSAB0HOqvf8tyxs3ppoM7HJo/3mkKudZ+b8g1aI5cCaEOSge1DbIuSuP2kDMsqfBDXfPzcupJiMHbATRLVqbOXw2aEncQKorojcGXIFt1xm7oRXVQz3TRET6K9MMUaG158MzcnJyTUg5O68sJPbZN8QJ0daWO6//YJM5gSRNlg5wRHTgBOKbIIc+i3obEQOVSTu2f+bZo2y/Isa1xiS1k1DAEMfYUVF5LC2tcLF0J1107YYJMoDcrqNfWbs04qx3CbAmLRihZCVkP4eLtyYGm7oO0N/mbT/9vOnCXK0zaTIYcsqRQ7dfbRIEbnrAy9w3z+TLzQ+Ra5SWrbapPK0rhQAXFvctob2WzkL6/mWb1mbT7Q8aNObfgFOa8MH3aVlFv5aHMu9/+dR5lLk1Lg3wYtIo/6qrVOHpJNgNhN191NlTWI53ZH3R40yawU5p+8Hojyr9k6lK3n0BtOhiiSWg1vyzbsv7efPUCOqAxZLABAbGwiGRORokc0jR3Vuihy10I2R22mKHNa2Tri4bp7TdaSoUXnsDq5sfXVsJUKufOufRz3dFDkhvdCKQmpuhdP3f2mIcDoD17cLOVqkiJxuk40NRNxC40uRE3VvBrmdFABcW9K2HXjg1dtq/baJ822C57cwemgyYoXmge45Myi2RGLlKiJQH0NvEuclGbrdHYP7Au4QHptoYcQ6jYxYcxo22aZopVYuC8Fp4liFkQdtXOy7sQdtBjlo6f8aXcbTxIIzI55JcKw1tMgWkCM6myAHdb+HfYsEOeJYoUOzkLVC3WHUlhMuBsKWku9qocoLI1ZIB8jdHBFULLVyQlZCerEVhdRnXlwKoeiLS4GcdiKHD4jI3fwDuZ9pS0iRM+reuPI0J1D2ARwrYsyi98T582G8it9Zuo+VKwgsrpo3pwG6B/TjfhxhRO7mk6u0l6dAc0KIXnLlfTLQ1Gw9ZPhx6GlSOYjlDvTZz3+zsJLO0uF5ucHpEDrovhloRO7OC/44JxxKHwlFx+umvdNwZJ+2YMoWktXgzTeOvDpXKxk+GJGrm2YFYKHERZfp6lgoS25pOeMCHwHDh4vPb9HRIptFTtA5rt/548S3DIktPTB0P9jIVblXNoSKyIEfDyq5PGWOunYSDDg+Mg4fQFs1vdgAeSP7ZFQeDx8+S+G+GV0GyKFOLJhiRE7MiqYnyElS335pcDoUbrmfbx9ytEhu+R9/jjHgu09maTkuVmgJKXK8qLuk8kJdKQB3x7j8lNIx5Jbg56q9e/e2wE+8mrNyS0yQW2Yhdy5E3VMwxeaNtRv9ReT4i6/K7f4b7h0dDKOt16pJa9BJCTx+2DBSPi5Wy20S5qWqlo3ETx9+nGDzp69eMyKHcpqBcqraYCGfkYaP3xwRitaU0ScOkBUqAtpmZ1PkeBzeC8URE7FxbknRT/PeVKMJnOFBWqHIZpETdC6YgmctoNXtlpE5g4tTQd9CI3L4gE8xmh+SjftqmnGS5CaMQenFyLDjKQZReUAO63YBxXKajwa84XvAiJwxK5KeICdJrVsMrobDY692IUeL5H981RqPu87MLi66uvGpMtoSJsiJujeuPNSVAsB985x1aMcmSW7Qr5ND62xaeMba8a/RbGEW8W81fDfLbYLim+qOJL7/5G6rAsGHge9xopm3hYx/u7byzSHH31hE3sbEax2asXJLOps4fvfJbm/gumkuxRydaepu5LIG5fU84nAMp78yb672ISDX/fIw1hQWLHta9LLdi5xuxRxtD0ROjE0eC+SYPIbCkGPCkGPCkGPChCHHhCHHhAlDjglDjgmTjiOn8TpI/uCOV8KPfS3OI3JhJ/jEbV0xAyrmilVoiyBdwtc3ns7UR02/hX4nKEG2GV/VOe6q9DgFH6sWwvHp13i+dLvSMRg9bUz2UXocbeD56ihXxwD0DDcbJV1QY8xcTHN1rdIxkjTO1bknDKg0VwVOg84Eg+IShbgw/0remIY77uJ0i55Szcmk7Y6KUipOwN+lbjto0gSnTIlagoj65viISui8UHHiGXQkty1Nl6PtOchpPoCGLZ9T0dKF5X+v0K3J7ALijLliFdoioEtT5KoDlRg5Liw4JSWl2Ei0e3pJMupKldN5OKHlq7wC0pJ8vNV8otOx4iS3SAMX5n4+N8QpD671I0nFzIU0qrnBKUkumIxyzx3QbVy4+wWcBs4U565b3yBVKMEdPQ8T02Q7ndIaGiHH50K+KcfnwIVXXZQUuatzzyGFqVpCbqK+qrlHS3Jc8Aku3BGKE88guSRdWViwXV0bXti43Qq23zvyQ89BrnwO9LdqVovIJWwzqPy74tm9MVesQlsEdGmCnG6dfxJGTvOf50yOB0J/coGQIsO7gTIIlcyec023BvV0xoIKjEHVQmiIaNr3YuZCmmj0OwHsHxDth7q4HNFRtfAEOYM0Nyp0Ze4pkzSJC4ztJiJHrOF6LZ8x99g6Umy5ZyT6LnBBLeM9Q/TlE3CGmLBE93goTjyDqh1daAKXuup4ceOFqXC0IFD98JE7GuPieFSbMVNJxD4z0S/JTRmQDwbeTeFxis/Axl7zn5l89EEwRKdClHA5zyW7IXeF+xL90IXMBPeCTqPciFPZQ2tHHQw9BNesVa7/NQquww0NuYLnUQTkIxUUJxL9kt281dXbXZC30h8H75XfOAnSBXoYzgWrSe5IoX2V2Ri5KgHc6hBXyIMiBz8SqLPVF+Ouv0buNkikL0FfwQ3I6QJP8BQ5mrmQpgilSUS9nbCgjGSC0nyAHLwWsXfOmEbnedAgSdOQgH0zdsT5UO75EKVHuoS/qxewihjABtEIoLrQmgn6UobRD9XczARATjyDcWowhQtbvibIafYWPnzknI6VHJ+VRzTPRlYuURlQeGOdt7p87jFtzgU+A3uJbG911d9vGS9PdD9vSJ6bJyDHR/sVVsdUktMO56BNIotvrMNNiP/O2VcpHNJ4uedXr3M9qk1EXoVHuUIR+qR8okKio8euCzqv4OLSwPXaRI+0ZpJko64PV0Cu4LZUoGWpJ0aFIKdycMWhkG5NQGFRTD5oms9fhTpwYY6ujquFhcnRC2rAN+bzpeuoOcmArDWerjQuo5lL0+hzUCkqh9Wu84+Se6k6CkK2RPCa1WH2mcY04Y6rTNMkQhFcuF+h/jhYVAePtKIopzzRyAmGGC5TrHYNSNeKamm8IouP22eK+kJDnjKUep5DhjCSTxCMajSxoZdiwbz+4+OYYl639+NPvt2urvus8NI/Pv74B9Oj7V2R31WOVQMOQoLcLGKuVXPyJC7gIJ8IDSRcTm7F8B0CcrT1NF47UAuuhxtSLTqRaBJsCIdwUlQGKpXHuSYsqOSF+zURNTLqJPDyt9B6CLjUNAnShSfWBdwW/p3o1yAiV52SkpvkEGnIoCE7F6ZQ2qOgOyetODfEvcwYNVWHKJWudKCh8wKnpk9NgyjKr0HMXJJG44lMGBfmdKGEBnWJSqd0qHyUUunh4l0jplE5BBeSyJCmwbXBbaH54JzRiRudLEGuytMvrSTK/pxRLQNq0EhJlAf2UhFQyHPR4NsF5MgZbNAKwgq5S/u0l0KKuUsYOQ5FbqZHIeTrGch9cFCKHOpviFV0gYrgC1ohYC/TrTlnvLxqoYcriIgcn+HisauSOoVo73voMGVKh/+mvwRaURnILfE413JPx12FAnKo9GhHlLs7ZiZbQE5IAroI4VbG9P8LxJfi+IogR6K9BTUk6AFIPNIAEjoqrsIq8aULiVPXa6OxDYdIcL3gl7KFuCyDZkfT6ItC/NTkb3xT8np98lzEjL4+EUAR0lQkeNeIITJOgzXPViA1HQlyGhyuUUcqIIdjMnLrimoJBpDoi8YVub7eNRmIZ4qcUBMe6EIDiLrPfkWLYwuMyJkeNR1l9CjkVOjmydmsDNYKExmqBTVS5Mi9JcZyfHW8j1MeNX7tQA7nyuuT1jqckCCXQKN2/fEgV9fGyJFJFdzDibP+L3CH2IQS5OBialxVGItEOi4iCpZ6HqW3UobDCUqcWhrg08yNafDyM5Vg+VFBJPrCVStfiDQS0uDfCBUhDUFu1i1j+AbOwCAZSRCuyCCD3ChErZzNq1zxYJboi10IyjBM6ejoqFQ6npPW5MgP3JFPvv3226hft6ZJkWt0tAcjlzHnFu06YSIjOtJgvJwMq/BdSZHD5/Bp3ZptJNKlbRouONYGiWMV+IkWJgOgp4zIUaeoCwwo1Da2cnRSBfcs5EnJMkWOC/duSCSdTJSg9SB3UikeGZIxIjY0EAoYx3EoHc1cLaQhpGbMukXuIrChOEPSClwgzlxQCENT9cEJMQ0uvJz6P6xO+exzeOjbIEVOhVqcWDmiVrbTMTU+RfXVeBLkMvUlINHrYQBjrAlyrCicQ5svN7JyJkd7TCyHkJt7NLUw2/5UqjrR/mhxjs96rSq2shpsPxo+gC2q+vs16eUZME7I2Z4OwXme/rhyG3c8XXt1LuDkV1gSNStTHD7o1kUaVLPh7+15xuGDhB+Sa84xdem6SKIC7cuAwpLjW+5BQdVRjZAjdpEPdzpVkoxMI2RbFAXeRV8PnrDewEUHp+VGOZ2D6AwNH07r1kCIlOyyQ6tadaw4yQf8XKlbQFpKSgqa+Q7DYZou0B0dKOQzAi7kJqGJLzFzIU0CmitzARyi3dNhfHQQqAhIg4gN40xCfCFN+UIUy4GyYho8fIh2P1+SHFoGkV5xUYjoR5DUc2t21BtACz+keKaoFkp2FcJGUd9op9MlaFxHQgcg3FgT9IVf2oIwGCXkGiBw0+3FyPFH9nH1pke5vT0HOQiDPdJ1gcqA/ET3Q3iGojrEBQWraJIknAweJJfzyW54ykMXpXSMjNpGZsy1Gq/gtWQKgM7JV3nCXXqVzKbTQyb8kFzpLApWgdgjKBs9D0h2cw34ZY4pcuFkIjE8kszWkGwhEowmUzx86Xb0KAG5oxAlqQYawULux31A40roUDLvj7J0wr2eSKaH1huqY1wVHse0xsyFNPgRBn2SAJechktK184kTyxULsTSNFFITIOrhCZJIG/VnK/c0GSJ0ciVzyaF40kdnCNVSxfiGHA0aodRX/1xNzKgpshJasLztWGVaGz6SYpWs/fj+J8DMXIF//gkpdHR8MqHjVxzD6AWNJ4ProLBQ+C59kwst/XJQ5tybawLe7rZ3EPAtj1XAGP3u0Cu/c8ymHS71Ma0ASYusfNmghlyTLpb2MtLTBhyTBhyTJgw5Jgw5JgwYcgxYcgxYdJx5PQ3buhZ4zHpRuRugLDGY8KQY/L4IHf73//+t/D3z9999ytrWCZdjBwQ98//ofIdCGtYJt2A3K9UGHJMugO5f/7zf8UP//Pdz6xhmXQxcvpDJawtmXTriLUF5LL6jl9mgfbw5A6Q7Sk2mb+prpsm86dbWETIfTaMxHt7oi00hJ3OL019Wj7jAt5yfPh2NR8hd15mPv6XDTgjJgy5+yAnk4+yAMLi+potmiCb2yAgd/tl+RsTBp+OMJdZjzKXb+HP9JW/MdKc7Cp382WZ9Z/7uNdOgl8Wcn8tXDRugrm52Xt9ZT1xqw4mPQy5J05zm8zH/jZJHlpU8HK/PAG5rL790vl6PsK8f75msQzOy9y1wBjayIxbYT62ki/VxvWBS+Is+uVFmKO9yeRbOJSYdRlD7j7I9csErsb+C2/MK3siXUDuzmsy+bhYA9BUA+ef/dcI+X4esalG+8/I0CaC5NPNEWb70UWA3E6enGfCkGsLci+YBSUnJydpBeQMBcssZGZ7KHKjCXIrBOTwro0YsOsMOYZch5D7bZpsrpa7koLwqgEH6s9XG6rmyfyRz9TMk70pOlbd3u33VpiPruSuFEsdK0OOIdcEufpWkas501dmPdK8f1kWDBMmmMv8rz857r2RZifR8GGkudlJcfiQ1dds/80RMG4wnwOY4uGDgSHHkGuKXOrB04ZWkUOTIGjjXd3ukXLnZTL/gmUj5cNjYTA6fHKf4bEGcZLk7hTbfGEPUHGShCHHkGuC3LZt2053JB8cyzFhyHUIuUg9Q45J9yFXsm1bKmtMJt05fGDChCHHhCHHhAlDjglDjglDjiHHhCHHhCHHhAlDjsmDSH1STHxuD0dOF2gvfEl5tmQfXRNJVJxoLcsEpWTjDl6Pvsc+4PR9vzo5wb7Jl6Pj78RPNxhVw985P/1WgmM7v0ddrEnOWldFZKNKcccvtKpX48JUDnS3HuNeOi2V6i1s0pTQQotpPljQlseEXJh7XmufW5QS5/Hjx9vFP+rIXXXwrpTmp1TMX0X3JmgfcrpAxeog5axMk89BQaE14Yodhg4hp3Jx3JXW+CvBNZ6t5cY1KewhIKfxMm2cxp9btHHO4+1Axqc+2siVe5rcgdFKP+jj0n2V7UdO5bC+gU9Uij0uqladou6YlWtWcdXsVgFuUthDQA6U0Lb6uQVJwsTZjfft+ciRfWhQR4FvQ7vV6AL94l0UAZVoUxelx2ZoQO64m9LjmBZOJLn43cP7zJNNvQKd0iU9WL5wVp7gYN3Q/jd8gtNXbgqPPOjp9VrMAs7paANCrhRw1aF9bQpJuxaiftvBJxLfJiIHHZiIzKZmzYIK5HwD0oXikt3Ibi85dIOZZLJtDkUuXEk9c9XCyChlJN5BpzDRBVLg0wlO533w1jw0T3IVpkXIDyUJUhqRy5jpp0a76QQXqhzQZonR9pl0Dx+KHIdbBiN3dbsr2iwo22EXXHHMgPc93jUbkCMZ8Nn2p9Y5ZYrVxzWMhhpzgbNuBU6/ZuwC1MjTr1UtDA6ByjW01rkxFDlnQ09HDm3kG+NfBh0FkUzQIRf3CnCPjrvWKrZpyz3tdx1ygQZMUASnhdifw/HV+kRF8KHtZJNGtDkR3jCo8Z2eMXd1rI99JkR6iuDNSu8aATk44HEoxL8CkNNBk3NhUMAqwZnqb6xzyuOjiW+TIlc+2xtIctjGBdrvivd1olhnOwSkhUDpV2c77tqsXFBR5RkQuxa6jiJXHaYITknxQsgplY5HSUnVUYqAlDQSgyLN3Mt0NE9yFaaF5odc+yE3I3KlLgvKyj0dj8W7eP8aCHXTeHr/StpOqDtpGaSxLtAD8ljfkK2EPFxmXUP1jl2rXFBDM1BnQ8M5XROrXw7to/GCm6Fq4fp7GDnaBSJyCo9Yn9ajahG5Hm/lymeT7U2pO0IwQB3hrl9Qk4h6HxqwaqF3DYQU0BiKSC2+GYlUAZIpIUpx98BE5Xqt1NXswDc8yk5ATuPpVEZKORVmj/YhdMqTumV7tNN3qto4fMC4n+DCpuchm6JSQh+oHA7q9gUF7dEmQLerHHZwaBNoADVEZ5wAAA7hSURBVBXHj6Bpg1gTUjYgZ3/KIJSULbjuBGxXFCeEPPFVpDCaH5iyBl4lIvcLuh+wFUy0z0QRQIbiIG07ATnaMtSx6gJnVWSj3OEzrrcGmlTIIFvpni+pvm7NrArV3PkLarIVBzmMHO0CETnogRZjHzp6wMTZ2cb0eOTQDYUsPdQHbYiqRLcoeBGN54KacNSEEBKpHLCP8r6HI5oMMGwnG2jUpMVENY1ncrZDTttwzAZ9KCIHEZuW2Bgwlw3Y1qwWXXNRaohx4EGHD8RNZSh2gE1pIDu+KXagvde8GzKA/wQH6E28bzZY0NLNrjOVyIA0Rg7dE7QkI3KkbjuEPOlVCmN++IYTYznswunWbfbnqqB1wmbdom0n1J22DCq4OmaVKzh1XBx4TdVsaBkUywkZED2M1Yf7J2FB0vRr0dOvUeRwF1QYkVOjcLdVnxkPA9bxtr71PT+WQxMbs/KgjZGbS4uWIIfvd4TcbL/ikpKSYhpE52x2ISY/GxMSLuwID+5BiOUSFQFpycpWkFMcI44zebvSnmzzXKJFwb2wa6mJYwUL6/0zlJWoiERqaIXJA8AgskFE5KqLx4WfF7aEHC2pKXI0z2aRkw4fznuCSQq3P0U0iLY/vxCqQtpOvN1IyyC/AB63OFCCnIOfFiMnZED1EKuvcjgauL5q4dE13uoOI8en+jo7x3QRcZ09YoVGgjYuR42YIEEOOx/cC2jL0HqtOG5DNzke/oGlqvKadYsvIaOJcKVfPgrIy8CnkNCNIleO7nLRsRrQ8fLZZPcw2qnRqOkzwDPrU7VNkIOzQVAKdnV8vUEYLR9F9BFHCKqC50WOtSXkSEkSxwqagRkX8hSRE/PLVtIhD43lQDmI1tBEXz3Ky1Wqn2jhq3CznchWRCJUROTAR1bQWIVkIOpBq6/xmj/7BBc4HwbUHUcOWlb0GCX0r/rckp4Xy3nuSlmHrRyQdT7JxYhchWq2Y2w8jqeUEA27RBLkEgLS4omx4sIVq2PdoHUT6ZSrbp1S4bpqpuJgmP2pXF+JY9WtgSDdBR1Q+sWGOJWh4wmKHVxgcFoUcaYqF8XqzTPhcDPDBwQ36n8uDMoLEYxqNkTksWnFEO577IqBcB+UyA1pybHqaEkqB4+gdOLaUYDvXSPkKSIn5qfxAp0lwwcYvJzQecEIYS3cNuBS4a6jbScgR1smAYUiAcVREscKpUA6SCJkgE/ojNWHQBbtWI22mH0Q5Hi8JTuf6wujCDQnXB9i29/Wt6SHIYdn/U8bkE1KdoPx3GwjcvhAksMJnkvG29ES5HLEiQG00y2eC8kQAttq/PQhXVvqo/Q472VEDs3EeJxfiCZJcGnoOGCYiba5pUN/NN+A5l4SHU80tXL0UzUuj/reKi8UFoG3o5MaOrQpb1gLyOlpSWjjYTzaTlCsdkPTGEKeRuTESRK0mfF5T+MkSbmLexneKRmFXwA4fWJC1mUi5GjLIFMZ5eoY/K0ROb46ROlxLBAopRngE3pJ9bFNVbnAFQ+GHCZuvC2K6uJ5g3N/JHb1PQe5zhKu/Zv7PniZYdAh+mj7DpacYH/uwUqfdavHPmv1tbVDIwk7Q3x/IjGPHnIZWxoeAnJOu1LifTq6newDIocm0noqcSUYODBzqb79h2DknB895PQP4/vjqkNcxSmKbkfuAZN3LXLjjcj1f1SRY9KTpN6ZMlcSg63ckP4hDDkmXSrxttjIbQZ7h42cbQlDjkkXM2cH0IVAtJNrhwasnfLaJkOOSavhXGoStWyp8an3CbRLcktAcnNLDLlU6hlyTLpSYma6YVlbMpNKCkOOyQOI4X6n4918KHJuGDg3QM7AkGPShYFfM8gxK8ekC41gd1u5DqyCarvoYwpZnzLkTJHrwCqodojK4Rzr09+JY/UhyLl1uWPtyCqotkuiPUPu92Xl8B9dauXuswoKvZukVNpn0uVKeJmRR1mUqyJYDS7ZB6+n4tGbPX5J5EWg0u1kBRZe+KSOIosXmPwuJklm+uTSOZJ3utLK3W8VlCIyxUcR/CtdrlS1UBEc7+LofihEEclnOPgdilfStynRciSlX0O5l9OuQy5OmXTRWA5cl8KiuZ4uJalEcutTU1JTUlJTU0q60rG2ugpKF7igAq1GIn/PqsDvCYZDCtXs9Wi9JXpprAKfm36N162Zfi0BjUQylOuFhU/4ZbbkoKCgfNaxPUhyU02Yivchsrb1aKrzkWtuFVT09FN69CoiXa6EL0IYlc/21ngqiEvmhaXs4faZeNUAXPUrXfiEkYtGC5pYP/cc4Hzt7OxC6o2x3PY/EnnHIEg3Itd4FVS1r1LpdNQgLFeSInfPyykPLVUyfntCmBG5GrrwKZqx1vPcqLOds/MMCXMicjMfhpXjG62Cyp5zCq2hEpYrmSCH8dLTxVjoRX2vWbfwirAMsn4a5cyQ63kSYuc8A/7ZpTaxcg8DuaaroBSrY2PT1MJyJSlyDarZjrsOueFlp7pABfpqhm1aMnyYlSksfEpUBITmsV7uUeILyNkBcjE9Arkmq6DIKn3vGrpcyQQ5vA5qdT6xcva7XBzRxElpiKtjQLq48Em3Dn/RDBOGXBtF47W+gS+979dcteGbsJj0nFm4Ro6V71HIVXl6HEqJd1nfwJB7hIYPvsjG2YU8tBFr63J1ravSI/K+3yjIkOvB0mQjyqIQGLPGSLDq5nk5Jo8fc3xR82+a8/XxVEoYckw6LFfOHj6b3cZrc/+DSjxDjklH5eyXSM62Gzn2IjqTjrlU1ZdfHP7iiy++VPH69iHHrByTjhq5L8DIfQFmrp3IMSvH5EGRY46VSTchdxiQO9x+5B6qY730A+u5320sVw28QSjX/liu66wct+Iv+Osws57J468/uR/9WTvJ3cCtGKsWr9iCf9Oz+IItzWceMbote7dUbZhgbu2cdr/LuOVOBgbNA4sKWbkvrrSJuO6xctwKub+2W5ErmDJ887d7l/XLZ8h1j527dPZSdduI47vlO0m4FTYD93QncnXzbNFiCK7IwJDrHuaMPx9QOg05x41D8tuKXO0yC2ufSoScZsNIuV0hf/elr6aYD98DbFycIJ+xDJArQJcU81lP/Wtq/7IDr8rGxZqCk9Vrp5DxkQmycRd4bnHQRxZmq9T8GVCBP/NUHnjekfIZFzBy3wy+oPkIPhE3XDDV3Dq0QbgOlS13+W2ZBZQPmSyzGP/L7pHWkJGY8eqPLN4bim6ouNEVjy9zevRfj0LOqW7eX9RtQ45b8U5xVThGLmJ8yk9T5jTcHTM89ueN/dL568+vKrn88tiau6+tKqma567NGmrjG3N76K6iS/tMkYt4SnhlM2vQoZ/n9S/jlpsF5R54bqeInGaxbVrp3n0Iue8HnebjhqTp9v5AFFtVciXGiNwY27TaSTahhohn0rnlg4/pNtosVP/4/H5jxjbjPklZPKeBr5vmzwxmT0LOcPPlAC1GboTMEsRc1hJymsVztdSxou8Gjnu24u5rcPL2CzvJ9eBYsT3Jeqos67ktBoCo6euZm54tE/6EgOH6wNPc8jlaXrPY3SCgdH3oSepYfxx4DDIdK2y0efulnYgcETlUdgSUd/ul/dgkngFANfPcDWLGowvJ1VmD2GuiPQs5/szAk8TKhaLTP41p0bF+/9y4kGIxljvzbBnu9jsvbKmbBj2NkNskt7GxGfViHu7l2nlmzj+Qrw3fZC7rl25q5ZDcHHoS06Jb7CQid4ZcwS2XmTtB4psvD/clK3l1G/vM2KdtjFwlRh5lgspE7Eoz5u+O2cJterOBAdOzkNNtGvJfbRs+VG2dOiQdIVe198+jnpYi50+Ro71LDIvu0jILPCDmS3NycrWmsZzuyPujRpm1gpzT9wORvavaO9VsD0lyZeurYyvvi5w0Y4B99L9e2M946WHI8bXThrV1xHp3zE5Arm7azBSDxMrpFoN3hN6tieufJ0Gumci9bhoZseY0bLJN0UqtXNagzEaOlYt4ES/1R2fFADBduK5F5KQZg95D33uMBw89FjnwXv0aIWeblJycXNwoltt6yPDj0NOA3J0XdvJVG43I8WcGH9NffHVsTe2kmamlRzzVGLmCwOKqeXMaOTVwlGhebnD6Yiet7puBRuTuvOCvvTylHxk+HAlFx+umvdNwZJ+2YApWofazFO6b0WXCdS0hp5NmzMMRGRs8iIPXnoMcDpBMkJMhgSGAFDndgQmy4bFaQI77ZqTNjP/+gxE5zUcjrd0O4EmSp+UzLhArV7DMQu7c5CtJqpaNxE8ffpxg86evXjMix198VQ55okkSC/mMNHz85ojQS1Mt0MwHgmfDSPm4C7xwXUvImWSMqjb4MRw86Huulbu/CMj9XuWxHDwAcfrqJoeqqymL9SUGhlzXyd3HcvBQevbw4bMqE+LQ09bD6PF+fYyvs288Q67L5DF88qDnSw9jMb4/ouevfI5Fxdf72jk7G1dO90DkmPwOkTtLkDtsDOn0X36OFkF8flgfj4gD5nIZckw6TaopcaKZA7f6OWLu88+/LF07gyAXz5Bj0mlGToKcIBg5IO5LlS9FLoYhx6TzoBMcq3HUWk0d65f6EIpcKkOOSeeZuSsYuC/OSsM7hNznn1/ic53x8MHXwJBj0glioIsWshFyZyWPG/T6s2jAihYXoi9uneHb/h1aGXJMWrNzepWq0Vwwr8rOLsV/1KcmNT9crS+iwjbHZNJGKYkJ2UxiNNXZs9n6Rhjy93sQVuRMZEYqQ45JmyTXzhZkM3jRw8iLHjZdZqPXi47WwJBj0hlSj3Y3Hz/eNp4/Sx41HG7n432GHJP2STwizna8rbP+y8+Fx1t6hhyTrpMQbOTG29qVUuI+z25fBgw5Ju2TGIqcczWzcky6Z7w6HjP3HzFo3hcR90U739RkyDFpdzCHxLeerz7cESPHF82gwpBj0tZZkhD68iV6ynq2ur3vo9fTXTKT2NdTM2mzkBk3ilrnroBgyDG5D3T6Tl5zw5Bj0s3CkGPy+0BOf+OGnjUek25EjgkThhwThhwTJgw5Jgw5Jgw5JkwYckwYckyYMOSYMOSYMGHIMWHIMWHIMWHCkGPCkGPChCHH5Hcl/w8BjRR6N/l6dAAAAABJRU5ErkJggg==)
"""

"""
# Search-Summarize-Notify

The next section builds on the Hacker News example. Instead of just sending a notification on a match, this workflow will summarize the match first.

There are a number of alternative combinations. For example, the summaries could be built at index time. But this example will do everything on the fly when searching.
"""

from txtai.app import Application

workflow = """
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

summary:
  path: sshleifer/distilbart-cnn-12-6

tabular:
  idcolumn: url
  textcolumns:
    - title

textractor:
  join: true
  minlength: 100
  paragraphs: true

__main__.Slack:

workflow:
  index:
    schedule:
      cron: "* * * * * 0/5"
      elements:
        - front_page
      iterations: 1
    tasks:
      - batch: false
        extract:
          - hits
        method: get
        params:
          tags: null
        task: service
        url: https://hn.algolia.com/api/v1/search?hitsPerPage=50
      - action: tabular
      - action: upsert
  alert:
    schedule:
      cron: 0/1 * * * *
      elements:
        - select id url, id title from txtai where similar('software development library') and score >= 0.4 and id like 'http%'
      iterations: 1
    tasks:
      - action: search
      - action: tabular
      - action: textractor
      - action: summary
      - action: __main__.Slack
        unpack: false
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-10 17:19:48,847 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-10 17:19:48,857 [INFO] schedule: 'index' next run scheduled for 2022-02-10T17:19:50+00:00

#   2022-02-10 17:19:48,848 [INFO] schedule: 'alert' scheduler started with schedule 0/1 * * * *

#   2022-02-10 17:19:48,864 [INFO] schedule: 'alert' next run scheduled for 2022-02-10T17:20:00+00:00

#   2022-02-10 17:19:50,368 [INFO] schedule: 'index' max iterations (1) reached

#   2022-02-10 17:20:02,233 [INFO] __call__: Sending alert: ('https://datastation.multiprocess.io/blog/2022-02-08-the-world-of-postgresql-wire-compatibility.html', 'Every server-client database has a wire protocol. A wire protocol is the format for interactions between a database server and its clients. It does NOT encompass the actual query language itself, let alone database semantics. Proprietary databases like Oracle and IBM Db2 find value in developing their own drivers.', None)

#   2022-02-10 17:20:02,496 [INFO] schedule: 'alert' max iterations (1) reached


"""
And the result in Slack. See how the text is now the article summary vs. the title.

![](data:image/png;base64,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)
"""

"""
# Search-Summarize-Translate-Notify

One more example to really drive this home. Let's do the same as the last example and add a translate to French step.
"""

from txtai.app import Application

workflow = """
writable: true

embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true

summary:
  path: sshleifer/distilbart-cnn-12-6

tabular:
  idcolumn: url
  textcolumns:
    - title

textractor:
  join: true
  minlength: 100
  paragraphs: true

translation:

__main__.Slack:

workflow:
  index:
    schedule:
      cron: "* * * * * 0/5"
      elements:
        - front_page
      iterations: 1
    tasks:
      - batch: false
        extract:
          - hits
        method: get
        params:
          tags: null
        task: service
        url: https://hn.algolia.com/api/v1/search?hitsPerPage=50
      - action: tabular
      - action: upsert
  alert:
    schedule:
      cron: 0/1 * * * *
      elements:
        - select id url, id title from txtai where similar('software development library') and score >= 0.4 and id like 'http%'
      iterations: 1
    tasks:
      - action: search
      - action: tabular
      - action: textractor
      - action: summary
      - action: translation
        args:
          - fr
      - action: __main__.Slack
        unpack: false
"""

app = Application(workflow)
app.wait()
# Output:
#   2022-02-10 17:25:04,448 [INFO] schedule: 'index' scheduler started with schedule * * * * * 0/5

#   2022-02-10 17:25:04,449 [INFO] schedule: 'alert' scheduler started with schedule 0/1 * * * *

#   2022-02-10 17:25:04,451 [INFO] schedule: 'index' next run scheduled for 2022-02-10T17:25:05+00:00

#   2022-02-10 17:25:04,457 [INFO] schedule: 'alert' next run scheduled for 2022-02-10T17:26:00+00:00

#   2022-02-10 17:25:05,357 [INFO] schedule: 'index' max iterations (1) reached

#   2022-02-10 17:26:08,125 [INFO] __call__: Sending alert: ('https://datastation.multiprocess.io/blog/2022-02-08-the-world-of-postgresql-wire-compatibility.html', "Chaque base de donnÃ©es serveur-client a un protocole filaire. Un protocole filaire est le format pour les interactions entre un serveur de base de donnÃ©es et ses clients. Il n'inclut PAS le langage de requÃªte rÃ©el lui-mÃªme, et encore moins la sÃ©mantique de la base de donnÃ©es.", None)

#   2022-02-10 17:26:08,310 [INFO] schedule: 'alert' max iterations (1) reached


"""
And just like before, Slack has a summary and a link but this time in French!

![](data:image/png;base64,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)
"""

"""
# Wrapping up

This notebook covered how to build workflow notifications with txtai. There are many directions one could go with this. Build an activity feed, alert when semantic events occur and more. More ideas can be found in the [txtai application](https://huggingface.co/spaces/NeuML/txtai) on Hugging Face Spaces. 

Everything in this notebook can also be written in Python. The benefits of YAML workflows are that they require little to no-code. Work is ongoing as of txtai 4.1 to make workflows easier to containerize and ultimately run in serverless environments. Keep an eye on this!
"""



================================================
FILE: examples/29_Anatomy_of_a_txtai_index.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Anatomy of a txtai index

This notebook inspects the filesystem of a txtai embeddings index and gives an overview of the structure.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai
!apt-get update && apt-get install -y file xxd

"""
# Create index
Let's first create an index to inspect. We'll use the classic txtai example.

"""

from txtai.embeddings import Embeddings

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "objects": True})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run a search
embeddings.search("feel good story", 1)
# Output:
#   [{'id': '4',

#     'score': 0.08329004049301147,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
# Print index info

Embeddings indexes have an `info` method which prints metadata about the index. This can be used to see when the index was build, what settings were used and when it was last updated.
"""

# Print metadata
embeddings.info()
# Output:
#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2022-03-02T15:18:41Z",

#       "python": "3.7.12",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "4.3.0"

#     },

#     "content": "sqlite",

#     "dimensions": 768,

#     "objects": true,

#     "offset": 6,

#     "path": "sentence-transformers/nli-mpnet-base-v2",

#     "update": "2022-03-02T15:18:41Z"

#   }


"""
# Save index and review file structure

Next let's save the index and review the file structure. This section prints each file, and runs commands to show
"""

# Save the index
embeddings.save("index")

# Show basic details about index files
for f in ["config", "documents", "embeddings"]:
  !ls -l "index/{f}"
  !xxd "index/{f}" | head -5
  !file "index/{f}"
  !echo

# Output:
#   -rw-r--r-- 1 root root 295 Mar  2 15:18 index/config

#   00000000: 8004 951c 0100 0000 0000 007d 9428 8c04  ...........}.(..

#   00000010: 7061 7468 948c 2773 656e 7465 6e63 652d  path..'sentence-

#   00000020: 7472 616e 7366 6f72 6d65 7273 2f6e 6c69  transformers/nli

#   00000030: 2d6d 706e 6574 2d62 6173 652d 7632 948c  -mpnet-base-v2..

#   00000040: 0763 6f6e 7465 6e74 948c 0673 716c 6974  .content...sqlit

#   index/config: data

#   

#   -rw-r--r-- 1 root root 28672 Mar  2 15:18 index/documents

#   00000000: 5351 4c69 7465 2066 6f72 6d61 7420 3300  SQLite format 3.

#   00000010: 1000 0101 0040 2020 0000 0001 0000 0007  .....@  ........

#   00000020: 0000 0000 0000 0000 0000 0001 0000 0004  ................

#   00000030: 0000 0000 0000 0000 0000 0001 0000 0000  ................

#   00000040: 0000 0000 0000 0000 0000 0000 0000 0000  ................

#   index/documents: SQLite 3.x database, last written using SQLite version 3022000

#   

#   -rw-r--r-- 1 root root 18570 Mar  2 15:18 index/embeddings

#   00000000: 4978 4d70 0003 0000 0600 0000 0000 0000  IxMp............

#   00000010: 0000 1000 0000 0000 0000 1000 0000 0000  ................

#   00000020: 0100 0000 0049 7846 4900 0300 0006 0000  .....IxFI.......

#   00000030: 0000 0000 0000 0010 0000 0000 0000 0010  ................

#   00000040: 0000 0000 0001 0000 0000 0012 0000 0000  ................

#   index/embeddings: data

#   


"""
The directory has three files: *config*, *documents* and *embeddings*.

- config - The input configuration passed into the Embeddings object. Serialized with [Python's pickle format](https://docs.python.org/3/library/pickle.html).

- documents - [SQLite](https://www.sqlite.org/index.html) database. Stores the input text content and associated data.

- embeddings - The embeddings index file. This is an [Approximate Nearest Neighbor (ANN)](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor) index with either [Faiss](https://github.com/facebookresearch/faiss) (default), [Hnswlib](https://github.com/nmslib/hnswlib) or [Annoy](https://github.com/spotify/annoy), depending on the settings.
"""

"""
# Config

Given that the configuration file is serialized with Python pickle, it can be loaded in Python.
"""

import json
import pickle

with open("index/config", "rb") as config:
  print(json.dumps(pickle.load(config), sort_keys=True, indent=2))
# Output:
#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2022-03-02T15:18:41Z",

#       "python": "3.7.12",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "4.3.0"

#     },

#     "content": "sqlite",

#     "dimensions": 768,

#     "objects": true,

#     "offset": 6,

#     "path": "sentence-transformers/nli-mpnet-base-v2",

#     "update": "2022-03-02T15:18:41Z"

#   }


"""
Notice how this is the same output as `embeddings.info()`.
"""

"""
# Documents

The documents file is a SQLite database with three tables, documents, objects and sections. Let's take a look inside.
"""

import pandas as pd
import sqlite3

from IPython.display import display, Markdown

# Print details of a txtai SQLite document database
def showdb(path):
  db = sqlite3.connect(path)

  display(Markdown("## Tables"))
  df = pd.read_sql_query("select name FROM sqlite_master where type='table'", db)
  display(df.style.hide_index())

  for table in df["name"]:
    display(Markdown(f"## {table}"))
    df = pd.read_sql_query(f"select * from {table}", db)

    # Truncate large binary objects
    if "object" in df:
      df["object"] = df["object"].str.slice(0, 25)

    display(df.style.hide_index())

showdb("index/documents")
# Output:
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f686163de90>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f686163e850>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f686163e850>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f68631d1510>

"""
`documents` stores additional text fields as JSON, `objects` stores binary content and `sections` stores indexed text. The only table with data as of now is `sections`. `sections` stores the input (id, text, tags) elements along with internal ids and entry dates. 

We'll come back to `documents` and `objects`.
"""

"""
# Embeddings

Embeddings is the ANN index and what is queried when running similarity search. The default setting is to use Faiss. Let's inspect!
"""

import faiss
import numpy as np

# Query
query = "feel good story"

# Read index
index = faiss.read_index("index/embeddings")
print(index)
print(f"Total records: {index.ntotal}, dimensions: {index.d}")
print()

# Generate query embeddings and run query
queries = np.array([embeddings.transform((None, query, None))])
scores, ids = index.search(queries, 1)

# Lookup query result from original data array
result = data[ids[0][0]]

# Show results
print("Query:", query)
print("Results:", result, ids, scores)
# Output:
#   <faiss.swigfaiss.IndexIDMap; proxy of <Swig Object of type 'faiss::IndexIDMapTemplate< faiss::Index > *' at 0x7f68631cd750> >

#   Total records: 6, dimensions: 768

#   

#   Query: feel good story

#   Results: Maine man wins $1M from $25 lottery ticket [[4]] [[0.08329004]]


"""
# Index compression

txtai normally saves index files to a directory. Indexes can also be compressed. Nothing is different other than the files being in an compressed file format vs a directory.
"""

# Save index as tar.xz
embeddings.save("index.tar.xz")
!tar -tvJf index.tar.xz
!echo
!xz -l index.tar.xz
!echo

# Reload index
embeddings.load("index.tar.xz")

# Test search matches
embeddings.search("feel good story", 1)
# Output:
#   drwx------ root/root         0 2022-03-02 15:18 ./

#   -rw-r--r-- root/root       295 2022-03-02 15:18 ./config

#   -rw-r--r-- root/root     28672 2022-03-02 15:18 ./documents

#   -rw-r--r-- root/root     18570 2022-03-02 15:18 ./embeddings

#   

#   Strms  Blocks   Compressed Uncompressed  Ratio  Check   Filename

#       1       1     18.1 KiB     50.0 KiB  0.361  CRC64   index.tar.xz

#   

#   [{'id': '4',

#     'score': 0.08329004049301147,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
# Content storage

Let's add additional metadata and binary content to the index and see how that is stored in the SQLite database.
"""

import urllib

from IPython.display import Image

# Get an image
request = urllib.request.urlopen("https://raw.githubusercontent.com/neuml/txtai/master/demo.gif")

# Get data
data = request.read()

# Upsert new record having both text and an object
embeddings.upsert([("txtai", {"text": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.", "size": len(data), "object": data}, None)])

embeddings.save("index")
showdb("index/documents")
# Output:
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f68632cf7d0>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f6861966890>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f6861966890>
#   <IPython.core.display.Markdown object>
#   <pandas.io.formats.style.Styler at 0x7f686319be50>

"""
This section added a new record with metadata and binary content (truncated when printed here). The `documents` table enables additional fielded search with SQL. 
"""

embeddings.search("select * from txtai where size > 0")
# Output:
#   [{'data': '{"text": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.", "size": 47189}',

#     'entry': '2022-03-02 15:19:00.708223',

#     'id': 'txtai',

#     'indexid': 6,

#     'object': <_io.BytesIO at 0x7f6861408a70>,

#     'score': None,

#     'tags': None,

#     'text': 'txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.'}]

"""
Metadata fields can also be selected and combined with similarity queries.
"""

embeddings.search("select text, size, score from txtai where similar('machine learning') and score > 0.25 and size > 0")
# Output:
#   [{'score': 0.5479326844215393,

#     'size': 47189,

#     'text': 'txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.'}]

"""
The `objects` table enables additional binary content to be stored alongside an embeddings index. In some cases (image search), the object content is used to build embeddings.

Otherwise, it's the text field from sections. In both cases, associated binary objects are available at search time. 
"""

embeddings.search("select object from txtai where object is not null")
# Output:
#   [{'object': <_io.BytesIO at 0x7f6863246470>}]

"""
# Wrapping up

This notebook gave an overview of the txtai embeddings index file format. This hopefully gives a basic understanding of the architecture and/or helps with debugging when running into issues. 

See the following links for more information.

- [GitHub](https://github.com/neuml/txtai)
- [Embeddings documentation](https://neuml.github.io/txtai/embeddings)
"""



================================================
FILE: examples/30_Embeddings_SQL_custom_functions.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Embeddings SQL custom functions

txtai 4.0 added support for SQL-based embeddings queries. This feature combines natural language queries for similarity with concrete filtering rules. txtai now has support for user-defined SQL functions, making this feature even more powerful.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

"""
# Create index
Let's first recap how to create an index. We'll use the classic txtai example.

"""

from txtai.embeddings import Embeddings

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run a search
embeddings.search("feel good story", 1)
# Output:
#   [{'id': '4',

#     'score': 0.08329004049301147,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
# Custom SQL functions

Next, we'll recreate the index adding user-defined SQL functions. These functions are simply Python callable objects or functions that take an input and return values. Pipelines, workflows, custom tasks and any other callable object is supported.
"""

def clength(text):
  return len(text) if text else 0

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "functions": [clength]})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run a search using a custom SQL function
embeddings.search("select clength(text) clength, length(text) length, text from txtai where similar('feel good story')", 1)
# Output:
#   [{'clength': 42,

#     'length': 42,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
The function itself is simple, it's just alternate length function. But this example is just warming us up to what is possible and what is more exciting. 
"""

"""
# Pipelines in SQL

As mentioned above, any callable can be registered as a custom SQL function. Let's add a translate SQL function.
"""

from txtai.pipeline import Translation

# Translation pipeline
translate = Translation()

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "functions": [translate]})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

query = """
select
  text,
  translation(text, 'de', null) 'text (DE)',
  translation(text, 'es', null) 'text (ES)',
  translation(text, 'fr', null) 'text (FR)'
from txtai where similar('feel good story')
limit 1
"""

# Run a search using a custom SQL function
embeddings.search(query)
# Output:
#   [{'text': 'Maine man wins $1M from $25 lottery ticket',

#     'text (DE)': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket',

#     'text (ES)': 'Maine hombre gana $1M de billete de loterÃ­a de $25',

#     'text (FR)': 'Maine homme gagne $1M Ã  partir de $25 billet de loterie'}]

"""
And just like that we have translations through SQL! This is pretty ğŸ”¥ğŸ”¥ğŸ”¥

We can do more to make this easier though. Let's define a helper function to not require as many parameters. The default logic will require all function parameters each call, including parameters with default values.
"""

def translation(text, lang):
  return translate(text, lang)

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "functions": [translation]})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

query = """
select
  text,
  translation(text, 'de') 'text (DE)',
  translation(text, 'es') 'text (ES)',
  translation(text, 'fr') 'text (FR)'
from txtai where similar('feel good story')
limit 1
"""

# Run a search using a custom SQL function
embeddings.search(query)
# Output:
#   [{'text': 'Maine man wins $1M from $25 lottery ticket',

#     'text (DE)': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket',

#     'text (ES)': 'Maine hombre gana $1M de billete de loterÃ­a de $25',

#     'text (FR)': 'Maine homme gagne $1M Ã  partir de $25 billet de loterie'}]

"""
# Custom SQL functions with applications

Of course this is all available with YAML-configured applications.
"""

config = """
translation:

writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
  functions:
    - {name: translation, argcount: 2, function: translation}
"""

from txtai.app import Application

# Build application and index data
app = Application(config)
app.add([{"id": x, "text": row} for x, row in enumerate(data)])
app.index()

# Run search with custom SQL
app.search(query)
# Output:
#   [{'text': 'Maine man wins $1M from $25 lottery ticket',

#     'text (DE)': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket',

#     'text (ES)': 'Maine hombre gana $1M de billete de loterÃ­a de $25',

#     'text (FR)': 'Maine homme gagne $1M Ã  partir de $25 billet de loterie'}]

"""
# Wrapping up

This notebook introduced running user-defined custom SQL functions through embeddings SQL. This powerful feature can be used with any callable function including pipelines, tasks and workflows in tandem with similarity and rules filters.
"""



================================================
FILE: examples/31_Near_duplicate_image_detection.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Near duplicate image detection

This notebook will give an overview of how perceptual image hashes can be used to detect duplicate and near duplicate images.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline] textdistance

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz
!tar -xvzf tests.tar.gz

"""
# Generate hashes

The example below generates perceptual image hashes for a list of images.
"""

import glob

from PIL import Image

from txtai.pipeline import ImageHash

def show(image):
  width, height = image.size
  return image.resize((int(width / 2.25), int((width / 2.25) * height / width)))

# Get and scale images
images = [Image.open(image) for image in glob.glob('txtai/*jpg')]

# Create image pipeline
ihash = ImageHash()

# Generate hashes
hashes = ihash(images)
hashes
# Output:
#   ['ffffdf0700010100',

#    '78f8f8d8f8f8f8f0',

#    '00000006fefcfc30',

#    '000000c0feffff00',

#    '63263c183ce66742',

#    '60607072fe78cc00',

#    '0859dd04bfbfbf00',

#    '0000446c6f2f2724',

#    'ff9f010909010101',

#    'ff9d8140c070ffff']

"""
# Hash search

Next we'll generate a search hash to use to find similar near-duplicate images. This logic takes a section of an image and generates a hash for that
"""

# Get test image index
index = hashes.index('000000c0feffff00')

# Select portion of image
width, height = images[index].size

# Get dimensions for middle of image
left = (width - width/3)/2
top = (height - height/1.35)/2
right = (width + width/3)/2
bottom = (height + height/1.35)/2

# Crop image
search = images[index].crop((left, top, right, bottom))
show(search)
# Output:
#   <PIL.Image.Image image mode=RGB size=104x146 at 0x7FE86301FC50>

"""
Now let's compare the hash to all the image hashes using [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). We'll use the [textdistance](https://github.com/life4/textdistance) library for that. 
"""

import textdistance

# Find closest image hash using textdistance
shash = ihash(search)

# Calculate distances for search hash
distances = [int(textdistance.levenshtein.distance(h, shash)) for h in hashes]

# Show closest image hash
low = min(distances)
show(images[distances.index(low)])
# Output:
#   <PIL.Image.Image image mode=RGB size=311x197 at 0x7FE85DCA1D10>

"""
And as expected, the closest match is the original full image!
"""

"""
# Generate hashes with Embeddings indexes

Next we'll add a custom field with a perceptual image hash and a custom SQL function to calculate Levenshtein distance. An index of images is built and then a search query run using the distance from the same search hash.
"""

from txtai.embeddings import Embeddings

def distance(a, b):
  if a and not b:
    return len(a)
  if not a and b:
    return len(b)
  if not a and not b:
    return 0
  
  return int(textdistance.levenshtein.distance(a, b))

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True, "objects": "image", "functions": [distance]})

# Create an index for the list of text
embeddings.index([(uid, {"object": image, "text": ihash(image)}, None) for uid, image in enumerate(images)])

# Find image that is closest to hash
show(embeddings.search(f"select object from txtai order by distance(text, '{shash}')")[0]["object"])
# Output:
#   <PIL.Image.Image image mode=RGB size=311x197 at 0x7FE85DC89750>

"""
And just like above, the best match is the original full image.
"""

"""
# Wrapping up

This notebook introduced perceptual image hashing. These hashes can be used to detect near-duplicate images. This method is not backed by machine learning models and not intended to find conceptually similar images. But for tasks looking to find similar/near-duplicate images this method is fast and does the job!
"""



================================================
FILE: examples/32_Model_explainability.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Model explainability

Neural/transformers based approaches have recently made amazing advancements. But it is difficult to understand how models make decisions. This is especially important in sensitive areas where models are being used to drive critical decisions.

This notebook will cover how to gain a level of understanding of complex natural language model outputs.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline] shap

"""
# Semantic Search

The first example we'll cover is semantic search. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords. While this produces higher quality results, one advantage of keyword search is it's easy to understand why a result why selected. The keyword is there.

Let's see if we can gain a better understanding of semantic search output. 
"""

from txtai.embeddings import Embeddings

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run a search
embeddings.explain("feel good story", limit=1)
# Output:
#   [{'id': '4',

#     'score': 0.08329004049301147,

#     'text': 'Maine man wins $1M from $25 lottery ticket',

#     'tokens': [('Maine', 0.003297939896583557),

#      ('man', -0.03039500117301941),

#      ('wins', 0.03406312316656113),

#      ('$1M', -0.03121592104434967),

#      ('from', -0.02270638197660446),

#      ('$25', 0.012891143560409546),

#      ('lottery', -0.015372440218925476),

#      ('ticket', 0.007445111870765686)]}]

"""
The `explain` method above ran an embeddings query like `search` but also analyzed each token to determine term importance. Looking at the results, it appears that `win` is the most important term. Let's visualize it.
"""

from IPython.display import HTML

def plot(query):
  result = embeddings.explain(query, limit=1)[0]

  output = f"<b>{query}</b><br/>"
  spans = []
  for token, score in result["tokens"]:
    color = None
    if score >= 0.1:
      color = "#fdd835"
    elif score >= 0.075:
      color = "#ffeb3b"
    elif score >= 0.05:
      color = "#ffee58"
    elif score >= 0.02:
      color = "#fff59d"

    spans.append((token, score, color))

  if result["score"] >= 0.05 and not [color for _, _, color in spans if color]:
    mscore = max([score for _, score, _ in spans])
    spans = [(token, score, "#fff59d" if score == mscore else color) for token, score, color in spans]

  for token, _, color in spans:
    if color:
      output += f"<span style='background-color: {color}'>{token}</span> "
    else:
      output += f"{token} "

  return output

HTML(plot("feel good story"))
# Output:
#   <IPython.core.display.HTML object>

"""
Let's try some more queries!
"""

output = ""
for query in ["feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"]:
  output += plot(query) + "<br/><br/>"

HTML(output)
# Output:
#   <IPython.core.display.HTML object>

"""
There is also a batch method that can run bulk explainations more efficently. 
"""

queries = ["feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"]
results = embeddings.batchexplain(queries, limit=1)

for x, result in enumerate(results):
  print(result)
# Output:
#   [{'id': '4', 'text': 'Maine man wins $1M from $25 lottery ticket', 'score': 0.08329004049301147, 'tokens': [('Maine', 0.003297939896583557), ('man', -0.03039500117301941), ('wins', 0.03406312316656113), ('$1M', -0.03121592104434967), ('from', -0.02270638197660446), ('$25', 0.012891143560409546), ('lottery', -0.015372440218925476), ('ticket', 0.007445111870765686)]}]

#   [{'id': '1', 'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg", 'score': 0.24478264153003693, 'tokens': [("Canada's", -0.026454076170921326), ('last', 0.017057165503501892), ('fully', 0.007285907864570618), ('intact', -0.005608782172203064), ('ice', 0.009459629654884338), ('shelf', -0.029393181204795837), ('has', 0.0253918319940567), ('suddenly', 0.021642476320266724), ('collapsed,', -0.030680224299430847), ('forming', 0.01910528540611267), ('a', -0.00890059769153595), ('Manhattan-sized', -0.023612067103385925), ('iceberg', -0.009710296988487244)]}]

#   [{'id': '0', 'text': 'US tops 5 million confirmed virus cases', 'score': 0.1701308637857437, 'tokens': [('US', -0.02426217496395111), ('tops', -0.04896041750907898), ('5', -0.040287598967552185), ('million', -0.04737819731235504), ('confirmed', 0.02050541341304779), ('virus', 0.05511370301246643), ('cases', -0.029122650623321533)]}]

#   [{'id': '2', 'text': 'Beijing mobilises invasion craft along coast as Taiwan tensions escalate', 'score': 0.2714069187641144, 'tokens': [('Beijing', -0.040329575538635254), ('mobilises', -0.01986941695213318), ('invasion', 0.06464864313602448), ('craft', 0.044328778982162476), ('along', 0.021214008331298828), ('coast', -0.01738378405570984), ('as', -0.02182626724243164), ('Taiwan', -0.020671993494033813), ('tensions', -0.007258296012878418), ('escalate', -0.01663634181022644)]}]

#   [{'id': '3', 'text': 'The National Park Service warns against sacrificing slower friends in a bear attack', 'score': 0.28424495458602905, 'tokens': [('The', -0.022544533014297485), ('National', -0.005589812994003296), ('Park', 0.08145171403884888), ('Service', -0.016785144805908203), ('warns', -0.03266721963882446), ('against', -0.032368004322052), ('sacrificing', -0.04440906643867493), ('slower', 0.034766435623168945), ('friends', 0.0013159513473510742), ('in', -0.008420556783676147), ('a', 0.015498429536819458), ('bear', 0.08734165132045746), ('attack', -0.011731922626495361)]}]

#   [{'id': '2', 'text': 'Beijing mobilises invasion craft along coast as Taiwan tensions escalate', 'score': 0.24338798224925995, 'tokens': [('Beijing', -0.032770439982414246), ('mobilises', -0.04045189917087555), ('invasion', -0.0015233010053634644), ('craft', 0.017402753233909607), ('along', 0.004210904240608215), ('coast', 0.0028585344552993774), ('as', -0.0018710196018218994), ('Taiwan', 0.01866382360458374), ('tensions', -0.011064544320106506), ('escalate', -0.029331132769584656)]}]

#   [{'id': '4', 'text': 'Maine man wins $1M from $25 lottery ticket', 'score': 0.06539873033761978, 'tokens': [('Maine', 0.012625649571418762), ('man', -0.013015367090702057), ('wins', -0.022461198270320892), ('$1M', -0.041918568313121796), ('from', -0.02305116504430771), ('$25', -0.029282495379447937), ('lottery', 0.02279689908027649), ('ticket', -0.009147539734840393)]}]

#   [{'id': '5', 'text': 'Make huge profits without work, earn up to $100,000 a day', 'score': 0.033823199570178986, 'tokens': [('Make', 0.0013405345380306244), ('huge', 0.002276904881000519), ('profits', 0.02767787780612707), ('without', -0.007079385221004486), ('work,', -0.019851915538311005), ('earn', -0.026906955987215042), ('up', 0.00074811652302742), ('to', 0.007462538778781891), ('$100,000', -0.03565136343240738), ('a', -0.009965047240257263), ('day', -0.0021888017654418945)]}]


"""
Of course, this method is supported through YAML-based applications and the API.
"""

from txtai.app import Application

app = Application("""
writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
""")

app.add([{"id": uid, "text": text} for uid, text in enumerate(data)])
app.index()

app.explain("feel good story", limit=1)
# Output:
#   [{'id': '4',

#     'score': 0.08329004049301147,

#     'text': 'Maine man wins $1M from $25 lottery ticket',

#     'tokens': [('Maine', 0.003297939896583557),

#      ('man', -0.03039500117301941),

#      ('wins', 0.03406312316656113),

#      ('$1M', -0.03121592104434967),

#      ('from', -0.02270638197660446),

#      ('$25', 0.012891143560409546),

#      ('lottery', -0.015372440218925476),

#      ('ticket', 0.007445111870765686)]}]

"""
# Pipeline models

txtai pipelines are wrappers around Hugging Face pipelines with logic to easily integrate with txtai's workflow framework. Given that, we can use the [SHAP](https://github.com/slundberg/shap) library to explain predictions.

Let's try a sentiment analysis example.
"""

import shap

from txtai.pipeline import Labels

data = ["Dodgers lose again, give up 3 HRs in a loss to the Giants",
        "Massive dunk!!! they are now up by 15 with 2 minutes to go"]

labels = Labels(dynamic=False)

# explain the model on two sample inputs
explainer = shap.Explainer(labels.pipeline) 
shap_values = explainer(data)

shap.plots.text(shap_values[0, :, "NEGATIVE"])
# Output:
#   <IPython.core.display.HTML object>

shap.plots.text(shap_values[1, :, "NEGATIVE"])
# Output:
#   <IPython.core.display.HTML object>

"""
The [SHAP documentation](https://shap.readthedocs.io/en/latest/text_examples.html) provides a great list of additional examples for translation, text generation, summarization, translation and question-answering.

The SHAP library is pretty ğŸ”¥ğŸ”¥ğŸ”¥ Check it out for more!
"""

"""
# Wrapping up

This notebook briefly introduced model explainability. There is a lot of work in this area, expect a number of different methods to become available. Model explainability helps users gain a level of trust in model predictions. It also helps debug why a model is making a decision, which can potentially drive how to fine-tune a model to make better predictions. 

Keep an eye on this important area over the coming months!

"""



================================================
FILE: examples/33_Query_translation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Query translation

txtai supports two main types of queries: natural language statements and SQL statements. Natural language queries handles a search engine like query. SQL statements enable more complex filtering, sorting and column selection. Query translation bridges the gap between the two and enables filtering for natural language queries.

For example, the query:

```
Tell me a feel good story since yesterday
```

becomes

```sql
select * from txtai where similar("Tell me a feel good story") and
entry >= date('now', '-1 day')
```
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

"""
# Create index
Let's first recap how to create an index. We'll use the classic txtai example.

"""

from txtai.embeddings import Embeddings

data = ["US tops 5 million confirmed virus cases",
        "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
        "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
        "The National Park Service warns against sacrificing slower friends in a bear attack",
        "Maine man wins $1M from $25 lottery ticket",
        "Make huge profits without work, earn up to $100,000 a day"]

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

# Run a search
embeddings.search("feel good story", 1)
# Output:
#   [{'id': '4',

#     'score': 0.08329011499881744,

#     'text': 'Maine man wins $1M from $25 lottery ticket'}]

"""
# Query translation models

Next we'll explore how query translation models work with examples. 
"""

from txtai.pipeline import Sequences

sequences = Sequences("NeuML/t5-small-txtsql")

queries = [
  "feel good story",
  "feel good story since yesterday",
  "feel good story with lottery in text",
  "how many feel good story",
  "feel good story translated to fr",
  "feel good story summarized"
]

# Prefix to pass to T5 model
prefix = "translate English to SQL: "

for query in queries:
  print(f"Input: {query}")
  print(f"SQL: {sequences(query, prefix)}")
  print()

# Output:
#   Input: feel good story

#   SQL: select id, text, score from txtai where similar('feel good story')

#   

#   Input: feel good story since yesterday

#   SQL: select id, text, score from txtai where similar('feel good story') and entry >= date('now', '-1 day')

#   

#   Input: feel good story with lottery in text

#   SQL: select id, text, score from txtai where similar('feel good story') and text like '% lottery%'

#   

#   Input: how many feel good story

#   SQL: select count(*) from txtai where similar('feel good story')

#   

#   Input: feel good story translated to fr

#   SQL: select id, translate(text, 'fr') text, score from txtai where similar('feel good story')

#   

#   Input: feel good story summarized

#   SQL: select id, summary(text) text, score from txtai where similar('feel good story')

#   


"""
Looking at the query translations above gives an idea on how this model works.

[t5-small-txtsql](https://huggingface.co/NeuML/t5-small-txtsql) is the default model. Custom domain query syntax languages can be created using this same methodology, including for other languages. Natural language can be translated to functions, query clauses, column selection and more!
"""

"""
# Natural language filtering

Now it's time for this in action! Let's first initialize the embeddings index with the appropriate settings.
"""

from txtai.pipeline import Translation

def translate(text, lang):
  return translation(text, lang)

translation = Translation()

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2",
                         "content": True,
                         "query": {"path": "NeuML/t5-small-txtsql"},
                         "functions": [translate]})

# Create an index for the list of text
embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

query = "select id, score, translate(text, 'de') 'text' from txtai where similar('feel good story')"

# Run a search using a custom SQL function
embeddings.search(query)[0]
# Output:
#   {'id': '4',

#    'score': 0.08329011499881744,

#    'text': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket'}

"""
Note how the query model was provided as a embeddings index configuration parameter. Custom SQL functions were also added in. Let's now see if the same SQL statement can be run with a natural language query.
"""

embeddings.search("feel good story translated to de")[0]
# Output:
#   {'id': '4',

#    'score': 0.08329011499881744,

#    'text': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket'}

"""
Same result. Let's try a few more.
"""

embeddings.search("feel good story since yesterday")[0]
# Output:
#   {'id': '4',

#    'score': 0.08329011499881744,

#    'text': 'Maine man wins $1M from $25 lottery ticket'}

embeddings.search("feel good story with lottery in text")[0]
# Output:
#   {'id': '4',

#    'score': 0.08329011499881744,

#    'text': 'Maine man wins $1M from $25 lottery ticket'}

"""
For good measure, a couple queries with filters that return no results.
"""

embeddings.search("feel good story with missing in text")
# Output:
#   []

embeddings.search("feel good story with field equal 14")
# Output:
#   []

"""
# Query translation with applications

Of course this is all available with YAML-configured applications.
"""

config = """
translation:

writable: true
embeddings:
  path: sentence-transformers/nli-mpnet-base-v2
  content: true
  query:
    path: NeuML/t5-small-txtsql
  functions:
    - {name: translate, argcount: 2, function: translation}
"""

from txtai.app import Application

# Build application and index data
app = Application(config)
app.add([{"id": x, "text": row} for x, row in enumerate(data)])
app.index()

# Run search query
app.search("feel good story translated to de")[0]
# Output:
#   {'id': '4',

#    'score': 0.08329011499881744,

#    'text': 'Maine Mann gewinnt $1M von $25 Lotterie-Ticket'}

"""
# Wrapping up

This notebook introduced natural language filtering with query translation models. This powerful feature adds filtering and pipelines to natural language statements. Custom domain-specific query languages can be created to enable rich queries natively in txtai.
"""



================================================
FILE: examples/34_Build_a_QA_database.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build a QA database

Conversational AI is a growing field that could potentially automate much of the customer service industry. Full automation is still a ways away (most of us have been on a call with an automated agent and just want to get to a person) but it certainly can be a solid first line before human intervention.

This notebook presents a process to answer user questions using a txtai embeddings instance. It's not conversational AI but instead looks to find the closest existing question to a user question. This is useful in cases where there are a list of frequently asked questions. 
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai datasets

"""
# Load the dataset

We'll use a Hugging Face dataset of web questions for this example. The dataset has a list of questions and answers. The code below loads the dataset and prints a couple examples to get an idea of how the data is formatted.
"""

from datasets import load_dataset

ds = load_dataset("web_questions", split="train")

for row in ds.select(range(5)):
  print(row["question"], row["answers"])
# Output:
#   what is the name of justin bieber brother? ['Jazmyn Bieber', 'Jaxon Bieber']

#   what character did natalie portman play in star wars? ['PadmÃ© Amidala']

#   what state does selena gomez? ['New York City']

#   what country is the grand bahama island in? ['Bahamas']

#   what kind of money to take to bahamas? ['Bahamian dollar']


"""
# Create index

Next, we'll create a txtai index. The question will be the indexed text. We'll also store full content so we can access the answer at query time.

"""

from txtai.embeddings import Embeddings

# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

# Map question to text and store content
embeddings.index([(uid, {"url": row["url"], "text": row["question"], "answer": ", ".join(row["answers"])}, None) for uid, row in enumerate(ds)])

"""
# Asking questions

Now that the index is built, let's ask some questions! We'll use txtai SQL to select the fields we want to return.

See the list of questions asked and best matching question-answer combo.
"""

def question(text):
  return embeddings.search(f"select text, answer, score from txtai where similar('{text}') limit 1")

question("What is the timezone of NYC?")
# Output:
#   [{'answer': 'North American Eastern Time Zone',

#     'score': 0.8904051184654236,

#     'text': 'what time zone is new york under?'}]

question("Things to do in New York")
# Output:
#   [{'answer': "Chelsea Art Museum, Brooklyn Bridge, Empire State Building, The Broadway Theatre, American Museum of Natural History, Central Park, St. Patrick's Cathedral, Japan Society of New York, FusionArts Museum, American Folk Art Museum",

#     'score': 0.8308358192443848,

#     'text': 'what are some places to visit in new york?'}]

question("Microsoft founder")
# Output:
#   [{'answer': 'Bill Gates',

#     'score': 0.6617322564125061,

#     'text': 'who created microsoft windows?'}]

question("Apple founder university")
# Output:
#   [{'answer': 'Reed College',

#     'score': 0.5137897729873657,

#     'text': 'what college did steve jobs attend?'}]

question("What country uses the Yen?")
# Output:
#   [{'answer': 'Japanese yen',

#     'score': 0.6663530468940735,

#     'text': 'what money do japanese use?'}]

question("Show me a list of Pixar movies")
# Output:
#   [{'answer': "A Bug's Life, Toy Story 2, Ratatouille, Cars, Up, Toy Story, Monsters, Inc., The Incredibles, Finding Nemo, WALL-E",

#     'score': 0.653051495552063,

#     'text': 'what does pixar produce?'}]

question("What is the timezone of Florida?")
# Output:
#   [{'answer': 'North American Eastern Time Zone',

#     'score': 0.9672279357910156,

#     'text': 'where is the time zone in florida?'}]

question("Tell me an animal found offshore in Florida")
# Output:
#   [{'answer': 'Largemouth bass',

#     'score': 0.6526554822921753,

#     'text': 'what kind of fish do you catch in florida?'}]

"""
Not too bad! This database only has over 6,000 question-answer pairs. To improve quality a score filter could be put on the query to only return highly confident answers. But this gives an idea of what is possible.
"""

"""
# Run as an application

This can also be run as an application. See below.
"""

from txtai.app import Application

# Save index
embeddings.save("questions.tar.gz")

# Build application and index data
app = Application("path: questions.tar.gz")

# Run search query
app.search("select text, answer, score from txtai where similar('Tell me an animal found offshore in Florida') limit 1")[0]
# Output:
#   {'answer': 'Largemouth bass',

#    'score': 0.6526554822921753,

#    'text': 'what kind of fish do you catch in florida?'}

"""
# Wrapping up

This notebook introduced a simple question matching service. This could be the foundation of an automated customer service agent and/or an online FAQ.

For a full example, see [codequestion](https://github.com/neuml/codequestion), which is an application that matches user questions to Stack Overflow question-answer pairs.
"""



================================================
FILE: examples/35_Pictures_are_worth_a_thousand_words.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Pictures are worth a thousand words

One of the hottestğŸ”¥ models as of June 2022 is [DALL-E mini](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy). There are a number of projects and examples utilizing this model as seen [here](https://huggingface.co/spaces/dalle-mini/dalle-mini), [here](https://github.com/borisdayma/dalle-mini) and [here](https://github.com/kuprel/min-dalle). There have even been mainstream news articles covering this model.

This notebook presents a workflow to build a summary of a webpage and then generate an image of this summary. While there are a number of potential use cases for text to image models, this example is focused on showing the power of workflows and also provides interesting insights into how DALL-E mini "sees" the world.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. 

All credit for the DALL-E mini models and code goes to https://github.com/borisdayma/dalle-mini and https://github.com/kuprel/min-dalle.
"""

%%capture
!pip install git+https://github.com/neuml/txtai tika min-dalle ipyplot

"""
# Build a DALL-E pipeline

Let's first construct a txtai pipeline that generates images using DALL-E mini.
"""

from min_dalle import MinDalle

from txtai.pipeline import Pipeline

class Dalle(Pipeline):
  def __init__(self):
    self.model = MinDalle(is_mega=False, is_verbose=False)

  def __call__(self, texts, seed, prefix):
    results = []
    for text in texts:
      text = prefix + text
      results.append(self.model.generate_image(text, seed))

    return results


"""
# Build DALL-E workflow

Next we'll define a txtai workflow as YAML. This workflow extracts text at a specified URL, builds a summary and then generates an image for the summary text. 

This workflow can be run from Python as shown below or as a [API service](https://neuml.github.io/txtai/api/).
"""

from txtai.app import Application

app = Application("""
__main__.Dalle:
summary:
  path: sshleifer/distilbart-cnn-12-6
textractor:
  join: true
  lines: false
  minlength: 100
  paragraphs: true
  sentences: false
workflow:
  draw:
    tasks:
    - action: textractor
      task: url
    - action: summary
      args: [0, 60, 0]
    - action: __main__.Dalle
      args: [1024, "Illustration of "]
""")


"""
# Generate webpage summary images

Now that the workflow is up, let's generate some images! The following example generates images for a set of Wikipedia articles. Give this a try for articles, recipes or any other descriptive web page.

It's also fun to generate images for random Wikipedia pages using this url: https://en.wikipedia.org/wiki/Special:Random
"""

%%capture
# Build list of Wikipedia article URLs
captions = ["Catholic Church", "Magic Kingdom", "Epcot", "Mountain", "Tundra", "War of 1812", "Chinese Cuisine", "Indian Cuisine", "Italian Cuisine",
            "Romanian cuisine", "Football", "Artificial Intelligence", "Galaxy", "Fjord", "Island"]
urls = [f"https://en.wikipedia.org/wiki/{url.replace(' ', '_')}" for url in captions]

# Run workflow to generate images with DALL-E mini
images = list(app.workflow("draw", urls))
# Output:
#   2022-07-03 02:41:15,891 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Catholic_Church to /tmp/wiki-catholic_church.

#   2022-07-03 02:41:15,891 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Catholic_Church to /tmp/wiki-catholic_church.

#   2022-07-03 02:41:16,293 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Magic_Kingdom to /tmp/wiki-magic_kingdom.

#   2022-07-03 02:41:16,293 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Magic_Kingdom to /tmp/wiki-magic_kingdom.

#   2022-07-03 02:41:16,391 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Epcot to /tmp/wiki-epcot.

#   2022-07-03 02:41:16,391 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Epcot to /tmp/wiki-epcot.

#   2022-07-03 02:41:16,462 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Mountain to /tmp/wiki-mountain.

#   2022-07-03 02:41:16,462 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Mountain to /tmp/wiki-mountain.

#   2022-07-03 02:41:16,518 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Tundra to /tmp/wiki-tundra.

#   2022-07-03 02:41:16,518 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Tundra to /tmp/wiki-tundra.

#   2022-07-03 02:41:16,582 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/War_of_1812 to /tmp/wiki-war_of_1812.

#   2022-07-03 02:41:16,582 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/War_of_1812 to /tmp/wiki-war_of_1812.

#   2022-07-03 02:41:18,738 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Chinese_Cuisine to /tmp/wiki-chinese_cuisine.

#   2022-07-03 02:41:18,738 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Chinese_Cuisine to /tmp/wiki-chinese_cuisine.

#   2022-07-03 02:41:18,799 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Indian_Cuisine to /tmp/wiki-indian_cuisine.

#   2022-07-03 02:41:18,799 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Indian_Cuisine to /tmp/wiki-indian_cuisine.

#   2022-07-03 02:41:18,944 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Italian_Cuisine to /tmp/wiki-italian_cuisine.

#   2022-07-03 02:41:18,944 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Italian_Cuisine to /tmp/wiki-italian_cuisine.

#   2022-07-03 02:41:19,100 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Romanian_cuisine to /tmp/wiki-romanian_cuisine.

#   2022-07-03 02:41:19,100 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Romanian_cuisine to /tmp/wiki-romanian_cuisine.

#   2022-07-03 02:41:19,172 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Football to /tmp/wiki-football.

#   2022-07-03 02:41:19,172 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Football to /tmp/wiki-football.

#   2022-07-03 02:41:19,524 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Artificial_Intelligence to /tmp/wiki-artificial_intelligence.

#   2022-07-03 02:41:19,524 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Artificial_Intelligence to /tmp/wiki-artificial_intelligence.

#   2022-07-03 02:41:19,706 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Galaxy to /tmp/wiki-galaxy.

#   2022-07-03 02:41:19,706 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Galaxy to /tmp/wiki-galaxy.

#   2022-07-03 02:41:19,808 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Fjord to /tmp/wiki-fjord.

#   2022-07-03 02:41:19,808 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Fjord to /tmp/wiki-fjord.

#   2022-07-03 02:41:19,859 [MainThread  ] [INFO ]  Retrieving https://en.wikipedia.org/wiki/Island to /tmp/wiki-island.

#   2022-07-03 02:41:19,859 [INFO] getRemoteFile: Retrieving https://en.wikipedia.org/wiki/Island to /tmp/wiki-island.


import ipyplot
ipyplot.plot_images(images, captions, img_width=256)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
# Wrapping up

This notebook walked through an example on how to build a txtai workflow to generate webpage summary images. DALL-E mini is a fascinating model and it's a lot of fun to "see" how the model works. Give it a try yourself!
"""



================================================
FILE: examples/36_Run_txtai_in_native_code.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Run txtai in native code

txtai currently has two main methods of execution: Python or via a HTTP API. There are API bindings for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).

This notebook presents a way to run txtai as part of a native executable with the [Python C API](https://docs.python.org/3/c-api/index.html). We'll run an example in C and even call txtai from assembly code!

Before diving into this notebook, it's important to emphasize that connecting to txtai via the HTTP API has a number of major advantages. This includes decoupling from Python, the ability to offload txtai to a different machine and scaling with cloud compute. With that being said, this notebook demonstrates an additional way to integrate txtai along with providing an informative and perhaps academic programming exercise.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline] sacremoses

# Remove tensorflow as it's not used and prints noisy log messages
!pip uninstall -y tensorflow

# Install python3.7-dev and nasm
!apt-get install python3.7-dev nasm

"""
# Workflow configuration

This configuration builds a workflow to translate input text to French. More information on workflows can be found in [txtai's documentation](https://neuml.github.io/txtai/workflow).

"""

%%writefile config.yml
summary:
  path: sshleifer/distilbart-cnn-12-6

textractor:
  join: true
  lines: false
  minlength: 100
  paragraphs: true
  sentences: false
  tika: false

translation:

workflow:
  summary:
    tasks:
    - action: textractor
      task: url
    - action: summary

  translate:
    tasks:
    - action: translation
      args: 
      - fr
# Output:
#   Writing config.yml


"""
# Python C API

Next we'll build an interface to txtai workflows with the Python C API. This logic will load Python, create a txtai application instance and add methods to run workflows.

Some assumptions are made:

- txtai is installed and available
- A workflow is available in a file named `config.yml`
- The workflow only returns the first element

These assumptions are for brevity. This example could be expanded on and built into a more robust, full-fledged library.

While this example is in C, Rust has a well-maintained and popular library for interfacing with Python, [PyO3](https://github.com/PyO3/pyo3). Interfacing with the Python C API is also possible in Java, JavaScript and Go but not as straighforward.
"""

%%writefile workflow.c
#include <Python.h>

// Global instances
PyObject *module = NULL, *app = NULL;

/**
 * Create txtai module.
 */
PyObject* txtai() {
    PyObject* module = NULL;
    module = PyImport_ImportModule("txtai.app");
    return module;
}

/**
 * Create txtai application instance.
 */
PyObject* application() {
    PyObject* app = NULL;
    app = PyObject_CallMethod(module, "Application", "z", "config.yml");
    return app;
}

/**
 * Run txtai workflow.
 */
PyObject* run(char** args) {
    PyObject* result = NULL;
    result = PyObject_CallMethod(app, "workflow", "z[z]", args[0], args[1]);
    return result;
}

/**
 * Cleanup Python objects.
 */
void cleanup() {
    // Ensure Python instance exists
    if (Py_IsInitialized()) {
        PyErr_Print();

        Py_CLEAR(app);
        Py_CLEAR(module);

        Py_FinalizeEx();
    }
}

/**
 * Initialize a txtai application and run a workflow.
 */
const char* workflow(char** args) {
    PyObject* result = NULL;

    // Create application instance if it doesn't already exist
    if (!Py_IsInitialized()) {
        // Start Python Interpreter
        Py_Initialize();

        // Create txtai module
        module = txtai();

        // Handle errors
        if (!module) {
            cleanup();
            return NULL;
        }

        // Create txtai application
        app = application();

        // Handle errors
        if (!app) {
            cleanup();
            return NULL;
        }
    }

    // Run workflow
    result = run(args);

    // Handle errors
    if (!result) {
        cleanup();
        return NULL;
    }

    // Get first result
    const char *text = PyUnicode_AsUTF8(PyIter_Next(result));

    // Cleanup result
    Py_CLEAR(result);

    return text;
}
# Output:
#   Writing workflow.c


"""
# Run txtai workflow in C

Let's now write a C program to run a workflow using command line arguments as input. 
"""

%%writefile main.c
#include <stdio.h>

extern char* workflow(char** argv);
extern void cleanup();

/**
 * Run a txtai workflow and print results.
 */
int main(int argc, char** argv) {
    if (argc < 3) {
        printf("Usage: workflow <name> <element>\n");
        return 1;
    }

    // Run workflow using command line arguments
    char* text = workflow(argv + 1);
    if (text) {
        printf("%s\n", text);
    }

    // Cleanup
    cleanup();

    return 0;
}
# Output:
#   Writing main.c


"""
# Compile and run

Time to compile this all into an executable and run!
"""

!cc -c main.c -I/usr/include/python3.7m
!cc -c workflow.c -I/usr/include/python3.7m
!cc -o workflow workflow.o main.o -lpython3.7m

!./workflow translate "I'm running machine translation using a transformers model in C!"
# Output:
#   J'exÃ©cute la traduction automatique Ã  l'aide d'un modÃ¨le de transformateurs en C!


"""
And there it is, a translation workflow from English to French in a native executable, all backed by Transformers models. Any workflow YAML can be loaded and run in C using this method, which is pretty powerful.

Embedding txtai in native executable adds libpython as a dependency (libraries from 3rd party modules such as PyTorch and NumPy also load dynamically). See output of ldd below.
This opens up an avenue to embed txtai in native code provided it is acceptable to add libpython as a project dependency. 

As mentioned above, connecting to a txtai HTTP API instance is a less tightly coupled way to accomplish the same thing.
"""

!ldd workflow | grep python
# Output:
#   	libpython3.7m.so.1.0 => /usr/lib/x86_64-linux-gnu/libpython3.7m.so.1.0 (0x00007efcba85e000)


"""
# Machine learning in Assembly?

Now for a more academic exercise perhaps bringing you back to a computer organization/logic class from college. Let's see if we can run the same program in assembly!
"""

%%writefile main.asm
global main

; External C library functions
extern puts

; External txtai functions
extern workflow, cleanup

; Default to REL mode
default REL

section .data
    message:    db  "Usage: workflow <name> <element>", 0

section .text

; Print a usage message
usage:
    mov     rdi, message
    call    puts
    jmp     done

; Main function
main:
    ; Enter
    sub     rsp, 8

    ; Read argc - require workflow name and element (plus program name)
    cmp     rdi, 3
    jl      usage

    ; Run txtai workflow with argv params (skip program name) and print result
    lea     rdi, [rsi + 8]
    call    workflow
    mov     rdi, rax
    call    puts

done:
    ; Close txtai application instance
    call    cleanup

    ; Exit
    add     rsp, 8
    ret
# Output:
#   Writing main.asm


# Build workflow executable
!nasm -felf64 main.asm
!cc -c workflow.c -I/usr/include/python3.7m
!cc -o workflow -no-pie workflow.o main.o -lpython3.7m

!./workflow translate "I'm running machine translation using a transformers model with assembler!"
# Output:
#   J'exÃ©cute la traduction automatique Ã  l'aide d'un modÃ¨le de transformateurs avec assembleur!


"""
Just as before, the input text is translated to French using a machine translation model. But this time the code executing the logic was in assembly!

Probably not terribly useful but using the lowest level of code possible proves that any higher-level native code can do the same. 
"""

"""
# Multiple workflow calls

Everything up to this point has been a single workflow call. Much of the run time is spent on loading models as part of the txtai workflow. The next example will run a series of workflow calls and compare how long it takes vs a single workflow command line call. Once again in assembly.
"""

%%writefile main.asm
global main

; External C library functions
extern printf

; External txtai functions
extern workflow, cleanup

; Default to REL mode
default REL

section .data
    format:     db  "action: %s", 10, "input:  %s", 10, "output: %s", 10, 10, 0
    summary:    db  "summary", 0
    translate:  db  "translate", 0
    text1:      db  "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.", 0
    text2:      db  "Traditional search systems use keywords to find data", 0
    url1:       db  "https://github.com/neuml/txtai", 0
    url2:       db  "https://github.com/neuml/paperai", 0

section .text

; Run txtai workflow and print results
%macro txtai 2
    ; Workflow name and element
    push    %2
    push    %1

    ; Run workflow
    lea     rdi, [rsp]
    call    workflow

    ; Print action-input-output
    mov     rdi, format
    mov     rsi, [rsp]
    mov     rdx, [rsp + 8]
    mov     rcx, rax
    call    printf

    ; Restore stack
    add     rsp, 16
%endmacro

; Main function
main:
    ; Enter
    sub     rsp, 8

    ; Run workflows
    txtai   translate, text1	
    txtai   translate, text2
    txtai   summary, url1
    txtai   summary, url2

done:
    ; Close txtai application instance
    call    cleanup

    ; Exit
    add     rsp, 8
    ret
# Output:
#   Overwriting main.asm


!time ./workflow translate "I'm running machine translation using a transformers model with assembler!"
# Output:
#   J'exÃ©cute la traduction automatique Ã  l'aide d'un modÃ¨le de transformateurs avec assembleur!

#   

#   real	0m19.208s

#   user	0m11.256s

#   sys	0m3.224s


# Build workflow executable
!nasm -felf64 main.asm
!cc -c workflow.c -I/usr/include/python3.7m
!cc -no-pie -o workflow workflow.o main.o -lpython3.7m

!time ./workflow
# Output:
#   action: translate

#   input:  txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.

#   output: txtai exÃ©cute des workflows d'apprentissage automatique pour transformer les donnÃ©es et construire des applications de recherche sÃ©mantique alimentÃ©es par l'IA.

#   

#   action: translate

#   input:  Traditional search systems use keywords to find data

#   output: Les systÃ¨mes de recherche traditionnels utilisent des mots-clÃ©s pour trouver des donnÃ©es

#   

#   action: summary

#   input:  https://github.com/neuml/txtai

#   output: txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords. API bindings for JavaScript, Java, Rust and Go. Cloud-native architecture scales out with container orchestration systems (e. g. Kubernetes)

#   

#   action: summary

#   input:  https://github.com/neuml/paperai

#   output: paperai is an AI-powered literature discovery and review engine for medical/scientific papers. Paperai was used to analyze the COVID-19 Open Research Dataset (CORD-19) paperai and NeuML have been recognized in the following articles: Cord-19 Kaggle Challenge Awards Machine-Learning Experts Delve Into 47,000 Papers on Coronavirus Family.

#   

#   

#   real	0m22.478s

#   user	0m13.776s

#   sys	0m3.218s


"""
As we can see, running 4 workflow actions is about the same runtime as a single action when accounting for model load times.
"""

"""
# Wrapping up

This notebook walked through an example on how to run txtai with native code. While the HTTP API is a better route to go, this is another way to work with txtai!
"""



================================================
FILE: examples/37_Embeddings_index_components.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Embeddings index components

The main components of txtai are `embeddings`, `pipeline`, `workflow` and an `api`. The following shows the top level view of the txtai src tree.

```
Abbreviated listing of src/txtai
 ann
 api
 database
 embeddings
 pipeline
 scoring
 vectors
 workflow
```

One might ask, why are `ann`, `database`, `scoring` and `vectors` top level packages and not under the `embeddings` package? The `embeddings` package provides the glue between these components, making everything easy to use. The reason is that each of these packages are modular and can be used on their own! 

This notebook will go through a series of examples demonstrating how these components can be used standalone as well as combined together to build custom search indexes.

_Note: This is intended as a deep dive into txtai `embeddings` components. There are much simpler high-level APIs for standard use cases._
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai datasets

"""
# Load dataset

This example will use the `ag_news` dataset, which is a collection of news article headlines.
"""

from datasets import load_dataset

dataset = load_dataset("ag_news", split="train")

"""
# Approximate nearest neighbor (ANN) and Vectors

In this section, we'll use the `ann` and `vectors` package to build a similarity index over the `ag_news` dataset.

The first step is vectorizing the text. We'll use a `sentence-transformers` model. 
"""

import numpy as np

from txtai.vectors import VectorsFactory

model = VectorsFactory.create({"path": "sentence-transformers/all-MiniLM-L6-v2"}, None)

embeddings = []

# List of all text elements
texts = dataset["text"]

# Create embeddings buffer, vector model has 384 features
embeddings = np.zeros(dtype=np.float32, shape=(len(texts), 384))

# Vectorize text in batches
batch, index, batchsize = [], 0, 128
for text in texts:
  batch.append(text)

  if len(batch) == batchsize:
    vectors = model.encode(batch)
    embeddings[index : index + vectors.shape[0]] = vectors
    index += vectors.shape[0]
    batch = []

# Last batch
if batch:
    vectors = model.encode(batch)
    embeddings[index : index + vectors.shape[0]] = vectors

# Normalize embeddings
embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]

# Print shape
embeddings.shape
# Output:
#   (120000, 384)

"""
Next we'll build a vector index using these embeddings!
"""

from txtai.ann import ANNFactory

# Create Faiss index using normalized embeddings
ann = ANNFactory.create({"backend": "faiss"})
ann.index(embeddings)

# Show total
ann.count()
# Output:
#   120000

"""
Now let's run a search.
"""

query = model.encode(["best planets to explore for life"])
query /= np.linalg.norm(query)

for uid, score in ann.search(query, 3)[0]:
  print(uid, texts[uid], score)
# Output:
#   17752 Rocky Road: Planet hunting gets closer to Earth Astronomers have discovered the three lightest planets known outside the solar system, moving researchers closer to the goal of finding extrasolar planets that resemble Earth. 0.599043607711792

#   16158 Earth #39;s  #39;big brothers #39; floating around stars Washington - A new class of planets has been found orbiting stars besides our sun, in a possible giant leap forward in the search for Earth-like planets that might harbour life. 0.5688529014587402

#   45029 Coming Soon: "Good" Jupiters Most of the extrasolar planets discovered to date are gas giants like Jupiter, but their orbits are either much closer to their parent stars or are highly eccentric. Planet hunters are on the verge of confirming the discovery of Jupiter-size planets with Jupiter-like orbits. Solar systems that contain these "good" Jupiters may harbor habitable Earth-like planets as well. 0.5606889724731445


"""
And there it is, a full vector search system without using the `embeddings` package.

Just as a reminder, the following much simpler code does the same thing with an Embeddings instance.
"""

from txtai.embeddings import Embeddings

embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2"})
embeddings.index((x, text, None) for x, text in enumerate(texts))

for uid, score in embeddings.search("best planets to explore for life"):
  print(uid, texts[uid], score)
# Output:
#   17752 Rocky Road: Planet hunting gets closer to Earth Astronomers have discovered the three lightest planets known outside the solar system, moving researchers closer to the goal of finding extrasolar planets that resemble Earth. 0.599043607711792

#   16158 Earth #39;s  #39;big brothers #39; floating around stars Washington - A new class of planets has been found orbiting stars besides our sun, in a possible giant leap forward in the search for Earth-like planets that might harbour life. 0.568852961063385

#   45029 Coming Soon: "Good" Jupiters Most of the extrasolar planets discovered to date are gas giants like Jupiter, but their orbits are either much closer to their parent stars or are highly eccentric. Planet hunters are on the verge of confirming the discovery of Jupiter-size planets with Jupiter-like orbits. Solar systems that contain these "good" Jupiters may harbor habitable Earth-like planets as well. 0.560688853263855


"""
# Database

When the `content` parameter is enabled, an Embeddings instance stores both vector content and raw content in a database. But the `database` package can be used standalone too.
"""

from txtai.database import DatabaseFactory

# Load content into database
database = DatabaseFactory.create({"content": True})
database.insert((x, row, None) for x, row in enumerate(dataset))

# Show total
database.search("select count(*) from txtai")
# Output:
#   [{'count(*)': 120000}]

"""
The full txtai [SQL query syntax](https://neuml.github.io/txtai/embeddings/query/#sql) is available, including working with dynamically created fields.
"""

database.search("select count(*), label from txtai group by label")
# Output:
#   [{'count(*)': 30000, 'label': 0},

#    {'count(*)': 30000, 'label': 1},

#    {'count(*)': 30000, 'label': 2},

#    {'count(*)': 30000, 'label': 3}]

"""
Let's run a query to find text containing the word planets.
"""

for row in database.search("select id, text from txtai where text like '%planets%' limit 3"):
  print(row["id"], row["text"])
# Output:
#   100 Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.

#   102 Redesigning Rockets: NASA Space Propulsion Finds a New Home (SPACE.com) SPACE.com - While the exploration of the Moon and other planets in our solar system is nbsp;exciting, the first task for astronauts and robots alike is to actually nbsp;get to those destinations.

#   272 Sharpest Image Ever Obtained of a Circumstellar Disk Reveals Signs of Young Planets MAUNA KEA, Hawaii -- The sharpest image ever taken of a dust disk around another star has revealed structures in the disk which are signs of unseen planets.     Dr...


"""
Since this is just a SQL database, text search is quite limited. The query above just retrieved results with the word planets in it.
"""

"""
# Scoring

Since the original txtai release, there has been a `scoring` package. The main use case for this package is building a weighted sentence embeddings vector when using word vector models. But this package can also be used standalone to build BM25, TF-IDF and/or SIF text indexes.
"""

from txtai.scoring import ScoringFactory

# Build index
scoring = ScoringFactory.create({"method": "bm25", "terms": True, "content": True})
scoring.index((x, text, None) for x, text in enumerate(texts))

# Show total
scoring.count()
# Output:
#   120000

for row in scoring.search("planets explore life earth", 3):
  print(row["id"], row["text"], row["score"])
# Output:
#   16327 3 Planets Are Found Close in Size to Earth, Making Scientists Think 'Life' A trio of newly discovered worlds are much smaller than any other planets previously discovered outside of the solar system. 17.768332448130707

#   16158 Earth #39;s  #39;big brothers #39; floating around stars Washington - A new class of planets has been found orbiting stars besides our sun, in a possible giant leap forward in the search for Earth-like planets that might harbour life. 17.65941968170793

#   16620 New Planets could advance search for Life Astronomers in Europe and the United States have found two new planets about 20 times the size of Earth beyond the solar system. The discovery might be a giant leap forward in  17.65941968170793


"""
The search above ran a BM25 search across the dataset. The search will return more keyword/literal results. With proper query construction, the results can be decent.

Comparing the vector search results earlier and these results are a good lesson in the differences between keyword and vector search.
"""

"""
# Database and Scoring

Earlier we showed how the `ann` and `vectors` components can be combined to build a vector search engine. Can we combine the `database` and `scoring` components to add keyword search to a database? Yes!
"""

def search(query, limit=3):
  # Get similar clauses, if any
  similar = database.parse(query).get("similar")
  return database.search(query, [scoring.search(args[0], limit * 10) for args in similar] if similar else None, limit)

# Rebuild scoring - only need terms index
scoring = ScoringFactory.create({"method": "bm25", "terms": True})
scoring.index((x, text, None) for x, text in enumerate(texts))

for row in search("select id, text, score from txtai where similar('planets explore life earth') and label = 0"):
  print(row["id"], row["text"], row["score"])
# Output:
#   15363 NASA to Announce New Class of Planets Astronomers have discovered four new planets in a week's time, an exciting end-of-summer flurry that signals a sharper era in the hunt for new worlds.    While none of these new bodies would be mistaken as Earth's twin, some appear to be noticeably smaller and more solid - more like Earth and Mars - than the gargantuan, gaseous giants identified before... 12.582923259697132

#   15900 Astronomers Spot Smallest Planets Yet American astronomers say they have discovered the two smallest planets yet orbiting nearby stars, trumping a small planet discovery by European scientists five days ago and capping the latest round in a frenzied hunt for other worlds like Earth.    All three of these smaller planets belong to a new class of "exoplanets" - those that orbit stars other than our sun, the scientists said in a briefing Tuesday... 12.563928231067155

#   15879 Astronomers see two new planets US astronomers find the smallest worlds detected circling other stars and say it is a breakthrough in the search for life in space. 12.078383982352994


"""
And there it is, scoring-based similarity search with the same syntax as standard txtai vector queries, including additional filters!

txtai is built on vector search, machine learning and finding results based on semantic meaning. It's been well-discussed from a functionality standpoint how vector search has many advantages over keyword search. The one advantage keyword search has is speed. 
"""

"""
# Wrapping up

This notebook walked through each of the packages used by an Embeddings index. The Embeddings index makes this all transparent and easy to use. But each of the components do stand on their own and can be individually integrated into a project!
"""



================================================
FILE: examples/38_Introducing_the_Semantic_Graph.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Introducing the Semantic Graph

One of the main use cases of txtai is semantic search over a corpus of data. Semantic search provides an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords. Within an Embeddings instance sits a wealth of implied knowledge and relationships between rows. Many approximate nearest neighbor (ANN) indexes are even backed by graphs. What if we are able to tap into this knowledge?

Semantic graphs, also known as knowledge graphs or semantic networks, build a graph network with semantic relationships connecting the nodes. In txtai, they can take advantage of the relationships inherently learned within an embeddings index. This opens exciting possibilities for exploring relationships, such as topics and interconnections in a dataset. 

This notebook introduces the semantic graph.


"""

"""
# Install dependencies

Install `txtai` and all dependencies. We'll install the graph extra for graph functionality, pipeline extra for object detection and similarity extra to load models with the sentence-transformers library.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph,pipeline,similarity] datasets ipyplot

"""
# Graph basics

First we'll build a basic [graph](https://en.wikipedia.org/wiki/Graph_theory) and show how it can be used to explore relationships.

The code below builds a graph of animals and relationships between them. We'll add nodes and relationships along with running a couple analysis functions.
"""

import networkx as nx

from txtai.graph import GraphFactory

# Create graph
graph = GraphFactory.create({"backend": "networkx"})
graph.initialize()

# Add nodes
nodes = [(0, "dog"), (1, "fox"), (2, "wolf"), (3, "zebra"), (4, "horse")]
labels = {uid:text for uid, text in nodes}
for uid, text in nodes:
  graph.addnode(uid, text=text)

# Add relationships
edges = [(0, 1, 1), (0, 2, 1), (1, 2, 1), (2, 3, 0.25), (3, 4, 1)]
for source, target, weight in edges:
  graph.addedge(source, target, weight=weight)

# Print centrality and path between 0 and 4
print("Centrality:", {labels[k]:v for k, v in graph.centrality().items()})
print("Path (dog->horse):", " -> ".join([labels[uid] for uid in graph.showpath(0, 4)]))

# Visualize graph
nx.draw(graph.backend, nx.shell_layout(graph.backend), labels=labels, with_labels=True,
        node_size=2000, node_color="#03a9f4", edge_color="#cfcfcf", font_color="#fff")
# Output:
#   Centrality: {'wolf': 0.75, 'dog': 0.5, 'fox': 0.5, 'zebra': 0.5, 'horse': 0.25}

#   Path (dog->horse): dog -> wolf -> zebra -> horse

#   <Figure size 432x288 with 1 Axes>

"""
The visualization shows the layout of the graph. A centrality and path function were also run. Centrality shows the most central or related nodes. In this case, the `wolf` node has the highest score. We also ran a path function to show how the graph is traversed from `dog` to `horse`.
"""

"""
# Build a Semantic Graph

While txtai graphs can be standalone, with nodes and relationships manually added, the real power comes in indexing an embeddings instance.

The following section builds an embeddings index over the `ag_news` dataset. `ag_news` contains news headlines from the mid 2000s. This configuration sets the familiar vector model and content settings.

Column expressions is a feature starting with txtai 5.0. Column expressions alias expressions allowing SQL statements to use those references as a shorthand for the expression.

Next comes the graph. The configuration sets the maximum number of connections to add per node (15) along with a minimum similarity score (0.1). Topic modeling parameters are also added which we'll cover later.
"""

from datasets import load_dataset

from txtai.embeddings import Embeddings

# Create embeddings instance with a semantic graph
embeddings = Embeddings({
  "path": "sentence-transformers/all-MiniLM-L6-v2",
  "content": True,
  "functions": [
    {"name": "graph", "function": "graph.attribute"},
  ],
  "expressions": [
      {"name": "category", "expression": "graph(indexid, 'category')"},
      {"name": "topic", "expression": "graph(indexid, 'topic')"},
      {"name": "topicrank", "expression": "graph(indexid, 'topicrank')"}
  ],
  "graph": {
      "limit": 15,
      "minscore": 0.1,
      "topics": {
          "categories": ["Society & Culture", "Science & Mathematics", "Health", "Education & Reference", "Computers & Internet", "Sports",
                         "Business & Finance", "Entertainment & Music", "Family & Relationships", "Politics & Government"]
      }
  }
})

# Load dataset
dataset = load_dataset("ag_news", split="train")
rows = dataset["text"]

# Index dataset
embeddings.index((x, text, None) for x, text in enumerate(rows))

"""
The embeddings index is now created. Let's explore!
"""

"""
# Topic modeling

[Topic modeling](https://en.wikipedia.org/wiki/Topic_model) is an unsupervised method to identify abstract topics within a dataset. The most common way to do topic modeling is to use clustering algorithms to group nodes with the closest proximity.

A number of excellent topic modeling libraries exist in Python today. [BERTopic](https://github.com/MaartenGr/BERTopic) and [Top2Vec](https://github.com/ddangelov/Top2Vec) are two of the most popular. Both use [sentence-transformers](https://github.com/UKPLab/sentence-transformers) to encode data into vectors, [UMAP](https://github.com/lmcinnes/umap) for dimensionality reduction and [HDBSCAN](https://github.com/scikit-learn-contrib/hdbscan) to cluster nodes.

Given that an embeddings index has already encoded and indexed data, we'll take a different approach. txtai builds a graph running a query for each node against the index. In addition to topic modeling, this also opens up much more functionality which will be covered later.

Topic modeling in txtai is done using [community detection](https://en.wikipedia.org/wiki/Community_structure) algorithms. Similar nodes are group together. There are settings to control how much granularity is used to group nodes. In other words, topics can be very specific or broad, depending on these settings. Topics are labeled by building a BM25 index over each topic and finding the most common terms associated with the topic.

Let's take a closer look at the topics created with this embeddings index.
"""

# Store reference to graph
graph = embeddings.graph
len(embeddings.graph.topics)
# Output:
#   1919

list(graph.topics.keys())[:5]
# Output:
#   ['kerry_john_bush_president',

#    'sox_red_boston_series',

#    'oil_opec_prices_said',

#    'dollar_reuters_against_euro',

#    'darfur_sudan_region_said']

"""
The section above shows the number of topics in the index and top 5 topics. Keep in mind that `ag_news` is from the mid 2000s and that is evident with the top topics.

Given that we added functions to run SQL functions to get the topic for each row, we can use that to explore topics.

Each topic is associated with a list of associated matching ids. Those ids are ranked based on the importance to the topic in a field named `topicrank`. The section below prints the best matching text for the topic `sox_red_boston_series`. 
"""

print(embeddings.search("select text from txtai where topic = 'sox_red_boston_series' and topicrank = 0", 1)[0]["text"])
# Output:
#   Red Sox heading to the World Series The Boston Red Sox have won the American League Championship Series and are heading to the World Series for the first time since 1986.


"""
In addition to topics, higher level categories can be associated with topics. This enables having granular topics and encompassing categories for topics. For example, the topic of 'sox_red_boston_series' has a category of `Sports`. See below.
"""

for x, topic in enumerate(list(graph.topics.keys())[:5]):
  print(graph.categories[x], topic)
# Output:
#   Politics & Government kerry_john_bush_president

#   Sports sox_red_boston_series

#   Business & Finance oil_opec_prices_said

#   Business & Finance dollar_reuters_against_euro

#   Politics & Government darfur_sudan_region_said


"""
Topics and categories can also be used to filter results. See the difference when just querying for results similar to `book` and similar to `book` with a topic of `Sports`.
"""

print(embeddings.search("select text from txtai where similar('book')", 1)[0]["text"])
# Output:
#   A Guidebook for Every Taste LANNING a trip involves many difficult decisions, but near the top of my list is standing in a bookstore trying to choose from a daunting lineup of guidebooks, a purchase that brands the owner 


print(embeddings.search("select text from txtai where category='Sports' and similar('book')", 1)[0]["text"])
# Output:
#   Same story for Wildcats After a game about as artful as a dime-store novel, Virginia coach Pete Gillen turned to literature to express the trying time his No.


"""
# Graph analysis

Indexing an embeddings instance into a graph adds the ability to do network analysis. For example, the centrality of the graph can be analyzed to find the most common nodes. Alternatively, pagerank could also be run to rank the importance of nodes within the dataset. 

The section below runs graph centrality and shows the associated topic for the most central nodes. Not surprisingly, many of the topics are top topics.
"""

centrality = graph.centrality()

topics = list(graph.topics.keys())

for uid in list(centrality.keys())[:5]:
  topic = graph.attribute(uid, "topic")
  print(f"{topic} ({topics.index(topic)})")
# Output:
#   peoplesoft_oracle_takeover_bid (442)

#   darfur_sudan_region_said (4)

#   windows_microsoft_xp_service (12)

#   fallujah_us_city_iraqi (24)

#   eclipse_lunar_moon_total (615)


"""
# Walk the graph

Given that graphs are nodes and relationships, we can traverse the nodes using those relationships. The graph can be used to show how any two nodes are connected. 
"""

from IPython.display import HTML

def highlight(index, result):
  output = f"{index}. "
  spans = [(token, score, "#fff59d" if score > 0.025 else None) for token, score in result["tokens"]]

  if result["score"] >= 0.05 and not [color for _, _, color in spans if color]:
    mscore = max([score for _, score, _ in spans])
    spans = [(token, score, "#fff59d" if score == mscore else color) for token, score, color in spans]

  for token, _, color in spans:
    output += f"<span style='background-color: {color}'>{token}</span> " if color else f"{token} "

  return output

def showpath(source, target):
  path = graph.showpath(source, target)
  path = [graph.attribute(p, "text") for p in path]

  sections = []
  for x, p in enumerate(path):
      if x == 0:
          # Print start node
          sections.append(f"{x + 1}. {p}")

      if x < len(path) - 1:
          # Explain and highlight next path element
          results = embeddings.explain(p, [path[x + 1]], limit=1)[0]

          sections.append(highlight(x + 2, results))

  return HTML("<br/><br/>".join(sections))

showpath(82889, 67364)
# Output:
#   <IPython.core.display.HTML object>

"""
This shows how text about a `famous squirrel` and the `Red Sox winning the world series` are connected. Notice how the first match pivots to a node about a squirrel running on the field during a baseball game. From there, it's a relatively logical path to the end node. 

This is reminiscent of the game "six degrees of Kevin Bacon". Try running `showpath` with calls to `random.randint(0, len(rows) - 1)`, it's oddly addicting. This is a fun way to explore the interconnectivity of a dataset.
"""

"""
# Group images into topics

Topic modeling isn't limited to text. It supports any data that can be vectorized into an embeddings index. Next we'll create an embeddings index using the `imagenette` dataset, which is a small dataset for image object detection
"""

dataset = load_dataset("frgfm/imagenette", "160px", split="train")
rows = dataset["image"]

# Index with content and objects
embeddings = Embeddings({
  "method": "sentence-transformers",
  "path": "sentence-transformers/clip-ViT-B-32",
  "content": True,
  "objects": "image",
  "functions": [
      {"name": "graph", "function": "graph.attribute"},
  ],
  "expressions": [
      {"name": "topic", "expression": "graph(indexid, 'topic')"},
      {"name": "topicrank", "expression": "graph(indexid, 'topicrank')"}
  ],
  "graph": {
      "limit": 15,
      "minscore": 0.1,
      "topics": {
          "resolution": 1.0
      }
  }
})

embeddings.index((x, image, None) for x, image in enumerate(rows))

graph = embeddings.graph
list(graph.topics.keys())[:5]
# Output:
#   ['topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4']

"""
# Topic labeling for images

The index is now ready. Notice how the topic names are generic. Given there is no text associated, a different approach is needed. We'll use an object detection pipeline to label to best matching image per topic.
"""

import ipyplot

from PIL import Image

from txtai.pipeline import Objects

def labels():
  objects = Objects(classification=True, threshold=0.25)

  images = embeddings.search("select topic, object from txtai where topicrank=0 order by topic", 100)
  results = objects([result["object"] for result in images], flatten=True)

  return {images["topic"]: results[x][0].split(",")[0] for x, images in enumerate(images)}

def scale(image, factor=1):
  width, height = image.size
  return image.resize((int(width / factor), int((width / factor))))

images, labels = {}, labels()

for topic in list(graph.topics.keys())[:5]:
  for result in embeddings.search(f"select topic, object from txtai where topic = '{topic}' and topicrank = 0", len(graph.topics)):
    images[topic] = scale(result["object"])

ipyplot.plot_images(list(images.values()), [labels[topic] for topic in images], img_width=150)
# Output:
#   No model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).

#   Using a pipeline without specifying a model name and revision in production is not recommended.

#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
As we can see, each topic is now labeled. Imagenette is a labeled dataset, let's evaluate the accuracy of our topic modeling.
"""

def accuracy():
  correct, total = 0, 0
  labels = dataset["label"]

  for topic in graph.topics:
      label = labels[int(graph.topics[topic][0])]
      correct += sum(1 if labels[int(x)] == label else 0 for x in graph.topics[topic])
      total += len(graph.topics[topic])

  print("Accuracy:", correct/total)

accuracy()
# Output:
#   Accuracy: 0.9747597423170345


"""
Not bad, 97.48% accuracy using a totally unsupervised method not even intended for image classification!
"""

"""
# Walk the image graph

As we did before, let's walk the graph. We'll start with two images, `a person parachuting from the sky` and `someone holding a french horn`.
"""

images = []
for uid in graph.showpath(4352, 9111):
  images.append(scale(embeddings.search(f"select object from txtai where indexid = {uid} limit 1")[0]["object"]))

ipyplot.plot_images(images, img_width=150)
# Output:
#   <IPython.core.display.HTML object>
#   <IPython.core.display.HTML object>

"""
Very interesting! The first match is a person parachuting onto a football field, followed by a matching band on a field, finally leading to a person holding a french horn.
"""

"""
# Wrapping up

This notebook covered quite a lot! We introduced graphs, showed how they can be used to model semantic relationships and topics. This change makes it easier to run exploratory data analysis on a dataset with txtai and quickly gain insights. 

This is just the beginning of what is possible and there are a wide range of exciting new possibilities for txtai, stay tuned!
"""



================================================
FILE: examples/39_Classic_Topic_Modeling_with_BM25.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Classic Topic Modeling with BM25

txtai 5.0 introduced topic modeling via [semantic graphs](https://neuml.hashnode.dev/introducing-the-semantic-graph). Semantic graphs can be easily integrated into an embeddings instance to add topic modeling to a txtai index.

In addition to transformers-backed models, txtai also has support for traditional indexing methods. Given the modular design of txtai, traditional scoring methods like BM25 can be combined with graphs to build topic models. 

This notebook is all classic Python code on the CPU. No GPUs or machine learning models required!
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] datasets

"""
# Load dataset

This example will use the `ag_news` dataset, which is a collection of news article headlines.
"""

from datasets import load_dataset

dataset = load_dataset("ag_news", split="train")

"""
# Build BM25 Index

Since the original txtai release, there has been a `scoring` package. This package supports building standalone BM25, TF-IDF and/or SIF text indexes.
"""

from txtai.scoring import ScoringFactory

# List of all text elements
texts = dataset["text"]

# Build index
scoring = ScoringFactory.create({"method": "bm25", "terms": True})
scoring.index((x, text, None) for x, text in enumerate(texts))

# Show total
scoring.count()
# Output:
#   120000

"""
Let's test the index.
"""

for id, score in scoring.search("planets explore life earth", 3):
  print(id, texts[id], score)
# Output:
#   16327 3 Planets Are Found Close in Size to Earth, Making Scientists Think 'Life' A trio of newly discovered worlds are much smaller than any other planets previously discovered outside of the solar system. 20.72295380862701

#   16158 Earth #39;s  #39;big brothers #39; floating around stars Washington - A new class of planets has been found orbiting stars besides our sun, in a possible giant leap forward in the search for Earth-like planets that might harbour life. 19.917461045326878

#   16620 New Planets could advance search for Life Astronomers in Europe and the United States have found two new planets about 20 times the size of Earth beyond the solar system. The discovery might be a giant leap forward in  19.917461045326878


"""
Results look as expected. BM25 returns keyword-based results vs contextual matches.
"""

"""
# Build topic model

Now that we have a scoring index, we'll use it to build a graph.

Graphs have built-in methods to insert nodes and build a relationship index between the nodes. The `index` method takes a search parameter that can be any function that returns (id, score) pairs. This logic is built into embeddings instances. 

Graphs constructed via a BM25 index will have more literal relationships. In other words, it will be keyword-driven. Semantic graphs backed by embeddings will have contextual relationships.

The next section builds a graph to support topic modeling. We'll use a multiprocessing pool to maximize CPU usage.
"""

import os

from multiprocessing import Pool

from txtai.graph import GraphFactory

# Multiprocessing helper methods
SCORING = None

def create(search):
    global SCORING

    # Create a global scoring object
    SCORING = search

def run(params):
    query, limit = params
    return SCORING.search(query, limit)

def batchsearch(queries, limit):
    return pool.imap(run, [(query, limit) for query in queries])

# Build the graph
pool = None
with Pool(os.cpu_count(), initializer=create, initargs=(scoring,)) as pool:
    graph = GraphFactory.create({"topics": {}})
    graph.insert((x, text, None) for x, text in enumerate(texts))
    graph.index(batchsearch, None)

"""
Let's list the top 10 topics. Keep in mind this dataset is from 2004.
"""

list(graph.topics)[:10]
# Output:
#   ['kerry_bush_john_president',

#    'nhl_players_league_lockout',

#    'arafat_yasser_palestinian_leader',

#    'sharon_ariel_prime_minister',

#    'blair_tony_minister_prime',

#    'xp_windows_microsoft_sp2',

#    'athens_gold_medal_olympic',

#    'space_prize_million_spaceshipone',

#    'nikkei_tokyo_reuters_average',

#    'hostage_british_bigley_iraq']

"""
Topics map a list of ids for each matching text element ordered by topic relevance. Let's print the most relevant text element for a topic.
"""

uid = graph.topics["xp_windows_microsoft_sp2"][0]
graph.attribute(uid, "text")
# Output:
#   'Microsoft continues Windows XP SP2 distribution Continuing the roll-out of Windows XP Service Pack 2 (SP2), Microsoft Corp. on Wednesday began pushing the security-focused update to PCs running Windows XP Professional Edition '

"""
# Graph analysis

Given this is a standard txtai graph, analysis methods such as centrality and pagerank are available.
"""

centrality = list(graph.centrality().keys())
print("Top connection count:", [len(graph.edges(uid)) for uid in centrality[:5]], "\n")

# Print most central node/topic
print("Most central node:", graph.attribute(centrality[0], "text"))

topic = graph.attribute(centrality[0], "topic")
for uid in graph.topics[topic][:3]:
  print("->", graph.attribute(uid, "text"))
# Output:
#   Top connection count: [30, 30, 28, 28, 28] 

#   

#   Most central node: Manning Gets Chance to Start Giants Coach Tom Coughlin announced that rookie quarterback Eli Manning will start ahead of two-time M.V.P. Kurt Warner in Thursday's preseason game against Carolina.

#   -> Manning Replaces Warner As Giants QB (AP) AP - Eli Manning has replaced Kurt Warner as the New York Giants' starting quarterback.

#   -> Eli Manning replaces Warner at quarterback Eli Manning, the top pick in this year #39;s NFL draft, has been named the starting quarterback of the New York Giants. Coach Tom Coughlin made the announcement at a Monday news conference.

#   -> Giants to Start Manning Against Carolina (AP) AP - Eli Manning is going to get a chance to open the season as the New York Giants' starting quarterback.


"""
Notice the correlation between the number of connections and centrality.

Given that BM25 is keyword-driven, we expect that the most central node would be text that is duplicative in nature. And that is the case here.
"""

"""
# Walk the graph

Just like semantic graphs, relationship paths can be explored.
"""

from IPython.display import HTML

def showpath(source, target):
  path = graph.showpath(source, target)
  path = [graph.attribute(p, "text") for p in path]

  sections = []
  for x, p in enumerate(path):
    # Print start node
    sections.append(f"{x + 1}. {p}")

  return HTML("<br/><br/>".join(sections))

showpath(83978, 8107)
# Output:
#   <IPython.core.display.HTML object>

"""
Notice how the data pivots from the start node to the end node. If you've read the [Introducing the Semantic Graph](https://neuml.hashnode.dev/introducing-the-semantic-graph) article, you'll notice how this traversal is more literal in nature. In other words, the relationships are keyword-driven vs contextual.
"""

"""
# Wrapping up

This notebook demonstrated how graphs can index traditional indexes such as BM25. This method can also be applied to an external index provided a search function is available to build connections.

Semantic graphs backed by embeddings instances have a number of advantages and are recommended in most cases. But this is a classic way to do it - no machine learning models required!
"""



================================================
FILE: examples/40_Text_to_Speech_Generation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Text to speech generation

Text To Speech (TTS) models have made great strides in quality over the last few years. Unfortunately, it's not currently possible to use these libraries without installing a large number of dependencies.

The txtai TextToSpeech pipeline has the following objectives:

- Fast performance both on CPU and GPU
- Ability to batch large text values and stream it through the model
- Minimal install footprint
- All dependencies must be Apache 2.0 compatible

This notebook will go through a set of text to speech generation examples.


"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package. We'll also demonstrate running this pipeline as an application.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-audio,pipeline-data] onnxruntime-gpu librosa

# Install NLTK
import nltk
nltk.download('averaged_perceptron_tagger_eng')

"""
# Create a TextToSpeech instance

The TextToSpeech instance is the main entrypoint for generating speech from text. The pipeline is backed by models from the [ESPnet](https://github.com/espnet/espnet) project. ESPnet has a number of high quality TTS models available on the [Hugging Face Hub](https://huggingface.co/models?library=espnet&pipeline_tag=text-to-speech&sort=downloads).

This pipeline can use the following models on the Hugging Face Hub.

- [ljspeech-jets-onnx](https://huggingface.co/NeuML/ljspeech-jets-onnx)
- [ljspeech-vits-onnx](https://huggingface.co/NeuML/ljspeech-vits-onnx)
- [vctk-vits-onnx](https://huggingface.co/NeuML/vctk-vits-onnx)

The default model is `ljspeech-jets-onnx`. Each of the models above are ESPnet models exported to ONNX using [espnet_onnx](https://github.com/espnet/espnet_onnx). More on that process can be found in the links above.

"""

%%capture

from txtai.pipeline import TextToSpeech

# Create text-to-speech model
tts = TextToSpeech()

"""
# Generate speech

The first example shows how to generate speech from text. Let's give it a try!
"""

import librosa.display
import matplotlib.pyplot as plt

text = "Text To Speech models have made great strides in quality over the last few years."

# Generate raw waveform speech
speech, rate = tts(text), 22050

# Print waveplot
plt.figure(figsize=(15, 5))
plot = librosa.display.waveshow(speech[0], sr=speech[1])
# Output:
#   <Figure size 1500x500 with 1 Axes>

"""
The graph shows a plot of the audio. It clearly shows pauses between words and sentences as we would expect in spoken language. Now let's play the generated speech.
"""

from IPython.display import Audio, display

import os

import soundfile as sf

def play(speech):
  # Convert to MP3 to save space
  sf.write("speech.wav", speech[0], speech[1])
  !ffmpeg -i speech.wav -y -b:a 64 speech.mp3 2> /dev/null

  # Play speech
  display(Audio(filename="speech.mp3"))

play(speech)
# Output:
#   <IPython.lib.display.Audio object>

"""
# Transcribe audio back to text

Next we'll use [OpenAI Whisper](https://github.com/openai/whisper) to transcribe the generated audio back to text.
"""

from txtai.pipeline import Transcription

# Transcribe files
transcribe = Transcription("openai/whisper-base")

# Print result
transcribe(speech, rate)
# Output:
#   'Text to speech models have made great strides in quality over the last few years.'

"""
And as expected, the transcription matches the original text.
"""

"""
# Streaming speech generation

The TextToSpeech pipeline supports incrementally generating snippets of speech. This enables the pipeline to work with streaming LLM generation.
"""

text = "This is streaming speech generation. It's designed to take output tokens from a streaming LLM. It returns snippets of speech.".split()
for speech, _ in tts(text, stream=True):
  print(speech.shape)
# Output:
#   (32768,)

#   (31488,)

#   (26368,)


"""
# Audio books

The TextToSpeech pipeline is designed to work with large blocks of text. It could be used to build audio for entire chapters of books.

In the next example below, we'll read the beginning of the book the `Great Gatsby`. We'll load a new model that enables setting a speaker.
"""

# Beginning of The Great Gatsby from Project Gutenberg
# https://www.gutenberg.org/ebooks/64317

text = """
In my younger and more vulnerable years my father gave me some advice
that I've been turning over in my mind ever since.

â€œWhenever you feel like criticizing anyone,â€ he told me, â€œjust
remember that all the people in this world haven't had the advantages
that you've had.â€

He didn't say any more, but we've always been unusually communicative
in a reserved way, and I understood that he meant a great deal more
than that.
"""

tts = TextToSpeech("neuml/vctk-vits-onnx")
speech = tts(text, speaker=3)
play(speech)
# Output:
#   <IPython.lib.display.Audio object>

"""
# Text To Speech Workflow

In the last example, we'll cover building a text-to-speech workflow. This workflow is no different in that it connects multiple pipelines together, each of which are backed by machine learning models.

The workflow extracts text from a webpage, summarizes it and then generates audio of the summary.
"""

%%writefile workflow.yml
summary:
  path: sshleifer/distilbart-cnn-12-6

textractor:
  join: true
  lines: false
  minlength: 100
  paragraphs: true
  sentences: false

texttospeech:
  path: neuml/vctk-vits-onnx

workflow:
  tts:
    tasks:
    - action: textractor
      task: url
    - action: summary
    - action: texttospeech
      args:
        speaker: 15
# Output:
#   Overwriting workflow.yml


from txtai.app import Application

app = Application("workflow.yml")

speech = list(app.workflow("tts", ["https://en.wikipedia.org/wiki/Natural_language_processing"]))[0]

play(speech)
# Output:
#   <IPython.lib.display.Audio object>

"""
# Wrapping up

This notebook gave a brief introduction on text to speech models. The text to speech pipeline in txtai is designed to be easy to use and handles the most common text to speech tasks in English.  

This work is made possible by the excellent advancements in text to speech modeling. [ESPnet](https://github.com/espnet/espnet) is a great project and should be checked out for more advanced and a wider range of use cases. This pipeline was also made possible by the great work from [espnet_onnx](https://github.com/espnet/espnet_onnx) in building a framework to export models to ONNX.

Looking forward to seeing what the community dreams up using this pipeline!


"""



================================================
FILE: examples/41_Train_a_language_model_from_scratch.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Train a language model from scratch

txtai has a robust training pipeline that can fine-tune large language models (LLMs) for downstream tasks such as labeling text. txtai also has the ability to train language models from scratch.

The vast majority of time, fine-tuning a LLM yields the best results. But when making significant changes to the structure of a model, training from scratch is often required.

Examples of significant changes are:

- Changing the vocabulary size
- Changing the number of hidden dimensions
- Changing the number of attention heads or layers
- Create a custom model architecture

This notebook will show how to build a new tokenizer and train a small language model (known as a micromodel) from scratch.

"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-train] datasets sentence-transformers onnxruntime onnx

"""
# Load dataset

This example will use the `ag_news` dataset, which is a collection of news article headlines.
"""

from datasets import load_dataset

dataset = load_dataset("ag_news", split="train")

"""
# Train the tokenizer

The first step is to train the tokenizer. We could use an existing tokenizer but in this case, we want a smaller vocabulary.

"""

from transformers import AutoTokenizer

def stream(batch=10000):
    for x in range(0, len(dataset), batch):
        yield dataset[x: x + batch]["text"]

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokenizer = tokenizer.train_new_from_iterator(stream(), vocab_size=500, length=len(dataset))
tokenizer.model_max_length = 512

tokenizer.save_pretrained("bert")

"""
Let's test the tokenizer.
"""

print(tokenizer.tokenize("Red Sox defeat Yankees 5-3"))
# Output:
#   ['re', '##d', 'so', '##x', 'de', '##f', '##e', '##at', 'y', '##ank', '##e', '##es', '5', '-', '3']


"""
With a limited vocabulary size of 500, most words require multiple tokens. This limited vocabulary lowers the number of token representations the model needs to learn.
"""

"""
# Train the language model

Now it's time to train the model. We'll train a micromodel, which is an extremely small language model with a limited vocabulary. Micromodels, when paired with a limited vocabulary have the potential to work in limited compute environments like edge devices and microcontrollers.
"""

from transformers import AutoTokenizer, BertConfig, BertForMaskedLM

from txtai.pipeline import HFTrainer

config = BertConfig(
    vocab_size = 500,
    hidden_size = 50,
    num_hidden_layers = 2,
    num_attention_heads = 2,
    intermediate_size = 100,
)

model = BertForMaskedLM(config)
model.save_pretrained("bert")
tokenizer = AutoTokenizer.from_pretrained("bert")

train = HFTrainer()

# Train model
train((model, tokenizer), dataset, task="language-modeling", output_dir="bert",
      fp16=True, per_device_train_batch_size=128, num_train_epochs=10,
      dataloader_num_workers=2)

"""
# Sentence embeddings

Next let's take the language model and fine-tune it to build sentence embeddings. 
"""

%%capture
!wget https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/training/nli/training_nli_v2.py
!python training_nli_v2.py bert
!mv output/* bert-nli

"""
# Embeddings search

Now we'll build a txtai embeddings index using the fine-tuned model. We'll index the `ag_news` dataset. 
"""

from txtai.embeddings import Embeddings

# Get list of all text
texts = dataset["text"]

embeddings = Embeddings({"path": "bert-nli", "content": True})
embeddings.index((x, text, None) for x, text in enumerate(texts))

"""
Let's run a search and see how much the model has learned.
"""

embeddings.search("Boston Red Sox Cardinals World Series")
# Output:
#   [{'id': '76733',

#     'text': 'Red Sox sweep Cardinals to win World Series The Boston Red Sox ended their 86-year championship drought with a 3-0 win over the St. Louis Cardinals in Game Four of the World Series.',

#     'score': 0.8008379936218262},

#    {'id': '71169',

#     'text': 'Red Sox lead 2-0 over Cardinals of World Series The host Boston Red Sox scored a 6-2 victory over the St. Louis Cardinals, helped by Curt Schilling #39;s pitching through pain and seeping blood, in World Series Game 2 on Sunday night.',

#     'score': 0.7896029353141785},

#    {'id': '70100',

#     'text': 'Sports: Red Sox 9 Cardinals 7 after 7 innings BOSTON Boston has scored twice in the seventh inning to take an 9-to-7 lead over the St. Louis Cardinals in the World Series opener at Fenway Park.',

#     'score': 0.7735188603401184}]

"""
Not too bad. It's far from perfect but we can tell that it has some knowledge! This model was trained for 5 minutes, there is certainly room for improvement in training longer and/or with a larger dataset.

The standard `bert-base-uncased` model has 110M parameters and is around 440MB. Let's see how many parameters this model has.
"""

# Show number of parameters
parameters = sum(p.numel() for p in embeddings.model.model.parameters())
print(f"Number of parameters:\t\t{parameters:,}")
print(f"% of bert-base-uncased\t\t{(parameters / 110000000) * 100:.2f}%")
# Output:
#   Number of parameters:		94,450

#   % of bert-base-uncased		0.09%


!ls -lh bert-nli/pytorch_model.bin
# Output:
#   -rw-r--r-- 1 root root 386K Jan 11 20:52 bert-nli/pytorch_model.bin


"""
This model is 386KB and has only 0.1% of the parameters. With proper vocabulary selection, a small language model has potential.
"""

"""
# Quantization

If 386KB isn't small enough, we can quantize the model to get it down even further. 
"""

from txtai.pipeline import HFOnnx

onnx = HFOnnx()
onnx("bert-nli", task="pooling", output="bert-nli.onnx", quantize=True)

embeddings = Embeddings({"path": "bert-nli.onnx", "tokenizer": "bert-nli", "content": True})
embeddings.index((x, text, None) for x, text in enumerate(texts))
embeddings.search("Boston Red Sox Cardinals World Series")
# Output:
#   [{'id': '76733',

#     'text': 'Red Sox sweep Cardinals to win World Series The Boston Red Sox ended their 86-year championship drought with a 3-0 win over the St. Louis Cardinals in Game Four of the World Series.',

#     'score': 0.8008379936218262},

#    {'id': '71169',

#     'text': 'Red Sox lead 2-0 over Cardinals of World Series The host Boston Red Sox scored a 6-2 victory over the St. Louis Cardinals, helped by Curt Schilling #39;s pitching through pain and seeping blood, in World Series Game 2 on Sunday night.',

#     'score': 0.7896029353141785},

#    {'id': '70100',

#     'text': 'Sports: Red Sox 9 Cardinals 7 after 7 innings BOSTON Boston has scored twice in the seventh inning to take an 9-to-7 lead over the St. Louis Cardinals in the World Series opener at Fenway Park.',

#     'score': 0.7735188603401184}]

!ls -lh bert-nli.onnx
# Output:
#   -rw-r--r-- 1 root root 187K Jan 11 20:53 bert-nli.onnx


"""
We're down to 187KB with a quantized model!

"""

"""
# Train on BERT dataset

The [BERT paper](https://arxiv.org/abs/1810.04805) has all the information regarding training parameters and datasets used. Hugging Face Datasets hosts the `bookcorpus` and `wikipedia` datasets.

Training on this size of a dataset is out of scope for this notebook but example code is shown below on how to build the BERT dataset.

```python
bookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20220301.en", split="train")
wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])
dataset = concatenate_datasets([bookcorpus, wiki])
```

Then the same steps to train the tokenizer and model can be run. The dataset is 25GB compressed, so it will take some space and time to process!
"""

"""
# Wrapping up

This notebook covered how to build micromodels from scratch with txtai. Micromodels can be fully rebuilt in hours using the most up-to-date knowledge available. If properly constructed, prepared and trained, micromodels have the potential to be a viable choice for limited resource environments. They can also help when realtime response is more important than having the highest accuracy scores.

It's our hope that further research and exploration into micromodels leads to productive and useful models.
"""



================================================
FILE: examples/42_Prompt_driven_search_with_LLMs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Prompt-driven search with LLMs

This notebook revisits the RAG pipeline, which has been covered in a number of previous notebooks. This pipeline is a combination of a similarity instance (embeddings or similarity pipeline) to build a question context and a model that answers questions.

The RAG pipeline recently underwent a number of major upgrades to support the following.

- Ability to run embeddings searches. Given that content is supported, text can be retrieved from the embeddings instance.
- In addition to extractive qa, support text generation models, sequence to sequence models and custom pipelines

These changes enable embeddings-guided and prompt-driven search with Large Language Models (LLMs) ğŸ”¥ğŸ”¥ğŸ”¥
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai datasets

"""
# Create Embeddings and RAG instances

An Embeddings instance defines methods to represent text as vectors and build vector indexes for search.

The RAG pipeline is a combination of a similarity instance (embeddings or similarity pipeline) to build a question context and a model that answers questions. The model can be a prompt-driven large language model (LLM), an extractive question-answering model or a custom pipeline.

Let's run a basic example.

"""

%%capture

from txtai import Embeddings, RAG

# Create embeddings model with content support
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True})

# Create RAG instance
rag = RAG(embeddings, "google/flan-t5-base")

data = ["Giants hit 3 HRs to down Dodgers",
        "Giants 5 Dodgers 4 final",
        "Dodgers drop Game 2 against the Giants, 5-4",
        "Blue Jays beat Red Sox final score 2-1",
        "Red Sox lost to the Blue Jays, 2-1",
        "Blue Jays at Red Sox is over. Score: 2-1",
        "Phillies win over the Braves, 5-0",
        "Phillies 5 Braves 0 final",
        "Final: Braves lose to the Phillies in the series opener, 5-0",
        "Lightning goaltender pulled, lose to Flyers 4-1",
        "Flyers 4 Lightning 1 final",
        "Flyers win 4-1"]

def prompt(question):
  return f"""
    Answer the following question using the context below.
    Question: {question}
    Context:
  """

questions = ["What team won the game?", "What was score?"]

execute = lambda query: rag([(question, query, prompt(question), False) for question in questions], data)

for query in ["Red Sox - Blue Jays", "Phillies - Braves", "Dodgers - Giants", "Flyers - Lightning"]:
    print("----", query, "----")
    for answer in execute(query):
        print(answer)
    print()
# Output:
#   ---- Red Sox - Blue Jays ----

#   ('What team won the game?', 'Blue Jays')

#   ('What was score?', '2-1')

#   

#   ---- Phillies - Braves ----

#   ('What team won the game?', 'Phillies')

#   ('What was score?', '5-0')

#   

#   ---- Dodgers - Giants ----

#   ('What team won the game?', 'Giants')

#   ('What was score?', '5-4')

#   

#   ---- Flyers - Lightning ----

#   ('What team won the game?', 'Flyers')

#   ('What was score?', '4-1')

#   


"""
This code runs a series of questions. First it runs an embeddings filtering query to find the most relevant text. For example, `Red Sox - Blue Jays` finds text related to those teams. Then `What team won the game?` and `What was the score?` are asked.

This logic is the same logic found in Notebook 5 - Extractive QA with txtai but uses prompt-based QA vs extractive QA. 
"""

"""
# Embeddings-guided and Prompt-driven Search

Now for the fun stuff. Let's build an embeddings index for the `ag_news` dataset (a set of news stories from the mid 2000s). Then we'll use prompts to ask questions with embeddings results as the context.
"""

from datasets import load_dataset

dataset = load_dataset("ag_news", split="train")

# List of all text elements
texts = dataset["text"]

# Create an embeddings index over the dataset
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True})
embeddings.index((x, text, None) for x, text in enumerate(texts))

# Create RAG instance
rag = RAG(embeddings, "google/flan-t5-large")

"""
Now let's run a prompt-driven search!
"""

def prompt(question):
  return f"""Answer the following question using only the context below. Say 'no answer' when the question can't be answered.
Question: {question}
Context: """

def search(query, question=None):
  # Default question to query if empty
  if not question:
    question = query

  return rag([("answer", query, prompt(question), False)])[0][1]

question = "Who won the 2004 presidential election?"
answer = search(question)
print(question, answer)

nquestion = "Who did the candidate beat?"
print(nquestion, search(f"{question} {answer}. {nquestion}"))
# Output:
#   Who won the 2004 presidential election? George W. Bush

#   Who did the candidate beat? John F. Kerry


"""
And there are the answers. Let's unpack how this works.

The first thing the RAG pipeline does is run an embeddings search to find the most relevant text within the index. A context string is then built using those search results.

After that, a prompt is generated, run and the answer printed. Let's see what a full prompt looks like.
"""

text = prompt(question)
text += "\n" + "\n".join(x["text"]for x in embeddings.search(question))
print(text)
# Output:
#   Answer the following question using only the context below. Say 'no answer' when the question can't be answered.

#   Question: Who won the 2004 presidential election?

#   Context: 

#   Right- and left-click politics The 2004 presidential race ended last week in a stunning defeat for Massachusetts Senator John F. Kerry, as incumbent President George W. Bush cruised to an easy victory.

#   2004 Presidential Endorsements (AP) AP - Newspaper endorsements in the 2004 presidential campaign between President Bush, a Republican, and Sen. John Kerry, a Democrat.

#   Presidential Campaign to Nov. 2, 2004 (Reuters) Reuters - The following diary of events\leading up to the presidential election on Nov. 2.


"""
The prompt has the information needed to determine the answers to the questions.
"""

"""
# Additional examples

Before moving on, a couple more example questions.
"""

question = "Who won the World Series in 2004?"
answer = search(question)
print(question, answer)

nquestion = "Who did they beat?"
print(nquestion, search(f"{question} {answer}. {nquestion}"))
# Output:
#   Who won the World Series in 2004? Boston

#   Who did they beat? St Louis


search("Tell me something interesting?")
# Output:
#   'herrings communicate by farting'

"""
Whhaaaattt???  Is this a model hallucination?

Let's run an embeddings query and see if that text is in the results.
"""

answer = "herrings communicate by farting"
for x in embeddings.search("Tell me something interesting?"):
  if answer in x["text"]:
    start = x["text"].find(answer)
    print(x["text"][start:start + len(answer)])
# Output:
#   herrings communicate by farting


"""
Sure enough it is. It appears that the FLAN-T5 model has a bit of an immature sense of humor ğŸ˜ƒ
"""

"""
# External API Integration

In addition to support for Hugging Face models, the RAG pipeline also supports custom question-answer models. This could be a call to the OpenAI API (GPT-3), Cohere API, Hugging Face API or using langchain to manage that. All that is needed is a Callable object or a function!

Let's see an example that uses the Hugging Face API to answer questions. We'll use the original sports dataset to demonstrate.
"""

import requests

data = ["Giants hit 3 HRs to down Dodgers",
        "Giants 5 Dodgers 4 final",
        "Dodgers drop Game 2 against the Giants, 5-4",
        "Blue Jays beat Red Sox final score 2-1",
        "Red Sox lost to the Blue Jays, 2-1",
        "Blue Jays at Red Sox is over. Score: 2-1",
        "Phillies win over the Braves, 5-0",
        "Phillies 5 Braves 0 final",
        "Final: Braves lose to the Phillies in the series opener, 5-0",
        "Lightning goaltender pulled, lose to Flyers 4-1",
        "Flyers 4 Lightning 1 final",
        "Flyers win 4-1"]

def prompt(question):
  return f"""
    Answer the following question using the context below.
    Question: {question}
    Context:
  """

# Submits a series of prompts to the Hugging Face API.
# This call can easily be switched to use the OpenAI API (GPT-3), Cohere API or a library like langchain.
def api(prompts):
  response = requests.post("https://api-inference.huggingface.co/models/google/flan-t5-base",
                           json={"inputs": prompts})

  return [x["generated_text"] for x in response.json()]

# Create embeddings model with content support
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True})

# Create RAG instance, submit prompts to the Hugging Face inference API
rag = RAG(embeddings, api)

questions = ["What team won the game?", "What was score?"]

execute = lambda query: rag([(question, query, prompt(question), False) for question in questions], data)

for query in ["Red Sox - Blue Jays", "Phillies - Braves", "Dodgers - Giants", "Flyers - Lightning"]:
    print("----", query, "----")
    for answer in execute(query):
        print(answer)
    print()

# Output:
#   ---- Red Sox - Blue Jays ----

#   ('What team won the game?', 'Blue Jays')

#   ('What was score?', '2-1')

#   

#   ---- Phillies - Braves ----

#   ('What team won the game?', 'Phillies')

#   ('What was score?', '5-0')

#   

#   ---- Dodgers - Giants ----

#   ('What team won the game?', 'Giants')

#   ('What was score?', '5-4')

#   

#   ---- Flyers - Lightning ----

#   ('What team won the game?', 'Flyers')

#   ('What was score?', '4-1')

#   


"""
Everything matches with first example above in [Create Embeddings and RAG instances](#Create-Embeddings-and-RAG-instances) except the prompts are run as an external API call.

The Embeddings instance can also swap out the vectorization, database and vector store components with external API services. Check out the [txtai documentation](https://neuml.github.io/txtai/embeddings/configuration/) documentation for more information.


"""

"""
# Wrapping up

This notebook covered how to run embeddings-guided and prompt-driven search with LLMs. This functionality is a major step forward towards `Generative Semantic Search` for txtai. More to come, stay tuned!
"""



================================================
FILE: examples/43_Embeddings_in_the_Cloud.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Embeddings in the Cloud

Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. 

In addition to local storage, embeddings can be synced with cloud storage. Given that txtai is a fully encapsulated index format, cloud sync is simply a matter of moving a group of files to and from cloud storage. This can be object storage such as AWS S3/Azure Blob/Google Cloud or the [Hugging Face Hub](https://hf.co/models). More details on available options can be found in the [documentation](https://neuml.github.io/txtai/embeddings/configuration/cloud/). There is also an [article](https://medium.com/neuml/serverless-vector-search-with-txtai-96f6163ab972) available that covers how to build and store indexes in cloud object storage.

This notebook will cover an example of loading embeddings indexes from the Hugging Face Hub.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Integration with the Hugging Face Hub

The Hugging Face Hub has a vast array of models, datasets and example applications available to jumpstart your project. This now includes txtai indexes ğŸ”¥ğŸ”¥ğŸ”¥

Let's load the embeddings used in the standard [Introducing txtai](https://huggingface.co/NeuML/txtai-intro) example.

"""

%%capture
from txtai.embeddings import Embeddings

# Load the index from the Hub
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-intro")

"""
Notice the two fields, `provider` and `container`. The `provider` field tells txtai to look for an index in the Hugging Face Hub. The `container` field sets the target repository.
"""

print("%-20s %s" % ("Query", "Best Match"))
print("-" * 50)

# Run an embeddings search for each query
for query in ("feel good story", "climate change", "public health story", "war", "wildlife", "asia", "lucky", "dishonest junk"):
    # Get to the top result
    result = embeddings.search(query, 1)[0]

    # Print text
    print("%-20s %s" % (query, result["text"]))
# Output:
#   Query                Best Match

#   --------------------------------------------------

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day


"""
If you've seen txtai before, this is the classic example. The big difference though is the index was loaded from the Hugging Face Hub instead of being built dynamically.
"""

"""
# Wikipedia search with txtai

Let's try something more interesting using the [Wikipedia index available on the Hugging Face Hub](https://huggingface.co/NeuML/txtai-wikipedia)
"""

%%capture
from txtai.embeddings import Embeddings

# Load the index from the Hub
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

from IPython.display import HTML, display

def wrap():
  display(HTML("""<style>pre { white-space: pre-wrap; }</style>"""))

get_ipython().events.register('pre_run_cell', wrap)

"""
Now run a series of searches to show the kind of data available in this index.
"""

import json

for x in embeddings.search("Roman Empire", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "Roman Empire",

#     "text": "The Roman Empire ( ; ) was the post-Republican period of ancient Rome. As a polity, it included large territorial holdings around the Mediterranean Sea in Europe, North Africa, and Western Asia, and was ruled by emperors. From the accession of Caesar Augustus as the first Roman emperor to the military anarchy of the 3rd century, it was a Principate with Italia as the metropole of its provinces and the city of Rome as its sole capital. The Empire was later ruled by multiple emperors who shared control over the Western Roman Empire and the Eastern Roman Empire. The city of Rome remained the nominal capital of both parts until AD 476 when the imperial insignia were sent to Constantinople following the capture of the Western capital of Ravenna by the Germanic barbarians. The adoption of Christianity as the state church of the Roman Empire in AD 380 and the fall of the Western Roman Empire to Germanic kings conventionally marks the end of classical antiquity and the beginning of the Middle Ages. Because of these events, along with the gradual Hellenization of the Eastern Roman Empire, historians distinguish the medieval Roman Empire that remained in the Eastern provinces as the Byzantine Empire.",

#     "score": 0.8913329243659973

#   }


for x in embeddings.search("How does a car engine work", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "Internal combustion engine",

#     "text": "An internal combustion engine (ICE or IC engine) is a heat engine in which the combustion of a fuel occurs with an oxidizer (usually air) in a combustion chamber that is an integral part of the working fluid flow circuit. In an internal combustion engine, the expansion of the high-temperature and high-pressure gases produced by combustion applies direct force to some component of the engine. The force is typically applied to pistons (piston engine), turbine blades (gas turbine), a rotor (Wankel engine), or a nozzle (jet engine). This force moves the component over a distance, transforming chemical energy into kinetic energy which is used to propel, move or power whatever the engine is attached to. This replaced the external combustion engine for applications where the weight or size of an engine was more important.",

#     "score": 0.8664469122886658

#   }


for x in embeddings.search("Who won the World Series in 2022?", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "2022 World Series",

#     "text": "The 2022 World Series was the championship series of Major League Baseball's (MLB) 2022 season. The 118th edition of the World Series, it was a best-of-seven playoff between the American League (AL) champion Houston Astros and the National League (NL) champion Philadelphia Phillies. The Astros defeated the Phillies in six games to earn their second championship. The series was broadcast in the United States on Fox television and ESPN Radio. ",

#     "score": 0.8889098167419434

#   }


for x in embeddings.search("What was New York called under the Dutch?", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "Dutch Americans in New York City",

#     "text": "Dutch people have had a continuous presence in New York City for nearly 400 years, being the earliest European settlers. New York City traces its origins to a trading post founded on the southern tip of Manhattan Island by Dutch colonists in 1624. The settlement was named New Amsterdam in 1626 and was chartered as a city in 1653. Because of the history of Dutch colonization, Dutch culture, politics, law, architecture, and language played a formative role in shaping the culture of the city. The Dutch were the majority in New York City until the early 1700s and the Dutch language was commonly spoken until the mid to late-1700s. Many places and institutions in New York City still bear a colonial Dutch toponymy, including Brooklyn (Breukelen), Harlem (Haarlem), Wall Street (Waal Straat), The Bowery (bouwerij (\u201cfarm\u201d), and Coney Island (conyne).",

#     "score": 0.8840358853340149

#   }


"""
It's now probably clear how these results can be combined with another component (such as a LLM prompt) to build a conversational QA-based system!
"""

"""
# Filter by popularity

Let's try one last query. This is a generic query where there are a lot of matching results with similarity search alone.
"""

for x in embeddings.search("Boston", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "Boston (song)",

#     "text": "\"Boston\" is a song by American rock band Augustana, from their debut album All the Stars and Boulevards (2005). It was originally produced in 2003 by Jon King for their demo, Midwest Skies and Sleepless Mondays, and was later re-recorded with producer Brendan O'Brien for All the Stars and Boulevards.",

#     "score": 0.8729256987571716

#   }


"""
While the result is about Boston, it's not the most popular result. This is where the `percentile` field comes in to help. The results can be filtered based on the number of page views.


"""

for x in embeddings.search("SELECT id, text, score, percentile FROM txtai WHERE similar('Boston') AND percentile >= 0.99", 1):
  print(json.dumps(x, indent=2))
# Output:
#   <IPython.core.display.HTML object>
#   {

#     "id": "Boston",

#     "text": "Boston, officially the City of Boston, is the state capital and most populous city of the Commonwealth of Massachusetts, as well as the cultural and financial center of the New England region of the United States. It is the 24th-most populous city in the country. The city boundaries encompass an area of about  and a population of 675,647 as of 2020. It is the seat of Suffolk County (although the county government was disbanded on July 1, 1999). The city is the economic and cultural anchor of a substantially larger metropolitan area known as Greater Boston, a metropolitan statistical area (MSA) home to a census-estimated 4.8\u00a0million people in 2016 and ranking as the tenth-largest MSA in the country. A broader combined statistical area (CSA), generally corresponding to the commuting area and including Providence, Rhode Island, is home to approximately 8.2\u00a0million people, making it the sixth most populous in the United States.",

#     "score": 0.8668985366821289,

#     "percentile": 0.9999025135905505

#   }


"""
This query adds an additional filter to only match results for the Top 1% of visited Wikipedia pages. 
"""

"""
# Wrapping up

This notebook covered how to load embeddings indexes from cloud storage. The Hugging Face Hub is a great resource for sharing models, datasets, example applications and now txtai embeddings indexes. This is especially useful when indexing time is long or requires significant GPU resources.

Looking forward to seeing what embeddings indexes the community shares in the coming months!
"""



================================================
FILE: examples/44_Prompt_templates_and_task_chains.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Prompt templates and task chains

txtai has long had support for workflows. Workflows connect the input and outputs of machine learning models together to create powerful transformation and processing functions.

There has been a recent surge in interest in "model prompting", which is the process of building a natural language description of a task and passing it to a large language model (LLM). txtai has recently improved support for task templating, which builds string outputs from a set of parameters.

This notebook demonstrates how txtai workflows can be used to apply prompt templates and chain those tasks together.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api]

"""
# Prompt workflow

First, we'll look at building a workflow with a series of model prompts. This workflow creates a conditional translation using a statement and target language. Another task reads that output text and detects the language.

This workflow uses a sequences pipeline. The sequences pipeline loads a Hugging Face sequence to sequence model for inference, in this case [FLAN-T5](https://huggingface.co/google/flan-t5-base). The sequences pipeline takes a prompt as input and outputs the model inference result.

It's important to note that a pipeline is simply a callable function. It can easily be replaced with a call to an external API.
"""

from txtai.pipeline import Sequences
from txtai.workflow import Workflow, TemplateTask

# Create sequences pipeline
sequences = Sequences("google/flan-t5-large")

# Define workflow or chaining of tasks together.
workflow = Workflow([
    TemplateTask(
        template="Translate '{statement}' to {language} if it's English",
        action=sequences
    ),
    TemplateTask(
        template="What language is the following text? {text}",
        action=sequences
    )
])

inputs = [
    {"statement": "Hello, how are you", "language": "French"},
    {"statement": "Hallo, wie geht's dir", "language": "French"}
]

print(list(workflow(inputs)))
# Output:
#   ['French', 'German']


"""
Let's recap what happened here. The first workflow task conditionally translates text to a language if it's English.

The first statement is `Hello, how are you` with a target language of French. So the statement is translated to French.

The second statement is German, so it's not converted to French.

The next step asks the model what the language is and it correctly prints `French` and `German`.
"""

"""
# Prompt Workflow as YAML

The same workflow above can be created with YAML configuration.
"""

%%writefile workflow.yml

sequences:
  path: google/flan-t5-large

workflow:
  chain:
    tasks:
      - task: template
        template: Translate '{statement}' to {language} if it's English
        action: sequences
      - task: template
        template: What language is the following text? {text}
        action: sequences
# Output:
#   Writing workflow.yml


from txtai import Application

app = Application("workflow.yml")
print(list(app.workflow("chain", inputs)))
# Output:
#   ['French', 'German']


"""
As expected, the same result! This is a matter of preference on how you want to create a workflow. One advantage of YAML workflows is that an API can easily be created from the workflow file.
"""

"""
# Prompt Workflow via an API call

Let's say you want the workflow to be available via an API call. Well good news, txtai has a built in API mechanism using FastAPI. 
"""

# Start an API service
!CONFIG=workflow.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 60

import requests

# Run API request
requests.post("http://localhost:8000/workflow", json={"name": "chain", "elements": inputs}).json()
# Output:
#   ['French', 'German']

"""
Just like the previous steps, except through an API call. Let's run via cURL for good measure.
"""

%%bash

curl -s -X POST "http://localhost:8000/workflow" \
     -H "Content-Type: application/json" \
     --data @- << EOF
{
  "name": "chain",
  "elements": [
    {"statement": "Hello, how are you", "language": "French"},
    {"statement": "Hallo, wie geht's dir", "language": "French"}
  ]
}
EOF
# Output:
#   ["French","German"]

"""
One last time, the same output is shown.

If your primary development environment isn't Python, txtai does have API bindings for [JavaScript](https://github.com/neuml/txtai.js), [Rust](https://github.com/neuml/txtai.rs), [Go](https://github.com/neuml/txtai.go) and [Java](https://github.com/neuml/txtai.java).

More information on the API is available [here](https://neuml.github.io/txtai/api/).
"""

"""
# Conversational chain

Conversational search is another big area of focus in 2023. [txtchat](https://github.com/neuml/txtchat) is a framework for building conversational search applications. It relies heavily on txtai. Let's see a conversational example.
"""

%%writefile search.yml

writable: false
cloud:
  provider: huggingface-hub
  container: neuml/txtai-intro

rag:
  path: google/flan-t5-large
  output: reference

workflow:
  search:
    tasks:
      - task: rag
        template: |
          Answer the following question using only the context below. Give a detailed answer.
          Say 'I don't have data on that' when the question can't be answered.
          Question: {text}
          Context: 
        action: rag
      - task: template
        template: "{answer}\n\nReference: {reference}"
        rules:
          answer: I don't have data on that
# Output:
#   Overwriting search.yml


app = Application("search.yml")
print(list(app.workflow("search", ["Tell me something about North America"])))
# Output:
#   ["Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\n\nReference: 1"]


"""
The first thing the code above does is run an embeddings search to build a conversational context. That context is then used to build a prompt and inference is run against the LLM. 

The next task formats the outputs with a reference to the best matching record. In this case, it's only an id of 1. But this can be much more useful if the id is a URL or there is logic to format the id back to a unique reference string.

The [txtchat](https://github.com/neuml/txtchat) project has much more on this, check it out!
"""

"""
# Wrapping up

This notebook covered how to build prompt templates and task chains through a series of results. txtai has long had a robust and efficient workflow framework for connecting models together. This can be small and simple models and/or prompting with large models. Go ahead and give it a try!
"""



================================================
FILE: examples/45_Customize_your_own_embeddings_database.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Customize your own embeddings database

txtai supports a number of different database and vector index backends, including external databases. With modern hardware, it's amazing how far a single node index can take us. Easily into the hundreds of millions and even billions of records.

txtai provides maximum flexibility in creating your own embeddings database. Sensible defaults are used out of the box. So unless you seek out this configuration, it's not necessary. This notebook will explore the options available when you do want to customize your embeddings database.

More on [embeddings configuration settings can be found here](https://neuml.github.io/txtai/embeddings/configuration). 
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[database,similarity] datasets

"""
# Load dataset

This example will use the `ag_news` dataset, which is a collection of news article headlines. We'll use a subset of 25,000 headlines.
"""

import timeit

from datasets import load_dataset

def timer(embeddings, query="red sox"):
  elapsed = timeit.timeit(lambda: embeddings.search(query), number=250)
  print(f"{elapsed / 250} seconds per query")

dataset = load_dataset("ag_news", split="train")["text"][:25000]

"""
# NumPy

Let's start with the simplest possible embeddings database. This will just be a thin wrapper around vectorizing text with sentence-transformers, storing the results as a NumPy array and running similarity queries.
"""

from txtai.embeddings import Embeddings

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "backend": "numpy"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

embeddings.search("red sox")
# Output:
#   [(19831, 0.6780003309249878),

#    (18302, 0.6639199256896973),

#    (16370, 0.6617192029953003)]

embeddings.info()
# Output:
#   {

#     "backend": "numpy",

#     "build": {

#       "create": "2023-05-16T13:38:32Z",

#       "python": "3.10.11",

#       "settings": {

#         "numpy": "1.22.4"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "dimensions": 384,

#     "offset": 25000,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2023-05-16T13:38:32Z"

#   }


"""
The embeddings instance above vectorizes the text and stores the content as a NumPy array. Array index positions are returned with similarity scores. While the same can easily be done using sentence-transformers, using the txtai framework makes it easy to swap out different options as seen next.
"""

"""
# SQLite and NumPy

The next combination we'll test is a SQLite database with a NumPy array.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": "sqlite", "backend": "numpy"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

"""
Now let's run a search.
"""

embeddings.search("red sox")
# Output:
#   [{'id': '19831',

#     'text': 'Boston Red Sox Team Report - September 6 (Sports Network) - Two of the top teams in the American League tangle in a possible American League Division Series preview tonight, as the West-leading Oakland Athletics host the wild card-leading Boston Red Sox for the first of a three-game set at the ',

#     'score': 0.6780003309249878},

#    {'id': '18302',

#     'text': 'BASEBALL: RED-HOT SOX CLIP THE ANGELS #39; WINGS BOSTON RED SOX fans are enjoying their best week of the season. While their beloved team swept wild-card rivals Anaheim in a three-game series to establish a nine-game winning streak, the hated New York Yankees endured the heaviest loss in their history.',

#     'score': 0.6639199256896973},

#    {'id': '16370',

#     'text': 'Boston Red Sox Team Report - September 1 (Sports Network) - The red-hot Boston Red Sox hope to continue rolling as they continue their three-game set with the Anaheim Angels this evening at Fenway Park.',

#     'score': 0.6617192029953003}]

embeddings.info()
# Output:
#   {

#     "backend": "numpy",

#     "build": {

#       "create": "2023-05-16T13:38:52Z",

#       "python": "3.10.11",

#       "settings": {

#         "numpy": "1.22.4"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "content": "sqlite",

#     "dimensions": 384,

#     "offset": 25000,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2023-05-16T13:38:52Z"

#   }


"""
Same results as before. The only difference is the content is now available via the associated SQLite database. 

Let's inspect the ANN object to see how it looks. 
"""

print(embeddings.ann.backend.shape)
print(type(embeddings.ann.backend))
# Output:
#   (25000, 384)

#   <class 'numpy.memmap'>


"""
As expected, it's a NumPy array. Let's calculate how long a search query takes to execute.

"""

timer(embeddings)
# Output:
#   0.028768999292000445 seconds per query


"""
Not too bad at all!


"""

"""
# SQLite and PyTorch

Let's now try a PyTorch backend.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": "sqlite", "backend": "torch"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

"""
Let's run a search again.
"""

embeddings.search("red sox")
# Output:
#   [{'id': '19831',

#     'text': 'Boston Red Sox Team Report - September 6 (Sports Network) - Two of the top teams in the American League tangle in a possible American League Division Series preview tonight, as the West-leading Oakland Athletics host the wild card-leading Boston Red Sox for the first of a three-game set at the ',

#     'score': 0.678000271320343},

#    {'id': '18302',

#     'text': 'BASEBALL: RED-HOT SOX CLIP THE ANGELS #39; WINGS BOSTON RED SOX fans are enjoying their best week of the season. While their beloved team swept wild-card rivals Anaheim in a three-game series to establish a nine-game winning streak, the hated New York Yankees endured the heaviest loss in their history.',

#     'score': 0.6639199256896973},

#    {'id': '16370',

#     'text': 'Boston Red Sox Team Report - September 1 (Sports Network) - The red-hot Boston Red Sox hope to continue rolling as they continue their three-game set with the Anaheim Angels this evening at Fenway Park.',

#     'score': 0.6617191433906555}]

embeddings.info()
# Output:
#   {

#     "backend": "torch",

#     "build": {

#       "create": "2023-05-16T13:39:19Z",

#       "python": "3.10.11",

#       "settings": {

#         "torch": "2.0.0+cu118"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "content": "sqlite",

#     "dimensions": 384,

#     "offset": 25000,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2023-05-16T13:39:19Z"

#   }


"""
And once against inspect the ANN object.
"""

print(embeddings.ann.backend.shape)
print(type(embeddings.ann.backend))
# Output:
#   torch.Size([25000, 384])

#   <class 'torch.Tensor'>


"""
As expected, this time the backend is a Torch tensor. Next we'll calculate the average search time.
"""

timer(embeddings)
# Output:
#   0.0198183048359997 seconds per query


"""
A bit faster since Torch uses the GPU to compute the similarity matrix.
"""

"""
# SQLite and Faiss

Now lets run the same code with the standard txtai settings of Faiss + SQLite.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

embeddings.search("red sox")
# Output:
#   [{'id': '19831',

#     'text': 'Boston Red Sox Team Report - September 6 (Sports Network) - Two of the top teams in the American League tangle in a possible American League Division Series preview tonight, as the West-leading Oakland Athletics host the wild card-leading Boston Red Sox for the first of a three-game set at the ',

#     'score': 0.6780003309249878},

#    {'id': '18302',

#     'text': 'BASEBALL: RED-HOT SOX CLIP THE ANGELS #39; WINGS BOSTON RED SOX fans are enjoying their best week of the season. While their beloved team swept wild-card rivals Anaheim in a three-game series to establish a nine-game winning streak, the hated New York Yankees endured the heaviest loss in their history.',

#     'score': 0.6639199256896973},

#    {'id': '16370',

#     'text': 'Boston Red Sox Team Report - September 1 (Sports Network) - The red-hot Boston Red Sox hope to continue rolling as they continue their three-game set with the Anaheim Angels this evening at Fenway Park.',

#     'score': 0.6617192029953003}]

embeddings.info()
# Output:
#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2023-05-16T13:39:47Z",

#       "python": "3.10.11",

#       "settings": {

#         "components": "IVF632,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "content": true,

#     "dimensions": 384,

#     "offset": 25000,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2023-05-16T13:39:47Z"

#   }


timer(embeddings)
# Output:
#   0.00825659705599992 seconds per query


"""
Everything lines up with the previous examples. Note that Faiss is faster, given it's a vector index. For 25,000 records, the different is negligible but vector index performance increases rapidly for datasets in the million+ range.
"""

"""
# SQLite and HNSW

While txtai strives to keep things as simple as possible with many common default settings out of the box, customizing the backend options can lead to increased performance. The next example will store vectors in a HNSW index and customize the index options.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True, "backend": "hnsw", "hnsw": {"m": 32}})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

embeddings.search("red sox")
# Output:
#   [{'id': '19831',

#     'text': 'Boston Red Sox Team Report - September 6 (Sports Network) - Two of the top teams in the American League tangle in a possible American League Division Series preview tonight, as the West-leading Oakland Athletics host the wild card-leading Boston Red Sox for the first of a three-game set at the ',

#     'score': 0.678000271320343},

#    {'id': '18302',

#     'text': 'BASEBALL: RED-HOT SOX CLIP THE ANGELS #39; WINGS BOSTON RED SOX fans are enjoying their best week of the season. While their beloved team swept wild-card rivals Anaheim in a three-game series to establish a nine-game winning streak, the hated New York Yankees endured the heaviest loss in their history.',

#     'score': 0.6639199256896973},

#    {'id': '16370',

#     'text': 'Boston Red Sox Team Report - September 1 (Sports Network) - The red-hot Boston Red Sox hope to continue rolling as they continue their three-game set with the Anaheim Angels this evening at Fenway Park.',

#     'score': 0.6617191433906555}]

embeddings.info()
# Output:
#   {

#     "backend": "hnsw",

#     "build": {

#       "create": "2023-05-16T13:40:21Z",

#       "python": "3.10.11",

#       "settings": {

#         "efconstruction": 200,

#         "m": 32,

#         "seed": 100

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "content": true,

#     "deletes": 0,

#     "dimensions": 384,

#     "hnsw": {

#       "m": 32

#     },

#     "metric": "ip",

#     "offset": 25000,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2023-05-16T13:40:21Z"

#   }


timer(embeddings)
# Output:
#   0.006280380824000531 seconds per query


"""
Once again, everything matches up with the previous examples. There is a negligible performance difference vs Faiss.

Hnswlib powers a number of popular vector databases. It's definitely an option worth evaluating.
"""

"""
# External Vectorization

txtai has a number of built-in vectorizers backed by Hugging Face Transformers and Sentence Transformers. Just like other txtai modules, vectorization can also be customized.

The next example uses the Hugging Face Inference API to vectorize text.

"""

import numpy as np
import requests

BASE = "https://api-inference.huggingface.co/pipeline/feature-extraction"

def transform(inputs):
  # Your API provider of choice
  response = requests.post(f"{BASE}/sentence-transformers/all-MiniLM-L6-v2", json={"inputs": inputs})
  return np.array(response.json(), dtype=np.float32)

embeddings = Embeddings({"transform": transform, "backend": "numpy", "content": True})
embeddings.index([(0, "sunny", None), (1, "rainy", None)])
embeddings.search("nice day")  
# Output:
#   [{'id': '0', 'text': 'sunny', 'score': 0.28077083826065063},

#    {'id': '1', 'text': 'rainy', 'score': 0.18051263689994812}]

"""
# Configuration storage

Configuration is passed to an embeddings instance as a dictionary. When saving an embeddings instance, the default behavior is to save configuration as a pickled object. JSON can alternatively be used.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": True, "format": "json"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

# Save embeddings
embeddings.save("index")

!cat index/config.json
# Output:
#   {

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "content": true,

#     "format": "json",

#     "dimensions": 384,

#     "backend": "faiss",

#     "offset": 25000,

#     "build": {

#       "create": "2023-05-16T13:40:49Z",

#       "python": "3.10.11",

#       "settings": {

#         "components": "IVF632,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "5.6.0"

#     },

#     "update": "2023-05-16T13:40:49Z"

#   }

"""
Looking at the stored configuration, it's almost identical to an `embeddings.info()` call. This is by design, JSON configuration is designed to be human-readable. This is a good option when sharing an embeddings database on the [Hugging Face Hub](https://huggingface.co/models).
"""

"""
# SQLite vs DuckDB

The last thing we'll explore is the database backend.

[SQLite](https://sqlite.org/index.html) is a row-oriented database, [DuckDB](https://duckdb.org/) is column-oriented. This design difference is important to note and a factor to consider when evaluating the expected workload. Let's explore.
"""

# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": "sqlite"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

timer(embeddings, "SELECT text FROM txtai where id = 3980")
# Output:
#   0.00012401376399975562 seconds per query


timer(embeddings, "SELECT count(*), text FROM txtai group by text order by count(*) desc")
# Output:
#   0.03863514600000053 seconds per query


# Create embeddings instance
embeddings = Embeddings({"path": "sentence-transformers/all-MiniLM-L6-v2", "content": "duckdb"})

# Index data
embeddings.index((x, text, None) for x, text in enumerate(dataset))

timer(embeddings, "SELECT text FROM txtai where id = 3980")
# Output:
#   0.0038918176440001844 seconds per query


timer(embeddings, "SELECT count(*), text FROM txtai group by text order by count(*) desc")
# Output:
#   0.0198518766039997 seconds per query


"""
While the dataset of 25,000 rows is small, we can start to see the differences. SQLite has a much faster single row retrieval time. DuckDB does better with an aggregate query. This is a product of a row-oriented vs column oriented database and a factor to consider when developing a solution.
"""

"""
# Wrapping up

This notebook explored different combinations of database and vector index backends. With modern hardware, it's amazing how far a single node index can take us. Easily into the hundreds of millions and even billions of records. When a hardware bottleneck becomes an issue, external vector databases are one option to consider. Another is [building a distributed txtai embeddings cluster](https://neuml.github.io/txtai/api/cluster/).

There is power in simplicity. Many paid services try to convince us that signing up for an API account is the best place to start. In some cases, such as teams with very few to no developers, this is true. But for teams with developers, options like txtai should be evaluated.
"""



================================================
FILE: examples/46_Whats_new_in_txtai_6_0.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ğŸ’¡ What's new in txtai 6.0

txtai 6.0 brings a number of major feature enhancements. Highlights include:

- Embeddings
  - Sparse/keyword indexes
  - Hybrid search
  - Subindexes
  - Streamlined methods

- Large Language Models (LLMs)
  - Automatically instantiate the best available underlying model
  - Pass through parameters enabling immediate support as features are released upstream

These are just the big, high level changes. There are also many improvements and bug fixes.

This notebook will cover all the changes with examples.

**Standard upgrade disclaimer below**

6.0 is one of the largest, if not largest releases to date! While almost everything is backwards compatible, it's prudent to backup production indexes before upgrading and test before deploying.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] datasets

"""
# Sparse indexes

While dense vector indexes are by far the best option for semantic search systems, sparse keyword indexes can still add value. There may be cases where finding an exact match is important or we just want a fast index to quickly do an initial scan of the dataset.

Unfortunately, there aren't a ton of great options for a local Python-based keyword index library. Most of the options available don't scale and are highly inefficient, designed only for simple situations. With 6.0, txtai has added a performant sparse index component with speed and accuracy on par with Apache Lucene. A future article will discuss the engineering behind this.

Let's take a look. We'll use a [prompt dataset on the Hugging Face Hub](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts) for all examples.
"""

from datasets import load_dataset

import txtai

# Load dataset
ds = load_dataset("fka/awesome-chatgpt-prompts", split="train")

def stream():
  for row in ds:
    yield f"{row['act']} {row['prompt']}"

# Build sparse keyword index
embeddings = txtai.Embeddings(keyword=True, content=True)
embeddings.index(stream())

embeddings.search("Linux terminal", 1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.5932681465337526}]

"""
And there it is, a keyword index!

Couple things to unpack here. First, for those familar with txtai, notice that only a text field was yielded in the `stream` method. With 6.0, when ids aren't provided, they are automatically generated.

Next notice the score. Those familar with keyword scores (TF-IDF, BM25) will notice that the score seems low. That is because with a keyword index, the default score is normalized between 0 and 1.

More on these items later.
"""

"""
# Hybrid Search

The addition of sparse indexes enables hybrid search. Hybrid search combines the results from sparse and dense vector indexes for the best of both worlds.
"""

# Build hybrid index
embeddings = txtai.Embeddings(hybrid=True, content=True)
embeddings.index(stream())

embeddings.search("Linux terminal", 1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.6078515601252442}]

"""
Simple change with big impacts. This new index now has both a sparse and dense (using default `sentence-transformers/all-MiniLM-L6-v2` model) index. These scores are combined into a single score as seen above.

The scoring weights (also known as alpha) control the weighting between the sparse and dense index.
"""

embeddings.search("Linux terminal", 1, weights=1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.6224349737167358}]

embeddings.search("Linux terminal", 1, weights=0)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.5932681465337526}]

"""
A weight of 1 only uses the dense index and 0 only uses the sparse index. Notice the score with `weight = 0` is the same as the sparse index query earlier.
"""

"""
# Subindexes

While sparse and hybrid indexes are great new features, the prize of this release is the addition of subindexes. Subindexes will add a host of new ways to build txtai embeddings instances. Let's give a brief intro here.
"""

# Build index with subindexes
embeddings = txtai.Embeddings(
    content=True,
    defaults=False,
    indexes={
        "sparse": {
            "keyword": True
        },
        "dense":{

        }
    }
)
embeddings.index(stream())

# Run search
embeddings.search("select id, text, score from txtai where similar('Linux terminal', 'sparse') and similar('Linux terminal', 'dense')", 1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.6078515601252442}]

embeddings.search("select id, text, score from txtai where similar('Linux terminal', 'dense')", 1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.6224349737167358}]

embeddings.search("select id, text, score from txtai where similar('Linux terminal', 'sparse')", 1)
# Output:
#   [{'id': '0',

#     'text': 'Linux Terminal I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.5932681465337526}]

"""
Notice how the scores are the same as above. The three searches above run a hybrid search, dense and sparse search. This time though it's using subindexes. The top-level Embeddings only has an associated database.

Each of the sections in the `indexes` is a full embeddings index supporting all available options. For example, let's add a graph subindex.
"""

# Build index with graph subindex
embeddings = txtai.Embeddings(
    content=True,
    defaults=False,
    functions=[
        {"name": "graph", "function": "indexes.act.graph.attribute"}
    ],
    expressions=[
        {"name": "topic", "expression": "graph(indexid, 'topic')"},
    ],
    indexes={
        "act": {
            "keyword": True,
            "columns": {
                "text": "act"
            },
            "graph": {
                "topics": {}
            }
        },
        "prompt":{
            "columns": {
                "text": "prompt"
            }
        }
    }
)
embeddings.index(ds)

# Run search
embeddings.search("select id, act, prompt, score, topic from txtai where similar('Linux terminal')", 1)
# Output:
#   [{'id': '0',

#     'act': 'Linux Terminal',

#     'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.6382951796072414,

#     'topic': 'terminal_linux_sql'}]

"""
Notice the new `topic` field added to this query. That comes from the graph index, which runs topic modeling. Also notice that two indexes for two different columns are added.

Note that graph indexes are different in that they depend on a sparse or dense index being available. That is how the graph is automatically constructed. For good measure, let's add the graph to a dense index.
"""

# Build index with graph subindex
embeddings = txtai.Embeddings(
    content=True,
    defaults=False,
    functions=[
        {"name": "graph", "function": "indexes.act.graph.attribute"}
    ],
    expressions=[
        {"name": "topic", "expression": "graph(indexid, 'topic')"},
    ],
    indexes={
        "act": {
            "path": "intfloat/e5-small-v2",
            "columns": {
                "text": "act"
            },
            "graph": {
                "topics": {}
            }
        },
        "prompt":{
            "path": "sentence-transformers/all-MiniLM-L6-v2",
            "columns": {
                "text": "prompt"
            }
        }
    }
)
embeddings.index(ds)

# Run search
embeddings.search("select id, act, prompt, score, topic from txtai where similar('Linux terminal')", 1)
# Output:
#   [{'id': '0',

#     'act': 'Linux Terminal',

#     'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 1.0,

#     'topic': 'linux_terminal'}]

"""
Almost the same as above except the topic is different. This is due to the grouping of the vector index. Notice how the `act` column and `prompt` column are both vector indexes but specify different vector models. This opens up another possibility of weighting not only sparse vs vector but different vector models.
"""

embeddings.search("select id, act, prompt, score from txtai where similar('Linux terminal', 'act') and similar('Linux terminal', 'prompt')", 1)
# Output:
#   [{'id': '0',

#     'act': 'Linux Terminal',

#     'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'score': 0.7881423830986023}]

"""
As always, everything discussed so far is also supported with txtai application instances.
"""

# Build index with graph subindex
app = txtai.Application("""
writable: True
embeddings:
  content: True
  defaults: False
  functions:
    - name: graph
      function: indexes.act.graph.attribute
  expressions:
    - name: topic
      expression: graph(indexid, 'topic')
  indexes:
    act:
      path: intfloat/e5-small-v2
      columns:
        text: act
      graph:
        topics:
    prompt:
      path: sentence-transformers/all-MiniLM-L6-v2
      columns:
        text: prompt
""")

app.add(ds)
app.index()

app.search("select id, act, prompt, topic, score from txtai where similar('Linux terminal', 'act') and similar('Linux terminal', 'prompt')", 1)
# Output:
#   [{'id': '0',

#     'act': 'Linux Terminal',

#     'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',

#     'topic': 'linux_terminal',

#     'score': 0.7881423830986023}]

"""
# Streamlined methods

Much of this has been covered already but a number of changes were added to make it easier to search and index data. The existing interfaces are all still supported, this is all about ease of use.

See the code explanations below.
"""

# Top-level import includes Application and Embeddings
import txtai

app = txtai.Application("""writable: False""")
embeddings = txtai.Embeddings()

# Ids are automatically generated when omitted
embeddings.index(["test"])
print(embeddings.search("test"))

# UUID ids are also supported - use any of the methods in https://docs.python.org/3/library/uuid.html
embeddings = txtai.Embeddings(autoid="uuid5")
embeddings.index(["test"])
embeddings.search("test")
# Output:
#   [(0, 0.9999998807907104)]

#   [('4be0643f-1d98-573b-97cd-ca98a65347dd', 0.9999998807907104)]

"""
# Large Language Models (LLMs)

While the bulk of the changes in this release came with the embeddings package, LLMs also have important changes that make it easier to use.
"""

import torch

from txtai import LLM

# Create model and set dtype to use 16-bit floats
llm = LLM("tiiuae/falcon-rw-1b", torch_dtype=torch.bfloat16)

print(llm("Write a short list of things to do in Paris", maxlength=55))
# Output:
#   .

#   - Visit the Eiffel Tower.

#   - Visit the Louvre.

#   - Visit the Arc de Triomphe.

#   - Visit the Notre Dame Cathedral.

#   - Visit the Sacre Coeur Basilica


"""
The new `LLM` pipeline automatically detects the type of model and loads it using the best available method.

The pipeline framework now passes through keyword arguments to the underlying methods, which adds support for new Hugging Face features automatically as they are released.
"""

"""
# Wrapping up

This notebook gave a quick overview of txtai 6.0. Updated documentation and more examples will be forthcoming. There is much to cover and much to build on!

See the following links for more information.

- [6.0 Release on GitHub](https://github.com/neuml/txtai/releases/tag/v6.0.0)
- [Documentation site](https://neuml.github.io/txtai)
"""



================================================
FILE: examples/47_Building_an_efficient_sparse_keyword_index_in_Python.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Building an efficient sparse keyword index in Python

Semantic search is a new category of search built on recent advances in Natural Language Processing (NLP). Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.

While semantic search adds amazing capabilities, sparse keyword indexes can still add value. There may be cases where finding an exact match is important or we just want a fast index to quickly do an initial scan of a dataset.

Unfortunately, there aren't a ton of great options for a local Python-based keyword index library. Most of the options available don't scale and/or are highly inefficient, designed only for simple situations.

Given that Python is an interpreted language, it often gets a bad rap from a performance standpoint. In some cases, it's justified as Python can be memory hungry and has a global interpreter lock (GIL) that forces single thread execution. But it is possible to build performant Python on par with other languages.

This notebook will explore how to build an efficient sparse keyword index in Python and compare the results with other approaches.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install txtai pytrec_eval rank-bm25 elasticsearch==7.10.1
!pip uninstall -y tensorflow

"""
# Introducing the problem

At a high level, keyword indexes work by tokenizing text into lists of tokens per document. These tokens are aggregated into frequencies per document and stored in term frequency sparse arrays.

The term frequency arrays are sparse given that they only store a frequency when the token exists in a document. For example, if a token exists in 1 of 1000 documents, the sparse array only has a single entry. A dense array stores 1000 entries all with zeros except for one.

One simple approach to store a term frequency sparse array in Python would be having a dictionary of `{id: frequency}` per token. The problem with this approach is that Python has significant object overhead.

Let's inspect the size used for a single number.
"""

import sys

a = 100
sys.getsizeof(a)
# Output:
#   28

"""
28 bytes for a single integer. Compared to a native int/long which is 4 or 8 bytes, this is quite wasteful. Imagine having thousands of `id: frequency` mappings. Memory usage will grow fast.

Let's demonstrate. The code below runs a self contained Python process that creates a list of 10 million numbers.

Running as a separate process helps calculate more accurate memory usage stats.
"""

%%writefile arrays.py
import psutil

results = []
for x in range(int(1e7)):
  results.append(x)

print(f"MEMORY USAGE = {psutil.Process().memory_info().rss / (1024 * 1024)} MB")
# Output:
#   Writing arrays.py


!python arrays.py
# Output:
#   MEMORY USAGE = 394.640625 MB


"""
Approximately 395 MB of memory is used for this array. That seems high.
"""

"""
# Efficient numeric arrays in Python

Fortunately, Python has a module for building [efficient arrays of numeric values](https://docs.python.org/3/library/array.html). This module enables building arrays with the same native type.

Let's try doing that with a `long long` type, which takes 8 bytes.
"""

%%writefile arrays.py
from array import array

import psutil

results = array("q")
for x in range(int(1e7)):
  results.append(x)

print(f"MEMORY USAGE = {psutil.Process().memory_info().rss / (1024 * 1024)} MB")
# Output:
#   Overwriting arrays.py


!python arrays.py
# Output:
#   MEMORY USAGE = 88.54296875 MB


"""
As we can see, memory usage went from 395 MB to 89 MB. That's a 4x reduction which is in line with the earlier calculate of 28 bytes/number vs 8 bytes/number.
"""

"""
# Efficient processing of numeric data

Large computations in pure Python can also be painfully slow. Luckily, there is a robust landscape of options for numeric processing. The most popular framework is [NumPy](https://github.com/numpy/numpy). There is also [PyTorch](https://github.com/pytorch/pytorch) and other GPU-based tensor processing frameworks.

Below is a simple example that sorts an array in Python vs NumPy to demonstrate.
"""

import random
import time

data = [random.randint(1, 500) for x in range(1000000)]

start = time.time()
sorted(data, reverse=True)
print(time.time() - start)
# Output:
#   0.33922290802001953


import numpy as np

data = np.array(data)

start = time.time()
np.sort(data)[::-1]
print(time.time() - start)
# Output:
#   0.10296249389648438


"""
As we can see, sorting an array in NumPy is significantly faster. It might not seem like a lot but this adds up when run in bulk.
"""

"""
# Sparse keyword indexes in txtai

Now that we've discussed the key performance concepts, let's talk about how to apply this to building sparse keyword indexes.

Going back to the original approach for a term frequency sparse array, we see that using the Python array package is more efficient. In txtai, this method is used to build term frequency arrays for each token. This results in near native speed and memory usage.

The search method uses a number of NumPy methods to efficiently calculate query term matches. Each query is tokenized and those token term frequency arrays are retrieved to calculate query scores. These NumPy methods are all written in C and often drop the GIL. So once again, near native speed and the ability to use multithreading.

Read the [full implementation on GitHub](https://github.com/neuml/txtai/blob/master/src/python/txtai/scoring/terms.py) to learn more.

"""

"""
# Evaluating performance

First, a review of the landscape. As said in the introduction, there aren't a ton of good options. [Apache Lucene](https://github.com/apache/lucene) is by far the best traditional search index from a speed, performance and functionality standpoint. It's the base for Elasticsearch/OpenSearch and many other projects. But it requires Java.

Here are the options we'll explore.

- [Rank-BM25](https://github.com/dorianbrown/rank_bm25) project, the top result when searching for `python bm25`.

- [SQLite FTS5](https://www.sqlite.org/fts5.html) extension. This extension builds a sparse keyword index right in SQLite.

We'll use the BEIR dataset. We'll also use a [benchmarks script](https://raw.githubusercontent.com/neuml/txtai/master/examples/benchmarks.py) from the txtai project. This benchmarks script has methods to work with the BEIR dataset.

Couple important caveats on the benchmarks script.

- For the SQLite FTS implementation, each token is joined together with an `OR` clause. SQLite FTS [implicitly joins clauses together](https://www.sqlite.org/fts5.html) with `AND` clauses by default. By contrast, [Lucene's default operator](https://lucene.apache.org/core/9_7_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#Boolean_operators) is an `OR`.
- The Elasticsearch implementation uses 7.x as it's simpler to instantiate in a notebook.
- All methods except Elasticsearch use txtai's [unicode tokenizer](https://github.com/neuml/txtai/blob/master/src/python/txtai/pipeline/data/tokenizer.py) to tokenize text for consistency
"""

%%capture
import os

# Get benchmarks script
os.system("wget https://raw.githubusercontent.com/neuml/txtai/master/examples/benchmarks.py")

# Create output directory
os.makedirs("beir", exist_ok=True)

# Download subset of BEIR datasets
datasets = ["trec-covid", "nfcorpus", "webis-touche2020", "scidocs", "scifact"]
for dataset in datasets:
  url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip"
  os.system(f"wget {url}")
  os.system(f"mv {dataset}.zip beir")
  os.system(f"unzip -d beir beir/{dataset}.zip")

  # Remove existing benchmark data
if os.path.exists("benchmarks.json"):
  os.remove("benchmarks.json")

"""
Now let's run the benchmarks.
"""

# Remove existing benchmark data
if os.path.exists("benchmarks.json"):
  os.remove("benchmarks.json")

# Runs benchmark evaluation
def evaluate(method):
  for dataset in datasets:
    command = f"python benchmarks.py beir {dataset} {method}"
    print(command)
    os.system(command)

# Calculate benchmarks
for method in ["bm25", "rank", "sqlite"]:
  evaluate(method)
# Output:
#   python benchmarks.py beir trec-covid bm25

#   python benchmarks.py beir nfcorpus bm25

#   python benchmarks.py beir webis-touche2020 bm25

#   python benchmarks.py beir scidocs bm25

#   python benchmarks.py beir scifact bm25

#   python benchmarks.py beir trec-covid rank

#   python benchmarks.py beir nfcorpus rank

#   python benchmarks.py beir webis-touche2020 rank

#   python benchmarks.py beir scidocs rank

#   python benchmarks.py beir scifact rank

#   python benchmarks.py beir trec-covid sqlite

#   python benchmarks.py beir nfcorpus sqlite

#   python benchmarks.py beir webis-touche2020 sqlite

#   python benchmarks.py beir scidocs sqlite

#   python benchmarks.py beir scifact sqlite


import json
import pandas as pd

def benchmarks():
  # Read JSON lines data
  with open("benchmarks.json") as f:
    data = f.read()

  df = pd.read_json(data, lines=True).sort_values(by=["source", "search"])
  return df[["source", "method", "index", "memory", "search", "ndcg_cut_10", "map_cut_10", "recall_10", "P_10"]].reset_index(drop=True)

# Load benchmarks dataframe
df = benchmarks()

df[df.source == "trec-covid"].reset_index(drop=True)
# Output:
#          source  method   index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  trec-covid    bm25  101.96     997    0.28      0.58119     0.01247   

#   1  trec-covid  sqlite   60.16     880   23.09      0.56778     0.01190   

#   2  trec-covid    rank   61.75    3245   75.49      0.57773     0.01210   

#   

#      recall_10   P_10  

#   0    0.01545  0.618  

#   1    0.01519  0.610  

#   2    0.01550  0.632  

df[df.source == "nfcorpus"].reset_index(drop=True)
# Output:
#        source  method  index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  nfcorpus    bm25   2.64     648    1.08      0.30639     0.11728   

#   1  nfcorpus  sqlite   1.50     630   12.73      0.30695     0.11785   

#   2  nfcorpus    rank   2.75     700   23.78      0.30692     0.11711   

#   

#      recall_10     P_10  

#   0    0.14891  0.21734  

#   1    0.14871  0.21641  

#   2    0.15320  0.21889  

df[df.source == "webis-touche2020"].reset_index(drop=True)
# Output:
#                source  method   index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  webis-touche2020    bm25  374.66    1137    0.37      0.36920     0.14588   

#   1  webis-touche2020  sqlite  220.46    1416   34.61      0.37194     0.14812   

#   2  webis-touche2020    rank  224.07   10347   81.22      0.39861     0.16492   

#   

#      recall_10     P_10  

#   0    0.22736  0.34694  

#   1    0.22890  0.35102  

#   2    0.23770  0.36122  

df[df.source == "scidocs"].reset_index(drop=True)
# Output:
#       source  method  index  memory  search  ndcg_cut_10  map_cut_10  recall_10  \

#   0  scidocs    bm25  17.95     717    1.64      0.15063     0.08756    0.15637   

#   1  scidocs  sqlite  17.85     670   56.64      0.15156     0.08822    0.15717   

#   2  scidocs    rank  13.11    1056  162.99      0.14932     0.08670    0.15408   

#   

#        P_10  

#   0  0.0772  

#   1  0.0776  

#   2  0.0761  

df[df.source == "scifact"].reset_index(drop=True)
# Output:
#       source  method  index  memory  search  ndcg_cut_10  map_cut_10  recall_10  \

#   0  scifact    bm25   5.51     653    1.07      0.66324     0.61764    0.78761   

#   1  scifact  sqlite   1.85     631   20.28      0.66630     0.61966    0.79494   

#   2  scifact    rank   1.85     724   42.22      0.65618     0.61204    0.77400   

#   

#       P_10  

#   0  0.087  

#   1  0.088  

#   2  0.085  

"""
The sections above show the metrics per source and method.

The table headers list the `source (dataset)`, `index method`, `index time(s)`, `memory usage(MB)`, `search time(s)` and `NDCG@10`/`MAP@10`/`RECALL@10`/`P@10` accuracy metrics. The tables are sorted by `search time`.

As we can see, txtai's implementation has the fastest search times across the board. But it is slower when it comes to index time. The accuracy metrics vary slightly but are all about the same per method.

Memory usage stands out. SQLite and txtai both have around the same usage per source. Rank-BM25 memory usage can get out of hand fast. For example, `webis-touch2020`, which is only ~400K records, uses `10 GB` of memory compared to `700 MB` for the other implementations.
"""

"""
# Compare with Elasticsearch

Now that we've reviewed methods to build keyword indexes in Python, let's see how txtai's sparse keyword index compares to Elasticsearch.

We'll spin up an inline instance and run the same evaluations.
"""

%%capture
# Download and extract elasticsearch
os.system("wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz")
os.system("tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz")
os.system("chown -R daemon:daemon elasticsearch-7.10.1")

from subprocess import Popen, PIPE, STDOUT

# Start and wait for server
server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))
!sleep 30

# Add benchmark evaluations for Elasticsearch
evaluate("es")

# Reload benchmarks dataframe
df = benchmarks()
# Output:
#   python benchmarks.py beir trec-covid es

#   python benchmarks.py beir nfcorpus es

#   python benchmarks.py beir webis-touche2020 es

#   python benchmarks.py beir scidocs es

#   python benchmarks.py beir scifact es


df[df.source == "trec-covid"].reset_index(drop=True)
# Output:
#          source  method   index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  trec-covid    bm25  101.96     997    0.28      0.58119     0.01247   

#   1  trec-covid      es   71.24     636    2.09      0.59215     0.01261   

#   2  trec-covid  sqlite   60.16     880   23.09      0.56778     0.01190   

#   3  trec-covid    rank   61.75    3245   75.49      0.57773     0.01210   

#   

#      recall_10   P_10  

#   0    0.01545  0.618  

#   1    0.01590  0.636  

#   2    0.01519  0.610  

#   3    0.01550  0.632  

df[df.source == "nfcorpus"].reset_index(drop=True)
# Output:
#        source  method  index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  nfcorpus    bm25   2.64     648    1.08      0.30639     0.11728   

#   1  nfcorpus      es   3.95     627   11.47      0.30676     0.11761   

#   2  nfcorpus  sqlite   1.50     630   12.73      0.30695     0.11785   

#   3  nfcorpus    rank   2.75     700   23.78      0.30692     0.11711   

#   

#      recall_10     P_10  

#   0    0.14891  0.21734  

#   1    0.14894  0.21610  

#   2    0.14871  0.21641  

#   3    0.15320  0.21889  

df[df.source == "webis-touche2020"].reset_index(drop=True)
# Output:
#                source  method   index  memory  search  ndcg_cut_10  map_cut_10  \

#   0  webis-touche2020    bm25  374.66    1137    0.37      0.36920     0.14588   

#   1  webis-touche2020      es  168.28     629    0.62      0.37519     0.14819   

#   2  webis-touche2020  sqlite  220.46    1416   34.61      0.37194     0.14812   

#   3  webis-touche2020    rank  224.07   10347   81.22      0.39861     0.16492   

#   

#      recall_10     P_10  

#   0    0.22736  0.34694  

#   1    0.22889  0.35102  

#   2    0.22890  0.35102  

#   3    0.23770  0.36122  

df[df.source == "scidocs"].reset_index(drop=True)
# Output:
#       source  method  index  memory  search  ndcg_cut_10  map_cut_10  recall_10  \

#   0  scidocs    bm25  17.95     717    1.64      0.15063     0.08756    0.15637   

#   1  scidocs      es  11.07     632   10.25      0.14924     0.08671    0.15497   

#   2  scidocs  sqlite  17.85     670   56.64      0.15156     0.08822    0.15717   

#   3  scidocs    rank  13.11    1056  162.99      0.14932     0.08670    0.15408   

#   

#        P_10  

#   0  0.0772  

#   1  0.0765  

#   2  0.0776  

#   3  0.0761  

df[df.source == "scifact"].reset_index(drop=True)
# Output:
#       source  method  index  memory  search  ndcg_cut_10  map_cut_10  recall_10  \

#   0  scifact    bm25   5.51     653    1.07      0.66324     0.61764    0.78761   

#   1  scifact      es   2.90     625    9.62      0.66058     0.61518    0.78428   

#   2  scifact  sqlite   1.85     631   20.28      0.66630     0.61966    0.79494   

#   3  scifact    rank   1.85     724   42.22      0.65618     0.61204    0.77400   

#   

#         P_10  

#   0  0.08700  

#   1  0.08667  

#   2  0.08800  

#   3  0.08500  

"""
Once again txtai's implementation compares well with Elasticsearch. The accuracy metrics vary but are all about the same.

It's important to note that in internal testing with solid state storage, Elasticsearch and txtai's speed is about the same. These times for Elasticsearch being a little slower are a product of running in a Google Colab environment.
"""

"""
# Wrapping up

This notebook showed how to build an efficient sparse keyword index in Python. The benchmarks show that txtai provides a strong implementation both from an accuracy and speed standpoint, on par with Apache Lucene.

This keyword index can be used as a standalone index in Python or in combination with dense vector indexes to form a `hybrid` index.
"""



================================================
FILE: examples/48_Benefits_of_hybrid_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Benefits of hybrid search

Semantic search is a new category of search built on recent advances in Natural Language Processing (NLP). Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.

While semantic search adds amazing capabilities, sparse keyword indexes can still add value. There may be cases where finding an exact match is important or we just want a fast index to quickly do an initial scan of a dataset.

Both methods have their merits. What if we combine them together to build a unified `hybrid` search capability? Can we get the best of both worlds?

This notebook will explore the benefits of hybrid search.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install txtai pytrec_eval rank-bm25 elasticsearch
!pip uninstall -y tensorflow

"""
# Introducing semantic, keyword and hybrid search

Before diving into the benchmarks, let's briefly discuss how semantic and keyword search works.

Semantic search uses large language models to vectorize inputs into arrays of numbers. Similar concepts will have similar values. The vectors are typically stored in a vector database, which is a system that specializes in storing these numerical arrays and finding matches. Vector search transforms an input query into a vector and then runs a search to find the best conceptual results.

Keyword search tokenizes text into lists of tokens per document. These tokens are aggregated into token frequencies per document and stored in term frequency sparse arrays. At search time, the query is tokenized and the tokens of the query are compared to the tokens in the dataset. This is more a literal process. Keyword search is like string matching, it has no conceptual understanding, it matches on characters and bytes.

Hybrid search combines the scores from semantic and keyword indexes. Given that semantic search scores are typically 0 - 1 and keyword search scores are unbounded, a method is needed to combine the results.

The two methods supported in txtai are:

- [Convex Combination](https://en.wikipedia.org/wiki/Convex_combination) when sparse scores are normalized
- [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) when sparse scores aren't normalized

The default method in txtai is convex combination and we'll use that.
"""

"""
# Evaluating performance

Now it's time to benchmark the results. For these tests, we'll use the BEIR dataset. We'll also use a [benchmarks script](https://raw.githubusercontent.com/neuml/txtai/master/examples/benchmarks.py) from the txtai project. This benchmarks script has methods to work with the BEIR dataset.

We'll select a subset of the BEIR sources for brevity. For each source, we'll benchmark a `bm25` index, an `embeddings` index and a `hybrid` or combined index.
"""

%%capture
import os

# Get benchmarks script
os.system("wget https://raw.githubusercontent.com/neuml/txtai/master/examples/benchmarks.py")

# Create output directory
os.makedirs("beir", exist_ok=True)

# Download subset of BEIR datasets
datasets = ["nfcorpus", "fiqa", "arguana", "scidocs", "scifact"]
for dataset in datasets:
  url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip"
  os.system(f"wget {url}")
  os.system(f"mv {dataset}.zip beir")
  os.system(f"unzip -d beir beir/{dataset}.zip")

  # Remove existing benchmark data
if os.path.exists("benchmarks.json"):
  os.remove("benchmarks.json")

"""
Now let's run the benchmarks.
"""

# Remove existing benchmark data
if os.path.exists("benchmarks.json"):
  os.remove("benchmarks.json")

# Runs benchmark evaluation
def evaluate(method):
  for dataset in datasets:
    command = f"python benchmarks.py beir {dataset} {method}"
    print(command)
    os.system(command)

# Calculate benchmarks
for method in ["bm25", "embed", "hybrid"]:
  evaluate(method)
# Output:
#   python benchmarks.py beir nfcorpus bm25

#   python benchmarks.py beir fiqa bm25

#   python benchmarks.py beir arguana bm25

#   python benchmarks.py beir scidocs bm25

#   python benchmarks.py beir scifact bm25

#   python benchmarks.py beir nfcorpus embed

#   python benchmarks.py beir fiqa embed

#   python benchmarks.py beir arguana embed

#   python benchmarks.py beir scidocs embed

#   python benchmarks.py beir scifact embed

#   python benchmarks.py beir nfcorpus hybrid

#   python benchmarks.py beir fiqa hybrid

#   python benchmarks.py beir arguana hybrid

#   python benchmarks.py beir scidocs hybrid

#   python benchmarks.py beir scifact hybrid


import json
import pandas as pd

def benchmarks():
  # Read JSON lines data
  with open("benchmarks.json") as f:
    data = f.read()

  df = pd.read_json(data, lines=True).sort_values(by=["source", "ndcg_cut_10"], ascending=[True, False])
  return df[["source", "method", "ndcg_cut_10", "map_cut_10", "recall_10", "P_10", "index", "search", "memory"]].reset_index(drop=True)

# Load benchmarks dataframe
df = benchmarks()

df[df.source == "nfcorpus"].reset_index(drop=True)
# Output:
#        source  method  ndcg_cut_10  map_cut_10  recall_10     P_10  index  \

#   0  nfcorpus  hybrid      0.34531     0.13369    0.17437  0.25480  29.46   

#   1  nfcorpus   embed      0.30917     0.10810    0.15327  0.23591  33.64   

#   2  nfcorpus    bm25      0.30639     0.11728    0.14891  0.21734   2.72   

#   

#      search  memory  

#   0    3.57    2900  

#   1    3.33    2876  

#   2    0.96     652  

df[df.source == "fiqa"].reset_index(drop=True)
# Output:
#     source  method  ndcg_cut_10  map_cut_10  recall_10     P_10   index  search  \

#   0   fiqa  hybrid      0.36642     0.28846    0.43799  0.10340  233.90   68.42   

#   1   fiqa   embed      0.36071     0.28450    0.43188  0.10216  212.30   58.83   

#   2   fiqa    bm25      0.23559     0.17619    0.29855  0.06559   19.78   12.84   

#   

#      memory  

#   0    3073  

#   1    2924  

#   2     761  

df[df.source == "arguana"].reset_index(drop=True)
# Output:
#       source  method  ndcg_cut_10  map_cut_10  recall_10     P_10  index  \

#   0  arguana  hybrid      0.48467     0.40101    0.75320  0.07532  37.80   

#   1  arguana   embed      0.47781     0.38781    0.76671  0.07667  34.11   

#   2  arguana    bm25      0.45713     0.37118    0.73471  0.07347   3.39   

#   

#      search  memory  

#   0   21.22    2924  

#   1   10.21    2910  

#   2   10.95     663  

df[df.source == "scidocs"].reset_index(drop=True)
# Output:
#       source  method  ndcg_cut_10  map_cut_10  recall_10    P_10   index  \

#   0  scidocs   embed      0.21718     0.12982    0.23217  0.1146  127.63   

#   1  scidocs  hybrid      0.21104     0.12450    0.22938  0.1134  138.00   

#   2  scidocs    bm25      0.15063     0.08756    0.15637  0.0772   13.07   

#   

#      search  memory  

#   0    4.41    2929  

#   1    6.43    2999  

#   2    1.42     722  

df[df.source == "scifact"].reset_index(drop=True)
# Output:
#       source  method  ndcg_cut_10  map_cut_10  recall_10     P_10  index  \

#   0  scifact  hybrid      0.71305     0.66773    0.83722  0.09367  39.51   

#   1  scifact    bm25      0.66324     0.61764    0.78761  0.08700   4.40   

#   2  scifact   embed      0.65149     0.60193    0.78972  0.08867  35.15   

#   

#      search  memory  

#   0    2.35    2918  

#   1    0.93     658  

#   2    1.48    2889  

"""
The sections above show the metrics per source and method.

The table headers list the `source (dataset)`, `index method`, `NDCG@10`/`MAP@10`/`RECALL@10`/`P@10` accuracy metrics, `index time(s)`, `search time(s)` and `memory usage(MB)`. The tables are sorted by `NDCG@10` descending.

Looking at the results, we can see that `hybrid` search often performs better than `embeddings` or `bm25` individually. In some cases, as with scidocs, the combination performs worse. But in the aggregate, the scores are better. This holds true for the entire BEIR dataset. For some sources, `bm25` does best, some `embeddings` but overall the combined `hybrid` scores do the best.

Hybrid search isn't free though, it is slower as it has extra logic to combine the results. For individual queries, the results are often negligible.
"""

"""
# Wrapping up

This notebook covered ways to improve search accuracy using a hybrid approach. We evaluated performance over a subset of the BEIR dataset to show how hybrid search, in many situations, can improve overall accuracy.

Custom datasets can also be evaluated using this method as [specified in this link](https://github.com/beir-cellar/beir/wiki/Load-your-custom-dataset). This notebook and the associated benchmarks script can be reused to evaluate what method works best on your data.

"""



================================================
FILE: examples/49_External_database_integration.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# External database integration

txtai provides many default settings to help a developer quickly get started. For example, metadata is stored in SQLite, dense vectors in Faiss, sparse vectors in a terms index and graph data with NetworkX.

Each of these components is customizable and can be swapped with alternate implementations. This has been covered in [several previous notebooks](https://neuml.github.io/txtai/examples/#architecture).

This notebook will introduce how to store metadata in client-server RDBMS systems. In addition to SQLite and DuckDB, any [SQLAlchemy-supported database](https://docs.sqlalchemy.org/en/20/dialects/) with [JSON support](https://docs.sqlalchemy.org/en/20/core/type_basics.html#sqlalchemy.types.JSON) can now be used.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture

!pip install git+https://github.com/neuml/txtai#egg=txtai[database] elasticsearch==7.10.1 datasets

"""
# Install Postgres

Next, we'll install Postgres and start a Postgres instance.
"""

%%capture

# Install and start Postgres
!apt-get update && apt-get install postgresql
!service postgresql start
!sudo -u postgres psql -U postgres -c "ALTER USER postgres PASSWORD 'postgres';"

"""
# Load a dataset

Now we're ready to load a dataset. We'll use the `ag_news` dataset. This dataset consists of 120,000 news headlines.
"""

from datasets import load_dataset

# Load dataset
ds = load_dataset("ag_news", split="train")

"""
# Build an Embeddings instance with Postgres

Let's load this dataset into an embeddings database. We'll configure this instance to store metadata in Postgres. Note that the content parameter below is a [SQLAlchemy connection string](https://docs.sqlalchemy.org/en/20/core/engines.html).

This embeddings database will use the default vector settings and build that index locally.
"""

import txtai

# Create embeddings
embeddings = txtai.Embeddings(
    content="postgresql+psycopg2://postgres:postgres@localhost/postgres",
)

# Index dataset
embeddings.index(ds["text"])

"""
Let's run a search query and see what comes back.
"""

embeddings.search("red sox defeat yankees")
# Output:
#   [{'id': '63561',

#     'text': 'Red Sox Beat Yankees 6-4 in 12 Innings BOSTON - Down to their last three outs of the season, the Boston Red Sox rallied - against Mariano Rivera, the New York Yankees and decades of disappointment. Bill Mueller singled home the tying run off Rivera in the ninth inning and David Ortiz homered against Paul Quantrill in the 12th, leading Boston to a 6-4 victory Sunday over the Yankees that avoided a four-game sweep in the AL championship series...',

#     'score': 0.8104304671287537},

#    {'id': '63221',

#     'text': 'Red Sox Beat Yankees 6-4 in 12 Innings BOSTON - Down to their last three outs of the season, the Boston Red Sox rallied - against Mariano Rivera, the New York Yankees and decades of disappointment. Bill Mueller singled home the tying run off Rivera in the ninth inning and David Ortiz homered against Paul Quantrill in the 12th, leading Boston to a 6-4 victory over the Yankees on Sunday night that avoided a four-game sweep in the AL championship series...',

#     'score': 0.8097385168075562},

#    {'id': '66861',

#     'text': 'Record-Breaking Red Sox Clinch World Series Berth  NEW YORK (Reuters) - The Boston Red Sox crushed the New  York Yankees 10-3 Wednesday to complete an historic comeback  victory over their arch-rivals by four games to three in the  American League Championship Series.',

#     'score': 0.8003846406936646}]

"""
As expected, we get the standard `id, text, score` fields with the top matches for the query. The difference though is that all the database metadata normally stored in a local SQLite file is now stored in a Postgres server.

This opens up several possibilities such as row-level security. If a row isn't returned by the database, it won't be shown here. Alternatively, this search could optionally return only the ids and scores, which lets the user know a record exists they don't have access to.

As with other supported databases, underlying database functions can be called from txtai SQL.
"""

embeddings.search("SELECT id, text, md5(text), score FROM txtai WHERE similar('red sox defeat yankees')")
# Output:
#   [{'id': '63561',

#     'text': 'Red Sox Beat Yankees 6-4 in 12 Innings BOSTON - Down to their last three outs of the season, the Boston Red Sox rallied - against Mariano Rivera, the New York Yankees and decades of disappointment. Bill Mueller singled home the tying run off Rivera in the ninth inning and David Ortiz homered against Paul Quantrill in the 12th, leading Boston to a 6-4 victory Sunday over the Yankees that avoided a four-game sweep in the AL championship series...',

#     'md5': '1e55a78fdf0cb3be3ef61df650f0a50f',

#     'score': 0.8104304671287537},

#    {'id': '63221',

#     'text': 'Red Sox Beat Yankees 6-4 in 12 Innings BOSTON - Down to their last three outs of the season, the Boston Red Sox rallied - against Mariano Rivera, the New York Yankees and decades of disappointment. Bill Mueller singled home the tying run off Rivera in the ninth inning and David Ortiz homered against Paul Quantrill in the 12th, leading Boston to a 6-4 victory over the Yankees on Sunday night that avoided a four-game sweep in the AL championship series...',

#     'md5': 'a0417e1fc503a5a2945c8755b6fb18d5',

#     'score': 0.8097385168075562},

#    {'id': '66861',

#     'text': 'Record-Breaking Red Sox Clinch World Series Berth  NEW YORK (Reuters) - The Boston Red Sox crushed the New  York Yankees 10-3 Wednesday to complete an historic comeback  victory over their arch-rivals by four games to three in the  American League Championship Series.',

#     'md5': '398a8508692aed109bd8c56f067a8083',

#     'score': 0.8003846406936646}]

"""
Note the addition of the Postgres `md5` function to the query.

Let's save and show the files in the embeddings database.
"""

embeddings.save("vectors")
!ls -l vectors
# Output:
#   total 183032

#   -rw-r--r-- 1 root root       355 Sep  7 16:38 config

#   -rw-r--r-- 1 root root 187420123 Sep  7 16:38 embeddings


"""
Only the configuration and the local vectors index are stored in this case.
"""

"""
# External indexing

As mentioned previously, all of the main components of txtai can be replaced with custom components. For example, there are external integrations for storing dense vectors in [Weaviate](https://github.com/hsm207/weaviate-txtai) and [Qdrant](https://github.com/qdrant/qdrant-txtai) to name a few.

Next, we'll build an example that stores metadata in Postgres and builds a sparse index with Elasticsearch.
"""

"""
## Scoring component for Elasticsearch

First, we need to define a custom scoring component for Elasticsearch. While could have used an existing integration, it's important to show that creating a new component isn't a large LOE (~70 lines of code). See below.
"""

from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

from txtai.scoring import Scoring

class Elastic(Scoring):
  def __init__(self, config=None):
    # Scoring configuration
    self.config = config if config else {}

    # Server parameters
    self.url = self.config.get("url", "http://localhost:9200")
    self.indexname = self.config.get("indexname", "testindex")

    # Elasticsearch connection
    self.connection = Elasticsearch(self.url)

    self.terms = True
    self.normalize = self.config.get("normalize")

  def insert(self, documents, index=None):
    rows = []
    for uid, document, tags in documents:
        rows.append((index, document))

        # Increment index
        index = index + 1

    bulk(self.connection, ({"_index": self.indexname, "_id": uid, "text": text} for uid, text in rows))

  def index(self, documents=None):
    self.connection.indices.refresh(index=self.indexname)

  def search(self, query, limit=3):
    return self.batchsearch([query], limit)

  def batchsearch(self, queries, limit=3):
    # Generate bulk queries
    request = []
    for query in queries:
      req_head = {"index": self.indexname, "search_type": "dfs_query_then_fetch"}
      req_body = {
        "_source": False,
        "query": {"multi_match": {"query": query, "type": "best_fields", "fields": ["text"], "tie_breaker": 0.5}},
        "size": limit,
      }
      request.extend([req_head, req_body])

      # Run ES query
      response = self.connection.msearch(body=request, request_timeout=600)

      # Read responses
      results = []
      for resp in response["responses"]:
        result = resp["hits"]["hits"]
        results.append([(r["_id"], r["_score"]) for r in result])

      return results

  def count(self):
    response = self.connection.cat.count(self.indexname, params={"format": "json"})
    return int(response[0]["count"])

  def load(self, path):
    # No local storage
    pass

  def save(self, path):
    # No local storage
    pass

"""
## Elasticsearch server

As with Postgres, we'll install and start an Elasticsearch instance.
"""

%%capture
import os

# Download and extract elasticsearch
os.system("wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz")
os.system("tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz")
os.system("chown -R daemon:daemon elasticsearch-7.10.1")

from subprocess import Popen, PIPE, STDOUT

# Start and wait for serverw
server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))
!sleep 30

"""
Let's build the index. The only difference from the previous example is setting the custom `scoring` component.
"""

import txtai

# Create embeddings
embeddings = txtai.Embeddings(
    keyword=True,
    content="postgresql+psycopg2://postgres:postgres@localhost/postgres",
    scoring= "__main__.Elastic"
)

# Index dataset
embeddings.index(ds["text"])

"""
Below is the same search as shown before.
"""

embeddings.search("red sox defeat yankees")
# Output:
#   [{'id': '66954',

#     'text': 'Boston Red Sox make history Believe it, New England -- the Boston Red Sox are in the World Series. And they got there with the most unbelievable comeback of all, with four sweet swings after decades of defeat, shaming the dreaded New York Yankees.',

#     'score': 21.451942},

#    {'id': '69577',

#     'text': 'Passing thoughts on Yankees-Red Sox series The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game. The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game and it wasn #39;t even close.',

#     'score': 20.923117},

#    {'id': '67253',

#     'text': 'Sox Victorious At Last!! BOSTON -- After suffering decades of defeat and disappointment, the 2004 Boston Red Sox made history Wednesday night, beating the Yankees in the house that Ruth built and claiming the American League championship trophy.',

#     'score': 20.865997}]

"""
And once again we get the top matches. This time though the index is in Elasticsearch. Why are results and scores different? This is because this is a keyword index and it's using Elasticsearch's raw BM25 scores.

One enhancement to this component would be adding score normalization as seen in the standard scoring components.

For good measure, let's also show that the `md5` function can be called here too.
"""

embeddings.search("SELECT id, text, md5(text), score FROM txtai WHERE similar('red sox defeat yankees')")
# Output:
#   [{'id': '66954',

#     'text': 'Boston Red Sox make history Believe it, New England -- the Boston Red Sox are in the World Series. And they got there with the most unbelievable comeback of all, with four sweet swings after decades of defeat, shaming the dreaded New York Yankees.',

#     'md5': '29084f8640d4d72e402e991bc9fdbfa0',

#     'score': 21.451942},

#    {'id': '69577',

#     'text': 'Passing thoughts on Yankees-Red Sox series The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game. The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game and it wasn #39;t even close.',

#     'md5': '056983d301975084b49a5987185f2ddf',

#     'score': 20.923117},

#    {'id': '67253',

#     'text': 'Sox Victorious At Last!! BOSTON -- After suffering decades of defeat and disappointment, the 2004 Boston Red Sox made history Wednesday night, beating the Yankees in the house that Ruth built and claiming the American League championship trophy.',

#     'md5': '7838fcf610f0b569829c9bafdf9012f2',

#     'score': 20.865997}]

"""
Same results with the additional `md5` column, as expected.
"""

"""
# Explore the data stores

The last thing we'll do is see where and how this data is stored in Postgres and Elasticsearch.

Let's connect to the local Postgres instance and sample content from the `sections` table.
"""

%env DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres
%load_ext sql
# Output:
#   env: DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres

#   The sql extension is already loaded. To reload it, use:

#     %reload_ext sql


%%sql
select id, text from sections where text like '%Red Sox%' and text like '%Yankees%' and text like '%defeat%' limit 3;
# Output:
#    * postgresql://postgres:***@localhost:5432/postgres

#   3 rows affected.

#   [('66954', 'Boston Red Sox make history Believe it, New England -- the Boston Red Sox are in the World Series. And they got there with the most unbelievable comeback of all, with four sweet swings after decades of defeat, shaming the dreaded New York Yankees.'),

#    ('62732', "BoSox, Astros Play for Crucial Game 4 Wins (AP) AP - The Boston Red Sox entered this AL championship series hoping to finally overcome their bitter r ... (50 characters truncated) ... n-game defeat last October. Instead, they've been reduced to trying to prevent the Yankees from completing a humiliating sweep in their own ballpark."),

#    ('62752', "BoSox, Astros Play for Crucial Game 4 Wins The Boston Red Sox entered this AL championship series hoping to finally overcome their bitter rivals from ... (42 characters truncated) ... game defeat last October. Instead, they've been reduced to trying to prevent the Yankees from completing a humiliating sweep in their own ballpark...")]

"""
As expected, we can see content stored directly in Postgres!

Now let's check Elasticsearch.
"""

import json
import requests

response = requests.get("http://localhost:9200/_search?q=red+sox+defeat+yankees&size=3")
print(json.dumps(response.json(), indent=2))
# Output:
#   {

#     "took": 13,

#     "timed_out": false,

#     "_shards": {

#       "total": 1,

#       "successful": 1,

#       "skipped": 0,

#       "failed": 0

#     },

#     "hits": {

#       "total": {

#         "value": 3297,

#         "relation": "eq"

#       },

#       "max_score": 21.451942,

#       "hits": [

#         {

#           "_index": "testindex",

#           "_type": "_doc",

#           "_id": "66954",

#           "_score": 21.451942,

#           "_source": {

#             "text": "Boston Red Sox make history Believe it, New England -- the Boston Red Sox are in the World Series. And they got there with the most unbelievable comeback of all, with four sweet swings after decades of defeat, shaming the dreaded New York Yankees."

#           }

#         },

#         {

#           "_index": "testindex",

#           "_type": "_doc",

#           "_id": "69577",

#           "_score": 20.923117,

#           "_source": {

#             "text": "Passing thoughts on Yankees-Red Sox series The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game. The Red Sox beat the Yankees at Yankee Stadium in a season-deciding game and it wasn #39;t even close."

#           }

#         },

#         {

#           "_index": "testindex",

#           "_type": "_doc",

#           "_id": "67253",

#           "_score": 20.865997,

#           "_source": {

#             "text": "Sox Victorious At Last!! BOSTON -- After suffering decades of defeat and disappointment, the 2004 Boston Red Sox made history Wednesday night, beating the Yankees in the house that Ruth built and claiming the American League championship trophy."

#           }

#         }

#       ]

#     }

#   }


"""
Same query results as what was run through the embeddings database.

Let's save the embeddings database and review what's stored.
"""

embeddings.save("elastic")
!ls -l elastic
# Output:
#   total 4

#   -rw-r--r-- 1 root root 155 Sep  7 16:39 config


"""
And all we have is the configuration. No `database`, `embeddings` or `scoring` files. That data is in Postgres and Elasticsearch!
"""

"""
# Wrapping up

This notebook showed how external databases and other external integrations can be used with embeddings databases. This architecture ensures that as new ways to index and store data become available, txtai can easily adapt.

This notebook also showed how creating a custom component is a low level of effort and can easily be done for a component without an existing integration.

"""



================================================
FILE: examples/50_All_about_vector_quantization.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# All about vector quantization

txtai supports a number of approximate nearest neighbor (ANN) libraries for vector storage. This includes [Faiss](https://github.com/facebookresearch/faiss), [Hnswlib](https://github.com/nmslib/hnswlib), [Annoy](https://github.com/spotify/annoy), [NumPy](https://github.com/numpy/numpy) and [PyTorch](https://github.com/pytorch/pytorch). Custom implementations can also be added.

The default ANN for txtai is Faiss. Faiss has by far the largest array of configurable options in building an ANN index. This article will cover quantization and different approaches that are possible along with the tradeoffs.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai pytrec_eval rank-bm25 elasticsearch psutil

"""
# Preparing the datasets

First, let's download a subset of the datasets from the BEIR evaluation framework. We'll also retrieve the standard txtai benchmark script. These will be used to help judge the accuracy of quantization methods.
"""

%%capture
import os

# Get benchmarks script
os.system("wget https://raw.githubusercontent.com/neuml/txtai/master/examples/benchmarks.py")

# Create output directory
os.makedirs("beir", exist_ok=True)

if os.path.exists("benchmarks.json"):
  os.remove("benchmarks.json")

# Download subset of BEIR datasets
datasets = ["nfcorpus", "arguana", "scifact"]
for dataset in datasets:
  url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip"
  os.system(f"wget {url}")
  os.system(f"mv {dataset}.zip beir")
  os.system(f"unzip -d beir beir/{dataset}.zip")

"""
# Evaluation

Next, we'll setup the scaffolding to run evaluations.
"""

import pandas as pd
import yaml

def writeconfig(dataset, quantize):
  sources = {"arguana": "IVF11", "nfcorpus": "IDMap", "scifact": "IVF6"}
  config = {
    "embeddings": {
      "batch": 8192,
      "encodebatch": 128,
      "faiss": {
          "sample": 0.05
      }
    }
  }

  if quantize and quantize[-1].isdigit() and int(quantize[-1]) < 4:
    # Use vector quantization for 1, 2 and 3 bit quantization
    config["embeddings"]["quantize"] = int(quantize[-1])
  elif quantize:
    # Use Faiss quantization for other forms of quantization
    config["embeddings"]["faiss"]["components"] = f"{sources[dataset]},{quantize}"

  # Derive name
  name = quantize if quantize else "baseline"

  # Derive config path and write output
  path = f"{dataset}_{name}.yml"
  with open(path, "w") as f:
    yaml.dump(config, f)

  return name, path

def benchmarks():
  # Read JSON lines data
  with open("benchmarks.json") as f:
    data = f.read()

  df = pd.read_json(data, lines=True).sort_values(by=["source", "ndcg_cut_10"], ascending=[True, False])
  return df[["source", "name", "ndcg_cut_10", "map_cut_10", "recall_10", "P_10", "disk"]].reset_index(drop=True)

# Runs benchmark evaluation
def evaluate(quantize=None):
  for dataset in datasets:
    # Build config based on requested quantization
    name, config = writeconfig(dataset, quantize)

    command = f"python benchmarks.py -d beir -s {dataset} -m embeddings -c \"{config}\" -n \"{name}\""
    os.system(command)


"""
# Establish a baseline

Before introducing vector quantization, let's establish a baseline of accuracy per source without quantization. The following table shows accuracy metrics along with the disk storage size in KB.
"""

evaluate()
benchmarks()
# Output:
#        source      name  ndcg_cut_10  map_cut_10  recall_10     P_10   disk

#   0   arguana  baseline      0.47886     0.38931    0.76600  0.07660  13416

#   1  nfcorpus  baseline      0.30893     0.10789    0.15315  0.23622   5517

#   2   scifact  baseline      0.65273     0.60386    0.78972  0.08867   7878

"""
# Quantization

The two main types of vector [quantization](https://en.wikipedia.org/wiki/Quantization_(signal_processing)) are scalar quantization and product quantization.

Scalar quantization maps floating point data to a series of integers. For example, 8-bit quantization splits the range of floats into 255 buckets. This cuts data storage down by 4 when working with 32-bit floats, since each dimension now only stores 1 byte vs 4. A more dramatic version of this is binary or 1-bit quantization, where the floating point range is cut in half, 0 or 1. The trade-off as one would expect is accuracy.

Product quantization is similar in that the process bins a floating point range into codes but it's more complex. This method splits vectors across dimensions into subvectors and runs those subvectors through a clustering algorithm. This can lead to a substantial reduction in data storage at the expense of accuracy like with scalar quantization. The [Faiss documentation](https://github.com/facebookresearch/faiss/wiki#research-foundations-of-faiss) has a number of great papers with more information on this method.

Quantization is available at the vector processing and datastore levels in txtai. In both cases, it requires an ANN backend that can support integer vectors. Currently, only Faiss, NumPy and Torch are supported.

Let's benchmark a variety of quantization methods.
"""

# Evaluate quantization methods
for quantize in ["SQ1", "SQ4", "SQ8", "PQ48x4fs", "PQ96x4fs", "PQ192x4fs"]:
  evaluate(quantize)

# Show benchmarks
benchmarks()
# Output:
#         source       name  ndcg_cut_10  map_cut_10  recall_10     P_10   disk

#   0    arguana   baseline      0.47886     0.38931    0.76600  0.07660  13416

#   1    arguana        SQ8      0.47781     0.38781    0.76671  0.07667   3660

#   2    arguana        SQ4      0.47771     0.38915    0.76174  0.07617   2034

#   3    arguana  PQ192x4fs      0.46322     0.37341    0.75391  0.07539   1260

#   4    arguana   PQ96x4fs      0.43744     0.35052    0.71906  0.07191    844

#   5    arguana        SQ1      0.42604     0.33997    0.70555  0.07055    795

#   6    arguana   PQ48x4fs      0.40220     0.31653    0.67852  0.06785    637

#   7   nfcorpus        SQ4      0.31028     0.10758    0.15417  0.23839    751

#   8   nfcorpus        SQ8      0.30917     0.10810    0.15327  0.23591   1433

#   9   nfcorpus   baseline      0.30893     0.10789    0.15315  0.23622   5517

#   10  nfcorpus  PQ192x4fs      0.30722     0.10678    0.15168  0.23467    433

#   11  nfcorpus   PQ96x4fs      0.29594     0.09929    0.13996  0.22693    262

#   12  nfcorpus        SQ1      0.26582     0.08579    0.12658  0.19907    237

#   13  nfcorpus   PQ48x4fs      0.25874     0.08100    0.11912  0.19567    177

#   14   scifact        SQ4      0.65299     0.60328    0.79139  0.08867   1078

#   15   scifact   baseline      0.65273     0.60386    0.78972  0.08867   7878

#   16   scifact        SQ8      0.65149     0.60193    0.78972  0.08867   2050

#   17   scifact  PQ192x4fs      0.64046     0.58823    0.78933  0.08867    622

#   18   scifact   PQ96x4fs      0.62256     0.57773    0.74861  0.08400    375

#   19   scifact        SQ1      0.58724     0.53418    0.73989  0.08267    338

#   20   scifact   PQ48x4fs      0.52292     0.46611    0.68744  0.07700    251

"""
# Review

Each of the sources above were run through a series of scalar and product quantization settings. The accuracy vs disk space trade off is clear to see.

Couple key points to highlight.

- The vector model outputs vectors with 384 dimensions
- Scalar quantization (SQ) was evaluated for 1-bit (binary), 4 and 8 bits
- 1-bit (binary) quantization stores vectors in [binary indexes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes)
- For product quantization (PQ), three methods were tested. 48, 96 and 192 codes respectively, all using 4-bit codes

In general, the larger the index size, the better the scores. There are a few exceptions to this but the differences are minimal in those cases. The smaller scalar and product quantization indexes are up to 20 times smaller.

It's important to note that the smaller scalar methods typically need a wider number of dimensions to perform competitively. With that being said, even at 384 dimensions, binary quantization still does OK. txtai supports scalar quantization precisions from 1 through 8 bits.

This is just a subset of the available quantization methods available in Faiss. More details can be found in the [Faiss documentation](https://github.com/facebookresearch/faiss/wiki/The-index-factory).
"""

"""
# Wrapping up

This notebook evaluated a variety of vector quantization methods. Quantization is an option to reduce storage costs at the expense of accuracy. Larger vector models (1024+ dimensions) will retain accuracy better with more aggressive quantization methods. As always, results will vary depending on your data.
"""



================================================
FILE: examples/51_Custom_API_Endpoints.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Custom API Endpoints

The [txtai API](https://neuml.github.io/txtai/api/) is a web-based service backed by [FastAPI](https://fastapi.tiangolo.com/). Semantic search, LLM orchestration and Language Model Workflows can all run through the API.

While the API is extremely flexible and complex logic can be executed through YAML-driven workflows, some may prefer to create an endpoint in Python.

This notebook introduces API extensions and shows how they can be used to define custom Python endpoints that interact with txtai applications.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api] datasets

"""
# Define the extension

First, we'll create an application that defines a persistent embeddings database and LLM. Then we'll combine those two into a RAG endpoint through the API.
"""

%%writefile app.yml

# Embeddings index
writable: true
embeddings:
  hybrid: true
  content: true

# LLM pipeline
llm:
  path: google/flan-t5-large
  torch_dtype: torch.bfloat16
# Output:
#   Writing app.yml


"""
The code below creates an API endpoint at `/rag`. This is a `GET` endpoint that takes a `text` parameter as input.
"""

%%writefile rag.py
from fastapi import APIRouter
from txtai.api import application, Extension


class RAG(Extension):
    """
    API extension
    """

    def __call__(self, app):
        app.include_router(RAGRouter().router)


class RAGRouter:
    """
    API router
    """

    router = APIRouter()

    @staticmethod
    @router.get("/rag")
    def rag(text: str):
        """
        Runs a retrieval augmented generation (RAG) pipeline.

        Args:
            text: input text

        Returns:
            response
        """

        # Run embeddings search
        results = application.get().search(text, 3)
        context = " ".join([x["text"] for x in results])

        prompt = f"""
        Answer the following question using only the context below.

        Question: {text}
        Context: {context}
        """

        return {
            "response": application.get().pipeline("llm", (prompt,))
        }
# Output:
#   Writing rag.py


"""
# Start the API instance

Let's start the API with the RAG extension.
"""

!CONFIG=app.yml EXTENSIONS=rag.RAG nohup uvicorn "txtai.api:app" &> api.log &
!sleep 60

"""
# Create the embeddings database

Next, we'll create the embeddings database using the `ag_news` dataset. This is a set of news stories from the mid 2000s.
"""

from datasets import load_dataset
import requests

ds = load_dataset("ag_news", split="train")

# API endpoint
url = "http://localhost:8000"
headers = {"Content-Type": "application/json"}

# Add data
batch = []
for text in ds["text"]:
  batch.append({"text": text})
  if len(batch) == 4096:
    requests.post(f"{url}/add", headers=headers, json=batch, timeout=120)
    batch = []

if batch:
    requests.post(f"{url}/add", headers=headers, json=batch, timeout=120)

# Build index
index = requests.get(f"{url}/index")

"""
# Run queries

Now that we have a knowledge source indexed, let's run a set of queries. The code below defines a method that calls the `/rag` endpoint and retrieves the response. Keep in mind this dataset is from 2004.

While the Python Requests library is used in this notebook, this is a simple web endpoint that can be called from any programming language.
"""

def rag(text):
    return requests.get(f"{url}/rag?text={text}").json()["response"]

rag("Who is the current President?")
# Output:
#   'George W. Bush'

rag("Who lost the presidential election?")
# Output:
#   'John Kerry'

rag("Who won the World Series?")
# Output:
#   'Boston'

rag("Who did the Red Sox beat to win the world series?")
# Output:
#   'Cardinals'

rag("What major hurricane hit the USA?")
# Output:
#   'Charley'

rag("What mobile phone manufacturer has the largest current marketshare?")
# Output:
#   'Nokia'

"""
# Wrapping up

This notebook showed how a txtai application can be extended with custom endpoints in Python. While applications have a robust workflow framework, it may be preferable to write complex logic in Python and this method enables that.
"""



================================================
FILE: examples/52_Build_RAG_pipelines_with_txtai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build RAG pipelines with txtai

Large Language Models (LLMs) have completely dominated the AI and machine learning space in 2023. The results have been amazing and the public imagination is almost endless.

While LLMs have been impressive, they are not problem free. The biggest challenge is with hallucinations. Hallucinations is the term for when a LLM generates output that is factually incorrect. The alarming part of this is that on a cursory glance, it actually sounds like good content. The default behavior of LLMs is to produce plausible answers even when no plausible answer exists. LLMs are not great at saying I don't know.

Retrieval augmented generation (RAG) helps reduce the risk of hallucinations by limiting the context in which a LLM can generate answers. This is typically done with a vector search query that hydrates a prompt with a relevant context. RAG is one of the most practical and production-ready use cases for *Generative AI*. It's so popular now, that some are creating their entire companies around it.

[txtai](https://github.com/neuml/txtai) has long had question-answering pipelines, which employ the same process of retrieving a relevant context. LLMs are now the preferred approach for analyzing that context and RAG pipelines are one of the main features of txtai. One of the other main features of txtai is that it's a vector database! You can build your prompts and limit your context all with one library. Hence the phrase *all-in-one embeddings database*.

This notebook shows how to build RAG pipelines with txtai.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline] autoawq

# Get test data
!wget -N https://github.com/neuml/txtai/releases/download/v6.2.0/tests.tar.gz
!tar -xvzf tests.tar.gz

# Install NLTK
import nltk
nltk.download(['punkt', 'punkt_tab'])

"""
# Start with the basics

Let's jump right in and start with a simple LLM pipeline. The [LLM pipeline](https://neuml.github.io/txtai/pipeline/text/llm/) supports local LLM models via [Hugging Face Transformers](https://github.com/huggingface/transformers) and [llama.cpp](https://github.com/abetlen/llama-cpp-python).

The LLM pipeline also supports [API services (i.e. OpenAI, Claude, Bedrock etc) via LiteLLM](https://github.com/BerriAI/litellm). The LLM pipeline automatically detects the underlying LLM framework from the `path` parameter.

"""

from txtai import LLM

# Create LLM
llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

"""
Next, we'll load a document to query. The [Textractor pipeline](https://neuml.github.io/txtai/pipeline/data/textractor/) has support for extracting text from common document formats (docx, pdf, xlsx).
"""

from txtai.pipeline import Textractor

# Create Textractor
textractor = Textractor()
text = textractor("txtai/document.docx")
print(text)
# Output:
#   txtai â€“ the all-in-one embeddings database

#   txtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

#   

#   Summary of txtai features:

#   Â· Vector search with SQL, object storage, topic modeling

#   Â· Create embeddings for text, documents, audio, images and video

#   Â· Pipelines powered by language models that run LLM prompts

#   Â· Workflows to join pipelines together and aggregate business logic

#   Â· Build with Python or YAML. API bindings available for JavaScript, Java, Rust and Go.

#   Â· Run local or scale out with container orchestration

#   

#   Examples

#   List of example notebooks.

#   |Notebook|Description|

#   |---|---|

#   |Introducing txtai |Overview of the functionality provided by txtai|

#   |Similarity search with images|Embed images and text into the same space for search|

#   |Build a QA database|Question matching with semantic search|

#   |Semantic Graphs|Explore topics, data connectivity and run network analysis|

#   

#   Install

#   The easiest way to install is via pip and PyPI

#   pip install txtai

#   Python 3.10+ is supported. Using a Python virtual environment is recommended.

#   See the detailed install instructions for more information covering optional dependencies, environment specific prerequisites, installing from source, conda support and how to run with containers.

#   

#   

#   

#   

#   Model guide

#   The following shows a list of suggested models.

#   |Component|Model(s)|

#   |---|---|

#   |Embeddings|all-MiniLM-L6-v2|

#   ||E5-base-v2|

#   |Image Captions|BLIP|

#   |Labels - Zero Shot|BART-Large-MNLI|

#   |Labels - Fixed|Fine-tune with training pipeline|

#   |Large Language Model (LLM)|Flan T5 XL|

#   ||Mistral 7B OpenOrca|

#   |Summarization|DistilBART|

#   |Text-to-Speech|ESPnet JETS|

#   |Transcription|Whisper|

#   |Translation|OPUS Model Series|


"""
Now we'll define a simple LLM pipeline. It takes a question and context (which in this case is the whole file), creates a prompt and runs it with the LLM.
"""

def execute(question, text):
  prompt = f"""<|im_start|>system
  You are a friendly assistant. You answer questions from users.<|im_end|>
  <|im_start|>user
  Answer the following question using only the context below. Only include information specifically discussed.

  question: {question}
  context: {text} <|im_end|>
  <|im_start|>assistant
  """

  return llm(prompt, maxlength=4096, pad_token_id=32000)

execute("Tell me about txtai in one sentence", text)
# Output:
#   'Txtai is an all-in-one embeddings database for semantic search, LLM orchestration, and language model workflows, offering features such as vector search, pipeline creation, workflow management, and API bindings for various programming languages.'

execute("What model does txtai recommend for transcription?", text)
# Output:
#   'The model that txtai recommends for transcription is Whisper.'

execute("I don't know anything about txtai, what would be the best thing to read?", text)
# Output:
#   'The best thing to read to learn about txtai is the "Introducing txtai" notebook, which provides an overview of the functionality provided by txtai. This notebook covers various features such as vector search with SQL, object storage, topic modeling, creating embeddings for text, documents, audio, images, and video, and running language model workflows. Additionally, you can explore other example notebooks like "Similarity search with images," "Build a QA database," and "Semantic Graphs" to learn more about specific use cases and features. To install txtai, use pip and PyPI with Python 3.10+, and follow the detailed install instructions for more information on optional dependencies and environment-specific prerequisites.'

"""
If this is the first time you've seen *Generative AI*, then these statements are ğŸ¤¯. Even if you've been in the space a while, it's still amazing how much a language model can understand and the high level of quality in it's answers.

While this use case is fun, lets try to scale it to a larger set of documents.
"""

"""
_Before continuing, it's important to note that txtai has multiple ways to run LLM inference. In the past, prior to "Chat Templates", it was expected that the submitted text had all the required chat tokens embedded. The same prompt above can also be written with chat messages. This is especially important when working with LLM APIs (i.e. OpenAI, Claude, Bedrock etc)._

```python
llm([
    {"role": "system": "You are a friendly assistant. You answer questions from users."}
    {"role": "user", "content": f"""
        Answer the following question using only the context below. Only include information specifically discussed.

        question: {question}
        context: {text} 
    """}
])
```

_See the [LLM pipeline documentation](https://neuml.github.io/txtai/pipeline/text/llm/) for more information._
"""

"""
# Build a RAG pipeline with vector search

Let's say we have a large number of documents, hundreds/thousands etc. We can't just put all those documents into a single prompt, we'll run out of GPU memory fast!

This is where retrieval augmented generation enters the picture. We can use a query step that finds the best candidates to add to the prompt.

Typically, this candidate query uses vector search but it can be anything that runs a search and returns results. In fact, many complex production systems have customized retrieval pipelines that feed a context into LLM prompts.

The first step in building our RAG pipeline is creating the knowledge store. In this case, it's a vector database of file content. The files will be split into paragraphs with each paragraph stored as a separate row.
"""

import os

from txtai import Embeddings

def stream(path):
  for f in sorted(os.listdir(path)):
    fpath = os.path.join(path, f)

    # Only accept documents
    if f.endswith(("docx", "xlsx", "pdf")):
      print(f"Indexing {fpath}")
      for paragraph in textractor(fpath):
        yield paragraph

# Document text extraction, split into paragraphs
textractor = Textractor(paragraphs=True)

# Vector Database
embeddings = Embeddings(content=True)
embeddings.index(stream("txtai"))
# Output:
#   Indexing txtai/article.pdf

#   Indexing txtai/document.docx

#   Indexing txtai/document.pdf

#   Indexing txtai/spreadsheet.xlsx


"""
The next step is defining the RAG pipeline. This pipeline takes the input question, runs a vector search and builds a context using the search results. The context is then inserted into a prompt template and run with the LLM.
"""

def context(question):
  context =  "\n".join(x["text"] for x in embeddings.search(question))
  return context

def rag(question):
  return execute(question, context(question))

rag("What model does txtai recommend for image captioning?")
# Output:
#   'Based on the provided context, txtai recommends the model "BLIP" for image captioning.'

result = rag("When was the BLIP model added for image captioning?")
print(result)
# Output:
#   The BLIP model was added for image captioning on 2022-03-17.


"""
As we can see, the result is similar to what we had before without vector search. The difference is that we only used a relevant portion of the documents to generate the answer.

As we discussed before, this is important when dealing with large volumes of data. Not all of the data can be added to a LLM prompt. Additionally, having only the most relevant context helps the LLM generate higher quality answers.
"""

"""
# Citations for LLMs

A healthy level of skepticism should be applied to answers generated by AI. We're far from the day where we can blindly trust answers from an AI model.

txtai has a couple approaches for generating citations. The basic approach is to take the answer and search the vector database for the closest match.
"""

for x in embeddings.search(result):
  print(x["text"])
# Output:
#   E5-base-v2

#   Image Captions BLIP

#   Labels - Zero Shot BART-Large-MNLI

#   Model Guide

#   |Component |Model(s)|Date Added|

#   |---|---|---|

#   |Embeddings |all-MiniLM-L6-v2|2022-04-15|

#   |Image Captions |BLIP|2022-03-17|

#   |Labels - Zero Shot |BART-Large-MNLI|2022-01-01|

#   |Large Language Model (LLM) |Mistral 7B OpenOrca|2023-10-01|

#   |Summarization |DistilBART|2021-02-22|

#   |Text-to-Speech |ESPnet JETS|2022-08-01|

#   |Transcription |Whisper|2022-08-01|

#   |Translation |OPUS Model Series|2021-04-06|

#   &"Times New Roman,Regular"&12&A

#   Notebook Description

#   Introducing txtai Overview of the functionality provided by txtai

#   Similarity search with 

#   images Embed images and text into the same space for search


"""
While the basic approach above works in this case, txtai has a more robust pipeline to handle citations and references.

The RAG pipeline is defined below. A RAG pipeline works in the same way as a LLM + Vector Search pipeline, except it has special logic for generating citations. This pipeline takes the answers and compares it to the context passed to the LLM to determine the most likely reference.
"""

from txtai import RAG

# RAG prompt
def prompt(question):
  return [{
    "query": question,
    "question": f"""
Answer the following question using only the context below. Only include information specifically discussed.

question: {question}
context:
"""
}]

# Create LLM with system prompt template
llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ", template="""<|im_start|>system
You are a friendly assistant. You answer questions from users.<|im_end|>
<|im_start|>user
{text} <|im_end|>
<|im_start|>assistant
""")

# Create RAG instance
rag = RAG(embeddings, llm, output="reference")

result = rag(prompt("What version of Python is supported?"), maxlength=4096, pad_token_id=32000)[0]
print("ANSWER:", result["answer"])
print("CITATION:", embeddings.search("select id, text from txtai where id = :id", limit=1, parameters={"id": result["reference"]}))
# Output:
#   ANSWER: Python 3.10+ is supported. Using a Python virtual environment is recommended. The easiest way to install is via pip and PyPI.

#   CITATION: [{'id': '24', 'text': 'Python 3.10+ is supported. Using a Python virtual environment is recommended.'}]


"""
And as we can see, not only is the answer to the statement shown, the RAG pipeline also provides a citation. This step is crucial in any line of work where answers must be verified (which is most lines of work).
"""

"""
_As with the LLM pipeline, the RAG pipeline also supports chat messages. See the [RAG pipeline documentation](https://neuml.github.io/txtai/pipeline/text/rag/) for more._
"""

"""
# Wrapping up

This notebook introduced retrieval augmented generation (RAG), explained why we need it and showed the options available for running RAG pipelines with txtai.

The advantages of building RAG pipelines with txtai are:

- **All-in-one database** - one library can handle LLM inference and vector search retrieval
- **Generating citations** - generating answers is useful but referencing where those answers came from is crucial in gaining the trust of users
- **Simple yet powerful** - building pipelines can be done in a small amount of Python. Options are available to build pipelines in YAML and/or run through the API
"""



================================================
FILE: examples/53_Integrate_LLM_Frameworks.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Integrate LLM Frameworks

The release of [BERT](https://arxiv.org/abs/1810.04805) in 2018 kicked off the language model revolution. The Transformers architecture succeeded RNNs and LSTMs to become the architecture of choice. Unbelievable progress was made in a number of areas: summarization, translation, text classification, entity classification and more. 2023 tooks things to another level with the rise of large language models (LLMs). Models with billions of parameters showed an amazing ability to generate coherent dialogue.

Looking ahead towards the next wave of innovation, we're due for another shift in model architecture. For example, the [Mamba paper](https://arxiv.org/abs/2312.00752) previews a possible future after Transformers.

With that in mind, [txtai](https://github.com/neuml/txtai) now has the capability to easily integrate additional LLM frameworks. While local models through Hugging Face Transformers continues to be the default choice, these additional LLM frameworks broaden the number of options available.

This notebook will demonstrate how txtai can integrate with [llama.cpp](https://github.com/ggerganov/llama.cpp), [LiteLLM](https://github.com/BerriAI/litellm) and custom generation methods. For custom generation, we'll show how to run inference with a `Mamba` model.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook is using optional libraries, we need to install the `pipeline-llm` extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-llm]

"""
# llama.cpp

First, we'll demonstrate how to load a model with llama.cpp. This framework is an extremely popular method with those who run local LLMs. It provides a number of innovations in running LLMs on CPUs, especially on Mac's.

The following example shows a retrieval augmented generation (RAG) pipeline with llama.cpp. txtai automatically loads llama.cpp models when working with GGUF files.
"""

from txtai import Embeddings, RAG, LLM

data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create embeddings
embeddings = Embeddings(content=True, autoid="uuid5")

# Create an index for the list of text
embeddings.index(data)

# Create LLM with llama.cpp - GGUF file is automatically downloaded
llm = LLM("TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf", verbose=True)

template = """<|im_start|>system
You are a friendly assistant. You answer questions from users.<|im_end|>
<|im_start|>user
Find the best matching text in the context for the question. The response should be the text from the context only.

Question:
{question}

Context:
{context}

Text:
<|im_end|>
<|im_start|>assistant
"""

# Create and run RAG instance
rag = RAG(embeddings, llm, output="reference", separator="\n", template=template)
result = rag("Tell me about someone lucky")

print("ANSWER:", result["answer"])
print("REFERENCE:", embeddings.search("select id, text from txtai where id = :id", parameters={"id": result["reference"]}))

# Output:
#   ANSWER: Maine man wins $1M from $25 lottery ticket

#   REFERENCE: [{'id': '37e5fae7-74c2-5f1c-bf69-2962dd7470d1', 'text': 'Maine man wins $1M from $25 lottery ticket'}]


"""
The code above builds an embeddings database, runs a vector search and passes those results to a LLM prompt. As expected, it prints the best answer and reference. The difference is that LLM inference is run through `llama.cpp` vs `transformers`.
"""

"""
# LiteLLM

LiteLLM is an abstraction framework designed to run with API-based LLMs. At the time of writing this article, LiteLLM supports over 100+ LLMs. See the [full list of providers](https://docs.litellm.ai/docs/providers) for all the options.

The following example shows a LLM call with the Hugging Face Inference API. This method automatically detects that this is a LiteLLM model string.
"""

# Hugging Face Inference API
llm = LLM("huggingface/roneneldan/TinyStories-1M")
print(llm("The cat and the dog.", maxlength=5))
# Output:
#   They are friends.


"""
Given that all these APIs require a paid account, we'll leave it to you to try other API models using your own authentication.
"""

"""
# Custom Generation

Last but certainly not least, we'll demonstrate how to add a custom generation framework. For this example, we'll use the recently released [mamba-chat](https://huggingface.co/havenhq/mamba-chat) model to build a RAG pipeline. You can read more about the model in this [GitHub Repository](https://github.com/havenhq/mamba-chat)

The following sections install support for Mamba models, define a Mamba Generation instance and run a Mamba-based RAG pipeline.
"""

%%capture
!pip install mamba-ssm

# Link CUDA libraries into environment
!export LC_ALL="en_US.UTF-8"
!export LD_LIBRARY_PATH="/usr/lib64-nvidia"
!export LIBRARY_PATH="/usr/local/cuda/lib64/stubs"
!ldconfig /usr/lib64-nvidia

import torch

from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

from txtai.pipeline import Generation


class MambaGeneration(Generation):
    def __init__(self, path, template=None, **kwargs):
        super().__init__(path, template, **kwargs)

        self.tokenizer = AutoTokenizer.from_pretrained(path)
        self.tokenizer.eos_token = "<|endoftext|>"
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = MambaLMHeadModel.from_pretrained(path, device="cuda", dtype=torch.float16)

    def execute(self, texts, maxlength, **kwargs):
        results = []
        for text in texts:
            # Tokenize prompt
            tokens = self.tokenizer(text, return_tensors="pt").to("cuda")["input_ids"]

            # Run inference
            output = self.model.generate(input_ids=tokens, max_length=maxlength, eos_token_id=self.tokenizer.eos_token_id, **kwargs)

            # Decode results
            output = self.tokenizer.batch_decode(output)
            output = output[0].split("<|assistant|>\n")[-1].replace("<|endoftext|>", "").strip()
            results.append(output)

        return results

llm = LLM("havenhq/mamba-chat", method="__main__.MambaGeneration")

template = """<|system|>You are a friendly assistant. You answer questions from users.</s>
<|user|>
Find the best matching text in the context for the question. The response should be the text from the context only.

Question:
{question}

Context:
{context}
</s>
<|assistant|>
"""

# Create and run RAG instance
rag = RAG(embeddings, llm, output="reference", separator="\n", template=template)
result = rag("Tell me something about about wildlife")

print("ANSWER:", result["answer"])
print("REFERENCE:", embeddings.search("select id, text from txtai where id = :id", parameters={"id": result["reference"]}))
# Output:
#   ANSWER: The National Park Service warns against sacrificing slower friends in a bear attack.

#   REFERENCE: [{'id': '7224f159-658b-5891-b06c-9a96cfa6a54d', 'text': 'The National Park Service warns against sacrificing slower friends in a bear attack'}]


"""
As expected, the best answer and reference is shown.

There is much to learn and validate about Mamba but it's important to note this model is only 2.8B parameters. The Mamba architecture is one to watch moving forward!
"""

"""
# Wrapping up

This notebook demonstrated how to run LLMs through txtai using alternate LLM frameworks. It's an exciting time in AI/NLP/Machine Learning. What new innovations will 2024 bring? Time will tell but txtai is ready to integrate them in!
"""



================================================
FILE: examples/54_API_Authorization_and_Authentication.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# API Authorization and Authentication

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows. txtai can run in Python, with YAML configuration and through an API service.

The default API service implementation runs without any security. This may be OK for a local prototype or if it's run on a small internal network. But in most cases, additional security measures should be taken.

This notebook will demonstrate how to add authorization, authentication and middleware dependencies to a txtai API service.
"""

"""
# Install dependencies

Install `txtai` and all dependencies. Since this notebook uses the API, we need to install the api extras package.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api]

"""
# Create an API Service

For this example, we'll load an existing txtai index from the Hugging Face Hub.
"""

%%writefile config.yml
cloud:
  provider: huggingface-hub
  container: neuml/txtai-intro

embeddings:
# Output:
#   Writing config.yml


"""
Next, we'll generate a test token to use for this notebook.
"""

import uuid
str(uuid.uuid5(uuid.NAMESPACE_DNS, "TokenTest"))
# Output:
#   'edd590d3-bfab-5425-8a85-79b01e3127ee'

"""
txtai has a default API token authorization method built-in. We'll set a token and start the service.

**It's important to note that this service is running via HTTP as this is only for demonstration purposes. HTTPS must be added either with a proxy service like NGINX or by passing a SSL cert to Uvicorn. See [this link](https://neuml.github.io/txtai/api/security/) for more.**
"""

!CONFIG=config.yml TOKEN=`echo -n 'edd590d3-bfab-5425-8a85-79b01e3127ee' | sha256sum | head -c 64` uvicorn "txtai.api:app" &> api.log &
!sleep 60

"""
# Connect to Service

First, we'll try a request with no token to see what happens.
"""

!curl -X GET -I 'http://localhost:8000/search?query=feel+good+story&limit=1'
# Output:
#   HTTP/1.1 401 Unauthorized

#   [1mdate[0m: Thu, 04 Jan 2024 15:08:38 GMT

#   [1mserver[0m: uvicorn

#   [1mcontent-length[0m: 40

#   [1mcontent-type[0m: application/json

#   


"""
As expected, we received a HTTP 401 saying the request is not authorized.

Now let's try an invalid token.
"""

!curl -X GET -I 'http://localhost:8000/search?query=feel+good+story&limit=1' -H 'Authorization: Bearer junk'
# Output:
#   HTTP/1.1 401 Unauthorized

#   [1mdate[0m: Thu, 04 Jan 2024 15:08:38 GMT

#   [1mserver[0m: uvicorn

#   [1mcontent-length[0m: 40

#   [1mcontent-type[0m: application/json

#   


"""
Once again, the request is rejected.

Let's try again, this time passing a valid API token.
"""

!curl -X GET 'http://localhost:8000/search?query=feel+good+story&limit=1' -H 'Authorization: Bearer edd590d3-bfab-5425-8a85-79b01e3127ee'
# Output:
#   [{"id":"4","text":"Maine man wins $1M from $25 lottery ticket","score":0.08329025655984879}]

"""
This time we get search results!
"""

"""
# Dependencies

Next, let's add a custom dependency to test out authentication. A dependency could integrate with external identity providers to validate user credentials such as OAuth, Active Directory, LDAP or another identity management service.

For this simple example, we'll validate user credentials using basic HTTP authentication. The code below checks if a specific username and password are provided. It's based on [this FastAPI example](https://fastapi.tiangolo.com/advanced/security/http-basic-auth/).
"""

%%writefile authentication.py

import secrets

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBasic, HTTPBasicCredentials

security = HTTPBasic()


class Authentication:
    def __call__(self, credentials: HTTPBasicCredentials = Depends(security)):
        user = credentials.username.encode("utf8")
        validuser = secrets.compare_digest(user, b"txtai")

        password = credentials.password.encode("utf8")
        validpassword = secrets.compare_digest(password, b"theembeddingsdb")

        if not (validuser and validpassword):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect user or password",
                headers={"WWW-Authenticate": "Basic"},
            )

        return credentials.username
# Output:
#   Writing authentication.py


"""
Now let's restart the application and add this dependency in.

**Once again, same note as above, this demonstration uses HTTP. Real use-cases must use HTTPS.**
"""

!killall -9 uvicorn
!CONFIG=config.yml DEPENDENCIES=authentication.Authentication uvicorn "txtai.api:app" &> api.log &
!sleep 30

!curl -X GET -I 'http://localhost:8000/search?query=feel+good+story&limit=1'
# Output:
#   HTTP/1.1 401 Unauthorized

#   [1mdate[0m: Thu, 04 Jan 2024 15:09:10 GMT

#   [1mserver[0m: uvicorn

#   [1mwww-authenticate[0m: Basic

#   [1mcontent-length[0m: 30

#   [1mcontent-type[0m: application/json

#   


"""
The request is rejected as expected. Next let's try an invalid username/password.
"""

!curl -X GET -I 'http://localhost:8000/search?query=feel+good+story&limit=1' -H "Authorization: Basic junk"
# Output:
#   HTTP/1.1 401 Unauthorized

#   [1mdate[0m: Thu, 04 Jan 2024 15:09:10 GMT

#   [1mserver[0m: uvicorn

#   [1mwww-authenticate[0m: Basic

#   [1mcontent-length[0m: 47

#   [1mcontent-type[0m: application/json

#   


"""
Once again, the request is rejected.

Now we'll add the expected username/password to the request. [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication) simply concats the username-password separated by a colon and then base64 encodes it.
"""

!echo -n txtai:theembeddingsdb | base64
# Output:
#   dHh0YWk6dGhlZW1iZWRkaW5nc2Ri


!curl -X GET 'http://localhost:8000/search?query=feel+good+story&limit=1' -H "Authorization: Basic dHh0YWk6dGhlZW1iZWRkaW5nc2Ri"
# Output:
#   [{"id":"4","text":"Maine man wins $1M from $25 lottery ticket","score":0.08329025655984879}]

"""
Now that we have the correct username/password, a response is returned!
"""

"""
# Wrapping up

This notebook introduced how to add authorization, authentication and middleware dependencies to a txtai API service. As noted multiple times, ensure that HTTPS is enabled when using this in production environments.

For more advanced authentication methods, check out the [FastAPI security documentation](https://fastapi.tiangolo.com/tutorial/security/).

"""



================================================
FILE: examples/55_Generate_knowledge_with_Semantic_Graphs_and_RAG.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Generate knowledge with Semantic Graphs and RAG

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Embeddings databases are a union of vector indexes (sparse and dense), graph networks and relational databases. This enables vector search with SQL, topic modeling, retrieval augmented generation and more.

Semantic graphs were introduced in late 2022 with txtai 5.0, right before the start of the LLM era. This notebook picks back up where that work left off, opening a new frontier in semantic graph integration.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] autoawq

"""
# Create a Semantic Graph

First, we're going to create a semantic graph using data from `txtai-wikipedia`, a full Wikipedia txtai embeddings database. We'll use the top 100K articles sorted by popularity.
"""

from txtai import Embeddings

# Create embeddings instance with a semantic graph
embeddings = Embeddings({
  "autoid": "uuid5",
  "path": "intfloat/e5-base",
  "instructions": {
    "query": "query: ",
    "data": "passage: "
  },
  "content": True,
  "graph": {
      "approximate": False,
      "topics": {}
  }
})

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 100000
"""

embeddings.index(wikipedia.search(query))

"""
The code above creates a new embeddings database with a graph component. This adds a separate graph index alongside the database and vector index components.

A semantic graph automatically creates edges between graph nodes using the vector model associated with the embeddings database. Relationships can also be manually specified.

Next, we'll run a search and return a filtered graph containing only the results of a search. This is a new feature as of txtai 7.0.
"""

graph = embeddings.search("Large Language Models", 50, graph=True)

"""
Let's repeat that again, this ran a search and created it's own graph of the search results. ğŸ”¥ğŸ”¥ğŸ”¥ This opens up a lot of interesting possibilities!

Time to plot the search results.
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')} ({x})" for x in graph.scan()}
    options = {
        "node_size": 750,
        "node_color": "#0277bd",
        "edge_color": "#454545",
        "font_color": "#fff",
        "font_size": 6,
        "alpha": 1.0
    }

    fig, ax = plt.subplots(figsize=(17, 8))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

    plt.show()

plot(graph)
# Output:
#   <Figure size 1700x800 with 1 Axes>

"""
This graph shows all the interconnections between the search results. The numbers on each node are the internal node id (same as the index id in an embeddings index).

Let's explore the path between two of these nodes.
"""

for x in graph.showpath(49740, 96081):
    print(graph.node(x))
# Output:
#   {'id': 'Linguistic relativity', 'text': "The idea of linguistic relativity, also known as the Sapirâ€“Whorf hypothesis, the Whorf hypothesis, or Whorfianism, is a principle suggesting that the structure of a language influences its speakers' worldview or cognition, and thus individuals' languages determine or shape their perceptions of the world. ", 'topic': 'language_word_languages_also', 'topicrank': 6, 'score': 0.794975996017456}

#   {'id': 'Input hypothesis', 'text': 'The input hypothesis, also known as the monitor model, is a group of five hypotheses of second-language acquisition developed by the linguist Stephen Krashen in the 1970s and 1980s. Krashen originally formulated the input hypothesis as just one of the five hypotheses, but over time the term has come to refer to the five hypotheses as a group. The hypotheses are the input hypothesis, the acquisitionâ€“learning hypothesis, the monitor hypothesis, the natural order hypothesis and the affective filter hypothesis. The input hypothesis was first published in 1977.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 88, 'score': 0.7856757044792175}

#   {'id': 'Natural language processing', 'text': 'Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 33, 'score': 0.7930918335914612}

#   {'id': 'ML (programming language)', 'text': 'ML (Meta Language) is a functional programming language. It is known for its use of the polymorphic Hindleyâ€“Milner type system, which automatically assigns the data types of most expressions without requiring explicit type annotations (type inference), and ensures type safety; there is a formal proof that a well-typed ML program does not cause runtime type errors. ML provides pattern matching for function arguments, garbage collection, imperative programming, call-by-value and currying. While a general-purpose programming language, ML is used heavily in programming language research and is one of the few languages to be completely specified and verified using formal semantics. Its types and pattern matching make it well-suited and commonly used to operate on other formal languages, such as in compiler writing, automated theorem proving, and formal verification.', 'topic': 'programming_language_computer_level', 'topicrank': 58, 'score': 0.7853577136993408}

#   {'id': 'Domain-specific language', 'text': 'A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.', 'topic': 'programming_language_computer_level', 'topicrank': 29, 'score': 0.780302882194519}


"""
The logic above walked the graph starting at the node for `Linguistic relativity (49740)` and ending at `Domain-specific language (96081)`.

The nodes in between are semantically related and were used to find a semantic path between the nodes.
"""

"""
# RAG with Semantic Graphs

Retrieval augmented generation (RAG) helps reduce the risk of hallucinations by limiting the context in which a LLM can generate answers. This is typically done with a vector search query that hydrates a prompt with a relevant context.

Can we use the semantic graph to power a RAG system? Of course but how?

Looking at the graph above, the most connected nodes are in the middle of the graph. These nodes are the ones that best overall cover the topic in the original query.

Where does this differ from a vector query? Well this method not only looks at semantic similarity between the search and results but the similarity within the results. It's effectively judging the results to see which result is most similar to other results.

Let's show the top 10 central nodes in the graph.
"""

for x in list(graph.centrality().keys())[:10]:
    print(graph.node(x))
# Output:
#   {'id': 'Generative pre-trained transformer', 'text': 'Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 11, 'score': 0.8267543315887451}

#   {'id': 'Large language model', 'text': 'A large language model (LLM) is a large-scale language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation. LLMs are artificial neural networks (mainly transformers) and are (pre)trained using self-supervised learning and semi-supervised learning.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 43, 'score': 0.9012120962142944}

#   {'id': 'Natural language processing', 'text': 'Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 33, 'score': 0.7930918335914612}

#   {'id': 'BERT (language model)', 'text': 'Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that "in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model."', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 78, 'score': 0.8038361072540283}

#   {'id': 'GPT-4', 'text': 'Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI\'s API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and "data licensed from third-party providers" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 57, 'score': 0.8085479140281677}

#   {'id': 'DALL-E', 'text': 'DALLÂ·E, DALLÂ·E 2, and DALLÂ·E 3 are text-to-image models developed by OpenAI using deep learning methodologies to generate digital images from natural language descriptions, called "prompts". ', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 79, 'score': 0.8013206124305725}

#   {'id': 'GPT-3', 'text': 'Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence- and convolution-based architectures with a technique known as "attention". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context and a hitherto-unprecedented 175 billion parameters, requiring 800GB of storage space, and has demonstrated strong "zero-shot" and "few-shot" learning abilities on many tasks.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 52, 'score': 0.8063850402832031}

#   {'id': 'LLaMA', 'text': 'LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. ', 'score': 0.8551499843597412}

#   {'id': 'LaMDA', 'text': "LaMDA (Language Model for Dialogue Applications) is a family of conversational large language models developed by Google. Originally developed and introduced as Meena in 2020, the first-generation LaMDA was announced during the 2021 Google I/O keynote, while the second generation was announced the following year. In June 2022, LaMDA gained widespread attention when Google engineer Blake Lemoine made claims that the chatbot had become sentient. The scientific community has largely rejected Lemoine's claims, though it has led to conversations about the efficacy of the Turing test, which measures whether a computer can pass for a human. In February 2023, Google announced Bard, a conversational artificial intelligence chatbot powered by LaMDA, to counter the rise of OpenAI's ChatGPT.", 'topic': 'artificial_learning_intelligence_based', 'topicrank': 67, 'score': 0.8128454685211182}

#   {'id': 'ChatGPT', 'text': 'ChatGPT (Chat Generative Pre-trained Transformer) is a chatbot developed by OpenAI and launched on November 30, 2022. Based on a large language model, it enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. Successive prompts and replies, known as prompt engineering, are considered at each conversation stage as a context.', 'topic': 'artificial_learning_intelligence_based', 'topicrank': 72, 'score': 0.7816742062568665}


"""
Overall, these all pass the eye test. If we scroll over to see the associated score with the original query we'll note that the highest scores aren't necessarily the most central nodes.

Conceptually this makes sense. We searched for `Large Language Models`, so that page makes sense to score highly. Then we have the GPT page which is even more central. When we think of core concepts with LLMs and the most popular incarnation, ChatGPT, this also makes sense.

Now let's load a LLM and use the central graph nodes as context for RAG.
"""

from txtai import LLM
llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

def facts(graph):
    question = "List 5 facts from this text"
    text = "\n".join(graph.node(x)["text"] for x in list(graph.centrality().keys())[:10])

    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    print(llm(prompt, maxlength=4096))

facts(graph)
# Output:
#   1. GPTs are a type of large language model (LLM) and a prominent framework for generative artificial intelligence.

#   2. They are artificial neural networks used in natural language processing tasks and are based on the transformer architecture.

#   3. GPTs are pre-trained on large data sets of unlabelled text and can generate novel human-like content.

#   4. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.

#   5. LLMs acquire their abilities by using massive amounts of data to learn billions of parameters during training and consume large computational resources during their training and operation.


"""
An interesting summary on LLMs. It's no surprise that GPT and LLM are almost interchangable!
"""

"""
# Explore Wikipedia

RAG with Semantic Graphs is great when we don't have a direct question but instead want to explore a topic. We can start with a topic and this method can tell us about the data. Often one of the hardest parts of search is knowing what to look for.

Let's explore some history.
"""

query = """
SELECT id, text FROM txtai
WHERE similar(:x)
LIMIT 5000
"""

embeddings.index(wikipedia.search(query, parameters={"x": "Vikings in the middle ages"}))

graph = embeddings.search("Viking raids in Ireland", 50, graph=True)
plot(graph)
# Output:
#   <Figure size 1700x800 with 1 Axes>

"""
Let's learn about the Viking Age in Ireland.
"""

facts(graph)
# Output:
#   1. The Battle of Tara was fought between the Gaelic Irish of Meath, led by MÃ¡el Sechnaill mac Domnaill, and the Norse Vikings of Dublin, led by AmlaÃ­b CuarÃ¡n, in 980 near the Hill of Tara in Ireland.

#   2. The Vikings had formed temporary alliances with certain Irish clans between 950-980 AD, which allowed them to continue their raids and plunder of the island.

#   3. The Battle of Tara was a devastating defeat for the Vikings, leading to the Irish regaining control of Dublin.

#   4. The First Viking Age in Ireland began in 795 with Viking raiders targeting Gaelic Irish coastal settlements.

#   5. The Norse Kingdom of Dublin, the earliest and longest-lasting Norse kingdom in Ireland, was established by Vikings in the 9th century and became the biggest slave port in Western Europe under their rule.


"""
Interesting, I definitely learned a few things!

Now let's learn about the Roman Empire and it's armies.
"""

embeddings.index(wikipedia.search(query, parameters={"x": "Roman Empire"}))

graph = embeddings.search("Roman soldiers", 50, graph=True)
plot(graph)
# Output:
#   <Figure size 1700x800 with 1 Axes>

facts(graph)
# Output:
#   1. Vexillarius is a term referring to a type of Roman soldier.

#   2. A numerus was a unit of the Roman army.

#   3. Roman legionaries were professional heavy infantrymen who conquered and defended territories during the late Republic and Principate eras.

#   4. Comitatenses and later Palatini were units of the field armies of the late Roman Empire, replacing the legionaries.

#   5. A hastiliarius was a weapons instructor in the Roman Empire who trained troops in standard weapons and fighting techniques.


"""
# Explore ArXiv

There is also a full ArXiv txtai index available on the Hugging Face Hub. This has the full metadata set (titles + abstracts). Let's take a look at this.
"""

# Load dataset
arxiv = Embeddings()
arxiv.load(provider="huggingface-hub", container="neuml/txtai-arxiv")

query = """
SELECT id, text FROM txtai
WHERE similar(:x)
LIMIT 1000
"""

embeddings.index(arxiv.search(query, parameters={"x": "Retrieval-Augmented Generation"}))

graph = embeddings.search("Best ways to populate context", 50, graph=True)
plot(graph)
# Output:
#   <Figure size 1700x800 with 1 Axes>

facts(graph)
# Output:
#   1. Context Tuning for Retrieval Augmented Generation (RAG) improves tool retrieval and plan generation by employing a smart context retrieval system to fetch relevant information.

#   2. Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models aims to improve downstream performance of smaller models by generating synthetic data.

#   3. Query Expansion by Prompting Large Language Models leverages the generative abilities of LLMs for query expansion using prompts like zero-shot, few-shot, and Chain-of-Thought (CoT).

#   4. In-Context Retrieval-Augmented Language Models prepend grounding documents to the input without modifying the LM architecture, providing significant LM gains across model sizes and diverse corpora.

#   5. Generate rather than Retrieve: Large Language Models are Strong Context Generators proposes a generate-then-read (GenRead) method that prompts a large language model to generate context-based documents for answering questions.


"""
This wouldn't be an article about RAG without exploring the literature available on RAG! Definitely some interesting articles here.

Last we'll run my "go-to" ArXiv demo query on the search for ET ğŸ‘½.
"""

embeddings.index(arxiv.search(query, parameters={"x": "Alien life"}))

graph = embeddings.search("Alien encounters on Earth", 50, graph=True)
plot(graph)
# Output:
#   <Figure size 1700x800 with 1 Axes>

facts(graph)
# Output:
#   1. Earth has been thoroughly surveilled.

#   2. Earth will be contacted in due course.

#   3. SETI beyond half the distance that Earth's EM has reached (~35-50 LY) is futile.

#   4. The very quiescence of the galaxy paradoxically implies that that Drake's N = many, and that there is a system of galactic governance.

#   5. Search strategies are proposed to detect the described probe-node-land base communications pathway.


"""
The literature in ArXiv is pretty direct on what's going to happen! ğŸ‘½
"""

"""
# Topic Generation

The last item we'll cover in this article is topic generation. Semantic graphs have a built-in method to create topic labels using a BM25 index. This works well enough to give a general idea on what the documents in an article discuss.

Can we do better with a LLM?
"""

def topic(graph):
    topic = list(graph.topics.keys())[0]

    text = "\n".join(graph.node(x)["text"] for x in graph.topics[topic])

    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Label the following text with a topic name in a couple words.

    text: {text}
    topic: <|im_end|>
    <|im_start|>assistant
    """

    print(f"Topic: \"{topic}\", Generated Topic: \"{llm(prompt, maxlength=4096)}\"")

topic(graph)
# Output:
#   Topic: "from_reported_reports_earth", Generated Topic: "UFOs and Alien Encounters"


"""
The code above took the article text for a topic and run it through a LLM prompt. Looking at the original topic, we have some idea on what it has to do with but the LLM is much more direct.

The BM25 index is fast. The LLM is not. It's a tradeoff but an option to consider.
"""

"""
# Wrapping up

This notebook (re)introduced the semantic graph and ways that it can be used standalone as well as with LLMs/RAG.

Semantic graphs, as constructed in txtai, are a novel approach that few if any other systems have. So expect some disruption once this becomes more popular and known!
"""



================================================
FILE: examples/56_External_vectorization.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# External vectorization

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Vectorization is the process of transforming data into numbers using machine learning models. Input data is run through a model and fixed dimension vectors are returned. These vectors can then be loaded into a vector database for similarity search.

txtai is an open-source first system. Given it's own open-source roots, like-minded projects such as [sentence-transformers](https://github.com/UKPLab/sentence-transformers) are prioritized during development. But that doesn't mean txtai can't work with Embeddings API services.

This notebook will show to use txtai with external vectorization.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Create an Embeddings dataset

The first thing we'll do is pre-compute an embeddings dataset. In addition to Embeddings APIs, this can also be used during internal testing to tune index and database settings.
"""

from txtai import Embeddings

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

# Query for Top 10,000 most popular articles
query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 10000
"""

data = wikipedia.search(query)

# Encode vectors using same vector model as Wikipedia
vectors = wikipedia.batchtransform(x["text"] for x in data)

# Build dataset of id, text, embeddings
dataset = []
for i, row in enumerate(data):
  dataset.append({"id": row["id"], "article": row["text"], "embeddings": vectors[i]})


"""
# Build an Embeddings index with external vectors

Next, we'll create an Embedding index with an external transform function set.

The external transform function can be any function or callable object. This function takes an array of data and returns an array of embeddings.
"""

def transform(inputs):
  return wikipedia.batchtransform(inputs)

def stream():
  for row in dataset:
    # Index vector
    yield row["id"], row["embeddings"]

    # Index metadata
    yield {"id": row["id"], "article": row["article"]}

embeddings = Embeddings(transform="__main__.transform", content=True)
embeddings.index(stream())

"""
ğŸš€ Notice how fast creating the index was compared to indexing. This is because there is no vectorization! Now let's run a query.
"""

embeddings.search("select id, article, score from txtai where similar(:x)", parameters={"x": "operating system"})
# Output:
#   [{'id': 'Operating system',

#     'article': 'An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.',

#     'score': 0.8955847024917603},

#    {'id': 'MacOS',

#     'article': "macOS (;), originally Mac\xa0OS\xa0X, previously shortened as OS\xa0X, is an operating system developed and marketed by Apple Inc. since 2001. It is the primary operating system for Apple's Mac computers. Within the market of desktop and laptop computers, it is the second most widely used desktop OS, after Microsoft Windows and ahead of all Linux distributions, including ChromeOS.",

#     'score': 0.8666583299636841},

#    {'id': 'Linux',

#     'article': 'Linux is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds. Linux is typically packaged as a Linux distribution (distro), which includes the kernel and supporting system software and libraries, many of which are provided by the GNU Project. Many Linux distributions use the word "Linux" in their name, but the Free Software Foundation uses and recommends the name "GNU/Linux" to emphasize the use and importance of GNU software in many distributions, causing some controversy.',

#     'score': 0.839817225933075}]

"""
All as expected! This method can also be used with existing datasets on the Hugging Face Hub.
"""

"""
# Integrate with Embeddings API services

Next, we'll integrate with an Embeddings API service to build vectors.

The code below interfaces with the Hugging Face Inference API. This can easily be switched to OpenAI, Cohere or even your own local API.
"""

import numpy as np
import requests

BASE = "https://api-inference.huggingface.co/pipeline/feature-extraction"

def transform(inputs):
  # Your API provider of choice
  response = requests.post(f"{BASE}/sentence-transformers/nli-mpnet-base-v2", json={"inputs": inputs})
  return np.array(response.json(), dtype=np.float32)

data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, " +
  "forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends " +
  "in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

embeddings = Embeddings({"transform": transform, "backend": "numpy", "content": True})
embeddings.index(data)
embeddings.search("feel good story", 1)
# Output:
#   [{'id': '4',

#     'text': 'Maine man wins $1M from $25 lottery ticket',

#     'score': 0.08329013735055923}]

"""
This is the classic txtai tutorial example. Except this time, vectorization is run with an external API service!
"""

"""
# Wrapping up

This notebook showed how txtai can integrate with external vectorization. This can be a dataset with pre-computed embeddings and/or an Embeddings API service.

Each of txtai's components can be fully customized and vectorization is no exception. Flexibility and customization for the win!
"""



================================================
FILE: examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build knowledge graphs with LLM-driven entity extraction

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Embeddings databases are a union of vector indexes (sparse and dense), graph networks and relational databases. This enables vector search with SQL, topic modeling, retrieval augmented generation and more.

Up until txtai 7.0, semantic graphs only supported automatic relationship detection. Now relationships can be loaded directly into a txtai database. This notebook will demonstrate how to work with this new feature.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] autoawq

"""
# Load Wikipedia database

The [txtai-wikipedia](https://huggingface.co/NeuML/txtai-wikipedia) database stores all Wikipedia article abstracts as of January 2024. This database is a great way to explore a wide variety of topics. It also has the number of page views integrated in, which enables pulling frequently viewed or popular articles on a topic.

For this example, we'll work with a couple articles related to `Viking raids in France`.
"""

from txtai import Embeddings

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai WHERE similar('Viking raids in France') and percentile >= 0.5
"""

results = wikipedia.search(query, 5)

for x in results:
    print(x)

"""
# LLM-driven entity extraction

Now that we have a couple relevant articles, let's go through and run an entity extraction process. For this task, we'll have a LLM prompt to do the work.
"""

import json

from txtai import LLM

# Load LLM
llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

data = []
for result in results:
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Extract an entity relationship graph from the following text. Output as JSON

    Nodes must have label and type attributes. Edges must have source, target and relationship attributes.

    text: {result['text']} <|im_end|>
    <|im_start|>assistant
    """

    try:
        data.append(json.loads(llm(prompt, maxlength=4096)))
    except:
        pass

"""
# Build an embeddings database

Now that we've extracted entities from the documents, next we'll review and load a graph network using these entity-relationships.
"""

def stream():
    nodes = {}

    for row in data.copy():
        # Create nodes
        for node in row["nodes"]:
            if node["label"] not in nodes:
                node["id"] = len(nodes)
                nodes[node["label"]] = node

        for edge in row["edges"]:
            source = nodes.get(edge["source"])
            target = nodes.get(edge["target"])

            if source and target:
                if "relationships" not in source:
                    source["relationships"] = []

                source["relationships"].append({"id": target["id"], "relationship": edge["relationship"]})

    return nodes.values()

# Create embeddings instance with a semantic graph
embeddings = Embeddings(
    autoid = "uuid5",
    path = "intfloat/e5-base",
    instructions = {
        "query": "query: ",
        "data": "passage: "
    },
    columns = {
        "text": "label"
    },
    content = True,
    graph = {
        "approximate": False,
        "topics": {}
    }
)

embeddings.index(stream())

"""
# Show the network

Now let's visualize the entity-relationship network. This period might not be that familar to most, unless you've watched the Vikings TV series, in which case it should make sense.
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'text')} ({x})" for x in graph.scan()}
    lookup = {
      "Person": "#d32f2f",
      "Location": "#0277bd",
      "Event": "#e64980",
      "Role": "#757575"
    }

    colors = []
    for x in graph.scan():
      value = embeddings.search("select type from txtai where id = :x", parameters={"x": x})[0]["type"]
      colors.append(lookup.get(value, "#7e57c2"))

    options = {
        "node_size": 2000,
        "node_color": colors,
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 11,
        "alpha": 1.0
    }

    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=3, iterations=250)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

plot(embeddings.graph)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
# Graph traversal

The last thing we'll cover is extracting a specific path from the graph. Let's show a path from node 8 to node 5 requiring a specific relationship type to start.

A new feature of txtai 7.0 is the ability to return a graph of search results. This is a powerful addition as not only do we get the search results but we get how the search results relate to each other.
"""

# Traverse graph looking for certain nodes and edge values
g = embeddings.graph.search("""
  MATCH P=(A{id: 8})-[R1]->()-[*1..3]->(D{id:5})
  WHERE
    R1.relationship == "has_object"
  RETURN P
""", graph=True)

plot(g)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
# Wrapping up

This notebook showed how knowledge graphs can be created with LLM-driven entity extraction. Automatically derived relationships via semantic similarity and manually specified ones are a powerful combination!
"""



================================================
FILE: examples/58_Advanced_RAG_with_graph_path_traversal.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Advanced RAG with graph path traversal

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Embeddings databases are a union of vector indexes (sparse and dense), graph networks and relational databases. This enables vector search with SQL, topic modeling, retrieval augmented generation (RAG) and more.

A standard RAG process typically runs a single vector search query and returns the closest matches. Those matches are then passed into a LLM prompt and used to limit the context and help ensure more factually correct answers are generated. This works well with most simple cases. More complex use cases, require a more advanced approach.

This notebook will demonstrate how semantic graphs can be used to build a more comprehensive context for LLM generation. It will cover an example of writing a short book on English history from the fall of the Roman Empire to the Norman conquest.

"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] autoawq

"""
# Build a knowledge base

The first step we'll take in writing this book is collecting information. We'll use the [txtai-wikipedia](https://huggingface.co/NeuML/txtai-wikipedia) database, which stores all Wikipedia article abstracts as of January 2024. This database is a great way to explore a wide variety of topics. It also has the number of page views integrated in, which enables pulling frequently viewed or popular articles on a topic.

For this example, we'll load the top 100,000 most popular articles across all of Wikipedia.
"""

from txtai import Embeddings

# Create embeddings instance with a semantic graph
embeddings = Embeddings({
  "autoid": "uuid5",
  "path": "intfloat/e5-base",
  "instructions": {
    "query": "query: ",
    "data": "passage: "
  },
  "content": True,
  "graph": {
      "approximate": False,
      "topics": {}
  }
})

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 100000
"""

embeddings.index(wikipedia.search(query))

"""
# Collect articles of interest

Next, we'll build a graph query that traverses topics of interest for our book. Given that this book is about the history of England from the fall of Rome to the Norman conquest, we'll walk the graph and pull in articles relevant to that period of time.

The following builds a graph path traversal query that finds the top 20 paths between these topics.
"""

g = embeddings.graph.search("""
MATCH P=({id: "Roman Empire"})-[*1..3]->({id: "Saxons"})-[*1..3]->({id: "Vikings"})-[*1..3]->({id: "Battle of Hastings"})
RETURN P
LIMIT 20
""", graph=True)

"""
A new feature of txtai 7.0 is the ability to return a graph of search results. This is a powerful addition as not only do we get the search results but we get how the search results relate to each other.

Now lets build a visualization showing what we've collected.
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}
    colors = ["#D32F2F", "#0277bd", "#7e57c2", "#757575"]

    results = embeddings.batchsimilarity(labels.values(), ["Roman Empire", "Germanic barbarians", "Vikings", "Normans invade"])
    colors = [colors[x[0][0]] for x in results]

    options = {
        "node_size": 2000,
        "node_color": colors,
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 11,
        "alpha": 1.0,
    }

    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

    plt.show()

plot(g)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
Each one of the nodes in the graph above is a Wikipedia article abstract. For example, the `Saxons` entry is the Wikipedia abstract text for the Saxons article. The `Roman Empire` gives a summary of the Roman Empire and so forth.

Clearly, we have aggregated an extremely relevant set of articles with the graph path query. This is a solid knowledge base related to the fall of Rome, Saxons, Vikings and Norman conquest.
"""

"""
# Write a short history book

Now that we have our collection of articles, let's write our book!

We'll load a LLM and build a prompt that uses the graph network of articles and a user input command to build the book.
"""

from txtai import LLM

llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

def rag(question, text):
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    return llm(prompt, maxlength=4096)

context = "\n".join(g.attribute(node, "text") for node in list(g.scan()))

print(rag("Write a book covering the end of Roman Britain, Saxons, Vikings and Norman conquest of England", context))

# Output:
#   Title: "The Fall and Rise of Britain: From Roman Rule to the Norman Conquest"

#   

#   Chapter 1: The Fall of the Western Roman Empire

#   The Western Roman Empire fell due to a combination of factors, including the effectiveness and numbers of the army, the health and numbers of the Roman population, the strength of the economy, the competence of the emperors, internal struggles for power, religious changes, and the efficiency of the civil administration. Barbarian invasions and climatic changes also played a significant role in the collapse.

#   

#   Chapter 2: Roman Britain

#   Roman Britain was the territory that became the Roman province of Britannia after the Roman conquest of Britain, consisting of a large part of the island of Great Britain. The occupation lasted from AD 43 to AD 410.

#   

#   Chapter 3: The End of Roman Rule in Britain

#   Roman rule in Britain ended in different parts of the country at different times and under different circumstances. The recall of Roman troops to Gaul by Constantine III in 407 left Britain vulnerable to barbarian attacks. Around 410, the Romano-British expelled the Roman magistrates from Britain, leading to the fall of Roman rule.

#   

#   Chapter 4: Sub-Roman Britain

#   Sub-Roman Britain is the period of late antiquity in Great Britain between the end of Roman rule and the Anglo-Saxon settlement. This period saw the decay of locally made wares from a previous higher standard under the Roman Empire.

#   

#   Chapter 5: The Saxon Settlement

#   The Saxons were a group of Germanic peoples who played a major role in the fall of the Western Roman Empire and the establishment of the post-Roman kingdoms. They settled in Britain, contributing to the decline of Romano-British culture and the rise of a new Germanic culture.

#   

#   Chapter 6: The Viking Age

#   The Viking Age was a period during the Middle Ages when Norsemen, known as Vikings, undertook large-scale raiding, colonizing, conquest, and trading throughout Europe. They also voyaged as far as the Mediterranean, North Africa, the Middle East, Greenland, and North America.

#   

#   Chapter 7: The Norman Conquest of England

#   The Battle of Hastings, fought on 14 October 1066, marked the beginning of the Norman Conquest of England. The Normans, led by William, Duke of Normandy, defeated the English army under King Harold Godwinson, leading to a decisive Norman victory and the end of Anglo-Saxon rule in England.

#   

#   Chapter 8: The Impact of the Norman Conquest on England

#   The Norman Conquest of England had a profound impact on the country's culture, language, and governance. The Normans introduced feudalism, the use of the Norman French language, and a new system of land ownership and administration.

#   

#   Chapter 9: The Legacy of the Saxons, Vikings, and Normans on Modern Britain

#   The Saxons, Vikings, and Normans left a lasting legacy on modern Britain, shaping its language, culture, and political landscape. Their influence can still be seen in modern British society, language, and institutions.


"""
_It's important to note that txtai has multiple ways to run LLM inference. In the past, prior to "Chat Templates", it was expected that the submitted text had all the required chat tokens embedded. The same prompt above can also be written with chat messages. This is especially important when working with LLM APIs (i.e. OpenAI, Claude, Bedrock etc)._

```python
llm([
    {"role": "system": "You are a friendly assistant. You answer questions from users."}
    {"role": "user", "content": f"""
        Answer the following question using only the context below. Only include information specifically discussed.

        question: {question}
        context: {text} 
    """}
])
```

_See the [LLM pipeline documentation](https://neuml.github.io/txtai/pipeline/text/llm/) for more information._
"""

"""
# Wrapping up

Just that like that, we have a short book of history covering the early medieval period in England! ğŸ“–ğŸ›¡ï¸ğŸ—¡ï¸ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿

It's quite amazing when we think about it. This notebook took the full set of Wikipedia articles, walked a graph to pull articles related to a series of topics and used that to build a short book!

It's also a stark contrast to a standard RAG process. There is no way we'd be able to get the breadth of knowledge with a simple vector search query. The graph path traversal is the key advancement here. This is just the beginning, more to come!
"""



================================================
FILE: examples/59_Whats_new_in_txtai_7_0.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ğŸ’¡ What's new in txtai 7.0

txtai 7.0 brings a number of major feature enhancements. Highlights include:

- Semantic Graph 2.0
  - Graph search
  - Advanced graph traversal
  - Graph RAG

- Embeddings
  - Default configuration format now JSON
  - Move ids storage outside of configuration when content is disabled

- Pipelines
  - Training support for LoRA / QLoRA

- API
  - Binary transport support

These are just the big, high level changes. There are also many improvements and bug fixes.

This notebook will cover all the changes with examples.

**Standard upgrade disclaimer below**

While everything is backwards compatible, it's prudent to backup production indexes before upgrading and test before deploying.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api,graph,pipeline-train] datasets autoawq

"""
# Semantic Graph 2.0

The biggest change and reason this is a major release is the addition of a number of new graph-driven patterns. Let's jump right into with that.

## Graph search

The first feature we'll test out is running a search that returns results as a graph. With this change, not only do we get search results, we get how these search results relate to each other.

We'll use a [prompt dataset on the Hugging Face Hub](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts) for all examples.
"""

from datasets import load_dataset

import txtai

# Load dataset
ds = load_dataset("fka/awesome-chatgpt-prompts", split="train")

def stream():
  for row in ds:
    yield (row["act"], f"{row['act']} {row['prompt']}")

# Build sparse keyword index
embeddings = txtai.Embeddings(content=True, graph={"approximate": False})
embeddings.index(stream())

graph = embeddings.search("Linux terminal", 5, graph=True)

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')} ({x})" for x in graph.scan()}
    options = {
        "node_size": 1500,
        "node_color": "#0277bd",
        "edge_color": "#454545",
        "font_color": "#fff",
        "font_size": 9,
        "alpha": 1.0
    }

    fig, ax = plt.subplots(figsize=(16, 5))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

    plt.show()

plot(graph)
# Output:
#   <Figure size 1600x500 with 1 Axes>

"""
We now see a graph of not only the search results but how they relate to each other!
"""

"""
## Advanced graph traversal

Before 7.0, the main way to traverse a graph was via the `showpath` method. This method finds the shortest path between two graph nodes. What if we want more control over how a path is traversed? Enter advanced graph traversal.
"""

g = embeddings.graph.search("""
MATCH P=({id: "Poet"})-[*1..2]->({id: "Rapper"})
RETURN P
LIMIT 5
""", graph=True)

plot(g)
# Output:
#   <Figure size 1600x500 with 1 Axes>

"""
The query above finds the top 5 connections between a `Poet` and a `Rapper` using a graph query. Graph queries use the [openCypher](https://github.com/opencypher/openCypher) query standard.
"""

"""
## Graph RAG

Graph path traversal opens up a different type of RAG process. A standard RAG process typically runs a single vector search query and returns the closest matches. Those matches are then passed into a LLM prompt and used to limit the context and help ensure more factually correct answers are generated. Graphs enable more complex analysis.

We'll use the graph path from the previous example for a more complex RAG query.
"""

from txtai import LLM

llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

def rag(question, text):
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    return llm(prompt, maxlength=4096)

context = "\n".join(g.attribute(node, "text") for node in list(g.scan()))

print(rag("What roles are similar to both a rapper and poet?", context))

# Output:
#   The roles that are similar to both a rapper and poet are:

#   

#   1. Composer: A composer creates music for various forms of art, including songs and poems. They work with different instruments and tools to bring the lyrics to life, making the music harmonious and engaging.

#   

#   2. Novelist: A novelist creates captivating stories with engaging characters and plotlines. They can write in various genres, such as science fiction, romance, or historical fiction. The goal is to write a story that keeps readers engaged and entertained.

#   

#   3. Movie Critic: A movie critic evaluates and reviews movies, discussing aspects like plot, themes, acting, direction, and more. They aim to express their feelings about the movie and how it impacted them, while also providing constructive criticism.

#   

#   4. Motivational Speaker: A motivational speaker inspires and empowers their audience by sharing words of wisdom and encouragement. They can talk about various topics, but the goal is to make their audience feel motivated and inspired to achieve their goals.


"""
Let's compare these results with the results from a standard RAG query. We'll pull the 6 most similar rows to have the same sized dataset as what is in the graph above.
"""

question = "What roles are most similar role to both a rapper and poet?"
context = "\n".join(x["text"] for x in embeddings.search(question, limit=6))
print(rag(question, context))
# Output:
#   The roles most similar to both a rapper and poet are the Composer and the Song Recommender. Both roles involve creating music or recommending songs based on given lyrics or themes.


"""
As we can see, the Graph RAG approach yields a more comprehensive answer. The standard RAG answer isn't bad, it's just not as complete.
"""

"""
# Embeddings

There are a couple backwards compatible changes to the embeddings database format. The default configuration format moving forward is `json`. While `pickle` configuration is still supported, txtai is moving towards a readable configuration format. This is to have maximum compatability with the Hugging Face Hub, when uploading models. The `pickle` format is generally not recommended when sharing indexes.
"""

import json
import pickle

from txtai import Embeddings

# Create a default index
embeddings = Embeddings()
embeddings.index(["test1", "test2"])
embeddings.save("index")

# Read standard configuration
with open("index/config.json") as f:
    print(json.dumps(json.load(f), sort_keys=True, default=str, indent=2))

# Read ids
with open("index/ids", "rb") as f:
    print("ID List:", pickle.load(f))
# Output:
#   {

#     "autoid": 2,

#     "backend": "faiss",

#     "build": {

#       "create": "2024-02-21T16:23:26Z",

#       "python": "3.8.18",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "7.0.0"

#     },

#     "dimensions": 384,

#     "offset": 2,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2024-02-21T16:23:26Z"

#   }

#   ID List: [0, 1]


"""
When no configuration is specified, notice that a `config.json` file is created along with an `ids` file. Ids are no longer stored within the configuration both for `json` and `pickle` configuration. When loading an existing index, the ids are automatically read and moved when saving a new version.
"""

"""
# LoRA / QLoRA support

Two new parameters have been added to the `HFTrainer` pipeline, `lora` and `quantize`. When both of those are enabled, models are trained using QLoRA. Custom settings are also supported.
"""

from txtai.pipeline import HFTrainer

trainer = HFTrainer()
model, _ = trainer(
    "ahxt/LiteLlama-460M-1T",
    [{"label": 0, "text": "sample text"}],
    maxlength=16,
    task="language-generation",
    quantize=True,
    lora=True,
)
# Output:
#   trainable params: 8,355,840 || all params: 470,041,600 || trainable%: 1.7776809541963945

#     0%|          | 0/3 [00:00<?, ?it/s]
#   {'train_runtime': 0.3832, 'train_samples_per_second': 7.829, 'train_steps_per_second': 7.829, 'train_loss': 9.008923212687174, 'epoch': 3.0}


"""
# Binary transport support

The API added support for reading and writing binary content. These changes will be pushed to the API clients in a future release. These changes include:

- Images and media content
- Encoding binary JSON and using MessagePack
"""

%%writefile index.yml

embeddings:
# Output:
#   Overwriting index.yml


!CONFIG=index.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 90

import requests

requests.post("http://localhost:8000/add", json=["test"])
requests.get("http://localhost:8000/index")

print(requests.get("http://localhost:8000/count", headers={"Accept": "application/json"}).content)
print(requests.get("http://localhost:8000/count", headers={"Accept": "application/msgpack"}).content)
# Output:
#   b'0'

#   b'\x00'


"""
Notice the subtle but important difference between the two outputs. The first response is a `0` character as JSON. The second response is the `\x00` character and can be intrepreted as a `0` using MessagePack. See below.
"""

import msgpack
print(msgpack.loads(requests.get("http://localhost:8000/count", headers={"Accept": "application/msgpack"}).content))
# Output:
#   0


"""
# Wrapping up

This notebook gave a quick overview of txtai 7.0. Updated documentation and more examples will be forthcoming. There is much to cover and much to build on!

See the following links for more information.

- [7.0 Release on GitHub](https://github.com/neuml/txtai/releases/tag/v7.0.0)
- [Documentation site](https://neuml.github.io/txtai)
"""



================================================
FILE: examples/60_Advanced_RAG_with_guided_generation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Advanced RAG with guided generation

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

A standard RAG process typically runs a single vector search query and returns the closest matches. Those matches are then passed into a LLM prompt and used to limit the context and help ensure more factually correct answers are generated. This works well with most simple cases. More complex use cases, require a more advanced approach.

This notebook will demonstrate how constrained or guided generation can be applied to better control LLM output.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai autoawq outlines

"""
# Define the RAG process

The first step we'll take is to define the RAG process. The following code creates a LLM instance, defines method that takes a question and context then prompts an LLM.
"""

from txtai import LLM

llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

def rag(question, text):
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    return llm(prompt, maxlength=4096)


"""
Let's run a simple RAG call to get the idea of the default behavior.
"""

# Manually generated context. Replace with an Embedding search or other request. See prior examples on txtai's documentation site for more.
context = """
England's terrain chiefly consists of low hills and plains, especially in the centre and south.
The Battle of Hastings was fought on 14 October 1066 between the Norman army of William, the Duke of Normandy, and an English army under the Anglo-Saxon King Harold Godwinson
Bounded by the Atlantic Ocean on the east, Brazil has a coastline of 7,491 kilometers (4,655 mi).
Spain pioneered the exploration of the New World and the first circumnavigation of the globe.
Christopher Columbus lands in the Caribbean in 1492.
"""

print(rag("List the countries discussed", context))
# Output:
#   1. England

#   2. Brazil

#   3. Spain


"""
# Guided Generation

The next step is defining how to guide generation. For this step, we'll use the [Outlines](https://github.com/outlines-dev/outlines) library. Outlines is a library for controlling how tokens are generated. It applies logic to enforce schemas, regular expressions and/or specific output formats such as JSON.

For our first example, we'll guide generation with a model that has answers and citations. With this multi-answer and multi-citation model, we can generate multiple answers along with associated references on how those answers were derived.
"""

from typing import List

from outlines.integrations.transformers import JSONPrefixAllowedTokens
from pydantic import BaseModel

class Response(BaseModel):
    answers: List[str]
    citations: List[str]

# Define method that guides LLM generation
prefix_allowed_tokens_fn=JSONPrefixAllowedTokens(
    schema=Response,
    tokenizer_or_pipe=llm.generator.llm.pipeline.tokenizer,
    whitespace_pattern=r" ?"
)

def rag(question, text):
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    return llm(prompt, maxlength=4096, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)


"""
Couple things to unpack here.

First, note the method `prefix_allowed_tokens_fn`. This method applies a [Pydantic](https://github.com/pydantic/pydantic) model to constrain/guide how the LLM generates tokens. Next, see how that constrain can be applied to txtai's LLM pipeline.

Let's try it out.
"""

import json

json.loads(rag("List the countries discussed", context))
# Output:
#   {'answers': ['England', 'Brazil', 'Spain'],

#    'citations': ["England's terrain chiefly consists of low hills and plains, especially in the centre and south.",

#     'The Battle of Hastings was fought on 14 October 1066 between the Norman army of William, the Duke of Normandy, and an English army under the Anglo-Saxon King Harold Godwinson.',

#     'Bounded by the Atlantic Ocean on the east, Brazil has a coastline of 7,491 kilometers (4,655 mi).',

#     'Spain pioneered the exploration of the New World and the first circumnavigation of the globe.',

#     'Christopher Columbus lands in the Caribbean in 1492.']}

"""
This is pretty ğŸ”¥

See how not only are the answers generated as they were previously but the answers are now list of answers. And there is a list of citations supporting how the answers were generated! This is also valid JSON.
"""

"""
# Extracting information models

In our last example, we'll define a more complex model to help with extracting structured information.
"""

class Response(BaseModel):
    countries: List[str]
    geography: List[str]
    years: List[str]
    people: List[str]

prefix_allowed_tokens_fn=JSONPrefixAllowedTokens(
    schema=Response,
    tokenizer_or_pipe=llm.generator.llm.pipeline.tokenizer,
    whitespace_pattern=r" ?"
)

def rag(question, text):
    prompt = f"""<|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information specifically discussed.

    question: {question}
    context: {text} <|im_end|>
    <|im_start|>assistant
    """

    return llm(prompt, maxlength=4096, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)


json.loads(rag("List the entities discussed", context))
# Output:
#   {'countries': ['England', 'Brazil', 'Spain'],

#    'geography': ['low hills and plains', 'Atlantic Ocean', 'New World'],

#    'years': ['1066', '1492'],

#    'people': ['William, the Duke of Normandy',

#     'Anglo-Saxon King Harold Godwinson',

#     'Christopher Columbus']}

"""
This is extremely guided generation. This is constraining the output of the LLM into a very specific model of information. It's quite impressive and simple to get started with!
"""

"""
# Wrapping up

Guided generation adds a number of different options to the RAG toolkit. It's a flexible approach that can enable more advanced functionality and/or fine-tuned control over how content is created.

It does add overhead which may or may not be acceptable depending on the use case. Expect new methods with improved efficiency and accuracy coming in the future. The space continues to advance forward fast!
"""



================================================
FILE: examples/61_Integrate_txtai_with_Postgres.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Integrate txtai with Postgres

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

The default persistence methods for txtai are local and file-based. `SQLite` for content, `Faiss` for vectors and `NetworkX` for graph data. The main value add of txtai is getting up and running quickly with a minimal amount of external dependencies.

Another key feature of txtai is being able to quickly move from prototyping to production. This notebook will demonstrate how txtai can integrate with [Postgres](https://www.postgresql.org/), a powerful, production-ready and open source object-relational database system. After txtai persists content to Postgres, we'll show it can be directly queried with SQL from any Postgres client ğŸ”¥
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture

# Install txtai and dependencies
!pip install git+https://github.com/neuml/txtai#egg=txtai[ann,database,graph]

"""
# Install Postgres

Next, we'll install Postgres and start a Postgres instance. This will also install the `pgvector` extension to enable vector search and storage.

*Note: With local environments, consider running Postgres as a Docker container.*
"""

%%capture

# Install Postgres and pgvector
!apt-get update && apt install postgresql postgresql-server-dev-14
!git clone --branch v0.6.2 https://github.com/pgvector/pgvector.git
!cd pgvector && make && make install

# Start database
!service postgresql start
!sudo -u postgres psql -U postgres -c "ALTER USER postgres PASSWORD 'pass';"

"""
# Build an Embeddings database
First, we'll load the 100K most popular articles from Wikipedia. This example will store vectors and content in Postgres.
"""

from txtai import Embeddings

# URL set in code for demo purposes. Use environment variables in production.
url = "postgresql+psycopg2://postgres:pass@localhost/postgres"

# Create embeddings
embeddings = Embeddings(
    content=url,
    backend="pgvector",
    pgvector={
        "url": url
    }
)

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 100000
"""

# Index dataset
embeddings.index(wikipedia.search(query))

embeddings.search("Tell me about a mythical horse", 1)
# Output:
#   [{'id': 'Unicorn',

#     'text': 'The unicorn is a legendary creature that has been described since antiquity as a beast with a single large, pointed, spiraling horn projecting from its forehead.',

#     'score': 0.6493861675262451}]

embeddings.search("What is the main ingredient in Ketchup?", 1)
# Output:
#   [{'id': 'Ketchup',

#     'text': 'Ketchup or catsup is a table condiment with a sweet and sour flavor. The unmodified term ("ketchup") now typically refers to tomato ketchup, although early recipes for various different varieties of ketchup contained mushrooms, oysters, mussels, egg whites, grapes or walnuts, among other ingredients. ',

#     'score': 0.6998806595802307}]

"""
Now, let's explore how the data is stored in further detail. We'll save the database and take a look.
"""

%env TOKENIZERS_PARALLELISM=false

# Commit results to the database
embeddings.save("test")
embeddings.close()

!ls test
!cat test/config.json
# Output:
#   env: TOKENIZERS_PARALLELISM=false

#   config.json

#   {

#     "content": "postgresql+psycopg2://postgres:pass@localhost/postgres",

#     "backend": "pgvector",

#     "pgvector": {

#       "url": "postgresql+psycopg2://postgres:pass@localhost/postgres"

#     },

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "dimensions": 384,

#     "offset": 100000

#   }

"""
Note how there is only a configuration file with the database details. Let's explore the Postgres database.

"""

!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from sections order by indexid LIMIT 1"
!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from vectors order by indexid LIMIT 1"
# Output:
#    indexid |    id     |                    text                     | tags |           entry            

#   ---------+-----------+---------------------------------------------+------+----------------------------

#          0 | Main Page | Welcome to Wikipedia,                      +|      | 2024-04-24 17:02:55.335643

#            |           | the free encyclopedia that anyone can edit.+|      | 

#            |           |  articles in English                        |      | 

#   (1 row)

#   

#    indexid |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    embedding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    

#   ---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#          0 | [0.006224656,0.010993197,-0.020153109,0.046154346,-0.0067520724,0.011807963,-0.014358369,0.028947443,-0.01586261,0.09301224,0.055368096,0.040113293,0.01760968,0.013503182,-0.016870098,-0.012503422,-0.008597899,0.026588641,-0.08471918,-0.04503593,0.060253955,-0.0031328013,0.11977076,0.024629632,-0.022491025,-0.03793595,0.0061959964,0.0072981887,0.00796152,-0.027184578,-0.10998758,0.07487334,0.08721705,-0.003783297,2.4116285e-05,0.01077903,0.009361188,0.020106519,0.025032397,0.026869925,-0.0111307595,-0.09997612,0.019141614,0.015201731,-0.000267303,0.12866877,-0.04923438,-0.059046485,0.013421367,0.04622524,-0.024259627,-0.01578652,-0.057170264,-0.058802612,0.055165343,-0.016757669,-0.11283095,-0.010731938,-0.03237114,-0.09259695,0.026988069,0.044143338,-0.062587716,0.12294508,0.017795669,-0.0022635453,-0.000859233,0.083078146,0.009367941,-0.08944039,0.020436421,0.03098923,0.08612748,0.06908283,-0.011492079,-0.057584476,0.007815615,0.07544962,-0.07507969,-0.031771705,0.06140103,0.045389958,0.0010914068,-0.014727611,-0.0048347805,-0.008656534,-0.011555558,-0.027875263,0.044627354,-0.041978024,0.0023647482,-0.068146914,0.10593634,0.00459672,-0.0043201237,-0.003424118,0.03128756,-0.031586997,0.006560516,0.047244154,0.06694147,-0.0024762554,-0.034794953,0.0685723,-0.115641095,-0.07013151,-0.029626537,0.05652201,-0.0047345776,-0.08600411,-0.02712477,0.06307427,-0.020134196,-0.05487676,-0.010889723,-0.012801617,0.0070655714,0.01565348,0.06468041,-0.0027397969,-0.052258722,0.025590474,0.0004874447,0.0567211,-0.01091167,-0.016167942,-0.056139555,-3.360483e-33,0.06776523,0.07398922,0.021548307,0.0522404,-0.063432045,0.009229409,-0.09691417,-0.05641955,-0.055137824,-0.04107145,0.055179913,0.06995813,-0.043880165,-0.060650233,0.0026008217,0.026983975,-0.03595947,0.11371047,0.018100752,0.027147746,0.034160867,0.050208986,-0.03631904,-0.0035121094,-0.06707664,0.0063270847,0.008653185,-0.11846966,0.0075610424,0.0021807787,0.015405556,-0.017979044,-0.04683855,-0.015657036,-0.04022284,-0.017653259,0.036167417,-0.042651214,0.005417714,-0.09401236,-0.032427326,-0.017289782,-0.04060469,0.015479944,-0.011029847,0.0719449,0.0051068533,-0.06828172,0.08947456,-0.04907346,0.016336497,0.032021444,-0.04396127,-0.0036975099,0.05712386,0.089574024,0.070081435,0.06301272,0.0010954054,0.01729774,0.047785297,0.09727387,-0.0016659853,0.01636651,-0.011960934,0.08042555,-0.044158004,0.015542993,0.039673716,-0.023705207,-0.0705359,-0.0060911574,0.00903571,0.056801423,-0.04288321,0.075399615,-0.060097944,-0.070493154,-0.039201744,0.02091038,0.021842068,-0.05289644,0.032512337,-0.04969522,0.014612853,0.03174613,0.019972498,-0.018262003,0.10062693,0.03011935,0.058835424,0.008656911,-0.0693178,-0.0003107625,-0.035816267,8.551262e-34,-0.08153472,-0.109941624,0.0010887359,0.058030974,-0.05916192,-0.017850945,-0.11167397,0.107069634,0.041289993,-0.019890498,-0.0026840265,-0.098246865,-0.052144755,0.06551777,-0.04089334,-0.028716525,0.03241684,-0.06699939,-0.09264921,0.124276765,-0.07738617,-0.008159692,-0.04621176,-0.003925314,-0.028218271,0.004362621,0.0662335,0.05448178,-0.049431797,0.001068466,-0.06137263,-0.07963246,-0.056226213,-0.050934546,-0.08301799,-0.03711833,0.001591565,-0.042424005,-0.020324506,-0.05731789,0.043870095,0.040546007,0.008878352,-0.06366917,-0.030763976,-0.028072147,-0.101492785,0.0049343915,0.008031439,-0.029323548,0.052847456,-0.015137294,0.02956745,-0.10447449,0.035620738,0.0025644598,-0.032033995,-0.075608805,0.060582157,-0.03741465,-0.05037591,0.024472151,-0.19789998,0.14253329,0.027650978,0.025539117,-0.09750419,0.07298771,0.017285004,-0.05192262,0.022350593,-0.033429917,0.034556758,-0.06539779,0.03567383,-0.023474114,0.04871094,0.0045313137,0.024971936,-0.09835969,-0.0012533984,-0.0019569201,0.015049834,0.009462184,0.026953677,0.045278806,0.024228122,0.0009281798,-0.023020085,-0.024668744,0.012790688,-0.021721361,-0.026636621,0.06193553,-0.026421277,-2.0252372e-08,-0.009053995,-0.022655917,-0.05537436,0.09484807,0.006692196,-0.05305789,0.019044574,0.050973132,-0.004839857,0.07959955,-0.0562942,0.043415457,-0.051326852,0.04842562,-0.022588398,0.05024676,0.022963611,0.0699145,0.016860556,0.009093765,0.04256753,0.016887672,0.04927324,-0.0092127,0.013626016,0.045127258,-0.005615936,-0.039890807,0.0928744,-0.10173735,-0.11896241,-0.0064999964,-0.04553107,0.04174175,0.030028146,0.042279907,0.000696869,0.0012388963,-0.02783201,-0.048495434,0.04485023,0.009498387,-0.009533283,0.036473263,0.028421838,0.01844087,0.01952972,-0.021264598,0.08634994,-0.022555202,0.060081147,0.013670229,0.1497612,-0.021247797,-0.008887135,0.0306203,-0.007087516,0.024898361,0.05220465,0.012215928,0.02570937,0.046506412,0.08931996,-0.0010894431]

#   (1 row)

#   


"""
Note how the text content and vectors are stored in this Postgres instance.
"""

"""
# Graph Embeddings

Next, we'll rebuild the same embeddings database and enable a graph component. This graph component will also persist content to Postgres.
"""

from txtai import Embeddings

# Create embeddings
embeddings = Embeddings(
    content=url,
    backend="pgvector",
    pgvector={
        "url": url
    },
    graph={
        "backend": "rdbms",
        "url": url,
        "approximate": False,
    }
)

# Index dataset
embeddings.index(wikipedia.search(query))

"""
Now, as with prior graph examples, let's build a new graph with a query. Then we'll plot that subgraph.
"""

g = embeddings.graph.search("""
MATCH P=({id: "Roman Empire"})-[*1..3]->({id: "Saxons"})-[*1..3]->({id: "Vikings"})-[*1..3]->({id: "Battle of Hastings"})
RETURN P
LIMIT 20
""", graph=True)

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}
    colors = ["#D32F2F", "#0277bd", "#7e57c2", "#757575"]

    results = embeddings.batchsimilarity(labels.values(), ["Roman Empire", "Germanic Barbarians", "Viking conquest and siege", "Norman Conquest of England"])
    colors = [colors[x[0][0]] for x in results]

    options = {
        "node_size": 2000,
        "node_color": colors,
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 11,
        "alpha": 1.0,
    }

    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=512, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

    plt.show()

plot(g)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
ğŸš€ Very exciting. A full graph embeddings database all backed by Postgres!

Let's explore the database once again.
"""

embeddings.save("test")

!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from sections order by indexid LIMIT 1"
!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from vectors order by indexid LIMIT 1"
!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from nodes LIMIT 1"
!PGPASSWORD=pass psql -h localhost -U postgres -c "SELECT * from edges LIMIT 1"
# Output:
#    indexid |    id     |                    text                     | tags |           entry            

#   ---------+-----------+---------------------------------------------+------+----------------------------

#          0 | Main Page | Welcome to Wikipedia,                      +|      | 2024-04-24 17:24:04.402629

#            |           | the free encyclopedia that anyone can edit.+|      | 

#            |           |  articles in English                        |      | 

#   (1 row)

#   

#    indexid |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    embedding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    

#   ---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#          0 | [0.006224656,0.010993197,-0.020153109,0.046154346,-0.0067520724,0.011807963,-0.014358369,0.028947443,-0.01586261,0.09301224,0.055368096,0.040113293,0.01760968,0.013503182,-0.016870098,-0.012503422,-0.008597899,0.026588641,-0.08471918,-0.04503593,0.060253955,-0.0031328013,0.11977076,0.024629632,-0.022491025,-0.03793595,0.0061959964,0.0072981887,0.00796152,-0.027184578,-0.10998758,0.07487334,0.08721705,-0.003783297,2.4116285e-05,0.01077903,0.009361188,0.020106519,0.025032397,0.026869925,-0.0111307595,-0.09997612,0.019141614,0.015201731,-0.000267303,0.12866877,-0.04923438,-0.059046485,0.013421367,0.04622524,-0.024259627,-0.01578652,-0.057170264,-0.058802612,0.055165343,-0.016757669,-0.11283095,-0.010731938,-0.03237114,-0.09259695,0.026988069,0.044143338,-0.062587716,0.12294508,0.017795669,-0.0022635453,-0.000859233,0.083078146,0.009367941,-0.08944039,0.020436421,0.03098923,0.08612748,0.06908283,-0.011492079,-0.057584476,0.007815615,0.07544962,-0.07507969,-0.031771705,0.06140103,0.045389958,0.0010914068,-0.014727611,-0.0048347805,-0.008656534,-0.011555558,-0.027875263,0.044627354,-0.041978024,0.0023647482,-0.068146914,0.10593634,0.00459672,-0.0043201237,-0.003424118,0.03128756,-0.031586997,0.006560516,0.047244154,0.06694147,-0.0024762554,-0.034794953,0.0685723,-0.115641095,-0.07013151,-0.029626537,0.05652201,-0.0047345776,-0.08600411,-0.02712477,0.06307427,-0.020134196,-0.05487676,-0.010889723,-0.012801617,0.0070655714,0.01565348,0.06468041,-0.0027397969,-0.052258722,0.025590474,0.0004874447,0.0567211,-0.01091167,-0.016167942,-0.056139555,-3.360483e-33,0.06776523,0.07398922,0.021548307,0.0522404,-0.063432045,0.009229409,-0.09691417,-0.05641955,-0.055137824,-0.04107145,0.055179913,0.06995813,-0.043880165,-0.060650233,0.0026008217,0.026983975,-0.03595947,0.11371047,0.018100752,0.027147746,0.034160867,0.050208986,-0.03631904,-0.0035121094,-0.06707664,0.0063270847,0.008653185,-0.11846966,0.0075610424,0.0021807787,0.015405556,-0.017979044,-0.04683855,-0.015657036,-0.04022284,-0.017653259,0.036167417,-0.042651214,0.005417714,-0.09401236,-0.032427326,-0.017289782,-0.04060469,0.015479944,-0.011029847,0.0719449,0.0051068533,-0.06828172,0.08947456,-0.04907346,0.016336497,0.032021444,-0.04396127,-0.0036975099,0.05712386,0.089574024,0.070081435,0.06301272,0.0010954054,0.01729774,0.047785297,0.09727387,-0.0016659853,0.01636651,-0.011960934,0.08042555,-0.044158004,0.015542993,0.039673716,-0.023705207,-0.0705359,-0.0060911574,0.00903571,0.056801423,-0.04288321,0.075399615,-0.060097944,-0.070493154,-0.039201744,0.02091038,0.021842068,-0.05289644,0.032512337,-0.04969522,0.014612853,0.03174613,0.019972498,-0.018262003,0.10062693,0.03011935,0.058835424,0.008656911,-0.0693178,-0.0003107625,-0.035816267,8.551262e-34,-0.08153472,-0.109941624,0.0010887359,0.058030974,-0.05916192,-0.017850945,-0.11167397,0.107069634,0.041289993,-0.019890498,-0.0026840265,-0.098246865,-0.052144755,0.06551777,-0.04089334,-0.028716525,0.03241684,-0.06699939,-0.09264921,0.124276765,-0.07738617,-0.008159692,-0.04621176,-0.003925314,-0.028218271,0.004362621,0.0662335,0.05448178,-0.049431797,0.001068466,-0.06137263,-0.07963246,-0.056226213,-0.050934546,-0.08301799,-0.03711833,0.001591565,-0.042424005,-0.020324506,-0.05731789,0.043870095,0.040546007,0.008878352,-0.06366917,-0.030763976,-0.028072147,-0.101492785,0.0049343915,0.008031439,-0.029323548,0.052847456,-0.015137294,0.02956745,-0.10447449,0.035620738,0.0025644598,-0.032033995,-0.075608805,0.060582157,-0.03741465,-0.05037591,0.024472151,-0.19789998,0.14253329,0.027650978,0.025539117,-0.09750419,0.07298771,0.017285004,-0.05192262,0.022350593,-0.033429917,0.034556758,-0.06539779,0.03567383,-0.023474114,0.04871094,0.0045313137,0.024971936,-0.09835969,-0.0012533984,-0.0019569201,0.015049834,0.009462184,0.026953677,0.045278806,0.024228122,0.0009281798,-0.023020085,-0.024668744,0.012790688,-0.021721361,-0.026636621,0.06193553,-0.026421277,-2.0252372e-08,-0.009053995,-0.022655917,-0.05537436,0.09484807,0.006692196,-0.05305789,0.019044574,0.050973132,-0.004839857,0.07959955,-0.0562942,0.043415457,-0.051326852,0.04842562,-0.022588398,0.05024676,0.022963611,0.0699145,0.016860556,0.009093765,0.04256753,0.016887672,0.04927324,-0.0092127,0.013626016,0.045127258,-0.005615936,-0.039890807,0.0928744,-0.10173735,-0.11896241,-0.0064999964,-0.04553107,0.04174175,0.030028146,0.042279907,0.000696869,0.0012388963,-0.02783201,-0.048495434,0.04485023,0.009498387,-0.009533283,0.036473263,0.028421838,0.01844087,0.01952972,-0.021264598,0.08634994,-0.022555202,0.060081147,0.013670229,0.1497612,-0.021247797,-0.008887135,0.0306203,-0.007087516,0.024898361,0.05220465,0.012215928,0.02570937,0.046506412,0.08931996,-0.0010894431]

#   (1 row)

#   

#    ID |                                                        _metadata                                                        

#   ----+-------------------------------------------------------------------------------------------------------------------------

#    0  | {"id": "Main Page", "data": "Welcome to Wikipedia,\nthe free encyclopedia that anyone can edit.\n articles in English"}

#   (1 row)

#   

#       ID    |           _metadata            | Source | Target 

#   ----------+--------------------------------+--------+--------

#    __0__179 | {"weight": 0.7411725521087646} | 0      | 179

#   (1 row)

#   


"""
As we saw before, text content and vectors are stored in the database. Additionally, we now have graph node and edge data.

Let's take this up a level.
"""

"""
# Vector search with Postgres

Now that the data is persisted in Postgres, can we run vector search without loading the txtai database locally? Yes!
"""

# Query with a search string
query = str(list(embeddings.transform("Roman Empire")))
query = f"""
SELECT id, (embedding <#> '{query}') * -1 AS score, text FROM sections s \
JOIN vectors v ON s.indexid = v.indexid \
ORDER by score desc LIMIT 5
"""

!PGPASSWORD=pass psql -h localhost -U postgres -c "{query}"
# Output:
#                id              |       score        |                                                                                                                                                                                                                                                                                                            text                                                                                                                                                                                                                                                                                                            

#   -----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#    Roman Empire                | 0.6798915266990662 | The Roman Empire was the post-Republican state of ancient Rome and is generally understood to mean the period and territory ruled by the Romans following Octavian's assumption of sole rule under the Principate in 31 BC. It included territory in Europe, North Africa, and Western Asia, and was ruled by emperors. The fall of the Western Roman Empire in 476 conventionally marks the end of classical antiquity and the beginning of the Middle Ages.

#    History of the Roman Empire | 0.6295346617698669 | The history of the Roman Empire covers the history of ancient Rome from the fall of the Roman Republic in 27 BC until the abdication of Romulus Augustulus in AD 476 in the West, and the Fall of Constantinople in the East in AD 1453. Ancient Rome became a territorial empire while still a republic, but was then ruled by Roman emperors beginning with Augustus, becoming the Roman Empire following the death of the last republican dictator, the first emperor's adoptive father Julius Caesar.

#    Roman Republic              | 0.6280723810195923 | The Roman Republic was the era of classical Roman civilization beginning with the overthrow of the Roman Kingdom (traditionally dated to 509Â BC) and ending in 27Â BC with the establishment of the Roman Empire. During this period, Rome's control expanded from the city's immediate surroundings to hegemony over the entire Mediterranean world.

#    Latin Empire                |  0.613895058631897 | The Latin Empire, also referred to as the Latin Empire of Constantinople, was a feudal Crusader state founded by the leaders of the Fourth Crusade on lands captured from the Byzantine Empire. The Latin Empire was intended to replace the Byzantine Empire as the Western-recognized Roman Empire in the east, with a Catholic emperor enthroned in place of the Eastern Orthodox Roman emperors. The main objective of the Latin Empire was planned by Venice, which promoted the creation of this state for their self-benefit.

#    Augustus                    | 0.6021512150764465 | Gaius Julius Caesar Augustus (born Gaius Octavius; 23 September 63 BC â€“ 19 August AD 14), also known as Octavianus or Octavian, was the founder of the Roman Empire; he reigned as the first Roman emperor from 27Â BC until his death in ADÂ 14. The reign of Augustus initiated an imperial cult as well as an era associated with imperial peace, the Pax Romana or Pax Augusta, in which the Roman world was largely free of armed conflict aside from expansionary wars and the Year of the Four Emperors. The Principate system of imperial rule established by Augustus lasted until the Crisis of the Third Century.

#   (5 rows)

#   


"""
Just like that, vector search without a local txtai instance! The only part needed is a way to vectorize the search query. This can easily be replaced with an API vectorization service.

We can also find the most similar rows for an existing row.
"""

# Find top n results closest to an existing row
query = """
SELECT id, text FROM sections s \
JOIN vectors v ON s.indexid = v.indexid \
WHERE v.indexid != 738 ORDER by v.embedding <#> (SELECT embedding FROM vectors WHERE indexid=738) LIMIT 5
"""

!PGPASSWORD=pass psql -h localhost -U postgres -c "{query}"
# Output:
#                   id                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        

#   ----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#    History of the Roman Empire      | The history of the Roman Empire covers the history of ancient Rome from the fall of the Roman Republic in 27 BC until the abdication of Romulus Augustulus in AD 476 in the West, and the Fall of Constantinople in the East in AD 1453. Ancient Rome became a territorial empire while still a republic, but was then ruled by Roman emperors beginning with Augustus, becoming the Roman Empire following the death of the last republican dictator, the first emperor's adoptive father Julius Caesar.

#    Roman Republic                   | The Roman Republic was the era of classical Roman civilization beginning with the overthrow of the Roman Kingdom (traditionally dated to 509Â BC) and ending in 27Â BC with the establishment of the Roman Empire. During this period, Rome's control expanded from the city's immediate surroundings to hegemony over the entire Mediterranean world.

#    Roman Kingdom                    | The Roman Kingdom, also referred to as the Roman monarchy or the regal period of ancient Rome, was the earliest period of Roman history when the city and its territory were ruled by kings. According to tradition, the Roman Kingdom began with the city's founding  753 BC, with settlements around the Palatine Hill along the river Tiber in central Italy, and ended with the overthrow of the kings and the establishment of the Republic  509 BC.

#    Byzantine Empire                 | The Byzantine Empire, also referred to as the Eastern Roman Empire, was the continuation of the Roman Empire centered in Constantinople during Late Antiquity and the Middle Ages. The eastern half of the Empire survived the conditions that caused the fall of the West in the 5th century AD, and continued to exist until the fall of Constantinople to the Ottoman Empire in 1453. During most of its existence, the empire remained the most powerful economic, cultural, and military force in the Mediterranean world. The term "Byzantine Empire" was only coined following the empire's demise: its citizens referred to the polity as the "Roman Empire" and to themselves as "Romans". Due to the imperial seat's move from Rome to Byzantium, the adoption of state Christianity, and the predominance of Greek instead of Latin, modern historians continue to make a distinction between the earlier "Roman Empire" and the later "Byzantine Empire".

#    Fall of the Western Roman Empire | The fall of the Western Roman Empire, also called the fall of the Roman Empire or the fall of Rome, was the loss of central political control in the Western Roman Empire, a process in which the Empire failed to enforce its rule, and its vast territory was divided into several successor polities. The Roman Empire lost the strengths that had allowed it to exercise effective control over its Western provinces; modern historians posit factors including the effectiveness and numbers of the army, the health and numbers of the Roman population, the strength of the economy, the competence of the emperors, the internal struggles for power, the religious changes of the period, and the efficiency of the civil administration. Increasing pressure from invading barbarians outside Roman culture also contributed greatly to the collapse. Climatic changes and both endemic and epidemic disease drove many of these immediate factors. The reasons for the collapse are major subjects of the historiography of the ancient world and they inform much modern discourse on state failure.

#   (5 rows)

#   


"""
# Wrapping up

From prototyping to production, txtai is ready for all that can be thrown it's way. As always, this functionality is just the beginning and will continue to improve over time. But this is a big deal and has impacts for future services such as [txtai.cloud](https://txtai.cloud) â˜ï¸

Stay tuned!

"""



================================================
FILE: examples/62_RAG_with_llama_cpp_and_external_API_services.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# RAG with llama.cpp and external API services

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

txtai has been and always will be a local-first framework. It was originally designed to run models on local hardware using Hugging Face Transformers. As the AI space has evolved over the last year, so has txtai. Additional LLM inference frameworks have been available for a while using llama.cpp and external API services (via LiteLLM). Recent changes have added the ability to use these frameworks for vectorization and made it easier to use for LLM inference.

This notebook will demonstrate how to run retrieval-augmented-generation (RAG) processes (vectorization and LLM inference) with llama.cpp and external API services.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture

# Install txtai and dependencies
!pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-llm]

"""
# Embeddings with llama.cpp vectorization

The first example will build an Embeddings database backed by [llama.cpp](https://github.com/ggerganov/llama.cpp) vectorization.

The llama.cpp project states: _The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud_.

Let's give it a try.
"""

from txtai import Embeddings

# Create Embeddings with llama.cpp GGUF model
embeddings = Embeddings(
    path="second-state/All-MiniLM-L6-v2-Embedding-GGUF/all-MiniLM-L6-v2-Q4_K_M.gguf",
    content=True
)

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 10000
"""

# Index dataset
embeddings.index(wikipedia.search(query))

"""
Now that the Embeddings database is ready, let's run a search query.
"""

embeddings.search("Inventors of electric-powered devices")
# Output:
#   [{'id': 'Thomas Edison',

#     'text': 'Thomas Alva Edison (February 11, 1847October 18, 1931) was an American inventor and businessman. He developed many devices in fields such as electric power generation, mass communication, sound recording, and motion pictures. These inventions, which include the phonograph, the motion picture camera, and early versions of the electric light bulb, have had a widespread impact on the modern industrialized world. He was one of the first inventors to apply the principles of organized science and teamwork to the process of invention, working with many researchers and employees. He established the first industrial research laboratory.',

#     'score': 0.6758285164833069},

#    {'id': 'Nikola Tesla',

#     'text': 'Nikola Tesla (; , ;  1856\xa0â€“ 7 January 1943) was a Serbian-American inventor, electrical engineer, mechanical engineer, and futurist. He is best-known for his contributions to the design of the modern alternating current (AC) electricity supply system.',

#     'score': 0.6077840328216553},

#    {'id': 'Alexander Graham Bell',

#     'text': 'Alexander Graham Bell (, born Alexander Bell; March 3, 1847 â€“ August 2, 1922) was a  Scottish-born Canadian-American inventor, scientist and engineer who is credited with patenting the first practical telephone. He also co-founded the American Telephone and Telegraph Company (AT&T) in 1885.',

#     'score': 0.4573010802268982}]

"""
As we can see, this Embeddings database works just like any other Embeddings database. The difference is that it's using a llama.cpp model for vectorization instead of PyTorch.
"""

"""
# RAG with llama.cpp

LLM inference with llama.cpp is not a new txtai feature. A recent change added support for conversational messages in additional to standard prompts. This abstracts away having to understand prompting formats.

Let's run a retrieval-augmented-generation (RAG) process fully backed by llama.cpp models.

_It's important to note that conversational messages work with all LLM backends supported by txtai (transformers, llama.cpp, litellm)._
"""

from txtai import LLM

# LLM instance
llm = LLM(path="TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf")

# Question and context
question = "Write a list of invented electric-powered devices"
context = "\n".join(x["text"] for x in embeddings.search(question))

# Pass messages to LLM
response = llm([
    {"role": "system", "content": "You are a friendly assistant. You answer questions from users."},
    {"role": "user", "content": f"""
Answer the following question using only the context below. Only include information specifically discussed.

question: {question}
context: {context}
"""}
])
print(response)
# Output:
#   Based on the given context, here's a list of invented electric-powered devices:

#   

#   1. Electric light bulb by Thomas Edison

#   2. Phonograph by Thomas Edison

#   3. Motion picture camera by Thomas Edison

#   4. Alternating current (AC) electricity supply system by Nikola Tesla

#   5. Telephone by Alexander Graham Bell


"""
And just like that, RAG with llama.cppğŸ¦™!
"""

"""
# Embeddings with external vectorization

Next, we'll show how an Embeddings database can integrate with external API services via [LiteLLM](https://github.com/BerriAI/litellm) .

In the LiteLLM project's own words: _LiteLLM handles loadbalancing, fallbacks and spend tracking across 100+ LLMs. All in the OpenAI format._

Let's first startup a local API service to use for this demo.
"""

%%capture

# Download models
!wget https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-Q4_K_M.gguf
!wget https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q4_K_M.gguf

# Start local API services
!nohup python -m llama_cpp.server --n_gpu_layers -1 --model all-MiniLM-L6-v2-Q4_K_M.gguf --host 127.0.0.1 --port 8000 &> vector.log &
!nohup python -m llama_cpp.server --n_gpu_layers -1 --model mistral-7b-openorca.Q4_K_M.gguf --chat_format chatml --host 127.0.0.1 --port 8001 &> llm.log &
!sleep 30

"""
Now let's connect and use this local service to generate vectors for a new Embeddings database. Note that the local service responds in OpenAI's response format, hence the `path` setting below.
"""

from txtai import Embeddings

# Create Embeddings instance with external vectorization
embeddings = Embeddings(
    path="openai/gpt-4-turbo",
    content=True,
    vectors={
        "api_base": "http://localhost:8000/v1",
        "api_key": "sk-1234"
    }
)

# Load dataset
wikipedia = Embeddings()
wikipedia.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

query = """
SELECT id, text FROM txtai
order by percentile desc
LIMIT 10000
"""

# Index dataset
embeddings.index(wikipedia.search(query))

embeddings.search("Inventors of electric-powered devices")
# Output:
#   [{'id': 'Thomas Edison',

#     'text': 'Thomas Alva Edison (February 11, 1847October 18, 1931) was an American inventor and businessman. He developed many devices in fields such as electric power generation, mass communication, sound recording, and motion pictures. These inventions, which include the phonograph, the motion picture camera, and early versions of the electric light bulb, have had a widespread impact on the modern industrialized world. He was one of the first inventors to apply the principles of organized science and teamwork to the process of invention, working with many researchers and employees. He established the first industrial research laboratory.',

#     'score': 0.6758285164833069},

#    {'id': 'Nikola Tesla',

#     'text': 'Nikola Tesla (; , ;  1856\xa0â€“ 7 January 1943) was a Serbian-American inventor, electrical engineer, mechanical engineer, and futurist. He is best-known for his contributions to the design of the modern alternating current (AC) electricity supply system.',

#     'score': 0.6077840328216553},

#    {'id': 'Alexander Graham Bell',

#     'text': 'Alexander Graham Bell (, born Alexander Bell; March 3, 1847 â€“ August 2, 1922) was a  Scottish-born Canadian-American inventor, scientist and engineer who is credited with patenting the first practical telephone. He also co-founded the American Telephone and Telegraph Company (AT&T) in 1885.',

#     'score': 0.4573010802268982}]

"""
Like the previous example with llama.cpp, this Embeddings database behaves exactly the same. The main difference is that content is sent to an external service for vectorization.
"""

"""
# RAG with External API services

For our last task, we'll run a retrieval-augmented-generation (RAG) process fully backed by an external API service.
"""

from txtai import LLM

# LLM instance
llm = LLM(path="openai/gpt-4-turbo", api_base="http://localhost:8001/v1", api_key="sk-1234")

# Question and context
question = "Write a list of invented electric-powered devices"
context = "\n".join(x["text"] for x in embeddings.search(question))

# Pass messages to LLM
response = llm([
    {"role": "system", "content": "You are a friendly assistant. You answer questions from users."},
    {"role": "user", "content": f"""
Answer the following question using only the context below. Only include information specifically discussed.

question: {question}
context: {context}
"""}
])
print(response)

# Output:
#   Based on the given context, a list of invented electric-powered devices includes:

#   

#   1. Phonograph by Thomas Edison

#   2. Motion Picture Camera by Thomas Edison

#   3. Early versions of the Electric Light Bulb by Thomas Edison

#   4. AC (Alternating Current) Electricity Supply System by Nikola Tesla

#   5. Telephone by Alexander Graham Bell


"""
# Wrapping up

txtai supports a number of different vector and LLM backends. The default method uses PyTorch models via the Hugging Face Transformers library. This notebook demonstrated how llama.cpp and external API services can also be used.

These additional vector and LLM backends enable maximum flexibility and scalability. For example, vectorization can be fully offloaded to an external API service or another local service. llama.cpp has great support for macOS devices, alternate accelerators such AMD ROCm / Intel GPUs and has been known to run on Raspberry Pi devices.

It's exciting to see the confluence of all these new advances coming together. Stay tuned for more!
"""



================================================
FILE: examples/63_How_RAG_with_txtai_works.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How RAG with txtai works

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Large Language Models (LLMs) have captured the public's attention with their impressive capabilities. The Generative AI era has reached a fever pitch with some predicting the coming rise of superintelligence.

LLMs are far from perfect though and we're still a ways away from true AI. The biggest challenge is with hallucinations. Hallucinations is the term for when a LLM generates output that is factually incorrect. The alarming part of this is that on a cursory glance, it actually sounds like factual content. The default behavior of LLMs is to produce plausible answers even when no plausible answer exists. LLMs are not great at saying I don't know.

Retrieval Augmented Generation (RAG) helps reduce the risk of hallucinations by limiting the context in which a LLM can generate answers. This is typically done with a search query that hydrates a prompt with a relevant context. RAG has been one of the most practical use cases of the Generative AI era.

txtai has a multiple ways to run RAG pipelines as follows.

- Embeddings instance and LLM. Run the embeddings search and plug the search results into a LLM prompt.
- RAG (aka Extractor) pipeline which automatically adds a search context to LLM prompts.
- RAG FastAPI service with YAML

This notebook will cover all these methods and shows how RAG with txtai works.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline] autoawq

"""
# Components of a RAG pipeline

Before using txtai's RAG pipeline, we'll show how each of the underlying components work together. In this example, we'll load the [txtai Wikipedia embeddings database](https://huggingface.co/NeuML/txtai-wikipedia) and a LLM. From there, we'll run a RAG process.
"""

from txtai import Embeddings, LLM

# Load Wikipedia Embeddings database
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

# Create LLM
llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")

"""
Next, we'll create a prompt template to use for the RAG pipeline. The prompt has a placeholder for the question and context.
"""

# Prompt template
prompt = """<|im_start|>system
You are a friendly assistant. You answer questions from users.<|im_end|>
<|im_start|>user
Answer the following question using only the context below. Only include information
specifically discussed.

question: {question}
context: {context} <|im_end|>
<|im_start|>assistant
"""

"""
After that, we'll generate the context using an embeddings (aka vector) query. This query finds the top 3 most similar matches to the question **"How do you make beer ğŸº?"**
"""

question = "How do you make beer?"

# Generate context
context = "\n".join([x["text"] for x in embeddings.search(question)])
print(context)
# Output:
#   Brewing is the production of beer by steeping a starch source (commonly cereal grains, the most popular of which is barley) in water and fermenting the resulting sweet liquid with yeast.  It may be done in a brewery by a commercial brewer, at home by a homebrewer, or communally. Brewing has taken place since around the 6th millennium BC, and archaeological evidence suggests that emerging civilizations, including ancient Egypt, China, and Mesopotamia, brewed beer. Since the nineteenth century the brewing industry has been part of most western economies.

#   Beer is produced through steeping a sugar source (commonly Malted cereal grains) in water and then fermenting with yeast. Brewing has taken place since around the 6th millennium BC, and archeological evidence suggests that this technique was used in ancient Egypt. Descriptions of various beer recipes can be found in Sumerian writings, some of the oldest known writing of any sort. Brewing is done in a brewery by a brewer, and the brewing industry is part of most western economies. In 19th century Britain, technological discoveries and improvements such as Burtonisation and the Burton Union system significantly changed beer brewing.

#   Craft beer is a beer that has been made by craft breweries, which typically produce smaller amounts of beer, than larger "macro" breweries, and are often independently owned. Such breweries are generally perceived and marketed as emphasising enthusiasm, new flavours, and varied brewing techniques.


"""
Now we'll take the question and context and put that into the prompt.
"""

print(llm(prompt.format(question=question, context=context)))
# Output:
#   To make beer, you need to steep a starch source, such as malted cereal grains (commonly barley), in water. This process creates a sweet liquid called wort. Then, yeast is added to the wort, which ferments the liquid and produces alcohol and carbon dioxide. The beer is then aged, filtered, and packaged for consumption. This process has been used since around the 6th millennium BC and has been a part of most western economies since the 19th century.


"""
Looking at the generated answer, we can see it's based on the context above. The LLM generates a paragraph of text using the context as input. While this same answer could be directly asked of the LLM, this helps ensure the answer is based on known factual data.
"""

"""
_Before continuing, it's important to note that txtai has multiple ways to run LLM inference. In the past, prior to "Chat Templates", it was expected that the submitted text had all the required chat tokens embedded. The same prompt above can also be written with chat messages. This is especially important when working with LLM APIs (i.e. OpenAI, Claude, Bedrock etc)._

```python
llm([
    {"role": "system": "You are a friendly assistant. You answer questions from users."}
    {"role": "user", "content": f"""
        Answer the following question using only the context below. Only include information specifically discussed.

        question: {question}
        context: {text} 
    """}
])
```

_See the [LLM pipeline documentation](https://neuml.github.io/txtai/pipeline/text/llm/) for more information._
"""

"""
# The RAG Pipeline

txtai has a RAG pipeline that makes this even easier. The logic to generate the context and join it context with the prompt is built in. Let's try that.
"""

from txtai import RAG

# Create RAG pipeline using existing components. LLM parameter can also be a model path.
rag = RAG(embeddings, llm, template=prompt)

"""
Let's ask a question similar to the last one. This time we'll ask **"How do you make wineğŸ·?"**
"""

print(rag("How do you make wine?", maxlength=2048)["answer"])
# Output:
#   To make wine, follow these steps:

#   

#   1. Select the fruit: Choose high-quality grapes or other fruit for wine production.

#   

#   2. Fermentation: Introduce yeast to the fruit, which will consume the sugar present in the juice and convert it into ethanol and carbon dioxide.

#   

#   3. Monitor temperature and oxygen levels: Control the temperature and speed of fermentation, as well as the levels of oxygen present in the must at the start of fermentation.

#   

#   4. Primary fermentation: This stage lasts from 5 to 14 days, during which the yeast consumes the sugar and produces alcohol and carbon dioxide.

#   

#   5. Secondary fermentation (optional): If desired, allow the wine to undergo a secondary fermentation, which can last another 5 to 10 days.

#   

#   6. Fermentation location: Choose the appropriate fermentation vessel, such as stainless steel tanks, open wooden vats, wine barrels, or wine bottles for sparkling wines.

#   

#   7. Bottle and age the wine: Transfer the finished wine into bottles and allow it to age, if desired, to develop flavors and complexity.

#   

#   Remember that wine can be made from various fruits, but grapes are most commonly used, and the term "wine" generally refers to grape wine when used without a qualifier.


"""
_As with the LLM pipeline, the RAG pipeline also supports chat messages. See the [RAG pipeline documentation](https://neuml.github.io/txtai/pipeline/text/rag/) for more._
"""

"""
# RAG API Endpoint

Did you know that txtai has a built-in framework for automatically generating FastAPI services? This can be done with a YAML configuration file.
"""

%%writefile config.yml

# Load Wikipedia Embeddings index
cloud:
  provider: huggingface-hub
  container: neuml/txtai-wikipedia

# RAG pipeline configuration
rag:
  path: TheBloke/Mistral-7B-OpenOrca-AWQ
  output: flatten
  template: |
    <|im_start|>system
    You are a friendly assistant. You answer questions from users.<|im_end|>
    <|im_start|>user
    Answer the following question using only the context below. Only include information
    specifically discussed.

    question: {question}
    context: {context} <|im_end|>
    <|im_start|>assistant
# Output:
#   Writing config.yml


"""
Note how the same prompt template and models are set. This time instead of doing that with Python, it's done with a YAML configuration file ğŸ”¥

Now let's start the API service using this configuration.
"""

!CONFIG=config.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 90

"""
Now let's run a RAG query using the API service. Keeping with the theme, we'll ask **"How do you make whisky ğŸ¥ƒ?"**
"""

!curl "http://localhost:8000/rag?query=how+do+you+make+whisky&maxlength=2048"
# Output:
#   To make whisky, follow these steps:


#   1. Choose the grains: Select the grains you want to use for your whisky, such as barley, corn, rye, or wheat.


#   2. Malt the grains (optional): If using barley, malt the grains by soaking them in water and allowing them to germinate. This process releases enzymes that help break down starches into fermentable sugars.


#   3. Mill the grains: Grind the grains to create a coarse flour, which will be mixed with water to create a mash.


#   4. Create the mash: Combine the milled grains with hot water in a large vessel, and let it sit for several hours to allow fermentation to occur. The mash should have a temperature of around 65Â°C (149Â°F) to encourage the growth of yeast.


#   5. Add yeast: Once the mash has cooled to around 30Â°C (86Â°F), add yeast to the mixture. The yeast will ferment the sugars in the mash, producing alcohol.


#   6. Fermentation: Allow the mixture to ferment for several days, during which the yeast will consume the sugars and produce alcohol and carbon dioxide.


#   7. Distillation: Transfer the fermented liquid, called "wash" to a copper still. Heat the wash in the still, and the alcohol will vaporize and rise through the still's neck. The vapors are then condensed back into a liquid form, creating a high-proof spirit.


#   8. Maturation: Transfer the distilled spirit to wooden casks, typically made of charred white oak. The spirit will mature in the casks for a specified period, usually ranging from 3 to 25 years. During this time, the wood imparts flavors and color to the whisky.


#   9. Bottling: Once the whisky has reached the desired maturity, it is bottled and ready for consumption.

"""
And as before, we get an answer bound by the search context provided to the LLM. This time it comes from an API service vs a direct Python method.
"""

"""
# RAG API Service with Docker

txtai builds Docker images with each release. There are also Docker files available to help configure API services.

The Dockerfile below builds an API service using the same config.yml.
"""

"""
```dockerfile
# Set base image
ARG BASE_IMAGE=neuml/txtai-gpu
FROM $BASE_IMAGE

# Copy configuration
COPY config.yml .

# Install latest version of txtai from GitHub
RUN \
    apt-get update && \
    apt-get -y --no-install-recommends install git && \
    rm -rf /var/lib/apt/lists && \
    python -m pip install git+https://github.com/neuml/txtai

# Run local API instance to cache models in container
RUN python -c "from txtai.api import API; API('config.yml')"

# Start server and listen on all interfaces
ENV CONFIG "config.yml"
ENTRYPOINT ["uvicorn", "--host", "0.0.0.0", "txtai.api:app"]
```
"""

"""
The following commands build and start a Docker API service.
"""

"""
```bash
docker build -t txtai-wikipedia --build-arg BASE_IMAGE=neuml/txtai-gpu .
docker run -d --gpus=all -it -p 8000:8000 txtai-wikipedia
```
"""

"""
This creates the same API service just this time it's through Docker. RAG queries can be run the same way.
"""

"""
```bash
curl "http://localhost:8000/rag?query=how+do+you+make+whisky&maxlength=2048"
```
"""

"""
# Wrapping up

This notebook covered the various ways to run retrieval augmented generation (RAG) with txtai. We hope you find txtai is one of the easiest and most flexible ways to get up and running fast!
"""



================================================
FILE: examples/64_Embeddings_index_format_for_open_data_access.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Embeddings index format for open data access

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

The main programming language with txtai is Python. A key tenet is that the underlying data in an embeddings index is accessible without txtai.

This notebook will demonstrate this through a series of examples.

"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph] datasets sqlite-vec

"""
# Load dataset

This example will use the `chatgpt-prompts` dataset.
"""

from datasets import load_dataset

dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")

"""
# Build an Embeddings index

Let's first build an embeddings index using txtai.
"""

from txtai import Embeddings

embeddings = Embeddings()
embeddings.index((x["act"], x["prompt"]) for x in dataset)
embeddings.save("txtai-index")

"""
Let's take a look at the index that was created
"""

!ls -l txtai-index
!echo
!file txtai-index/*
# Output:
#   total 268

#   -rw-r--r-- 1 root root    342 Sep  6 15:21 config.json

#   -rw-r--r-- 1 root root 262570 Sep  6 15:21 embeddings

#   -rw-r--r-- 1 root root   2988 Sep  6 15:21 ids

#   

#   txtai-index/config.json: JSON data

#   txtai-index/embeddings:  data

#   txtai-index/ids:         data


"""
The txtai embeddings index format is documented [here](https://neuml.github.io/txtai/embeddings/format/). Looking at the files above, we have configuration, embeddings data and ids storage. Ids storage is only used when content is disabled.

Let's inspect each file.
"""

import json

with open("txtai-index/config.json") as f:
  print(json.dumps(json.load(f), sort_keys=True, indent=2))
# Output:
#   {

#     "backend": "faiss",

#     "build": {

#       "create": "2024-09-06T15:21:11Z",

#       "python": "3.10.12",

#       "settings": {

#         "components": "IDMap,Flat"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "7.5.0"

#     },

#     "dimensions": 384,

#     "offset": 170,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2024-09-06T15:21:11Z"

#   }


import faiss

index = faiss.read_index("txtai-index/embeddings")
print(f"Total records {index.ntotal}")
# Output:
#   Total records 170


import msgpack

with open("txtai-index/ids", "rb") as f:
  print(msgpack.unpack(f)[5:10])
# Output:
#   ['JavaScript Console', 'Excel Sheet', 'English Pronunciation Helper', 'Spoken English Teacher and Improver', 'Travel Guide']


"""
Each file can be read without txtai. [JSON](https://www.json.org/json-en.html), [MessagePack](https://msgpack.org/index.html) and [Faiss](https://github.com/facebookresearch/faiss) all have libraries in multiple programming languages.
"""

"""
# Embeddings index with SQLite

In the next example, we'll use SQLite to store content and vectors courtesy of the [sqlite-vec](https://github.com/asg017/sqlite-vec) library.
"""

from txtai import Embeddings

embeddings = Embeddings(content=True, backend="sqlite")
embeddings.index((x["act"], x["prompt"]) for x in dataset)
embeddings.save("txtai-sqlite")

"""
Let's once again explore the generated index files.
"""

!ls -l txtai-sqlite
!echo
!file txtai-sqlite/*
# Output:
#   total 1696

#   -rw-r--r-- 1 root root     384 Sep  6 15:21 config.json

#   -rw-r--r-- 1 root root  126976 Sep  6 15:21 documents

#   -rw-r--r-- 1 root root 1605632 Sep  6 15:21 embeddings

#   

#   txtai-sqlite/config.json: JSON data

#   txtai-sqlite/documents:   SQLite 3.x database, last written using SQLite version 3037002, file counter 1, database pages 31, cookie 0x1, schema 4, UTF-8, version-valid-for 1

#   txtai-sqlite/embeddings:  SQLite 3.x database, last written using SQLite version 3037002, file counter 1, database pages 392, cookie 0x1, schema 4, UTF-8, version-valid-for 1


"""
This time note how there is a documents file with content stored in SQLite and a separate SQLite file for embeddings. Let's test it out.
"""

embeddings.search("teacher")
# Output:
#   [{'id': 'Math Teacher',

#     'text': 'I want you to act as a math teacher. I will provide some mathematical equations or concepts, and it will be your job to explain them in easy-to-understand terms. This could include providing step-by-step instructions for solving a problem, demonstrating various techniques with visuals or suggesting online resources for further study. My first request is "I need help understanding how probability works."',

#     'score': 0.3421396017074585},

#    {'id': 'Educational Content Creator',

#     'text': 'I want you to act as an educational content creator. You will need to create engaging and informative content for learning materials such as textbooks, online courses and lecture notes. My first suggestion request is "I need help developing a lesson plan on renewable energy sources for high school students."',

#     'score': 0.3267676830291748},

#    {'id': 'Philosophy Teacher',

#     'text': 'I want you to act as a philosophy teacher. I will provide some topics related to the study of philosophy, and it will be your job to explain these concepts in an easy-to-understand manner. This could include providing examples, posing questions or breaking down complex ideas into smaller pieces that are easier to comprehend. My first request is "I need help understanding how different philosophical theories can be applied in everyday life."',

#     'score': 0.30780404806137085}]

"""
The top N results as expected. Let's again inspect the files.
"""

import json

with open("txtai-sqlite/config.json") as f:
  print(json.dumps(json.load(f), sort_keys=True, indent=2))
# Output:
#   {

#     "backend": "sqlite",

#     "build": {

#       "create": "2024-09-06T15:21:13Z",

#       "python": "3.10.12",

#       "settings": {

#         "sqlite": "3.37.2",

#         "sqlite-vec": "v0.1.1"

#       },

#       "system": "Linux (x86_64)",

#       "txtai": "7.5.0"

#     },

#     "content": true,

#     "dimensions": 384,

#     "offset": 170,

#     "path": "sentence-transformers/all-MiniLM-L6-v2",

#     "update": "2024-09-06T15:21:13Z"

#   }


import sqlite3, sqlite_vec

db = sqlite3.connect("txtai-sqlite/documents")
print(db.execute("SELECT COUNT(*) FROM sections").fetchone()[0])

db = sqlite3.connect("txtai-sqlite/embeddings")
db.enable_load_extension(True)
sqlite_vec.load(db)
print(db.execute("SELECT COUNT(*) FROM vectors").fetchone()[0])
# Output:
#   170

#   170


"""
As in the previous example, each file can be read without txtai. [JSON](https://www.json.org/json-en.html), [SQLite](https://www.sqlite.org/) and [sqlite-vec](https://github.com/asg017/sqlite-vec) all have libraries in multiple programming languages.
"""

"""
# Graph storage

Starting with txtai 7.4, graphs are stored using MessagePack. The indexed file has a list of nodes and edges that can easily be imported.
"""

from txtai import Embeddings

embeddings = Embeddings(content=True, backend="sqlite", graph={"approximate": False})
embeddings.index((x["act"], x["prompt"]) for x in dataset)
embeddings.save("txtai-graph")

!ls -l txtai-graph
!echo
!file txtai-graph/*
# Output:
#   total 1816

#   -rw-r--r-- 1 root root     454 Sep  6 15:21 config.json

#   -rw-r--r-- 1 root root  126976 Sep  6 15:21 documents

#   -rw-r--r-- 1 root root 1605632 Sep  6 15:21 embeddings

#   -rw-r--r-- 1 root root  119970 Sep  6 15:21 graph

#   

#   txtai-graph/config.json: JSON data

#   txtai-graph/documents:   SQLite 3.x database, last written using SQLite version 3037002, file counter 1, database pages 31, cookie 0x1, schema 4, UTF-8, version-valid-for 1

#   txtai-graph/embeddings:  SQLite 3.x database, last written using SQLite version 3037002, file counter 1, database pages 392, cookie 0x1, schema 4, UTF-8, version-valid-for 1

#   txtai-graph/graph:       data


import msgpack

with open("txtai-graph/graph", "rb") as f:
  data = msgpack.unpack(f)
  print(data.keys())

  for key in data:
    if data[key]:
      print(key, data[key][100])
# Output:
#   dict_keys(['nodes', 'edges', 'categories', 'topics'])

#   nodes [100, {'id': 'Ascii Artist', 'text': 'I want you to act as an ascii artist. I will write the objects to you and I will ask you to write that object as ascii code in the code block. Write only ascii code. Do not explain about the object you wrote. I will say the objects in double quotes. My first object is "cat"'}]

#   edges [5, 100, {'weight': 0.39010339975357056}]


"""
# Wrapping up

This notebook gave an overview of the txtai embeddings index file format and how it supports open data access.

While txtai can be used as an all-in-one embeddings database, it can also be used for only one part of the stack such as data ingestion. For example, it can be used to populate a Postgres or SQLite database for downstream use. The options are there.
"""



================================================
FILE: examples/65_Speech_to_Speech_RAG.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Speech to Speech RAG

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

There are many articles, notebooks and examples covering how to perform vector search and/or retrieval augmented generation (RAG) with txtai. A lesser known component of txtai is it's built-in workflow component.

Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches. This allows large volumes of data to be processed efficiently.

This notebook will demonstrate how to to build a Speech to Speech (S2S) workflow with txtai.

_Note: This process is intended to run on local machines due to it's use of input and output audio devices._
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-audio] autoawq

"""
# Define the S2S RAG Workflow

The next section defines the Speech to Speech (S2S) RAG workflow. The objective of this workflow is to respond to a user request in near real-time.

txtai supports workflow definitions in Python and with YAML. We'll cover both methods.

The S2S workflow below starts with a microphone pipeline, which streams and processes input audio. The microphone pipeline has voice activity detection (VAD) built-in. When speech is detected, the pipeline returns the captured audio data. Next, the speech is transcribed to text and then passed to a RAG pipeline prompt. Finally, the RAG result is run through a text to speech (TTS) pipeline and streamed to an output audio device.
"""

import logging

from txtai import Embeddings, RAG
from txtai.pipeline import AudioStream, Microphone, TextToSpeech, Transcription
from txtai.workflow import Workflow, StreamTask, Task

# Enable DEBUG logging
logging.basicConfig()
logging.getLogger().setLevel(logging.DEBUG)

# Microphone
microphone = Microphone()

# Transcription
transcribe = Transcription("distil-whisper/distil-large-v3")

# Embeddings database
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

# Define prompt template
template = """
Answer the following question using only the context below. Only include information
specifically discussed. Answer the question without explaining how you found the answer.

question: {question}
context: {context}"""

# Create RAG pipeline
rag = RAG(
    embeddings,
    "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    system="You are a friendly assistant. You answer questions from users.",
    template=template,
    context=10
)

# Text to speech
tts = TextToSpeech("neuml/vctk-vits-onnx")

# Audio stream
audiostream = AudioStream()

# Define speech to speech workflow
workflow = Workflow(tasks=[
    Task(action=microphone),
    Task(action=transcribe, unpack=False),
    StreamTask(action=lambda x: rag(x, maxlength=4096, stream=True), batch=True),
    StreamTask(action=lambda x: tts(x, stream=True, speaker=15), batch=True),
    StreamTask(action=audiostream, batch=True)
])

while True:
    print("Waiting for input...")
    list(workflow([None]))

"""
Given that the input and outputs are audio, you'll have to use your imagination if you're reading this as an article.

[Check out this video](https://www.youtube.com/watch?v=tH8QWwkVMKA) to see the workflow in action! The following examples are run:

- Tell me about the Roman Empire
- Explain how faster than light travel could work
- Write a short poem about the Vikings
- Tell me about the Roman Empire in French
"""

"""
# S2S Workflow in YAML

A crucial feature of txtai workflows is that they can be defined with YAML. This enables building workflows in a low-code and/or no-code setting. These YAML workflows can then be "dockerized" and run.

Let's define the same workflow below.
"""

%%writefile s2s.yml
# Microphone
microphone:

# Transcription
transcription:
  path: distil-whisper/distil-large-v3

# Embeddings database
cloud:
  provider: huggingface-hub
  container: neuml/txtai-wikipedia

embeddings:

# RAG
rag:
  path: "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
  system: You are a friendly assistant. You answer questions from users.
  template: |
    Answer the following question using only the context below. Only include information
    specifically discussed. Answer the question without explaining how you found the answer.

    question: {question}
    context: {context}
  context: 10

# TTS
texttospeech:
  path: neuml/vctk-vits-onnx

# AudioStream
audiostream:

# Speech to Speech Chat workflow
workflow:
  s2s:
    tasks:
      - microphone
      - action: transcription
        unpack: False
      - task: stream
        action: rag
        args:
          maxlength: 4096
          stream: True
        batch: True
      - task: stream
        action: texttospeech
        args:
          stream: True
          speaker: 15
        batch: True
      - task: stream
        action: audiostream
        batch: True

from txtai import Application

app = Application("s2s.yml")
while True:
    print("Waiting for input...")
    list(app.workflow("s2s", [None]))

"""
Once again, the same idea, just a different way to do it. In the video demo, the following query was asked.

- As a Patriots fan, who would you guess is my favorite quarterback of all time is?
- I'm tall and run fast, what do you think the best soccer position for me is?
- I run slow, what do you think the best soccer position for me is?

With YAML workflows, it's possible to fully define the process outside of code such as with a web interface. Perhaps someday we'll see this with [txtai.cloud](https://txtai.cloud) ğŸ˜€
"""

"""
# Wrapping up

This notebook demonstrated how to build a Speech to Speech (S2S) workflow with txtai. While the workflow uses an off-the-shelf embeddings database, a custom embeddings database can easily be swapped in. From there, we have S2S with our own data!
"""



================================================
FILE: examples/66_Generative_Audio.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Generative Audio

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

txtai works with much more than just text! It has rich multimedia and multimodal capabilities.

This notebook will demonstrate how to build generative audio workflows. These workflows will generate a combined audio stream with text and relevant audio for a series of poems.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-audio] autoawq

"""
# Define a Generative Audio workflow

The next section defines a generative audio workflow. This workflow consists of a set of pipelines as follows:

- LLM
  - Llama 3 model used to describe the emotions of a given story or poem
- Text To Audio
  - Builds audio given a text prompt
- Text To Speech
  - Converts text to speech
- Audio Mixer
  - Joins multiple audio streams together into a single stream
"""

import logging

import soundfile as sf

from IPython.display import Audio, display

from txtai import LLM
from txtai.pipeline import AudioMixer, TextToAudio, TextToSpeech
from txtai.workflow import Workflow, Task, TemplateTask

# Enable DEBUG logging
logging.basicConfig()
logging.getLogger("txtai.workflow.base").setLevel(logging.DEBUG)
logging.getLogger("txtai.workflow.task.base").setLevel(logging.DEBUG)

def play(audio):
  # Convert to MP3 to save space
  sf.write("audio.wav", audio[0].T, audio[1])
  !ffmpeg -i audio.wav -y -b:a 64 audio.mp3 2> /dev/null

  # Play speech
  display(Audio(filename="audio.mp3"))
  return audio

# LLM
llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

# Text to Audio
# Important: The code for musicgen is licensed as MIT but model weights are CC-BY-NC
tta = TextToAudio("facebook/musicgen-stereo-small")

# Audio mixer
mixer = AudioMixer()

# Define prompt template
template = """
<|begin_of_text|><|start_header_id|>user<|end_header_id|>
Write 3-5 emotions, keywords and holidays to describe the following text. ONLY answer with a comma separated list and no preceding statement.

{text}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

# Background music subworkflow
music = Workflow([
    TemplateTask(
        template=template,
        action=llm
    ),
    Task(action=tta),
])


"""
# "The Raven" by Edgar Allan Poe

The first workflow will generate speech and corresponding background music for the first verse of "The Raven" by Edgar Allan Poe.

This poem is fitting given that Halloween was close at the time of publishing. ğŸƒğŸ‘»ğŸŒ•
"""

# Text to speech
tts = TextToSpeech("neuml/vctk-vits-onnx", rate=32000)

# Define the workflow
workflow = Workflow(tasks=[
    Task(action=[lambda x: tts(x, speaker=3), music], merge="hstack", unpack=False),
    Task(action=lambda x: mixer(x, scale2=0.5), unpack=False),
    Task(action=lambda x: [play(y) for y in x], unpack=False)
])

list(workflow(["""
Once upon a midnight dreary, while I pondered, weak and weary,

Over many a quaint and curious volume of forgotten loreâ€”

While I nodded, nearly napping, suddenly there came a tapping,

As of some one gently rapping, rapping at my chamber door.

â€™Tis some visitor, I muttered, "tapping at my chamber doorâ€” Only this and nothing more.â€
"""]))
# Output:
#   DEBUG:txtai.workflow.base:Running Task #0

#   DEBUG:txtai.workflow.task.base:Inputs: ['\nOnce upon a midnight dreary, while I pondered, weak and weary,\n\nOver many a quaint and curious volume of forgotten loreâ€”\n\nWhile I nodded, nearly napping, suddenly there came a tapping,\n\nAs of some one gently rapping, rapping at my chamber door.\n\nâ€™Tis some visitor, I muttered, "tapping at my chamber doorâ€” Only this and nothing more.â€\n']

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([0.00204471, 0.00245908, 0.00251085, ..., 0.00101355, 0.00124749,

#          0.00157734], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.task.base:Inputs: ['\nOnce upon a midnight dreary, while I pondered, weak and weary,\n\nOver many a quaint and curious volume of forgotten loreâ€”\n\nWhile I nodded, nearly napping, suddenly there came a tapping,\n\nAs of some one gently rapping, rapping at my chamber door.\n\nâ€™Tis some visitor, I muttered, "tapping at my chamber doorâ€” Only this and nothing more.â€\n']

#   DEBUG:txtai.workflow.base:Running Task #0

#   DEBUG:txtai.workflow.task.base:Inputs: ['\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWrite 3-5 emotions, keywords and holidays to describe the following text. ONLY answer with a comma separated list and no preceding statement.\n\n\nOnce upon a midnight dreary, while I pondered, weak and weary,\n\nOver many a quaint and curious volume of forgotten loreâ€”\n\nWhile I nodded, nearly napping, suddenly there came a tapping,\n\nAs of some one gently rapping, rapping at my chamber door.\n\nâ€™Tis some visitor, I muttered, "tapping at my chamber doorâ€” Only this and nothing more.â€\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n']

#   Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.

#   Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)

#   DEBUG:txtai.workflow.task.base:Outputs: ['melancholy, mystery, curiosity, introspection, Halloween']

#   DEBUG:txtai.workflow.base:Running Task #1

#   DEBUG:txtai.workflow.task.base:Inputs: ['melancholy, mystery, curiosity, introspection, Halloween']

#   `torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information.

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[-0.01685709, -0.0192524 , -0.01729976, ...,  0.02864039,

#            0.02873872,  0.02577066],

#          [-0.02714959, -0.0311739 , -0.02744334, ...,  0.2672284 ,

#            0.266621  ,  0.26353633]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[-0.01685709, -0.0192524 , -0.01729976, ...,  0.02864039,

#            0.02873872,  0.02577066],

#          [-0.02714959, -0.0311739 , -0.02744334, ...,  0.2672284 ,

#            0.266621  ,  0.26353633]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.base:Running Task #1

#   DEBUG:txtai.workflow.task.base:Inputs: [((array([0.00204471, 0.00245908, 0.00251085, ..., 0.00101355, 0.00124749,

#          0.00157734], dtype=float32), 32000), (array([[-0.01685709, -0.0192524 , -0.01729976, ...,  0.02864039,

#            0.02873872,  0.02577066],

#          [-0.02714959, -0.0311739 , -0.02744334, ...,  0.2672284 ,

#            0.266621  ,  0.26353633]], dtype=float32), 32000))]

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[-0.00638384, -0.00716712, -0.00613903, ..., -0.05081543,

#           -0.05321765, -0.0549943 ],

#          [-0.01153009, -0.01312787, -0.01121082, ..., -0.01355931,

#           -0.02704029, -0.03997342]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.base:Running Task #2

#   DEBUG:txtai.workflow.task.base:Inputs: [(array([[-0.00638384, -0.00716712, -0.00613903, ..., -0.05081543,

#           -0.05321765, -0.0549943 ],

#          [-0.01153009, -0.01312787, -0.01121082, ..., -0.01355931,

#           -0.02704029, -0.03997342]], dtype=float32), 32000)]

#   <IPython.lib.display.Audio object>
#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[-0.00638384, -0.00716712, -0.00613903, ..., -0.05081543,

#           -0.05321765, -0.0549943 ],

#          [-0.01153009, -0.01312787, -0.01121082, ..., -0.01355931,

#           -0.02704029, -0.03997342]], dtype=float32), 32000)]

#   [(array([[-0.00638384, -0.00716712, -0.00613903, ..., -0.05081543,

#             -0.05321765, -0.0549943 ],

#            [-0.01153009, -0.01312787, -0.01121082, ..., -0.01355931,

#             -0.02704029, -0.03997342]], dtype=float32),

#     32000)]

"""
This is quite amazing ğŸ”¥

From a single text verse, we not only generated speech, we also generated spooky background music to go along with it.

The LLM reads the text and writes a series of emotions, keywords and other descriptive words. That is then passed to a music generation model which creates the corresponding background music. Finally, an audio mixer pipeline joins the streams together and the audio is saved for playback.

This is the powerâš¡ of txtai workflows. Some may call it "agentic". Whatever we want to call it, it is able to combine multiple models small and large into a single execution flow.
"""

"""
# "A Visit from St. Nicholas" by Clement Clarke Moore

Next, we'll create audio for the classic Christmas tale, also known as "The Night Before Christmas" ğŸ…ğŸ„â„ï¸

We'll use a different voice this time, mine! This is the default voice for [txtai-speecht5-onnx](https://hf.co/neuml/txtai-speecht5-onnx).
"""

tts = TextToSpeech("neuml/txtai-speecht5-onnx", rate=32000)

# Define the workflow
workflow = Workflow(tasks=[
    Task(action=[tts, music], merge="hstack", unpack=False),
    Task(action=lambda x: mixer(x, scale2=0.05), unpack=False),
    Task(action=lambda x: [play(y) for y in x], unpack=False)
])

list(workflow(["""
'Twas the night before Christmas, when all through the house, not a creature was stirring, not even a mouse.

The stockings were hung by the chimney with care, in hopes that Saint Nicholas soon would be there.

The children were nestled all snug in their beds, while visions of sugar plums danced in their heads.

And mamma in her kerchief, and I in my cap, had just settled our brains, for a long winterâ€™s nap.

When out on the lawn there arose such a clatter, I sprang from my bed to see what was the matter.
"""]))
# Output:
#   DEBUG:txtai.workflow.base:Running Task #0

#   DEBUG:txtai.workflow.task.base:Inputs: ["\n'Twas the night before Christmas, when all through the house, not a creature was stirring, not even a mouse.\n\nThe stockings were hung by the chimney with care, in hopes that Saint Nicholas soon would be there.\n\nThe children were nestled all snug in their beds, while visions of sugar plums danced in their heads.\n\nAnd mamma in her kerchief, and I in my cap, had just settled our brains, for a long winterâ€™s nap.\n\nWhen out on the lawn there arose such a clatter, I sprang from my bed to see what was the matter.\n"]

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([-3.9214676e-05,  1.7410064e-04,  1.9779154e-04, ...,

#          -1.0386602e-03, -9.1643957e-04, -4.9463823e-04], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.task.base:Inputs: ["\n'Twas the night before Christmas, when all through the house, not a creature was stirring, not even a mouse.\n\nThe stockings were hung by the chimney with care, in hopes that Saint Nicholas soon would be there.\n\nThe children were nestled all snug in their beds, while visions of sugar plums danced in their heads.\n\nAnd mamma in her kerchief, and I in my cap, had just settled our brains, for a long winterâ€™s nap.\n\nWhen out on the lawn there arose such a clatter, I sprang from my bed to see what was the matter.\n"]

#   DEBUG:txtai.workflow.base:Running Task #0

#   DEBUG:txtai.workflow.task.base:Inputs: ["\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWrite 3-5 emotions, keywords and holidays to describe the following text. ONLY answer with a comma separated list and no preceding statement.\n\n\n'Twas the night before Christmas, when all through the house, not a creature was stirring, not even a mouse.\n\nThe stockings were hung by the chimney with care, in hopes that Saint Nicholas soon would be there.\n\nThe children were nestled all snug in their beds, while visions of sugar plums danced in their heads.\n\nAnd mamma in her kerchief, and I in my cap, had just settled our brains, for a long winterâ€™s nap.\n\nWhen out on the lawn there arose such a clatter, I sprang from my bed to see what was the matter.\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"]

#   DEBUG:txtai.workflow.task.base:Outputs: ['Peaceful, Hope, Joy, Christmas, Calm, Slumber, Wonder, Excitement']

#   DEBUG:txtai.workflow.base:Running Task #1

#   DEBUG:txtai.workflow.task.base:Inputs: ['Peaceful, Hope, Joy, Christmas, Calm, Slumber, Wonder, Excitement']

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[ 0.01238994,  0.00964349,  0.02490981, ..., -0.02256315,

#           -0.02624696, -0.01479813],

#          [-0.0111579 , -0.01307007,  0.00245246, ...,  0.02333916,

#            0.01998244,  0.02509145]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[ 0.01238994,  0.00964349,  0.02490981, ..., -0.02256315,

#           -0.02624696, -0.01479813],

#          [-0.0111579 , -0.01307007,  0.00245246, ...,  0.02333916,

#            0.01998244,  0.02509145]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.base:Running Task #1

#   DEBUG:txtai.workflow.task.base:Inputs: [((array([-3.9214676e-05,  1.7410064e-04,  1.9779154e-04, ...,

#          -1.0386602e-03, -9.1643957e-04, -4.9463823e-04], dtype=float32), 32000), (array([[ 0.01238994,  0.00964349,  0.02490981, ..., -0.02256315,

#           -0.02624696, -0.01479813],

#          [-0.0111579 , -0.01307007,  0.00245246, ...,  0.02333916,

#            0.01998244,  0.02509145]], dtype=float32), 32000))]

#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[ 5.8028224e-04,  6.5627520e-04,  1.4432818e-03, ...,

#            2.3652171e-04, -6.1154435e-04,  1.3553995e-03],

#          [-5.9710984e-04, -4.7940284e-04,  3.2041437e-04, ...,

#           -2.5115125e-03, -7.7958062e-04, -9.4321877e-05]], dtype=float32), 32000)]

#   DEBUG:txtai.workflow.base:Running Task #2

#   DEBUG:txtai.workflow.task.base:Inputs: [(array([[ 5.8028224e-04,  6.5627520e-04,  1.4432818e-03, ...,

#            2.3652171e-04, -6.1154435e-04,  1.3553995e-03],

#          [-5.9710984e-04, -4.7940284e-04,  3.2041437e-04, ...,

#           -2.5115125e-03, -7.7958062e-04, -9.4321877e-05]], dtype=float32), 32000)]

#   <IPython.lib.display.Audio object>
#   DEBUG:txtai.workflow.task.base:Outputs: [(array([[ 5.8028224e-04,  6.5627520e-04,  1.4432818e-03, ...,

#            2.3652171e-04, -6.1154435e-04,  1.3553995e-03],

#          [-5.9710984e-04, -4.7940284e-04,  3.2041437e-04, ...,

#           -2.5115125e-03, -7.7958062e-04, -9.4321877e-05]], dtype=float32), 32000)]

#   [(array([[ 5.8028224e-04,  6.5627520e-04,  1.4432818e-03, ...,

#              2.3652171e-04, -6.1154435e-04,  1.3553995e-03],

#            [-5.9710984e-04, -4.7940284e-04,  3.2041437e-04, ...,

#             -2.5115125e-03, -7.7958062e-04, -9.4321877e-05]], dtype=float32),

#     32000)]

"""
# Wrapping up

This notebook demonstrated how to build a series of Generative Audio workflows for poems. This capability has potential applications in the creative field.

Are we at a place where we can have a full pipeline that takes a prompt and generates a full multimedia video? Not quite but we're quite close. Interesting times certainly are ahead!
"""



================================================
FILE: examples/67_Whats_new_in_txtai_8_0.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ğŸ’¡ What's new in txtai 8.0

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

The 8.0 release brings a major new feature: Agents ğŸš€

Agents automatically create workflows to answer multi-faceted user requests. Agents iteratively prompt and/or interface with tools to
step through a process and ultimately come to an answer for a request.

This release also adds support for Model2Vec vectorization.

**Standard upgrade disclaimer below**

While everything is backwards compatible, it's prudent to backup production indexes before upgrading and test before deploying.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[agent] autoawq model2vec

"""
# Agents with txtai

The biggest change and reason this is a major release is the addition of agents. The following defines a basic agent. This agent has access to two embeddings databases (Wikipedia and ArXiv) and the web. Given the user's input request, the agent decides the best tool to solve the task.
"""

from datetime import datetime

from txtai import Agent

wikipedia = {
    "name": "wikipedia",
    "description": "Searches a Wikipedia database",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-wikipedia"
}

arxiv = {
    "name": "arxiv",
    "description": "Searches a database of scientific papers",
    "provider": "huggingface-hub",
    "container": "neuml/txtai-arxiv"
}

def today() -> str:
    """
    Gets the current date and time

    Returns:
        current date and time
    """

    return datetime.today().isoformat()

agent = Agent(
    tools=[today, wikipedia, arxiv, "websearch"],
    llm="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    max_iterations=10,
)

agent("Which city has the highest population, Boston or New York?", maxlength=16000)
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1mWhich city has the highest population, Boston or New York?[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use the tool 'web_search' to find the population of both cities.[0m

#   [33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'Population of Boston and New York'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the web search indicate that the population of Boston is 7,948 and the population of New York is 653,833. However, it seems that the question is asking about the city of Boston in New York, not the city of Boston in Massachusetts. I will use the tool 'wikipedia' to find the population of the city of New York.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Population of New York City'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search indicate that the population of New York City is approximately 8,804,190. However, this is the population of the entire city, not just the city of New York. I will use the tool 'wikipedia' to find the population of the city of New York, not the state of New York.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Population of New York City (borough)'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search indicate that the population of New York City is approximately 8,804,190, and the population of Brooklyn is approximately 2,592,149. Since Brooklyn is one of the five boroughs of New York City, I will assume that the population of the city of New York is approximately 8,804,190. However, this is still not the correct answer to the original question. I will use the tool 'wikipedia' to find the population of the city of Boston in Massachusetts, which is the other city mentioned in the original question.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Population of Boston, Massachusetts'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search indicate that the population of Boston, Massachusetts is approximately 675,647. This is the correct answer to the original question, which was to determine which city has the highest population, Boston or New York. Since the population of Boston is significantly lower than the population of New York City, which is approximately 8,804,190, the answer to the original question is New York City.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': 'New York City'}[0m

#   'New York City'

"""
How about that ğŸ”¥? 

Look at how the agent went through and systematically solved the problem. The agent ran a series of searches to gather the information it needed and then determined the final answer!
"""

"""
# Agentic RAG

Standard Retrieval Augmented Generation (RAG) runs a single vector search to obtain a context and builds a prompt with the context + input question. Agentic RAG is a more complex process that goes through multiple iterations. It can also utilize multiple databases to come to a final conclusion.

The example below aggregates information from multiple sources and builds a report on a topic.
"""

from IPython.display import display, Markdown

researcher = """
You're an expert researcher looking to write a paper on {topic}.
Search for websites, scientific papers and Wikipedia related to the topic.
Write a report with summaries and references (with hyperlinks).
Write the text as Markdown.
"""

display(Markdown(agent(researcher.format(topic="alien life"))))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   You're an expert researcher looking to write a paper on alien life.

#   Search for websites, scientific papers and Wikipedia related to the topic.

#   Write a report with summaries and references (with hyperlinks).

#   Write the text as Markdown.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will start by searching for Wikipedia articles related to alien life.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Alien life'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The search results from Wikipedia look promising. I will now search for scientific papers related to alien life using the arxiv tool.[0m

#   [33;1m>>> Calling tool: 'arxiv' with arguments: {'query': 'alien life'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The search results from arxiv look interesting. I will now search for websites related to alien life using the web_search tool.[0m

#   [33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'alien life'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have gathered information from Wikipedia, arxiv, and web search, I will write a report with summaries and references (with hyperlinks) in Markdown format.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "## Alien Life: A Report on the Search for Extraterrestrial Life\n\n### Introduction\n\nThe search for alien life has been ongoing for centuries, with scientists and enthusiasts alike searching for signs of life beyond Earth. While no conclusive evidence has been found yet, there are many promising leads and discoveries that suggest the possibility of life existing elsewhere in the universe.\n\n### Wikipedia\n\nAccording to Wikipedia, extraterrestrial life refers to life that does not originate from Earth. The search for alien life is an active area of research, with scientists using various methods to detect signs of life, such as analyzing the atmospheres of exoplanets and searching for biosignatures.\n\n### Arxiv\n\nThe arxiv database has numerous papers on the search for alien life, including studies on the possibility of life existing on exoplanets and the use of astrobiology to detect signs of life. One paper, 'Extraterrestrial Life and Censorship,' discusses the development of the theory of cosmic life and the attempts to censor evidence incompatible with this theory.\n\n### Web Search\n\nA web search for alien life yielded numerous results, including articles from reputable sources such as NASA and Live Science. These articles discuss the latest discoveries and findings in the search for alien life, including the potential for life existing on exoplanets and the use of astrobiology to detect signs of life.\n\n### Conclusion\n\nWhile the search for alien life is an ongoing and active area of research, there are many promising leads and discoveries that suggest the possibility of life existing elsewhere in the universe. As scientists continue to search for signs of life, it is possible that we may one day find evidence of alien life.\n\n### References\n\n* [Extraterrestrial life - Wikipedia](https://en.wikipedia.org/wiki/Extraterrestrial_life)\n* [What's the best evidence we've found for alien life?](https://www.livescience.com/space/extraterrestrial-life/whats-the-best-evidence-weve-found-for-alien-life)\n* [NASA Research Gives Guideline for Future Alien Life Search](https://www.nasa.gov/universe/nasa-research-gives-guideline-for-future-alien-life-search/)\n* [James Webb telescope sees potential signs of alien life in the atmosphere of a distant goldilocks water-world](https://www.livescience.com/space/exoplanets/james-webb-telescope-sees-potential-signs-of-alien-life-in-the-atmosphere-of-a-distant-goldilocks-water-world)\n* [Alien life in Universe: Scientists say finding it is 'only a matter of time'](https://www.bbc.com/news/science-environment-66950930)\n* [Extraterrestrial evidence: 10 incredible findings about aliens from 2020](https://www.livescience.com/alien-discoveries-2020.html)\n* [How Scientists Could Tell the World if They Find Alien Life](https://www.scientificamerican.com/article/how-scientists-could-tell-the-world-if-they-find-alien-life/)"}[0m

#   <IPython.core.display.Markdown object>

"""
Let's unpack what just happened here. The Agent reviewed multiple sources and aggregated the references into a single report. This is quite powerful ğŸ’ª

Note that depending on the LLM used, errors can be seen as the Agent tries to get the function parameters right. This example is using Llama 3.1 8B. A more powerful LLM will likely lead to even better results. Remember that txtai supports a number of LLM frameworks (Hugging Face, llama.cpp and LiteLLM APIs).
"""

"""
# Agent Teams

Can agents also be tools? Yes!

Next, we'll build a similar example but instead use an "Agent Team" to answer questions.
"""

from txtai import Agent, LLM

# Share the LLM instance across multiple agents
llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

websearcher = Agent(
    tools=["websearch"],
    llm=llm,
)

wikiman = Agent(
    tools=[{
        "name": "wikipedia",
        "description": "Searches a Wikipedia database",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-wikipedia"
    }],
    llm=llm,
)

researcher = Agent(
    tools=[{
        "name": "arxiv",
        "description": "Searches a database of scientific papers",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-arxiv"
    }],
    llm=llm,
)

agent = Agent(
    tools=[{
        "name": "websearcher",
        "description": "I run web searches, there is no answer a web search can't solve!",
        "target": websearcher
    }, {
        "name": "wikiman",
        "description": "Wikipedia has all the answers, I search Wikipedia and answer questions",
        "target": wikiman
    }, {
        "name": "researcher",
        "description": "I'm a science guy. I search arXiv to get all my answers.",
        "target": researcher
    }],
    llm=llm,
    max_iterations=10
)

display(Markdown(agent("""
Work with your team and build a comprehensive report on fundamental concepts about Signal Processing.
Write the output in Markdown.
""", maxlength=16000)))
# Output:
#   <IPython.core.display.Markdown object>

"""
ğŸ“š See the report above. It ran through similar logic as the first agent, except this time it ran with multiple agents! Note that the agent logging output is not included for brevity.
"""

"""
# Agents as a service

Agents are fully supported through txtai's application configuration via YAML. These services can be run standalone in Python or as a FastAPI service.
"""

%%writefile config.yml

agent:
    researcher:
        tools:
            - websearch
            - name: wikipedia
              description: Searches a Wikipedia database
              provider: huggingface-hub
              container: neuml/txtai-wikipedia
            - name: arxiv
              description: Searches a database of scientific papers
              provider: huggingface-hub
              container: neuml/txtai-arxiv

llm:
    path: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
# Output:
#   Writing config.yml


!CONFIG=config.yml nohup uvicorn "txtai.api:app" &> api.log &
!sleep 90

import requests

requests.post(
    "http://localhost:8000/agent",
    headers={"Content-Type": "application/json"},
    json={"name": "researcher", "text": "Tell me about the Roman Empire", "maxlength": 160000}
).json()
# Output:
#   'The Roman Empire was a vast and powerful state that existed from 27 BC to 476 AD, covering much of Europe, North Africa, and Western Asia. It was founded by Augustus Caesar and was ruled by a series of emperors, with its capital in Rome. The Roman Empire was known for its impressive architecture, engineering, and administrative achievements, including the construction of roads, bridges, and public buildings. It also had a significant impact on the development of law, governance, and culture in the ancient world.'

"""
ğŸ’¥ Look at that! A full API service from a simple configuration file. 
"""

"""
# Vectorization with Model2Vec

While the agent framework is the headline change, there is another major update - support for Model2Vec models. 

[Model2Vec](https://github.com/MinishLab/model2vec) is a technique to turn any sentence transformer into a really small static model, reducing model size by 15x and making the models up to 500x faster, with a small drop in performance.
"""

from txtai import Embeddings

# Data to index
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create an embeddings
embeddings = Embeddings(method="model2vec", path="minishlab/M2V_base_output")
embeddings.index(data)

uid = embeddings.search("climate change")[0][0]
data[uid]
# Output:
#   "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg"

"""
# Wrapping up

This notebook gave a quick overview of txtai 8.0. Updated documentation and more examples will be forthcoming. There is much to cover and much to build on!

See the following links for more information.

- [8.0 Release on GitHub](https://github.com/neuml/txtai/releases/tag/v8.0.0)
- [Documentation site](https://neuml.github.io/txtai)
"""



================================================
FILE: examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Analyzing Hugging Face Posts with Graphs and Agents

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

Embeddings databases can optionally build a graph network alongside stored vectors. The graph automatically infers relationships of the content using the computed vector embeddings. This provides an additional way to explore and analyze content. 

txtai 8.0 was recently released and added the ability to run agents. Agents automatically create workflows to answer multi-faceted user requests.

This notebook will demonstrate how to use these concepts with the [Hugging Face Posts dataset](https://huggingface.co/datasets/maxiw/hf-posts). If you're not familar with semantic graphs, graph traversal and agents in txtai, then check out the articles below before continuing on. 

Articles to review:
  - [Introducing the Semantic Graph](https://neuml.hashnode.dev/introducing-the-semantic-graph)
  - [Advanced RAG with graph path traversal](https://neuml.hashnode.dev/advanced-rag-with-graph-path-traversal)
  - [What's new in txtai 8.0](https://neuml.hashnode.dev/whats-new-in-txtai-80)
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[agent,graph] datasets autoawq

"""
# Hugging Face Posts

[Hugging Face Posts](https://huggingface.co/posts) is a microblog site. It has over 2,000 unique posts as of November 2024. The next section defines methods to yield records from this dataset.

Additionally, it loads an LLM to infer titles for each post.
"""

from datasets import load_dataset
from txtai import LLM

llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

def title(text):
    prompt = f"""
Create a simple, concise topic for the following text. Only return the topic name.

Text:
{text}
"""

    return llm([{"role": "user", "content": prompt}], maxlength=2048)

def hfposts():
    ds = load_dataset("maxiw/hf-posts", split="train")
    for row in ds:
        yield {
            "id": title(row["rawContent"]),
            "text": row["rawContent"],
            "author": row["author"]["name"],
            "date": row["publishedAt"],
            "url": f"https://hf.co{row['url']}",
            "reactions": sum(x["count"] for x in row["reactions"]),
            "views": row["totalUniqueImpressions"],
            "comments": row["numComments"]
        }


"""
Now, let's build an Embeddings index. We'll store vectors, content and build a graph network. The graph network automatically builds relationships between the records using vector similarity.
"""

from tqdm import tqdm
from txtai import Embeddings

embeddings = Embeddings(
    autoid="uuid5",
    path="intfloat/e5-large",
    instructions={"query": "query: ", "data": "passage: "},
    content=True,
    graph={"approximate": False, "minscore": 0.7},
)
embeddings.index(tqdm(hfposts()))

"""
With a modern GPU, this step shouldn't take long. The longest part of the process is running 2000+ LLM prompts to generate titles. Without that, it would only take a few seconds to build the embeddings index.

If you'd like to skip this step, you can instead load a pre-computed embeddings index from the Hugging Face Hub as follows.

```python
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-hfposts")
```

Let's run a sample search to verify the index is working.
"""

embeddings.search("transformers")
# Output:
#   [{'id': 'AI Music Generation',

#     'text': 'Love this new Space built by @enzostvs + @Xenova for Transformers.js: Generate your own AI music (In-browser generation) with AI Jukebox \n\nhttps://huggingface.co/spaces/enzostvs/ai-jukebox',

#     'score': 0.8460421562194824},

#    {'id': 'Kolmogorov Arnold Networks',

#     'text': 'Transformers are not all we need, that is being proven repeatedly now as more alternative frameworks emerge. Another such framework is Kolmogorov Arnold Network based Transformers. I break down exactly how these differ from Perceptron based Transformers and give you the link to my Colab where I create a model based on the research paper that absolutely destroys a standard Transformers based model. Check out the video here: https://www.youtube.com/watch?v=Sw0euxNZCc4',

#     'score': 0.8424240350723267},

#    {'id': 'GitHub Issue 8771',

#     'text': 'This issue is just a treasure ! A bit deprecated i guess, but things are in their historical context. (personally, still need more to understand better)\nhttps://github.com/huggingface/transformers/issues/8771\n\U0001fae1 to the man @stas ',

#     'score': 0.8417709469795227}]

"""
Looks good!
"""

"""
# Graph Analysis

Next up, graphs! We'll define methods to plot our graphs and subgraphs.
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}

    options = {
        "node_size": 700,
        "node_color": "#0277bd",
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 10,
        "alpha": 1.0,
    }

    # Draw graph
    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)

    # Disable axes and draw margins
    ax.axis("off")
    plt.margins(x=0.15)

    # Set background color
    ax.set_facecolor("#303030")
    fig.set_facecolor("#303030")

    plt.show()

def authorplot(author, limit):
    count = embeddings.search("SELECT count(*) count FROM txtai WHERE author=:author", 1, parameters={"author": author})[0]["count"]
    print(f"Total posts for `{author}` = {count}. Using up to {limit} posts for graph.")

    plot(embeddings.search(
        "SELECT id, text, score FROM txtai WHERE author=:author ORDER BY views desc",
        limit, parameters={"author": author}, graph=True
    ))

plot(embeddings.search("transformers", 25, graph=True))
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
Everything looks good! The graph above shows a network of posts related to `transformers`.
"""

"""
# Plot by Author

Let's take a look at a couple authors. First up, the CEO of Hugging Face, [Clem Delangue](https://huggingface.co/clem).

Below is a graph of his 20 most popular posts. Any themes emerge?
"""

authorplot("clem", 20)
# Output:
#   Total posts for `clem` = 29. Using up to 20 posts for graph.

#   <Figure size 2000x900 with 1 Axes>

"""
Next, let's see what [Tom Aarsen](https://huggingface.co/tomaarsen), the primary developer of [Sentence Transformers](https://github.com/UKPLab/sentence-transformers), talks about.

_p.s. It's not surprising._
"""

authorplot("tomaarsen", 5)
# Output:
#   Total posts for `tomaarsen` = 14. Using up to 5 posts for graph.

#   <Figure size 2000x900 with 1 Axes>

"""
# Top authors by reactions

Now, let's take a look at the Top 3 authors with the most reactions and see what they're talking about.
"""

for row in embeddings.search("SELECT SUM(reactions), author FROM txtai GROUP BY author ORDER BY SUM(reactions) desc", 3):
    authorplot(row["author"], 20)
# Output:
#   Total posts for `merve` = 72. Using up to 20 posts for graph.

#   <Figure size 2000x900 with 1 Axes>
#   Total posts for `akhaliq` = 80. Using up to 20 posts for graph.

#   <Figure size 2000x900 with 1 Axes>
#   Total posts for `MonsterMMORPG` = 46. Using up to 20 posts for graph.

#   <Figure size 2000x900 with 1 Axes>

"""
ğŸ”¥ Quite interesting! Perhaps some pointers on topics to write articles and posts about?
"""

"""
# Top posts by reactions

Now, we'll plot the Top 25 most reacted to posts.
"""

plot(embeddings.search("SELECT id, text, score FROM txtai ORDER BY reactions desc", 25, graph=True))
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
Let's look at a post to show how it looks.
"""

embeddings.search("SELECT id, text, author, url, date, reactions, views, comments FROM txtai WHERE id='Building Large Language Models'")
# Output:
#   [{'id': 'Building Large Language Models',

#     'text': "A Little guide to building Large Language Models in 2024\n\nThis is a post-recording of a 75min lecture I gave two weeks ago on how to train a LLM from scratch in 2024. I tried to keep it short and comprehensive â€“ focusing on concepts that are crucial for training good LLM but often hidden in tech reports.\n\nIn the lecture, I introduce the students to all the important concepts/tools/techniques for training good performance LLM:\n* finding, preparing and evaluating web scale data\n* understanding model parallelism and efficient training\n* fine-tuning/aligning models\n* fast inference\n\nThere is of course many things and details missing and that I should have added to it, don't hesitate to tell me you're most frustrating omission and I'll add it in a future part. In particular I think I'll add more focus on how to filter topics well and extensively and maybe more practical anecdotes and details.\n\nNow that I recorded it I've been thinking this could be part 1 of a two-parts series with a 2nd fully hands-on video on how to run all these steps with some libraries and recipes we've released recently at HF around LLM training (and could be easily adapted to your other framework anyway):\n*`datatrove` for all things web-scale data preparation: https://github.com/huggingface/datatrove\n*`nanotron` for lightweight 4D parallelism LLM training: https://github.com/huggingface/nanotron\n*`lighteval` for in-training fast parallel LLM evaluations: https://github.com/huggingface/lighteval\n\nHere is the link to watch the lecture on Youtube: https://www.youtube.com/watch?v=2-SPH9hIKT8\nAnd here is the link to the Google slides: https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit#slide=id.p\n\nEnjoy and happy to hear feedback on it and what to add, correct, extend in a second part.",

#     'author': 'thomwolf',

#     'url': 'https://hf.co/posts/thomwolf/706415412818350',

#     'date': '2024-03-28T21:44:02.000Z',

#     'reactions': 79,

#     'views': 4844,

#     'comments': 2}]

"""
# Graph traversal

As we've now seen, embeddings indexes that have an associated graph network have relationships!

These relationships can also be utilized to explore the dataset. The query below walks relationships between the `Argilla Joins Hugging Face` and `Hugging Face Growth` nodes and plots the graph.

The search syntax below uses openCypher. Learn more about this in [txtai's documentation](https://neuml.github.io/txtai/embeddings/query/#graph-search).
"""

g = embeddings.graph.search("""
MATCH P=({id: "Argilla Joins Hugging Face"})-[*1..3]->({id: "Hugging Face Growth"})
RETURN P
LIMIT 30
""", graph=True)

plot(g)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
# Agents

By now, we should get the idea on graphs! Time to move on to agents. Agents automatically create workflows to answer multi-faceted user requests. Agents iteratively prompt and/or interface with tools to step through a process and ultimately come to an answer for a request.

Let's connect our database to an agent.
"""

from txtai import Agent

posts = {
    "name": "posts",
    "description": "Searches a database of technical blog posts",
    "target": embeddings
}

agent = Agent(
    tools=[posts],
    llm=llm,
    max_iterations=10,
)

from IPython.display import display, Markdown

def md(output):
    display(Markdown(output))

researcher = """
You're looking to learn more about {topic}. Do the following.
 - Search for posts related to the topic.
 - Write a report with references hyperlinks.
 - Write the text as Markdown.
"""

md(agent(researcher.format(topic="hugging face and open source AI")))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   You're looking to learn more about hugging face and open source AI. Do the following.

#    - Search for posts related to the topic.

#    - Write a report with references hyperlinks.

#    - Write the text as Markdown.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will start by searching for posts related to the topic of Hugging Face and Open Source AI.[0m

#   [33;1m>>> Calling tool: 'posts' with arguments: {'query': 'Hugging Face Open Source AI'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have the search results, I can write a report with references and hyperlinks.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "### Hugging Face and Open Source AI\n\nHugging Face is a leading player in the open source AI ecosystem. Their commitment to democratizing AI has led to significant growth in the community, with over 1 million public models available on their platform.\n\n#### Key Developments\n\n*   **Hugging Face's AI Initiative**: The company has committed $10 million in free GPUs to help developers create new AI technologies, promoting collaboration and transparency in the AI community.\n\n*   **Open Source Models with Hugging Face**: Hugging Face has been instrumental in making hundreds of thousands of already-trained open source models available for developers to assemble into new applications.\n\n*   **Hugging Face Competitions**: The company has launched an open-source platform for creating machine learning competitions, allowing developers to create and host competitions for free.\n\n#### References\n\n*   [Huggingface AI Ecosystem](https://huyenchip.com/2024/03/14/ai-oss.html)\n*   [Hugging Face's AI Initiative](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai)\n*   [Open Source Models with Hugging Face](https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/)\n*   [Hugging Face Competitions](https://github.com/huggingface/competitions)\n*   [Hugging Face Milestone](https://huggingface.co/spaces/cfahlgren1/hub-stats)"}[0m

#   <IPython.core.display.Markdown object>

"""
There is a lot to unpack here. The agent has access to the Embeddings database as a tool. The prompt asks the agent to research a specific topic, `hugging face and open source AI` and build a report. We can see the series of steps this undertook and the report output (with links pulled out from the posts!). 

Very cool ğŸ˜
"""

"""
Next we'll make a slight change to our agent. We'll give it a custom function instead of an embeddings database. This function still runs a search but in a more specific way, by author.
"""

def search(author):
    """
    Searches a database of blog posts by author.

    Args:
        author: lower case author id to search for
 
    Returns:
        list of blog post texts for author
    """

    return embeddings.search("SELECT text FROM txtai WHERE author=:author ORDER BY views desc", 20, parameters={"author": author})

agent = Agent(
    tools=[search],
    llm=llm,
    max_iterations=5,
)

md(agent("Summarize what clem talks about as a list of 5 items in Markdown format", maxlength=16000))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1mSummarize what clem talks about as a list of 5 items in Markdown format[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will first search the database of blog posts by Clem to find what he talks about, then I will summarize the results as a list of 5 items in Markdown format.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'author': 'clem'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will now summarize the results as a list of 5 items in Markdown format.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "- **5,000 new repos are created every day on Hugging Face**: The community is amazing and contributing to the growth of open-source AI.\n- **Hugging Face is profitable**: With 220 team members and most of their platform being free and open-source for the community.\n- **Clem is excited about Hugging Face's success**: He mentions that they have plenty of money in the bank and are profitable.\n- **Clem is hosting a live workshop**: With team members next week about the Enterprise Hugging Face hub, and 1,000 spots are available.\n- **Clem is advocating for open-source AI**: He believes that it creates healthy competition and pushes companies to build AI faster, attract the best scientists and engineers, and generate more visibility and community contributions."}[0m

#   <IPython.core.display.Markdown object>

"""
Even though we've seen LLM outputs a bazillion times by now, it's still amazing to see how good of a job it can do, under the right circumstances.

Let's show the previous graph again. Sounds about right?
"""

authorplot("clem", 20)
# Output:
#   Total posts for `clem` = 29. Using up to 20 posts for graph.

#   <Figure size 2000x900 with 1 Axes>

"""
Last but certainly not least, we'll do the same thing for Tom Aarsen.
"""

md(agent("Summarize what tomaarsen talks about", maxlength=20000))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1mSummarize what tomaarsen talks about[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use the tool'search' to find information about tomaarsen and then summarize what I found.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'author': 'tomaarsen'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will summarize the content by tomaarsen.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'author': 'tomaarsen'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will summarize the content by tomaarsen.

#   The content is about updates to the Sentence Transformers library, including new features and improvements.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: The content is about updates to the Sentence Transformers library, including new features and improvements.[0m

#   <IPython.core.display.Markdown object>

authorplot("tomaarsen", 5)
# Output:
#   Total posts for `tomaarsen` = 14. Using up to 5 posts for graph.

#   <Figure size 2000x900 with 1 Axes>

"""
# Wrapping up

This notebook demonstrated how to use graphs and agents to explore the [Hugging Face Posts dataset](https://huggingface.co/datasets/maxiw/hf-posts). It was very illuminating. ğŸ’¡

We hope you had as much fun as we did!
"""



================================================
FILE: examples/69_Granting_autonomy_to_agents.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Granting autonomy to agents

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

txtai 8.0 was recently released and added the ability to run agents. Agents automatically create workflows to answer multi-faceted user requests.

Agents connect a series of tools with a reasoning engine (i.e. LLM). We're giving the agent a degree of latitude to go through it's own internal logic to address a user's request.

This is a huge paradigm shift. We're talking about handing over control to a program and hoping it makes the right decisions itself. Perhaps there are some parallels to sending your kid to college - we hope we've raised them the right way to be able to make smart decisions ğŸ˜‚. 

This notebook will focus on examples that give agents autonomy to address requests. With this, we can start to the see the path ahead towards more and more automation of tasks.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[agent,graph] autoawq

"""
# Let's get creative

In the first example, we'll define an agent that has access to the [txtai-wikipedia](https://huggingface.co/NeuML/txtai-wikipedia) embeddings database. Standard retrieval augmented generation (RAG) and vector search are typically designed for a single search. Agents enable a much more creative and iterative approach to search.

This example won't ask the agent exactly what we're looking for. We'll ask the agent to tell us something interesting based on the agent's own ideas and research.
"""

from txtai import Agent

agent = Agent(
    tools=[{
        "name": "wikipedia",
        "description": "Searches a Wikipedia database",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-wikipedia"
    }],
    llm="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
    max_iterations=10,
)

from IPython.display import display, Markdown

answer = agent("""
I'm bored ğŸ¥±. Think about 2-3 disparate topics and use those to search wikipedia to generate something fascinating.
Write a report summarizing each article. Include a section with a list of article hyperlinks.
Write the text as Markdown.
""", maxlength=16000)

display(Markdown(answer))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   I'm bored ğŸ¥±. Think about 2-3 disparate topics and use those to search wikipedia to generate something fascinating.

#   Write a report summarizing each article. Include a section with a list of article hyperlinks.

#   Write the text as Markdown.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use Wikipedia to search for articles related to three disparate topics. The first topic will be the "Great Barrier Reef", the second topic will be "Dark Matter", and the third topic will be "The Voynich Manuscript".[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Great Barrier Reef'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search show that the Great Barrier Reef is a vast and diverse ecosystem that supports a wide variety of life. However, it is also facing many threats, including climate change, pollution, and overfishing.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'Dark Matter'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search show that Dark Matter is a hypothetical form of matter that does not interact with light or other electromagnetic radiation, and is implied by gravitational effects that cannot be explained by general relativity.[0m

#   [33;1m>>> Calling tool: 'wikipedia' with arguments: {'query': 'The Voynich Manuscript'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the Wikipedia search show that the Voynich Manuscript is an illustrated codex that has been carbon-dated to the early 15th century, and its origins, authorship, and purpose are still debated.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "### Report on Fascinating Topics\n\n#### The Great Barrier Reef\n\nThe Great Barrier Reef is the world's largest coral reef system, composed of over 2,900 individual reefs and 900 islands stretching for over 2,300 kilometers off the coast of Queensland, Australia. It is a vast and diverse ecosystem that supports a wide variety of life, including coral and other invertebrate species, bony fish, sharks, rays, marine mammals, marine turtles, sea snakes, and algae. However, it is also facing many threats, including climate change, pollution, and overfishing.\n\n#### Dark Matter\n\nDark Matter is a hypothetical form of matter that does not interact with light or other electromagnetic radiation. It is implied by gravitational effects that cannot be explained by general relativity, and is a topic of ongoing research in astrophysics and cosmology.\n\n#### The Voynich Manuscript\n\nThe Voynich Manuscript is an illustrated codex that has been carbon-dated to the early 15th century. Its origins, authorship, and purpose are still debated, and it is considered one of the most mysterious and enigmatic manuscripts in the world.\n\n### Article Hyperlinks\n\n* [Great Barrier Reef](https://en.wikipedia.org/wiki/Great_Barrier_Reef)\n* [Dark Matter](https://en.wikipedia.org/wiki/Dark_matter)\n* [Voynich Manuscript](https://en.wikipedia.org/wiki/Voynich_manuscript)"}[0m

#   <IPython.core.display.Markdown object>

"""
ğŸ’¥ Interesting indeed. The fundamental concept of search is we need to know what to look for. In this case, we didn't have that (i.e. we're bored ğŸ˜€).

Let's go to another example. This time we'll look at the [txtai-hfposts](https://huggingface.co/NeuML/txtai-hfposts) embeddings database. We'll ask the agent to research a specific topic then write a report about what was found.

It's important to mention that txtai agents support any [LLM supported by txtai](https://neuml.github.io/txtai/pipeline/text/llm/). So while all of the examples have been using `Llama 3.1 8B`, other LLMs local and remote are supported (i.e. OpenAI, AWS Bedrock, Anthropic). To change it up, we'll use one of the [Qwen models](https://huggingface.co/Qwen) which have been receiving a lot of positive reviews lately.
"""

agent = Agent(
    tools=[{
        "name": "hfposts",
        "description": "Searches a database of technical posts on Hugging Face",
        "provider": "huggingface-hub",
        "container": "neuml/txtai-hfposts"
    }],
    llm="Qwen/Qwen2.5-7B-Instruct-AWQ",
    max_iterations=10,
)

answer = agent("""
Read posts about medicine and write a report on what you learned.

The report should be a Markdown table with the following columns.
 - Name
 - Description
 - Link to content

Only include rows that have a valid web url.
""", maxlength=16000)

display(Markdown(answer))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   Read posts about medicine and write a report on what you learned.

#   

#   The report should be a Markdown table with the following columns.

#    - Name

#    - Description

#    - Link to content

#   

#   Only include rows that have a valid web url.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: First, I need to search for posts related to medicine using the `hfposts` tool.[0m

#   [33;1m>>> Calling tool: 'hfposts' with arguments: {'query': 'medicine'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have the posts related to medicine, I need to filter out the ones that have a valid web URL. I will go through each post and check if it contains a URL.[0m

#   [33;1m>>> Calling tool: 'hfposts' with arguments: {'query': 'medicine url'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have the posts with URLs, I need to extract the relevant information such as the name, description, and link to content. I will create a Markdown table with these details.[0m

#   [33;1m>>> Calling tool: 'hfposts' with arguments: {'query': 'medicine url'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have the posts with URLs, I can extract the relevant information and create a Markdown table. I will format the data into the required structure.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': '| Name | Description | Link to content |\n| --- | --- | --- |\n| OpenBioLLM Release | Introducing OpenBioLLM-Llama3-70B & 8B: The most capable openly available Medical-domain LLMs to date! | [https://huggingface.co/blog/aaditya/openbiollm](https://huggingface.co/blog/aaditya/openbiollm) |\n| Last Week in Medical AI: Top Research Papers/Models (September 1 - September 7, 2024) | Outperforms industry giants like GPT-4, Gemini, Meditron-70B, Med-PaLM-1, and Med-PaLM-2 in the biomedical domain. | [https://x.com/OpenlifesciAI/status/1832476252260712788](https://x.com/OpenlifesciAI/status/1832476252260712788) |\n| Last Week in Medical AI: Top Research Papers/Models (August 25 - August 31, 2024) | Includes MultiMed, a Multimodal Medical Benchmark, and A Foundation model for generating chest X-ray images. | [https://x.com/OpenlifesciAI/status/1829984701324448051](https://x.com/OpenlifesciAI/status/1829984701324448051) |\n| Last Week in Medical AI: Top Research Papers/Models (October 5 - October 12, 2024) | Introduces MMedAgent: Learning to Use Medical Tools with Multi-modal Agent. | [https://youtu.be/OD3C5jirszw](https://youtu.be/OD3C5jirszw) |\n| Last Week in Medical AI: Top Research Papers/Models (October 26 - November 2, 2024) | Google Presents MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making. | [https://x.com/OpenlifesciAI/status/1852685220912464066](https://x.com/OpenlifesciAI/status/1852685220912464066) '}[0m

#   <IPython.core.display.Markdown object>

"""
ğŸš€ Once again, very interesting! This time we asked the agent to go read about a topic and report back. The agent did that and left us with links to explore further.
"""

"""
# Autonomous Embeddings

For our last example, we're going to give an agent free rein to control an embeddings database.

First, we will create an empty embeddings database and tell the agent how to add and search for data.
"""

from txtai import Agent, Embeddings
from txtai.pipeline import Textractor
from txtai.workflow import Workflow, Task

# Empty embeddings database
embeddings = Embeddings(
    path="intfloat/e5-large",
    instructions={"query": "query: ", "data": "passage: "},
    content=True
)

# Textractor instance
textractor = Textractor(sections=True, headers={"user-agent": "Mozilla/5.0"})

def insert(elements):
    """
    Inserts elements into the embeddings database.

    Args:
        elements: list of strings to insert
    """

    def upsert(elements):
        embeddings.upsert(elements)
        return elements

    # Upsert workflow
    workflow = Workflow([Task(textractor), Task(upsert)])
    list(workflow(elements))
    return f"{elements} inserted successfully"

agent = Agent(
    tools=[insert, embeddings.search, "websearch"],
    llm="Qwen/Qwen2.5-7B-Instruct-AWQ",
    max_iterations=10
)

"""
Next, we'll define a workflow process. This creates the rules we want the agent to follow. Then we'll run two requests.
"""

prompt = """
Run the following process:
 1. Search your internal knowledge for {topic}
 2. If not found, find relevant urls and insert those as a list of strings ONLY. Then rerun the search for {topic}.
 3. Write a detailed report about {topic} with Markdown sections covering the major topics. Include a section with hyperlink references.
"""

answer = agent(prompt.format(topic="txtai"))
display(Markdown(answer))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   Run the following process:

#    1. Search your internal knowledge for txtai

#    2. If not found, find relevant urls and insert those as a list of strings ONLY. Then rerun the search for txtai.

#    3. Write a detailed report about txtai with Markdown sections covering the major topics. Include a section with hyperlink references.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: First, I will attempt to search for "txtai" in my internal knowledge base.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'query': 'txtai'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Since the search for "txtai" did not yield any results, I will proceed to find relevant URLs and insert them as a list of strings.[0m

#   [33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'txtai'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have found relevant URLs, I will insert them into the database as a list of strings.[0m

#   [33;1m>>> Calling tool: 'insert' with arguments: {'elements': ['https://github.com/neuml/txtai', 'https://pypi.org/project/txtai/', 'https://neuml.github.io/txtai/', 'https://neuml.com/', 'https://medium.com/neuml/introducing-txtai-the-all-in-one-embeddings-database-c721f4ff91ad', 'https://neuml.github.io/txtai/embeddings/', 'https://neuml.github.io/txtai/examples/']}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that the relevant URLs have been inserted, I will rerun the search for "txtai".[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'query': 'txtai'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': '# txtai Overview\n\ntxtai is an all-in-one open-source embeddings database designed for semantic search, LLM orchestration, and language model workflows. It combines vector indexes (both sparse and dense), graph networks, and relational databases to offer a comprehensive solution for various tasks.\n\n## Semantic Search\n\ntxtai supports semantic search, enabling users to find relevant information based on the meaning of the text rather than just keywords. This makes it suitable for applications such as recommendation systems, content retrieval, and more.\n\n## LLM Orchestration\n\ntxtai includes support for Large Language Models (LLMs) orchestration, which allows users to integrate and manage different LLMs within their workflows. This feature is particularly useful for tasks such as prompt chaining, summarization, transcription, translation, and more.\n\n## Embeddings Databases\n\ntxtai provides a unified interface for embedding, indexing, searching, and processing text and multimedia data. Users can create, index, and search embeddings using code snippets and examples provided in the documentation.\n\n## Hyperlink References\n\n- [txtai on GitHub](https://github.com/neuml/txtai)\n- [PyPI - txtai](https://pypi.org/project/txtai/)\n- [txtai Documentation](https://neuml.github.io/txtai/)\n- [txtai Examples](https://neuml.github.io/txtai/examples/)\n- [txtai Embeddings](https://neuml.github.io/txtai/embeddings/)'}[0m

#   <IPython.core.display.Markdown object>

answer = agent(prompt.format(topic="openscholar"))
display(Markdown(answer))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   Run the following process:

#    1. Search your internal knowledge for openscholar

#    2. If not found, find relevant urls and insert those as a list of strings ONLY. Then rerun the search for openscholar.

#    3. Write a detailed report about openscholar with Markdown sections covering the major topics. Include a section with hyperlink references.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: First, I will search my internal knowledge for "openscholar".[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'query': 'openscholar'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Since the search did not return any relevant information about "openscholar", I will find relevant URLs and insert them as a list of strings. Then, I will rerun the search for "openscholar".[0m

#   [33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'openscholar'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that I have found relevant URLs about "openscholar", I will insert these URLs into the database.[0m

#   [33;1m>>> Calling tool: 'insert' with arguments: {'elements': ['https://openscholar.allen.ai/', 'https://allenai.org/blog/openscholar', 'https://arxiv.org/abs/2411.14199', 'https://github.com/AkariAsai/OpenScholar', 'https://www.marktechpost.com/2024/11/22/the-allen-institute-for-ai-ai2-introduces-openscholar-an-open-ecosystem-for-literature-synthesis-featuring-advanced-datastores-and-expert-level-results/', 'https://venturebeat.com/ai/openscholar-the-open-source-a-i-thats-outperforming-gpt-4o-in-scientific-research/']}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Now that the relevant URLs have been inserted, I will rerun the search for "openscholar" to gather more detailed information.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: {'query': 'openscholar'}[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The search has returned relevant information about OpenScholar. I will now compile a detailed report about OpenScholar with Markdown sections covering the major topics.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "# OpenScholar Report\n\n## Introduction\nOpenScholar is an advanced AI system developed by the Allen Institute for AI and the University of Washington. It aims to revolutionize scientific research by processing millions of research papers and providing citation-backed answers to complex research questions.\n\n## Key Features\n- **Data Processing**: Capable of handling 45 million research papers instantly.\n- **Citation Accuracy**: Achieves high citation accuracy, challenging proprietary AI systems.\n- **Customizable Tools**: Offers specialized tools and built-in features for creating impactful websites.\n\n## Research Impact\n- **Data Utilization**: Integrates a vast corpus of scientific literature, making it the largest open-access corpus for scientific literature synthesis.\n- **Expert-Level Results**: Provides answers that match or exceed those of human experts in various scientific domains.\n\n## Future Directions\n- **Continued Development**: Ongoing efforts to improve the system's capabilities and expand its applications.\n- **Community Engagement**: Plans for expanding the user base and fostering a community around OpenScholar.\n\n## References\n- [OpenScholar](https://openscholar.allen.ai/)\n- [VentureBeat Article](https://venturebeat.com/ai/openscholar-the-open-source-a-i-thats-outperforming-gpt-4o-in-scientific-research/)\n- [Allen Institute for AI](https://allenai.org/blog/openscholar)"}[0m

#   <IPython.core.display.Markdown object>

"""
ğŸ”¥ Amazing.

Remember, we started with an empty embeddings database. Then we gave basic instructions on how to use the available tools. From there, the agent autonomously operated and answered user requests. The agent also stored what it learned for future requests. This gave the agent it's own internal memory.

Of course, we could program a process that implements this workflow. But think about the productivity gains we're opening up to so many more people. We're enabling people to control a process simply by pairing a set of tools with a description of what they want, in English.

Exciting times!
"""

"""
# Wrapping up

This notebook demonstrated ways to run agents in a more autonomous fashion. While the technology isn't perfect, we can certainly see the path ahead where new models will continue to do a better job. With the right agents and targeted tools, much can be done now though. 

Think about the differences between now and 6-12 months ago. Where will we be in another 6-12 months!
"""



================================================
FILE: examples/70_Getting_started_with_LLM_APIs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Getting started with LLM APIs

_Thank you to [Igor Ribeiro Lima](https://github.com/igorlima) for writing this notebook!_

This notebook demonstrates how to call different LLM APIs using just one set of common functions.

While each LLM has its own configuration, here's a cool perk of using [`txtai`](https://neuml.github.io/txtai/) plus [`litellm`](https://docs.litellm.ai/docs/) working behind the scenes. The big win with `txtai` is that it helps standardize inputs and outputs across several LLMs. **This means we can seamlessly call any LLM API**.

In this notebook, we use one set of common functions to call _**Gemini**_, _**VertexAI**_, _**Mistral**_, _**Cohere**_, _**AWS Bedrock**_, _**OpenAI**_, _**Claude by Anthropic**_, _**Groq**_, _and more_!

Isn't that amazing? One method for many different LLM APIs - **that's the beauty of these libraries**!
"""

"""
## Install dependencies

This session guides you through all the essential dependencies you'll need to run the Python script for various LLM APIs.

You can choose the dependencies based on your specific needs. Each dependency is carefully commented on in the code, so you'll know exactly which API it supports.

The core dependencies you'll typically need are: `txtai` and `txtai[pipeline]`.
"""

%%capture
# The main point here is to follow the pattern of other notebooks.
# For more information, kindly refer to the note in the bullet right below.

# !pip install txtai==8.1.0
# !pip install txtai[pipeline]
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]

# to use Vertex AI
# https://github.com/BerriAI/litellm/issues/5483
# !pip install google-cloud-aiplatform==1.75.0
!pip install google-cloud-aiplatform

# to use AWS Bedrock
# https://docs.litellm.ai/docs/providers/bedrock
# !pip install boto3==1.35.88
!pip install boto3

"""
- _**A friendly reminder**: things can change unexpectedly in the ever-evolving world of coding. One day, your code works flawlessly; the next, it might throw a tantrum for no apparent reason. That's why it's essential to specify the dependencies' versions using the latest available versions when writing the code._
  - <sup><sub>Even recognizing the importance of version control, each version has one line above as a comment in the code. It serves as a guide to help you track dependencies and ensures the code runs smoothly across different environments, even if something goes awry.</sub></sup>
  - <sup><sub>While there is [a trade-off](https://github.com/neuml/txtai/pull/844#issuecomment-2564294186) in limiting code to a specific version, noting today's version as a comment should help you protect against potential issues with future updates.</sub></sup>
"""

"""
## LLM API Configuration

This session is like a special Python dictionary that holds all the cool configurations for our LLM AI model. It includes details like environment variables, the model's name, text embedding parameters, and other fancy settings.

One important key here is `IS_ENABLED`. When it's set to `True`, it's like giving the model a green light to shine! But if you ever feel like taking a break or don't need it for a while, you can easily set this key to `False`, and the model will chill out.
"""

import os, getpass
from txtai import LLM, Embeddings

# https://neuml.github.io/txtai/install/
# https://neuml.github.io/txtai/pipeline/text/llm/#example
# https://neuml.github.io/txtai/embeddings/configuration/vectors/#method
# https://docs.litellm.ai/docs/embedding/supported_embedding
LLM_MODEL_CONFIG = {
  'GEMINI': {
    'IS_ENABLED': True,
    'ENV_VAR': ['GEMINI_API_KEY'],
    # https://docs.litellm.ai/docs/providers/gemini
    # https://ai.google.dev/gemini-api/docs/models/gemini
    'LLM_MODEL_NAME': 'gemini/gemini-pro',
    # https://github.com/BerriAI/litellm/tree/12c4e7e695edb07d403dd14fc768a736638bd3d1/litellm/llms/vertex_ai
    # https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/model_prices_and_context_window.json#L2625
    'TEXT_EMBEDDING_PATH': 'gemini/text-embedding-004'
  },
  'COHERE': {
    'IS_ENABLED': False,
    'ENV_VAR': ['COHERE_API_KEY'],
    # https://docs.litellm.ai/docs/providers/cohere
    'LLM_MODEL_NAME': 'command-light',
    'TEXT_EMBEDDING_PATH': 'cohere/embed-english-v3.0'
  },
  'AWS_BEDROCK': {
    'IS_ENABLED': False,
    'ENV_VAR': ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'AWS_REGION_NAME'],
    # https://docs.litellm.ai/docs/providers/bedrock
    'LLM_MODEL_NAME': 'bedrock/amazon.titan-text-lite-v1',
    'TEXT_EMBEDDING_PATH': 'amazon.titan-embed-text-v1'
  },
  'MISTRAL': {
    'IS_ENABLED': False,
    'ENV_VAR': ['MISTRAL_API_KEY'],
    # https://docs.litellm.ai/docs/providers/mistral
    'LLM_MODEL_NAME': 'mistral/mistral-tiny',
    'TEXT_EMBEDDING_PATH': 'mistral/mistral-embed'
  },
  'VERTEXAI': {
    'IS_ENABLED': False,
    'ENV_VAR': ['GOOGLE_APPLICATION_CREDENTIALS', 'GOOGLE_VERTEX_PROJECT', 'GOOGLE_VERTEX_LOCATION'],
    'ENV_VAR_SETUP': None,
    # https://docs.litellm.ai/docs/providers/vertex
    'LLM_MODEL_NAME': 'vertex_ai/gemini-pro',
    'TEXT_EMBEDDING_PATH': 'vertex_ai/text-embedding-004'
  },
  'GROQ': {
    'IS_ENABLED': False,
    'ENV_VAR': ['GROQ_API_KEY'],
    # https://docs.litellm.ai/docs/providers/groq
    # https://console.groq.com/docs/models
    'LLM_MODEL_NAME': 'groq/llama3-8b-8192',
    # the line below has been commented out for a reason.
    # to understand why, check out the Groq section right below.
    # https://groq.com/retrieval-augmented-generation-with-groq-api/
    # 'TEXT_EMBEDDING_PATH': 'jinaai/jina-embeddings-v2-base-en',
  },
  'OPENAI': {
    'IS_ENABLED': False,
    'ENV_VAR': ['OPENAI_API_KEY'],
    # https://docs.litellm.ai/docs/providers/openai
    # https://platform.openai.com/docs/models
    'LLM_MODEL_NAME': 'gpt-4o-mini-2024-07-18',
    'TEXT_EMBEDDING_PATH': 'text-embedding-3-small',
  },
  'CLAUDE': {
    'IS_ENABLED': False,
    # You might need the Voyage API key, depending on the embedding model you choose. Read more below!
    # 'ENV_VAR': ['ANTHROPIC_API_KEY', 'VOYAGE_API_KEY'],
    'ENV_VAR': ['ANTHROPIC_API_KEY'],
    # https://docs.litellm.ai/docs/providers/anthropic
    # https://docs.anthropic.com/en/docs/about-claude/models
    'LLM_MODEL_NAME': 'anthropic/claude-3-5-haiku-20241022',
    # the line below has been commented out for a reason.
    # to understand why, check out the Claude Anthropic section right below.
    # https://docs.anthropic.com/en/docs/build-with-claude/embeddings
    # 'TEXT_EMBEDDING_PATH': 'voyage/voyage-01',
  },
  'VOYAGE': {
    'IS_ENABLED': False,
    # You might need the Anthropic API key, depending on the LLM you choose. Read more below!
    # 'ENV_VAR': ['ANTHROPIC_API_KEY', 'VOYAGE_API_KEY'],
    'ENV_VAR': ['VOYAGE_API_KEY'],
    # the line below has been commented out for a reason.
    # to understand why, check out the Voyage AI section right below.
    # https://docs.litellm.ai/docs/providers/voyage
    # https://docs.voyageai.com/docs/introduction
    # 'LLM_MODEL_NAME': 'anthropic/claude-3-5-haiku-20241022',
    #
    # https://docs.voyageai.com/docs/embeddings
    'TEXT_EMBEDDING_PATH': 'voyage/voyage-01',
  }
}

import litellm
# # https://github.com/BerriAI/litellm/blob/11932d0576a073d83f38a418cbdf6b2d8d4ff46f/litellm/litellm_core_utils/get_llm_provider_logic.py#L322
litellm.suppress_debug_info = True
# https://docs.litellm.ai/docs/debugging/local_debugging#set-verbose
litellm.set_verbose=False

def customsetup():
  def vertexai():
    # https://docs.litellm.ai/docs/embedding/supported_embedding#usage---embedding
    # https://docs.litellm.ai/docs/providers/vertex
    litellm.vertex_project = os.environ['GOOGLE_VERTEX_PROJECT']
    litellm.vertex_location = os.environ['GOOGLE_VERTEX_LOCATION']

  LLM_MODEL_CONFIG['VERTEXAI']['ENV_VAR_SETUP'] = vertexai()

customsetup()

LLM_MODELS = {k: v for k, v in LLM_MODEL_CONFIG.items() if v['IS_ENABLED']}
ENV_VARS = [v['ENV_VAR'] for k, v in LLM_MODELS.items()]
ENV_VARS = [x for xs in ENV_VARS for x in xs] # flatten array

print("config set!")
# Output:
#   config set!


"""
### Vertex AI
"""

"""
When working with VertexAI in a Jupyter notebook, an environment variable called `GOOGLE_APPLICATION_CREDENTIALS` points to a credentials file. This variable is like giving VertexAI a secret map to find the JSON file containing your service account key.

Imagine you've set `GOOGLE_APPLICATION_CREDENTIALS` to `application_default_credentials.json`. The service account key is stored in a file with that exact name. And guess what? This file must be in the same place you're working _(your current working directory)_.

Here's how your directory might look:
```
.
â”œâ”€â”€ ..
â”œâ”€â”€ sample_data/
â””â”€â”€ application_default_credentials.json
```

If you use Google Colab, your directory structure will differ slightly, but don't worry! It'll look something like this:

```
.
â”œâ”€â”€ bin
â”œâ”€â”€ boot
â”œâ”€â”€ content/
â”‚   â”œâ”€â”€ sample_data/
â”‚   â””â”€â”€ application_default_credentials.json
â”œâ”€â”€ datalab
â”œâ”€â”€ dev
â”œâ”€â”€ etc
â”œâ”€â”€ home
â”œâ”€â”€ lib
â”œâ”€â”€ lib32
â””â”€â”€ lib64
```

"""

"""
### Groq
"""

"""
It's good to make the most of [free resources](https://github.com/BerriAI/litellm/issues/4922#issuecomment-2374234548) like this one! Of course, free options often come with some limitations, and this is no different. Currently, Groq doesn't offer an API for text embeddings.

The Groq team highlights a pre-trained model that can be used locally. You can learn more about it on the [Groq Blog](https://groq.com/retrieval-augmented-generation-with-groq-api/), where they discuss the `jinaai/jina-embeddings-v2-base-en`, a model that can be hosted locally.

This notebook primarily focuses on well-known online API models, so I've commented on the Groq line for text embedding. However, if you're curious, you can explore the model Groq recommends for local use by uncommenting it.
"""

"""
### Claude Anthropic
"""

"""
While Claude Anthropic isn't a free resource, they're well-known for their top-notch LLM outputs. The only tiny hiccup for this Notebook context is that they don't offer an API for text embeddings.

But don't worry. In their documentation, they recommend checking out the Voyage AI embedding model. You can find all the deets right [here](https://docs.anthropic.com/en/docs/build-with-claude/embeddings).

The Claude team highlights the Voyage AI embedding model on their documentation page. Voyage is an online resource, though the Claude Anthropic team does not host it.

This notebook is about well-known online API models. So, I've commented on the Claude config line for text embedding. But if you're curious and want to explore the model Claude recommends, uncomment that line and explore away!
"""

"""
### Voyage AI
"""

"""
Voyage AI provides only [top-notch embeddings](https://docs.voyageai.com/docs/introduction). While it doesn't offer [its LLM model](https://docs.voyageai.com/docs/embeddings), that doesn't diminish its offerings. The cool thing? Their embeddings are versatile and can be integrated into any model you choose. So, feel free to experiment and have fun with them! That's the beauty of this notebook - it's all about exploration and learning.
"""

"""
## Environment Variables

This session has scripts to reset or set environment variables _(env vars)_. Using env vars is a great way to keep those sensitive API KEY values safe and sound, away from unwanted eyes.
"""

"""
### Script to Reset Environment Variables

There are a bunch of reasons to reset the environment variable here. Let's go over a few:
- **Switching API Keys:** Sometimes, we need to change to a different API key.
- **Typos Happen:** We're all human, and typos can sneak in!
- **Skipping Variables:** Sometimes we need to skip one or another, so we set the env var to empty.
- **Fresh Start:** After tweaking a script or setup, it's always good to rerun with a fresh environment.

"""

if 'MISTRAL_API_KEY' in os.environ:
  del os.environ['MISTRAL_API_KEY']

for ENV_VARS in [v['ENV_VAR'] for k, v in LLM_MODEL_CONFIG.items()]:
  for ENV_VAR in ENV_VARS:
    if ENV_VAR in os.environ:
      del os.environ[ENV_VAR]
    print(f'unset env var: {ENV_VAR}')

# Output:
#   unset env var: GEMINI_API_KEY

#   unset env var: COHERE_API_KEY

#   unset env var: AWS_ACCESS_KEY_ID

#   unset env var: AWS_SECRET_ACCESS_KEY

#   unset env var: AWS_REGION_NAME

#   unset env var: MISTRAL_API_KEY

#   unset env var: GOOGLE_APPLICATION_CREDENTIALS

#   unset env var: GOOGLE_VERTEX_PROJECT

#   unset env var: GOOGLE_VERTEX_LOCATION


"""
### Script to Set Environment Variables

In this session, the script will stroll through the config dictionary to cherry-pick only the enabled LLM models.

Once your favorites have been gathered, the script will prompt you to set all the necessary environment variables.

The more LLM models you have enabled, the more prompts you'll see - but don't worry, each environment variable will prompt only once.
"""

for ENV_VAR in ENV_VARS:
  os.environ[ENV_VAR] = getpass.getpass(f"enter {ENV_VAR}: ") if not ENV_VAR in os.environ else os.environ[ENV_VAR]

LLM_MODELS_WITH_CUSTOM_SETUP = {k: v for k, v in LLM_MODELS.items() if 'ENV_VAR_SETUP' in v}
for LLM_MODEL, LLM_CONFIG in LLM_MODELS_WITH_CUSTOM_SETUP.items():
  print(f"running custom env var setup for {LLM_MODEL}...")
  LLM_CONFIG['ENV_VAR_SETUP']()
  print(f"done setup for {LLM_MODEL}")

print("all env var set!")
# Output:
#   enter GEMINI_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter COHERE_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter AWS_ACCESS_KEY_ID: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter AWS_SECRET_ACCESS_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter AWS_REGION_NAME: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter MISTRAL_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter GOOGLE_APPLICATION_CREDENTIALS: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter GOOGLE_VERTEX_PROJECT: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter GOOGLE_VERTEX_LOCATION: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter GROQ_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter OPENAI_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter ANTHROPIC_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   enter VOYAGE_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·

#   running custom env var setup for VERTEXAI...

#   done setup for VERTEXAI

#   all env var set!


"""
# Code Snippet

You can run the code snippet after installing the necessary dependencies and setting up the required environment variables!



"""

"""
## Introduction

This code snippet is designed to achieve only two tasks: _(i)_ run an LLM Pipeline; _(ii)_ text Embed using the LLM Model.
"""

"""
The code snippet has two tasks:
1. **Running an LLM Pipeline**:
   - To kickstart an LLM pipeline, you'll need a prompt and a model name. It's as simple as that!
2. **Text Embedding using the LLM Model**:
   - To embed text with the LLM model, you'll need some text and the name of the text embedding model.

In this script, we'll run an LLM pipeline for every given LLM and embed text using the provided LLM model.

Here's the heart of the script:
```python
LLM_MODEL_NAME = "model-name"
TEXT_EMBEDDING_PATH = "text-embedding-name"
# A text prompt to send through the LLM pipeline
LLM_PROMPT_INPUT = "Where is one place you'd go in Washington, DC?"
# The embeddings dataset is versatile! It plays with lists, datasets, or even generators.
EMBEDDING_DATA = [
    "US tops 5 million confirmed virus cases",
    "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
    "Beijing mobilises invasion craft along coast as Taiwan tensions escalate"
]
# LLM PIPELINE
llm = LLM(LLM_MODEL_NAME, method="litellm")
print(llm([{"role": "user", "content": LLM_PROMPT_INPUT}]))
# TEXT EMBEDDING
embeddings = Embeddings(path=TEXT_EMBEDDING_PATH, method="litellm")
embeddings.index(EMBEDDING_DATA) # create an index for the text list
for query in ("feel good story", "climate change"): # now, let's embark on an embeddings search for each query
    # extract the uid of the first result
    # search result format: (uid, score)
    uid = embeddings.search(query, 1)[0][0]
    # print the text
    print("%-20s %s" % (query, EMBEDDING_DATA[uid]))
```

- <sup><sub>**friendly reminder**: _the library's author often [points out](https://github.com/neuml/txtai/pull/844#issuecomment-2563561232) that it's [not necessary](https://github.com/neuml/txtai/issues/843#issuecomment-2563244810) to explicitly pass the second argument `method='litellm'`. When you're learning something new, it's okay to avoid relying on shortcuts or "magic" until you're more comfortable. Once you understand the library better, you can start using these convenient features to your advantage. In this introduction, I'm intentionally including the second argument `method='litellm'` in the function. However, I'm choosing to leave it out in the Playground section_.</sub></sup>
"""

"""
## Playground

Once you've installed the necessary dependencies and configured the environment variables, you can play with and explore the code snippet. Enjoy your coding journey! ğŸ˜Š

"""

# A text prompt to run through the LLM pipeline
# https://neuml.github.io/txtai/pipeline/text/llm/
LLM_PROMPT_INPUT = "Where is one place you'd go in Washington, DC?"

# The embeddings dataset is versatile! It plays with lists, datasets, or even generators.
# https://neuml.github.io/txtai/embeddings/
EMBEDDING_DATA = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# https://neuml.github.io/txtai/pipeline/text/llm/
def runllm(prompt="", path=None):
  if path:
    # A quick note: you can skip specifying the `method` argument.
    # There's an autodetection logic designed to recognize it as a `litellm` model.
    # 
    # llm = LLM(LLM_MODEL_NAME, method="litellm")
    # 
    llm = LLM(path)

    # OR: print(llm(LLM_PROMPT_INPUT, defaultrole="user"))
    print(llm([{"role": "user", "content": prompt}]))

# https://neuml.github.io/txtai/embeddings/
def runembeddings(data=None, path=None):
  if path:
    embeddings = Embeddings(
      path=path,
      # a quick note: you can skip specifying the `method` argument - there is autodetection logic
      # method="litellm"
    )
    # create an index for the list of text
    embeddings.index(data)

    print("." * 50)
    print("%-20s %s" % ("Query", "Best Match"))
    print("." * 50)
 
    # run an embeddings search for each query
    for query in ("feel good story", "climate change",
        "public health story", "war", "wildlife", "asia",
        "lucky", "dishonest junk"):
      # extract uid of first result
      # search result format: (uid, score)
      uid = embeddings.search(query, 1)[0][0]
      # print text
      print("%-20s %s" % (query, data[uid]))

# Let's LOOP THROUGH each enabled LLM and embedding model.
for LLM_MODEL, LLM_CONFIG in LLM_MODELS.items():
  LLM_MODEL_NAME = LLM_CONFIG['LLM_MODEL_NAME'] if 'LLM_MODEL_NAME' in LLM_CONFIG else None
  TEXT_EMBEDDING_PATH = LLM_CONFIG['TEXT_EMBEDDING_PATH'] if 'TEXT_EMBEDDING_PATH' in LLM_CONFIG else None
  print("-" * 50)
  print(LLM_MODEL)

  # https://neuml.github.io/txtai/pipeline/text/llm/
  runllm(prompt=LLM_PROMPT_INPUT, path=LLM_MODEL_NAME)

  # https://neuml.github.io/txtai/embeddings/
  runembeddings(data=EMBEDDING_DATA, path=TEXT_EMBEDDING_PATH)

print("-" * 50)
# Output:
#   --------------------------------------------------

#   GEMINI

#   The National Mall

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------

#   COHERE

#   As an AI chatbot, I do not have personal preferences or the ability to travel. However, I can suggest popular tourist destinations in Washington, DC, that many people enjoy visiting:

#   

#   1. The White House: The official residence and workplace of the President of the United States is a symbol of American democracy. Visitors can take a tour of the White House to learn about its history and see the beautiful architecture.

#   

#   2. National Mall: This iconic open park stretches from the United States Capitol to the Lincoln Memorial, featuring various monuments and memorials. Some notable landmarks include the Washington Monument, the Lincoln Memorial Reflecting Pool, the Vietnam Veterans Memorial, and the National World War II Memorial.

#   

#   3. Smithsonian Institution: The world's largest museum and research complex offers a wealth of knowledge and cultural experiences. It includes renowned museums such as the National Air and Space Museum, National Museum of Natural History, National Museum of African American History and Culture, and many more, all offering free admission.

#   

#   4. United States Capitol: A visit to the Capitol allows you to explore the seat of the United States Congress and learn about the legislative process. The Capitol also features impressive architecture and art, including the iconic Capitol Dome and the National Statuary Hall Collection.

#   

#   5. National Gallery of Art: Located on the National Mall, this renowned art museum houses a vast collection of paintings, sculptures, and other artworks from the Middle Ages to the present. It offers a chance to appreciate works by famous artists like Leonardo da Vinci, Rembrandt, and Claude Monet.

#   

#   These are just a few highlights, but Washington, DC, offers many more attractions, including historic sites, beautiful parks, vibrant neighborhoods, and cultural institutions, ensuring there's something for everyone to enjoy.

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       US tops 5 million confirmed virus cases

#   --------------------------------------------------

#   AWS_BEDROCK

#   I'd recommend the Smithsonian National Air and Space Museum. It is located in Washington, DC, and is the world's largest and most visited museum complex.

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------

#   MISTRAL

#   I would love to visit the Smithsonian National Museum of Natural History in Washington, DC. It's one of the most popular and largest museums in the world, with a vast array of exhibits showcasing natural history, including dinosaur fossils, mineral and gemstone collections, and marine life. I find the diversity and depth of knowledge presented in this museum fascinating. Additionally, it's free to the public, which makes it an accessible and enjoyable experience for everyone.

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      The National Park Service warns against sacrificing slower friends in a bear attack

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  The National Park Service warns against sacrificing slower friends in a bear attack

#   war                  The National Park Service warns against sacrificing slower friends in a bear attack

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------

#   VERTEXAI

#   Washington, DC, is a fascinating city with a wealth of historical and cultural attractions. If I had the opportunity to visit, I would definitely make a stop at the Smithsonian National Air and Space Museum. This museum is home to a vast collection of aircraft and spacecraft, from the Wright brothers' first plane to the Apollo 11 command module. I am particularly interested in the history of space exploration, so I would be excited to see these iconic artifacts up close.

#   

#   In addition to the Air and Space Museum, there are many other places in Washington, DC, that I would like to visit. The National Mall is a beautiful park that is home to many of the city's most famous monuments, including the Washington Monument, the Lincoln Memorial, and the Vietnam Veterans Memorial. I would also like to visit the White House, the Capitol Building, and the Supreme Court. These buildings are all important symbols of American democracy, and I would be honored to have the opportunity to see them in person.

#   

#   Washington, DC, is a city with a rich history and a bright future. I am confident that it will continue to be a major center of culture and government for many years to come.

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------

#   GROQ

#   Washington, D.C. is a city with a plethora of amazing places to visit! But, if I had to pick just one place, I'd recommend the National Mall.

#   

#   The National Mall is a beautiful stretch of parkland that stretches from the Lincoln Memorial to the United States Capitol Building. It's home to some of the city's most iconic landmarks, including the Washington Monument, World War II Memorial, and Vietnam Veterans Memorial.

#   

#   You can take a leisurely stroll along the Mall, enjoy the scenic views of the city, and stop at one of the many museums or memorials along the way. The National Mall is also a great place to people-watch, with street performers and musicians adding to the lively atmosphere.

#   

#   Personally, I'd recommend starting at the Lincoln Memorial, where you can take in the stunning views of the Reflecting Pool and the Washington Monument. From there, you can walk to the World War II Memorial, where you can pay your respects to the millions of Americans who served in the war.

#   

#   Overall, the National Mall is a must-see destination in Washington, D.C. â€“ and it's completely free!

#   --------------------------------------------------

#   OPENAI

#   One iconic place to visit in Washington, DC, is the National Mall. It's home to many famous monuments and memorials, including the Lincoln Memorial, the Washington Monument, and the Vietnam Veterans Memorial. The expansive park offers a great opportunity to explore the history and culture of the nation, and it's a beautiful spot for walking, picnicking, and enjoying the iconic views.

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  US tops 5 million confirmed virus cases

#   war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------

#   CLAUDE

#   One great place to visit in Washington, DC is the Smithsonian National Air and Space Museum. It's located on the National Mall and features fascinating exhibits about aviation and space exploration, including famous aircraft and spacecraft like the Wright Brothers' plane and the Apollo 11 command module. The museum is free to enter and is very popular with visitors of all ages.

#   --------------------------------------------------

#   VOYAGE

#   ..................................................

#   Query                Best Match

#   ..................................................

#   feel good story      Maine man wins $1M from $25 lottery ticket

#   climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg

#   public health story  Maine man wins $1M from $25 lottery ticket

#   war                  The National Park Service warns against sacrificing slower friends in a bear attack

#   wildlife             The National Park Service warns against sacrificing slower friends in a bear attack

#   asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate

#   lucky                Maine man wins $1M from $25 lottery ticket

#   dishonest junk       Make huge profits without work, earn up to $100,000 a day

#   --------------------------------------------------




================================================
FILE: examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Analyzing LinkedIn Company Posts with Graphs and Agents

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

This notebook will analyze [NeuML's LinkedIn company posts](https://hf.co/datasets/neuml/neuml-linkedin-202501) over the last 12 months as of January 2025. It will build an embeddings database with an associated graph and topic modeling. The most popular topics will be explored using graphs and vector search queries.

From there we'll use Agents to investigate the dataset and find ways to increase engagement with our future posts using past history.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[agent,graph] datasets autoawq autoawq-kernels

"""
# LinkedIn Company Posts

The next section builds the LinkedIn company posts dataset for NeuML. First, it downloads the company posts dataset then it creates an embeddings database.

There is also custom logic to create topic names for the generated topic clusters vs the standard naming method.

_If you'd like to instead use your own company posts, see these [steps on the dataset page](https://hf.co/datasets/NeuML/neuml-linkedin-202501)._
"""

from datasets import load_dataset
from tqdm.auto import tqdm
from txtai import Embeddings, LLM

def title(text):
    prompt = f"""
Create a simple, concise topic for the following text. Only return the topic name.

Text:
{text}
"""

    return llm([{"role": "user", "content": prompt}], maxlength=2048)

def data():
    for x in posts:
        x["url"] = x.pop("Post link")
        x["text"] = x.pop("Post title")
        x["id"] = title(x["text"])
        yield {k.lower().replace(" ", ""): v for k, v in x.items()}

posts = load_dataset("neuml/neuml-linkedin-202501", split="train")

llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

embeddings = Embeddings(
    autoid="uuid5",
    path="intfloat/e5-large",
    instructions={"query": "query: ", "data": "passage: "},
    content=True,
    functions = [
        {"name": "graph", "function": "graph.attribute"}
    ],
    expressions = [
        {"name": "topic", "expression": "graph(indexid, 'topic')"}
    ],
    graph={
        "approximate": False,
        "topics": {"resolution": 6}
    },
)
embeddings.index(tqdm(data(), total=len(posts)))

"""
Now that the embeddings database is built, let's generate the topic names using an LLM.
"""

def topic(text):
    prompt = f"""
Create a simple, concise topic for the following text. Only return the topic name.

Text:
{text}"""

    return llm([{"role": "user", "content": prompt}], maxlength=5000)

topics = {}
for name, nodes in tqdm(embeddings.graph.topics.items()):
    name = topic("\n".join(embeddings.graph.attribute(x, "text") for x in nodes))
    topics[name] = nodes

    # Set topic on each node
    for node in nodes:
        embeddings.graph.addattribute(node, "topic", name)

embeddings.graph.topics = topics

"""
Both of the sections above should run within a minute with a modern GPU. Building the embeddings database and generating the topics can optionally be skipped using the following model from the Hugging Face Hub.

```python
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-neuml-linkedin")
```
"""

"""
# Popular topics

Now that we have an embeddings database, graph and topics, let's explore the data! The next section shows the Top 5 posts for each of the Top 5 most popular topics.
"""

from IPython.display import display, Markdown

# Get top 5 topics by popularity
popular = embeddings.search("SELECT topic FROM txtai GROUP BY topic ORDER BY sum(impressions) DESC LIMIT 5")

output = ""
for topic in popular:
    topic = topic["topic"]
    output += f"#### {topic}\n"
    results = embeddings.search(
        "SELECT text, createddate, impressions FROM txtai WHERE topic = :topic ORDER BY impressions DESC LIMIT 5",
        parameters={"topic": topic}
    )

    for i, x in enumerate(results):
        text = x["text"].replace("\n", " ")
        output += f"{i + 1}. _{x['createddate']} - {x['impressions']} views_ - {text}\n"

display(Markdown(output))
# Output:
#   <IPython.core.display.Markdown object>

"""
And lets also look at the top 5 most popular posts overall.
"""

output = ""
for i, x in enumerate(embeddings.search("SELECT id, text, createddate, impressions FROM txtai ORDER BY impressions DESC", 5)):
    text = x["text"].replace("\n", " ")
    output += f"{i + 1}. _{x['createddate']} - {x['impressions']} views_ - {text}\n"

display(Markdown(output))
# Output:
#   <IPython.core.display.Markdown object>

"""
A couple themes are shown in the data. Posts covering `Graph RAG`, `Vector Embeddings Innovations` and `medical literature related models` all had high engagement.

Let's plot a subgraph of the nodes related to `medical literature`.
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}

    options = {
        "node_size": 700,
        "node_color": "#0277bd",
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 10,
        "alpha": 1.0,
    }

    # Draw graph
    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)

    # Disable axes and draw margins
    ax.axis("off")
    plt.margins(x=0.15)

    # Set background color
    ax.set_facecolor("#303030")
    fig.set_facecolor("#303030")

    plt.show()

plot(embeddings.search("medical literature", 10, graph=True))
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
# Explore the data with Agents

Our last task will use Agents to explore the dataset. We'll create a method to search the dataset with a vector query and return the matches plus post engagement (impressions, likes, clicks etc). Then we'll hand the keys over the the agent and see what it can do.
"""

from txtai import Agent

def search(query):
    """
    Searches a database of posts. Returns results ordered by most popular

    Make sure to call this tool only with a string input, never use JSON.    

    Args:
        query: concepts to search for using similarity search
 
    Returns:
        list of search results with (text, impressions, likes, clicks and post urls) for each match
    """

    return embeddings.search(
        "SELECT text, impressions, likes, clicks, url FROM txtai WHERE similar(:query) ORDER BY impressions DESC",
        10, parameters={"query": query}
    )

def md(output):
    display(Markdown(output))

agent = Agent(
    tools=[search],
    llm=llm,
    max_iterations=15,
)

"""
First, let's ask the agent to explore ways to increase engagement with future social media posts.
"""

prompt = """
Analyze ALL available posts and find the topics discussed in the most popular posts.

With those findings, tell me some ideas to write about in my future social media
posts in a couple sentences.
"""

md(agent(prompt, maxlength=16000))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   Analyze ALL available posts and find the topics discussed in the most popular posts.

#   

#   With those findings, tell me some ideas to write about in my future social media

#   posts in a couple sentences.

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use the tool'search' to analyze the most popular posts and find the topics discussed in them.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: most popular posts[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will now extract the topics discussed in the most popular posts.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: txtai, RAG, Embeddings graph database, PubMedBERT, vector databases, semantic search apps, medical literature, Hacker News, GitHub, vector databases, open-source frameworks[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will now extract the topics discussed in the most popular posts.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: txtai, RAG, embeddings database, Wikipedia, vector databases, semantic search, LLM orchestration, language model workflows, Graph RAG, ArXiv, medical literature, text to sql, text to speech, RAG Data, multiprocessing, performance enhancements, txtai-wikipedia, vector similarity, path finding queries, txtai-wikipedia index, autonomous agents, retrieval augmented generation, multi-model workflows, default configuration[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will now use the extracted topics to generate ideas for future social media posts.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: Future social media post ideas: 1) Explore the power of Graph RAG and its applications in knowledge graph construction. 2) Discuss the benefits of using txtai's embeddings database for semantic search, LLM orchestration, and language model workflows. 3) Share success stories of using RAG for retrieval augmented generation and its potential in various industries. 4) Introduce the concept of autonomous agents and their potential in multi-model workflows. 5) Highlight the importance of open access to underlying data in a programmatic and platform-independent way.[0m

#   <IPython.core.display.Markdown object>

"""
ğŸš€ Interesting and certainly seems in line with our own previous analysis.

Next, let's ask the agent to tell us what it knows about NeuML. This is an example of `Agentic RAG`. It is like standard RAG but it enables an LLM to have multiple passes at the knowledge base before generating the response.
"""

md(agent("What would you say NeuML's core competencies are?", maxlength=16000))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1mWhat would you say NeuML's core competencies are?[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use the tool `search` to find the core competencies of NeuML. The tool can search a database of posts and return results ordered by most popular.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: NeuML core competencies[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the search tool indicate that NeuML's core competencies include:

#   

#   - Generative knowledge graphs

#   - Micromodels

#   - Cloud offering

#   - Consulting services

#   - Community engagement and training

#   

#   However, it's not clear what NeuML's core competencies are from the search results. Let me try to find more information.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: NeuML core competencies[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The search results still don't clearly indicate NeuML's core competencies. However, it seems that NeuML is building a sustainable and profitable company from the start and provides consulting services in situations where it's an interesting business problem. It also provides open-source development, advisory and strategic support services (i.e. Fractional CTO), and has a growing number of models available on the Hugging Face hub.

#   

#   Thought: I will try to get a clearer answer by looking at the specific services and products that NeuML offers.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: NeuML services[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: Unfortunately, the search results still don't provide a clear answer to what NeuML's core competencies are. However, it seems that NeuML provides a range of services, including consulting, advisory and strategic support services (i.e. Fractional CTO), open-source development, and more.

#   

#   Thought: Since I couldn't find a clear answer through the search tool, I will try to provide a final answer based on the information available.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: NeuML provides a range of services, including consulting, advisory and strategic support services (i.e. Fractional CTO), open-source development, and more.[0m

#   <IPython.core.display.Markdown object>

"""
Seems about right. Let's keep in mind this is using a database of company posts, so it only knows what the company tells it to know ğŸ˜‚.

Next, lets see what the agent believes are the strengths of txtai.
"""

md(agent("Tell me about the strengths of txtai", maxlength=16000))
# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1mTell me about the strengths of txtai[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will use the tool `search` to find information about the strengths of txtai.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: txtai strengths[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The strengths of txtai include its ability to automatically create graphs using semantic similarity, as well as its robust integration with Postgres, which allows for the persistence of vectors, sparse vectors, documents, graph edges, and nodes. Additionally, txtai has a growing ease of use, with a well-documented project and reliable, easy-to-use features out of the box. It also supports a variety of integrations, including Agents, RAG, and Vector Search, and has a focus on security with full Postgres integration.

#   

#   Thought: I also noticed that txtai has a focus on depth over breadth, ensuring that integrations are performant and support a large number of the underlying libraries' features. It is also built on the shoulders of open-source giants, such as Transformers, SQLite, NetworkX, Postgres, Faiss, and NumPy, and has a strong commitment to open access to underlying data in a programmatic and platform-independent way.

#   

#   Thought: Now that I have gathered information about the strengths of txtai, I will return the final answer.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "txtai's strengths include its ability to automatically create graphs using semantic similarity, robust integration with Postgres, ease of use, variety of integrations, focus on security, focus on depth over breadth, and commitment to open access to underlying data."}[0m

#   <IPython.core.display.Markdown object>

"""
Remember, this is based on what posts received the most impressions. It's a feedback loop with what users on LinkedIn believe is a strength as most engaging posts are prioritized. This is a complex topic as engagement is also driven by what LinkedIn chooses to share in user's feeds, which no one really knows the answer to ğŸ˜€.
"""

"""
# Wrapping up

This notebook demonstrated how to use graphs and agents to explore the [NeuML LinkedIn Posts dataset](https://huggingface.co/datasets/neuml/neuml-linkedin-202501). It presented ideas on how to increase future social media engagement by analyzing what's worked in the past. It was very illuminating ğŸ’¡.
"""



================================================
FILE: examples/72_Parsing_the_stars_with_txtai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Parsing the â­'s with txtai

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

In honor of txtai's recent milestone of reaching 10K â­'s on GitHub, this notebook will build an astronomical knowledge graph of known stars, planets, galaxies and other astronomical entities. It will show how a compendium of information can be built from a public knowledge source such as Wikipedia.

While agents and retrieval augmented generation (RAG) can do great things, they can do even greater things if provided with an assist from the humans in the form of compiled knowledge. It's not only better accuracy, it's also faster as we don't need to wait for an agent to figure out what we already know.

Let's dive in.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[agent,graph,pipeline-text] datasets autoawq autoawq-kernels

"""
# Build the Knowledge Graph

The first step is building our knowledge graph or compendium of information. We'll use [txtai-wikipedia](https://hf.co/neuml/txtai-wikipedia) and another source derived from wikipedia with known [stars by constellation](https://huggingface.co/datasets/NeuML/constellations).

We'll select articles about planets, constellations, galaxies and stars. We'll then extract entities using a [GLiNER pipeline](https://github.com/urchade/GLiNER). Lastly, we'll join those entries together with the constellation data.

To get an idea of what this data will look like, below is the schema for the `constellation` dataset.

The following is a description of each of the fields in this dataset. As we can see, there is a lot of identifiers but it also has data such as it's location, brightness and distance from earth.

| Field               | Description                                                       |
| ------------------- | ----------------------------------------------------------------- | 
| Constellation       | Name of the constellation the star is a part of                   |
| Name                | Proper name                                                       |
| Bayer               | Bayer designation                                                 |
| Flamsteed           | Flamsteed designation                                             |
| Variable Star       | Variable star designation                                         |
| Henry Draper Number | Henry Draper Catalogue designation number                         |
| Hipparcos Number    | Hipparcos Catalogue designation number                            |
| Right Ascension     | Right ascension for the Epoch/Equinox J2000.0                     |
| Declination         | Declination for the Epoch/Equinox J2000.0                         |
| Visual Magnitude    | Visual Magnitude (m or mv), also known as apparent magnitude      |
| Absolute Magnitude  | Absolute Magnitude (Mv)                                           | 
| Distance            | Distance in light-years from Earth                                |
| Spectral class      | Spectral class of the star in the stellar classification system   |
| Notes               | Common name(s) or alternate name(s); comments; notable properties |

"""

from txtai import Embeddings

embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

from txtai.pipeline import Entity

entity = Entity("gliner-community/gliner_medium-v2.5")

from datasets import load_dataset

def parsefloat(value):
    try:
        return float(value.replace("âˆ’", "-")) if value else value
    except ValueError:
        return value

def parse(star):
    text = star["Name"] if star["Name"] else f'HD-{star["Henry Draper Number"]}'

    output = {"id": star["Name"], "type": "star", "text": text}
    for field, value in star.items():
        field = field.lower().replace(" ", "_")
        output[field] = parsefloat(value)

    return output

# Star by constellation dataset
stars = load_dataset("neuml/constellations", split="train")

"""
The sections above loaded `txtai-wikipedia`, the `GLiNER` pipeline, and the `constellations` dataset. Now it's time to build the knowledge graph!
"""

from tqdm.auto import tqdm

labels = {
    "name of constellation": "constellation",
    "name of galaxy": "galaxy",
    "name of star in astronomy": "star",
    "name of planet": "planet"
}

def stream():
    # Seed with stars dataset
    rows = {star["Name"]: parse(star) for star in stars}

    # Get wikipedia results
    results = embeddings.search(
        "SELECT id, text, percentile FROM txtai WHERE similar('scientific article defining a constellation, galaxy, star or planet') AND score >= 0.8",
        embeddings.count()
    )

    # Aggregate into entity rows
    for row in tqdm(results):
        entities = entity(row["text"], labels=list(labels.keys()))
        entities = [(entity, labels[tag]) for entity, tag, score in entities if score >= 0.95 and entity != row["id"]]
        if entities or row["id"] in rows:
            # Collect relationships    
            relationships = set()
            for uid, tag in entities:
                if uid not in rows:
                    rows[uid] = {"id": uid, "type": tag, "text": tag}

                relationships.add(uid)

            # Add relationships and yield main record
            row["relationships"] = list(relationships)

            # Merge existing record (if any) and save
            rows[row["id"]] = {**rows[row["id"]], **row} if row["id"] in rows else row

    yield from rows.values()

astronomy = Embeddings(
    path = "intfloat/e5-base",
    instructions={"query": "query: ", "data": "passage: "},
    faiss = {"quantize": True, "sample": 0.05},
    content = True,
    graph = {"approximate": True, "topics": {}, "copyattributes": True}
)
astronomy.index(stream())
astronomy.save("txtai-astronomy")

"""
# Explore the Knowledge Graph

The next section creates a function to plot a knowledge graph. This function is specific to our astronomical data. It color codes galaxies, planets and stars by [spectral class](https://en.wikipedia.org/wiki/Stellar_classification).


"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}
    mapping = {
        "constellation": "#f44336", "galaxy": "#9575cd", "planet": "#9e9e9e", "star": "#ff9800",
        "O": "#92b5ff", "B": "#a2c0ff", "A": "#d5e0ff", "F": "#f9f5ff", "G": "#ffede3",
        "K": "#ffdab5", "M": "#ffb56c"
    }

    colors = []
    for x in graph.scan():
        etype, spectral  = graph.attribute(x, "type"), graph.attribute(x, "spectral_class")
        if etype == "star":
            etype = spectral[0] if spectral else etype
        
        colors.append(mapping.get(etype, "#03a9f4"))

    options = {
        "node_size": 700,
        "node_color": colors,
        "edge_color": "#454545",
        "alpha": 1.0,
    }

    # Draw graph
    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, with_labels=False, **options)

    # Calculate label positions
    pos = {node: (x, y - 0.1) for node, (x, y) in pos.items()}

    nx.draw_networkx_labels(
        graph.backend, pos, labels=labels, font_color="#efefef", font_size=10
    )

    # Disable axes and draw margins
    ax.axis("off")
    plt.margins(x=0.15)

    # Set background color
    ax.set_facecolor("#303030")
    fig.set_facecolor("#303030")

    plt.show()

"""
Now we're ready to run a query. `txtai` supports vector queries, SQL queries and Graph queries (using [openCypher](https://github.com/aplbrain/grand-cypher)). As of txtai 8.3, queries are automatically routed to the correct index based on type. 

The path traversal query below takes a starting node and plots the 10 relationships that are closest to earth.
"""

subgraph = astronomy.search("""
MATCH P=({id: "Andromeda"})-[]->(A)
WHERE A.distance > 0
RETURN P 
ORDER BY A.distance
LIMIT 10
""", graph=True)

plot(subgraph)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
We can also find stars with planets.
"""

subgraph = astronomy.search("""
MATCH P=(A {type: "star"})-[]->(B {type: "planet"})
WHERE B.id <> "Jupiter" AND B.id <> "Saturn"
RETURN P
""", graph=True)

plot(subgraph)

for x in subgraph.scan():
    print(subgraph.attribute(x, "id"), subgraph.attribute(x, "text"))
# Output:
#   <Figure size 2000x900 with 1 Axes>
#   WASP-21b planet

#   WASP-21 WASP-21 is a G-type star (spectral type G3V) that is reaching the end of its main sequence lifetime  approximately 850 light years from Earth in the constellation of Pegasus. The star is relatively metal-poor, having 40% of heavy elements compared to the Sun. Kinematically, WASP-21 belongs to the thick disk of the Milky Way. It has an exoplanet named WASP-21b.

#   WASP-13 WASP-13, also named Gloas, is a star in the Lynx constellation. The star is similar, in terms of metallicity and mass, to the Sun, although it is hotter and most likely older. The star was first observed in 1997, according to the SIMBAD database, and was targeted by SuperWASP after the star was observed by one of the SuperWASP telescopes beginning in 2006. Follow-up observations on the star led to the discovery of planet Cruinlagh in 2008; the discovery paper was published in 2009.

#   Cruinlagh planet

#   Gamma Cephei Gamma Cephei (Î³ Cephei, abbreviated Gamma Cep, Î³ Cep) is a binary star system approximately 45 light-years away in the northern constellation of Cepheus. The primary (designated Gamma Cephei A, officially named Errai , the traditional name of the system) is a stellar class K1 orange giant or subgiant star; it has a red dwarf companion (Gamma Cephei B). An exoplanet (designated Gamma Cephei Ab, later named Tadmor) has been confirmed to be orbiting the primary.

#   Tadmor planet


"""
Standard SQL queries are also supported. Let's show the stars that are brightest from Earth.
"""

astronomy.search("""
SELECT id, text, visual_magnitude FROM txtai
WHERE visual_magnitude IS NOT NULL
ORDER BY visual_magnitude
LIMIT 5
""")
# Output:
#   [{'id': 'Sirius',

#     'text': "Sirius is the brightest star in the night sky. Its name is derived from the Greek word  (Latin script: ), meaning  'glowing' or 'scorching'. The star is designated \xa0Canis Majoris, Latinized to Alpha Canis Majoris, and abbreviated \xa0CMa or Alpha\xa0CMa. With a visual apparent magnitude of âˆ’1.46, Sirius is almost twice as bright as Canopus, the next brightest star. Sirius is a binary star consisting of a main-sequence star of spectral type A0 or A1, termed Sirius\xa0A, and a faint white dwarf companion of spectral type\xa0DA2, termed Sirius\xa0B. The distance between the two varies between 8.2 and 31.5\xa0astronomical units as they orbit every 50\xa0years.",

#     'visual_magnitude': -1.46},

#    {'id': 'Canopus',

#     'text': 'Canopus is the brightest star in the southern constellation of Carina and the second-brightest star in the night sky. It is also designated Î±\xa0Carinae, which is romanized (transliterated) to Alpha\xa0Carinae. With a visual apparent magnitude of âˆ’0.74, it is outshone only by Sirius.',

#     'visual_magnitude': -0.72},

#    {'id': 'Arcturus',

#     'text': 'Arcturus is the brightest star in the northern constellation of BoÃ¶tes. With an apparent visual magnitude of âˆ’0.05, it is the fourth-brightest  star in the night sky, and the brightest in the northern celestial hemisphere. The name Arcturus originated from ancient Greece; it was then cataloged as Î±\xa0BoÃ¶tis by Johann Bayer in 1603, which is Latinized to Alpha\xa0BoÃ¶tis. Arcturus forms one corner of the Spring Triangle asterism.',

#     'visual_magnitude': -0.05},

#    {'id': 'Vega',

#     'text': "Vega is the brightest star in the northern constellation of Lyra. It has the Bayer designation Î± Lyrae, which is Latinised to Alpha Lyrae and abbreviated Alpha Lyr or Î± Lyr. This star is relatively close at only  from the Sun, and one of the most luminous stars in the Sun's neighborhood. It is the fifth-brightest star in the night sky, and the second-brightest star in the northern celestial hemisphere, after Arcturus.",

#     'visual_magnitude': 0.03},

#    {'id': 'Capella', 'text': 'Capella', 'visual_magnitude': 0.08}]

"""
# Retrieval Augmented Generation (RAG)

Next, we'll run a series of RAG queries using the knowledge graph.
"""

from txtai import LLM, RAG

# Define prompt template
template = """
Answer the following question using only the context below. Only include information
specifically discussed. Answer the question without explaining how you found the answer.

question: {question}
context: {context}"""

# Load LLM
llm = LLM("hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4")

# Create RAG pipeline
rag = RAG(
    astronomy,
    llm, 
    system="You are a friendly assistant. You answer questions from users.",
    template=template,
    context=10
)

print(rag("Tell me about the stars in the Andromeda constellation and how far they are from Earth", maxlength=4096)["answer"])
# Output:
#   The stars in the Andromeda constellation mentioned in the context include:

#   

#   1. Omicron Andromedae (Î¿ And), a blue-white B-type giant approximately 692 light years from Earth.

#   2. RT Andromedae, a variable star approximately 322 light years from Earth.

#   3. 8 Andromedae, a probable triple star system approximately 570 light years from Earth.

#   4. 64 Andromedae, a deep-yellow coloured G-type giant approximately 419 light years from Earth.

#   5. 3 Andromedae, a single star approximately 181 light years from Earth.

#   6. Epsilon Andromedae, a star approximately 155 light years from the Sun.

#   7. HD 166 or V439 Andromedae, a variable star approximately 45 light years away from Earth.

#   8. 7 Andromedae, a single, yellow-white hued star approximately 79.6 light years from Earth.

#   9. 18 Andromedae, a single star approximately 413 light years from Earth.

#   10. 12 Andromedae, a single star approximately 137 light years from Earth.

#   

#   These stars vary in their distance from Earth, ranging from 45 to 692 light years.


"""
Now let's try a Graph RAG query. Instead of vector search, we'll run a graph traversal query to generate the RAG context.
"""

subgraph = astronomy.search("""
MATCH P=({id: "Centaurus"})-[]->(B)
WHERE B.distance > 0
RETURN P
ORDER BY B.distance
LIMIT 10
""", graph=True)

plot(subgraph)

command = """
Write a summary of the stars discussed along with a short description about them.
Sort by distance to Earth.
"""

context = [subgraph.attribute(x, "text") for x in subgraph.scan()]
print(rag(command, context, maxlength=4096)["answer"])
# Output:
#   <Figure size 2000x900 with 1 Axes>
#   Here's a summary of the stars discussed along with a short description about them, sorted by distance to Earth:

#   

#   1. **Proxima Centauri**: The closest star to Earth after the Sun, located 4.25 light-years away. It's a small, low-mass star too faint to be seen with the naked eye.

#   

#   2. **Alpha Centauri (Rigil Kentaurus, Toliman, and Proxima Centauri)**: A triple star system located 4.25 light-years away. It consists of three stars: Rigil Kentaurus, Toliman, and Proxima Centauri.

#   

#   3. **Iota Centauri**: A star located approximately 51.5 light-years away. It has an apparent visual magnitude of +2.73, making it easily visible to the naked eye.

#   

#   4. **HD 113538 (Gliese 496.1)**: A star with two planetary companions located 53 light-years away. It is much too faint to be viewed with the naked eye.

#   

#   5. **HD 101930 (Gliese 3683)**: An orange-hued star with an orbiting exoplanet located 98 light-years away. It has a relatively large proper motion and is receding with a radial velocity of.

#   

#   6. **HD 114386**: A star with a pair of orbiting exoplanets located 91 light-years away. It has an apparent visual magnitude of 8.73, which means it cannot be viewed with the naked eye but can be seen with a telescope or good binoculars.

#   

#   7. **HD 125595**: A star with a close Neptunian companion located 92 light-years away. It is too faint to be viewed with the naked eye.

#   

#   8. **BPM 37093 (V886 Centauri)**: A variable white dwarf star of the DAV, or ZZ Ceti, type, located approximately  light-years away. It has a hydrogen atmosphere and an unusually high mass of approximately 1.1 times the Sun's.


"""
# Agents

Our last example is an Agent that searches the astronomy knowledge graph.
"""

from txtai import Agent

def search(query):
    """
    Searches a database of astronomy data.

    Make sure to call this tool only with a string input, never use JSON.    

    Args:
        query: concepts to search for using similarity search
 
    Returns:
        list of search results with for each match
    """

    return embeddings.search(
        "SELECT id, text, distance FROM txtai WHERE similar(:query)",
        10, parameters={"query": query}
    )

agent = Agent(
    tools=[search],
    llm=llm,
    max_iterations=10,
)

from IPython.display import display, Markdown

def md(output):
    display(Markdown(output))

researcher = """
{command}

Do the following.
 - Search for results related to the topic.
 - Analyze the results
 - Continue querying until conclusive answers are found
 - Write a Markdown report
"""

md(agent(researcher.format(command="""
Write a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.
"""), maxlength=16000))

# Output:
#   [32;20;1m======== New task ========[0m

#   [37;1m

#   

#   Write a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.

#   

#   

#   Do the following.

#    - Search for results related to the topic.

#    - Analyze the results

#    - Continue querying until conclusive answers are found

#    - Write a Markdown report

#   [0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: I will need to find the list of candidate stars that could potentially be habitable to life. To do this, I will use the `search` tool to look for related results.[0m

#   [33;1m>>> Calling tool: 'search' with arguments: candidate stars for habitable planets[0m

#   [33;1m=== Agent thoughts:[0m

#   [0mThought: The results from the `search` tool provide a list of candidate stars for habitable planets. However, they are not organized in a clear and concise manner. To create a detailed list with explanations, I will need to analyze the results and identify the key points.[0m

#   [33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': "Here is a list of 10 candidate stars that could potentially be habitable to life:\n\n1. Luyten's Star (red dwarf star) - has a confirmed exoplanet, Luyten b, which is a super-Earth and receives only 6% more starlight than Earth.\n\n2. Gliese 273 (red dwarf star) - has a confirmed exoplanet, Luyten b, which is a super-Earth and receives only 6% more starlight than Earth.\n\n3. Ross 128 (red dwarf star) - has a confirmed exoplanet, Ross 128 b, which is a super-Earth and receives only 6% more starlight than Earth.\n\n4. Proxima Centauri (red dwarf star) - has a confirmed exoplanet, Proxima Centauri b, which is a super-Earth and receives only 6% more starlight than Earth.\n\n5. Kepler-560 (binary star system) - has a confirmed exoplanet, Kepler-560b, which is a super-Earth and orbits within the habitable zone of the binary star system.\n\n6. K2-332 (M4V star) - has a confirmed exoplanet, K2-332 b, which is a potentially habitable Super-Earth or Mini-Neptune exoplanet and receives 1.17 times the light that Earth gets from the sun.\n\n7. Alpha Centauri A (G-type main-sequence star) - has a candidate exoplanet, Alpha Centauri Ab, which is a giant planet and orbits at approximately 1.1 AU away from the star.\n\n8. Kepler-298 (orange dwarf star) - has a confirmed exoplanet, Kepler-298d, which orbits within the habitable zone of the star and may be an ocean planet with a thick gas atmosphere.\n\n9. LHS 1140 (M-dwarf star) - has a confirmed exoplanet, LHS 1140 b, which is a super-Earth and orbits within the habitable zone of the star.\n\n10. Teegarden's Star (M-dwarf star) - has a confirmed exoplanet, Teegarden's Star b, which is a super-Earth and orbits within the habitable zone of the star.\n\nNote: The habitability of these stars and their exoplanets is still a topic of debate and research, and more studies are needed to confirm their potential for supporting life."}[0m

#   <IPython.core.display.Markdown object>

"""
# Wrapping up

This notebook parsed the â­'s with txtai. It showed how knowledge graphs can be built to drive RAG. We also covered how to use agents to build an "agentic" or "self-querying" RAG system.

Are we alone in the universe? No one knows for sure. Perhaps AI will figure it out for us ğŸ˜€

"""



================================================
FILE: examples/73_Chunking_your_data_for_RAG.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Chunking your data for RAG

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

One of the major workflows in `txtai` is Retrieval Augmented Generation (RAG). Large Language Models (LLM) are built to generate coherent sounding text. While in many cases it is factually accurate, that is not what they're built to do. RAG steps in to help inject smaller pieces of knowledge into a LLM prompt and increase the overall accuracy of responses. The `R` in RAG is very important.

This notebook will demonstrate how to extract, chunk and index text to support retrieval operations for RAG.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-data]

"""
# Data chunking and indexing

Let's dive right in and keep this example simple. The next section creates a [Textractor pipeline](https://neuml.github.io/txtai/pipeline/data/textractor/) and an [Embeddings database](https://neuml.github.io/txtai/embeddings/).

The `Textractor` extracts chunks of text from files and the `Embeddings` takes those chunks and builds an index/database. We'll use a [late chunker](https://docs.chonkie.ai/chunkers/late-chunker) backed by [Chonkie](https://github.com/chonkie-inc/chonkie).

Then, we'll build an indexing workflow that streams chunks from two files.
"""

from txtai import Embeddings
from txtai.pipeline import Textractor

# Text extraction pipeline with late chunking via Chonkie
textractor = Textractor(chunker="late")
embeddings = Embeddings(content=True)

def stream():
    urls = ["https://github.com/neuml/txtai", "https://arxiv.org/pdf/2005.11401"]
    for x, url in enumerate(urls):
        chunks = textractor(url)
 
        # Add all chunks - use the same document id for each chunk
        for chunk in chunks:
            yield x, chunk

        # Add the document metadata with the same document id
        # Can be any metadata. Can also be the entire document.
        yield x, {"url": url}

# Index the chunks and metadata
embeddings.index(stream())

"""
A key element of `txtai` that is commonly misunderstood is how to best store chunks of data and join them back to the main document. `txtai` allows re-using the same logical id multiple times.

Behind the scenes, each chunk gets it's own unique index id. The backend database stores chunks in a table called `sections` and data in a table called `documents`. This has been the case as far back as `txtai` 4.0. `txtai` also has the ability to store associated binary data in a table called `objects`. It's important to note that each associated `document` or `object` is only stored once. 

To illustrate, let's look at the first 20 rows in the embeddings database created.
"""

for x in embeddings.search("SELECT indexid, id, url, text from txtai", 20):
    print(x)
# Output:
#   {'indexid': 0, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': '**GitHub - neuml/txtai: ğŸ’¡ All-in-one open-source embeddings database for semantic search, LLM orchestration and language model workflows**\n\n*ğŸ’¡ All-in-one open-source embeddings database for semantic search, LLM orchestration and language model workflows - neuml/txtai*\n\n\n\n**All-in-one embeddings database** \ntxtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.\n\nEmbeddings databases are a union of vector indexes (sparse and dense), graph networks and relational databases.\n\nThis foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.\n\nBuild autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.\n\nSummary of txtai features:\n\n- ğŸ” Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing\n- ğŸ“„ Create embeddings for text, documents, audio, images and video\n- ğŸ’¡ Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more\n- â†ªï¸ï¸ Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.\n- ğŸ¤– Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems\n- âš™ï¸ Build with Python or YAML. API bindings available for [JavaScript](https://github.com/neuml/txtai.js) , [Java](https://github.com/neuml/txtai.java) , [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go) .\n- ğŸ”‹ Batteries included with defaults to get up and running fast\n- â˜ï¸ Run local or scale out with container orchestration\ntxtai is built with Python 3.10+, [Hugging Face Transformers](https://github.com/huggingface/transformers) , [Sentence Transformers](https://github.'}

#   {'indexid': 1, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi) . txtai is open-source under an Apache 2.0 license.\n\n*Interested in an easy and secure way to run hosted txtai applications? Then join the* [txtai.cloud](https://txtai.cloud) *preview to learn more.* \n\n## Why txtai?\nNew vector databases, LLM frameworks and everything in between are sprouting up daily. Why build with txtai?\n\n- Up and running in minutes with [pip](https://neuml.github.io/txtai/install/) or [Docker](https://neuml.github.io/txtai/cloud/) \n```\n# Get started in a couple lines\nimport txtai\n\nembeddings = txtai.Embeddings()\nembeddings.index(["Correct", "Not what we hoped"])\nembeddings.search("positive", 1)\n#[(0, 0.29862046241760254)]\n```\n\n- Built-in API makes it easy to develop applications using your programming language of choice\n```\n# app.yml\nembeddings:\n path: sentence-transformers/all-MiniLM-L6-v2\n```\n\n```\nCONFIG=app.yml uvicorn "txtai.api:app"\ncurl -X GET "http://localhost:8000/search?query=positive"\n```\n\n- Run local - no need to ship data off to disparate remote services\n- Work with micromodels all the way up to large language models (LLMs)\n- Low footprint - install additional dependencies and scale up when needed\n- [Learn by example](https://neuml.github.io/txtai/examples) - notebooks cover all available functionality\n\n## Use Cases\nThe following sections introduce common txtai use cases. A comprehensive set of over 60 [example notebooks and applications](https://neuml.github.io/txtai/examples) are also available.\n\n\n### Semantic Search\nBuild semantic/similarity/vector/neural search applications.'}

#   {'indexid': 2, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.\n\nGet started with the following examples.\n\n|Notebook|Description||\n|---|---|---|\n|[Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |Overview of the functionality provided by txtai||\n|[Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |Embed images and text into the same space for search||\n|[Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |Question matching with semantic search||\n|[Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |Explore topics, data connectivity and run network analysis||\n\n### LLM Orchestration\nAutonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).\n\nSee below to learn more.\n\n|Notebook|Description||\n|---|---|---|\n|[Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |Build model prompts and connect tasks together with workflows||\n|[Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |Integrate llama.cpp, LiteLLM and custom generation frameworks||\n|[Build knowledge graphs with LLMs](https://github.'}

#   {'indexid': 3, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |Build knowledge graphs with LLM-driven entity extraction||\n\n#### Agents\nAgents connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems.\n\ntxtai agents are built on top of the Transformers Agent framework. This supports all LLMs txtai supports (Hugging Face, llama.cpp, OpenAI / Claude / AWS Bedrock via LiteLLM).\n\nSee the link below to learn more.\n\n|Notebook|Description||\n|---|---|---|\n|[Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |Explore a rich dataset with Graph Analysis and Agents||\n|[Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |Agents that iteratively solve problems as they see fit||\n|[Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |Exploring how to improve social media engagement with AI||\n\n#### Retrieval augmented generation\nRetrieval augmented generation (RAG) reduces the risk of LLM hallucinations by constraining the output with a knowledge base as context. RAG is commonly used to "chat with your data".\n\nA novel feature of txtai is that it can provide both an answer and source citation.\n\n|Notebook|Description||\n|---|---|---|\n|[Build RAG pipelines with txtai](https://github.'}

#   {'indexid': 4, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |Guide on retrieval augmented generation including how to create citations||\n|[How RAG with txtai works](https://github.com/neuml/txtai/blob/master/examples/63_How_RAG_with_txtai_works.ipynb) |Create RAG processes, API services and Docker instances||\n|[Advanced RAG with graph path traversal](https://github.com/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) |Graph path traversal to collect complex sets of data for advanced RAG||\n|[Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |Full cycle speech to speech workflow with RAG||\n\n### Language Model Workflows\nLanguage model workflows, also known as semantic workflows, connect language models together to build intelligent applications.\n\nWhile LLMs are powerful, there are plenty of smaller, more specialized models that work better and faster for specific tasks. This includes models for extractive question-answering, automatic summarization, text-to-speech, transcription and translation.\n\n|Notebook|Description||\n|---|---|---|\n|[Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |Simple yet powerful constructs to efficiently process data||\n|[Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |Run abstractive text summarization||\n|[Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.'}

#   {'indexid': 5, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'ipynb) |Convert audio files to text||\n|[Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |Streamline machine translation and language detection||\n\n## Installation\nThe easiest way to install is via pip and PyPI\n\n```\npip install txtai\n```\n\nPython 3.10+ is supported. Using a Python [virtual environment](https://docs.python.org/3/library/venv.html) is recommended.\n\nSee the detailed [install instructions](https://neuml.github.io/txtai/install) for more information covering [optional dependencies](https://neuml.github.io/txtai/install/#optional-dependencies) , [environment specific prerequisites](https://neuml.github.io/txtai/install/#environment-specific-prerequisites) , [installing from source](https://neuml.github.io/txtai/install/#install-from-source) , [conda support](https://neuml.github.io/txtai/install/#conda) and how to [run with containers](https://neuml.github.io/txtai/cloud) .\n\n\n## Model guide\nSee the table below for the current recommended models. These models all allow commercial use and offer a blend of speed and performance.\n\n|Component|Model(s)|\n|---|---|\n|[Embeddings](https://neuml.github.io/txtai/embeddings) |[all-MiniLM-L6-v2](https://hf.co/sentence-transformers/all-MiniLM-L6-v2) |\n|[Image Captions](https://neuml.github.io/txtai/pipeline/image/caption) |[BLIP](https://hf.co/Salesforce/blip-image-captioning-base) |'}

#   {'indexid': 6, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': '|[Labels - Zero Shot](https://neuml.github.io/txtai/pipeline/text/labels) |[BART-Large-MNLI](https://hf.co/facebook/bart-large) |\n|[Labels - Fixed](https://neuml.github.io/txtai/pipeline/text/labels) |Fine-tune with [training pipeline](https://neuml.github.io/txtai/pipeline/train/trainer) |\n|[Large Language Model (LLM)](https://neuml.github.io/txtai/pipeline/text/llm) |[Llama 3.1 Instruct](https://hf.co/meta-llama/Llama-3.1-8B-Instruct) |\n|[Summarization](https://neuml.github.io/txtai/pipeline/text/summary) |[DistilBART](https://hf.co/sshleifer/distilbart-cnn-12-6) |\n|[Text-to-Speech](https://neuml.github.io/txtai/pipeline/audio/texttospeech) |[ESPnet JETS](https://hf.co/NeuML/ljspeech-jets-onnx) |\n|[Transcription](https://neuml.github.io/txtai/pipeline/audio/transcription) |[Whisper](https://hf.co/openai/whisper-base) |\n|[Translation](https://neuml.github.io/txtai/pipeline/text/translation) |[OPUS Model Series](https://hf.co/Helsinki-NLP) |\nModels can be loaded as either a path from the Hugging Face Hub or a local directory. Model paths are optional, defaults are loaded when not specified. For tasks with no recommended model, txtai uses the default models as shown in the Hugging Face Tasks guide.\n\nSee the following links to learn more.\n\n\n## Powered by txtai\nThe following applications are powered by txtai.'}

#   {'indexid': 7, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': "|Application|Description|\n|---|---|\n|[rag](https://github.com/neuml/rag) |Retrieval Augmented Generation (RAG) application|\n|[ragdata](https://github.com/neuml/ragdata) |Build knowledge bases for RAG|\n|[paperai](https://github.com/neuml/paperai) |Semantic search and workflows for medical/scientific papers|\n|[annotateai](https://github.com/neuml/annotateai) |Automatically annotate papers with LLMs|\nIn addition to this list, there are also many other [open-source projects](https://github.com/neuml/txtai/network/dependents) , [published research](https://scholar.google.com/scholar?q=txtai&hl=en&as_ylo=2022) and closed proprietary/commercial projects that have built on txtai in production.\n\n\n## Further Reading\n- [Tutorial series on Hashnode](https://neuml.hashnode.dev/series/txtai-tutorial) | [dev.to](https://dev.to/neuml/tutorial-series-on-txtai-ibg) \n- [What's new in txtai 8.0](https://medium.com/neuml/whats-new-in-txtai-8-0-2d7d0ab4506b) | [7.0](https://medium.com/neuml/whats-new-in-txtai-7-0-855ad6a55440) | [6.0](https://medium.com/neuml/whats-new-in-txtai-6-0-7d93eeedf804) | [5.0](https://medium.com/neuml/whats-new-in-txtai-5-0-e5c75a13b101) | [4.0](https://medium."}

#   {'indexid': 8, 'id': '0', 'url': 'https://github.com/neuml/txtai', 'text': 'com/neuml/whats-new-in-txtai-4-0-bbc3a65c3d1c) \n- [Getting started with semantic search](https://medium.com/neuml/getting-started-with-semantic-search-a9fd9d8a48cf) | [workflows](https://medium.com/neuml/getting-started-with-semantic-workflows-2fefda6165d9) | [rag](https://medium.com/neuml/getting-started-with-rag-9a0cca75f748) \n\n## Documentation\n[Full documentation on txtai](https://neuml.github.io/txtai) including configuration settings for embeddings, pipelines, workflows, API and a FAQ with common questions/issues is available.\n\n\n## Contributing\nFor those who would like to contribute to txtai, please see [this guide](https://github.com/neuml/.github/blob/master/CONTRIBUTING.md) .'}

#   {'indexid': 9, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\n\nPatrick Lewisâ€ â€¡, Ethan Perez?,\n\nAleksandra Piktusâ€ , Fabio Petroniâ€ , Vladimir Karpukhinâ€ , Naman Goyalâ€ , Heinrich KÃ¼ttlerâ€ ,\n\nMike Lewisâ€ , Wen-tau Yihâ€ , Tim RocktÃ¤schelâ€ â€¡, Sebastian Riedelâ€ â€¡, Douwe Kielaâ€ \n\nâ€ Facebook AI Research; â€¡University College London; ?New York University;\nplewis@fb.com\n\nAbstract\n\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on down-\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\nedge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-specific architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pre-\ntrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose fine-tuning recipe for retrieval-augmented generation\n(RAG) â€” models which combine pre-trained parametric and non-parametric mem-\nory for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\npare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We fine-tune and evaluate our models on a wide range of knowledge-\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract\narchitectures. For language generation tasks, we find that RAG models generate\nmore specific, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n\n1 Introduction\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52].'}

#   {'indexid': 10, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'While this development is exciting, such models do have down-\nsides: They cannot easily expand or revise their memory, canâ€™t straightforwardly provide insight into\ntheir predictions, and may produce â€œhallucinationsâ€ [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n\nar\nX\n\niv\n:2\n\n00\n5.\n\n11\n40\n\n1v\n4 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n2 \nA\n\npr\n 2\n\n02\n1\n\n\nThe\tDivine\nComedy\t(x) q\n\nQuery\nEncoder\n\nq(x)\n\nMIPS pÎ¸\n\nGenerator\xa0pÎ¸\n(Parametric)\n\nMargin-\nalize\n\nThis\t14th\tcentury\twork\nis\tdivided\tinto\t3\nsections:\t"Inferno",\n"Purgatorio"\t&\n"Paradiso"\t\t\t\t\t\t\t\t\t(y)\n\nEnd-to-End Backprop through q and\xa0pÎ¸\n\nBarack\tObama\twas\nborn\tin\tHawaii.(x)\n\nFact Verification: Fact Query\n\nsupports\t(y)\n\nQuestion Generation\n\nFact Verification:\nLabel Generation\n\nDocument\nIndex\n\nDefine\t"middle\tear"(x)\n\nQuestion Answering:\nQuestion Query\n\nThe\tmiddle\tear\tincludes\nthe\ttympanic\tcavity\tand\nthe\tthree\tossicles.\t\t(y)\n\nQuestion Answering:\nAnswer GenerationRetriever pÎ·\n\n(Non-Parametric)\nz4\n\nz3\nz2\n\nz1\n\nd(z)\n\nJeopardy Question\nGeneration:\n\nAnswer Query\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\nIndex) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we\ntreat z as a latent variable and marginalize over seq2seq predictions given different documents.\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the â€œworkhorse of NLP,â€ i.e. sequence-to-sequence (seq2seq) models.\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through'}

#   {'indexid': 11, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\n\nOur results highlight the benefits of combining parametric and non-parametric memory with genera-\ntion for knowledge-intensive tasksâ€”tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being'}

#   {'indexid': 12, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'extractive tasks, we find that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we find that our models generate responses that are more factual, specific, and\ndiverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1\n\n2 Methods\n\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them\nas additional context when generating the target sequence y. As shown in Figure 1, our models\nleverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated)\ndistributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized\n\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n\n2\n\n[https://github.com/huggingface/transformers/blob/master/examples/rag/](https://github.com/huggingface/transformers/blob/master/examples/rag/) \n[https://github.com/huggingface/transformers/blob/master/examples/rag/](https://github.com/huggingface/transformers/blob/master/examples/rag/) \n[https://huggingface.co/rag/](https://huggingface.co/rag/) \n\nby Î¸ that generates a current token based on a context of the previous iâˆ’ 1 tokens y1:iâˆ’1, the original\ninput x and a retrieved passage z.\n\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\nWe propose two models that marginalize over the latent documents in different ways to produce a'}

#   {'indexid': 13, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'distribution over generated text. In one approach, RAG-Sequence, the model uses the same document\nto predict each target token. The second approach, RAG-Token, can predict each target token based\non a different document. In the following, we formally introduce both models and then describe the\npÎ· and pÎ¸ components, as well as the training and decoding procedure.\n\n2.1 Models\n\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\nprobability for each document, which are then marginalized,\n\npRAG-Sequence(y|x) â‰ˆ\nâˆ‘\n\nzâˆˆtop-k(p(Â·|x))\n\npÎ·(z|x)pÎ¸(y|x, z) =\nâˆ‘\n\nzâˆˆtop-k(p(Â·|x))\n\npÎ·(z|x)\nNâˆ\ni\n\npÎ¸(yi|x, z, y1:iâˆ’1)\n\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\ntarget token and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever, and then the generator produces a distribution for the next output token for each document,\nbefore marginalizing, and repeating the process with the following output token, Formally, we define:\n\npRAG-Token(y|x) â‰ˆ\nNâˆ\ni\n\nâˆ‘\nzâˆˆtop-k(p(Â·|x))\n\npÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1)\n\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n\n2.2 Retriever: DPR\n\nThe retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\n\npÎ·(z|x) âˆ exp\n(\nd(z)>q(x)\n\n)\nd(z) = BERTd(z), q(x) = BERTq(x)'}

#   {'indexid': 14, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\ntop-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\n\n2.3 Generator: BART\n\nThe generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\nx with the retrieved content z when generating from BART, we simply concatenate them. BART was\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\nmodels [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth.\n\n2.4 Training\n\nWe jointly train the retriever and generator components without any direct supervision on what\ndocument should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj , yj), we\n\n3\n\n\nminimize the negative marginal log-likelihood of each target,\nâˆ‘\nj âˆ’ log p(yj |xj) using stochastic\n\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\nit requires the document index to be periodically updated as REALM does during pre-training [20].'}

#   {'indexid': 15, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'We do not find this step necessary for strong performance, and keep the document encoder (and\nindex) fixed, only fine-tuning the query encoder BERTq and the BART generator.\n\n2.5 Decoding\n\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate argmaxy p(y|x).\n\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\ntor with transition probability: pâ€²Î¸(yi|x, y1:iâˆ’1) =\n\nâˆ‘\nzâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To\n\ndecode, we can plug pâ€²Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder.\n\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\neach document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses\nY , some of which may not have appeared in the beams of all documents. To estimate the probability\nof an hypothesis y we run an additional forward pass for each document z for which y does not\nappear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across\nbeams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer\noutput sequences, |Y | can become large, requiring many forward passes. For more efficient decoding,\nwe can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\nbeen generated. We refer to this decoding procedure as â€œFast Decoding.â€\n\n3 Experiments\n\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint'}

#   {'indexid': 16, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': '100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\nk documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev\ndata. We now discuss experimental details for each task.\n\n3.1 Open-domain Question Answering\n\nOpen-domain question answering (QA) is an important real-world application and common testbed\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book\nQAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n\n3.2 Abstractive Question Answering\n\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages'}

#   {'indexid': 17, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'retrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n\n4\n\n\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\nâ€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages.\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses.\n\n3.3 Jeopardy Question Generation\n\nTo evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen-\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\nFor example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the first\ncountry to host this international sports competition twice.â€ As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task.\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\none for specificity. We define factuality as whether a statement can be corroborated by trusted external'}

#   {'indexid': 18, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'sources, and specificity as high mutual dependence between the input and output [33]. We follow\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\noptionsâ€”quuestion A is better, question B is better, both are good, or neither is good.\n\n3.4 Fact Verification\n\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\nexploring the RAG modelsâ€™ ability to handle classification rather than generation. We map FEVER\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\nretrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\ntwo variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n\n4 Results\n\n4.1 Open-domain Question Answering\n\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation flexibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of\n"open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s\nretriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-'}

#   {'indexid': 19, 'id': '1', 'url': 'https://arxiv.org/pdf/2005.11401', 'text': 'encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\n\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\na correct answer being generated, which is not possible with standard extractive approaches, leading\n\n5\n\n\nTable 1: Open-Domain QA Test Scores. For TQA,\nleft column uses the standard test set for Open-\nDomain QA, right column uses the TQA-Wiki\ntest set. See Appendix D for further details.\n\nModel NQ TQA WQ CT\n\nClosed\nBook\n\nT5-11B [52] 34.5 - /50.1 37.4 -\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\n\nOpen\nBook\n\nREALM [20] 40.4 - / - 40.7 46.8\nDPR [26] 41.5 57.9/ - 41.1 50.6\n\nRAG-Token 44.1 55.2/66.1 45.5 50.0\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\n\nTable 2: Generation and classification Test Scores.\nMS-MARCO SotA is [4], FEVER-3 is [68] and\nFEVER-2 is [57] *Uses gold context/evidence.\nBest model without gold access underlined.\n\nModel Jeopardy MSMARCO FVR3 FVR2\nB-1 QB-1 R-L B-1 Label Acc.\n\nSotA - - 49.8* 49.9* 76.8 92.2*\n\nBART 15.1 19.7 38.2 41.6 64.0 81.1\n\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\n\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\ncases for NQ, where an extractive model would score 0%.\n\n4.2 Abstractive Question Answering'}


"""
Note that the id/metadata is the same but the indexid and chunk text change with each row.
"""

"""
# Retrieval

Last thing here is to illustrate a couple retrieval operations. LLMs are great at generating answers when we properly bound the context. See the two examples below.
"""

print(embeddings.search("What is it called when LLM generation is bounded with factually correct data?", 1)[0]["text"])
# Output:
#   including less of emphasis on lightly editing a retrieved item, but on aggregating content from several

#   pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents

#   rather than related training pairs. This said, RAG techniques may work well in these settings, and

#   could represent promising future work.

#   

#   6 Discussion

#   

#   In this work, we presented hybrid generation models with access to parametric and non-parametric

#   memory. We showed that our RAG models obtain state of the art results on open-domain QA. We

#   found that people prefer RAGâ€™s generation over purely parametric BART, finding RAG more factual

#   and specific. We conducted an thorough investigation of the learned retrieval component, validating

#   its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model

#   without requiring any retraining. In future work, it may be fruitful to investigate if the two components

#   can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some

#   another objective. Our work opens up new research directions on how parametric and non-parametric

#   memories interact and how to most effectively combine them, showing promise in being applied to a

#   wide variety of NLP tasks.

#   

#   9

#   

#   

#   Broader Impact

#   

#   This work offers several positive societal benefits over previous work: the fact that it is more

#   strongly grounded in real factual knowledge (in this case Wikipedia) makes it â€œhallucinateâ€ less

#   with generations that are more factual, and offers more control and interpretability. RAG could be

#   employed in a wide variety of scenarios with direct benefit to society, for example by endowing it

#   with a medical index and asking it open-domain questions on that topic, or by helping people be more

#   effective at their jobs.

#   

#   With these advantages also come potential downsides: Wikipedia, or any potential external knowledge

#   source, will probably never be entirely factual and completely devoid of bias. Since RAG can be

#   employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably

#   to a lesser extent, including that it might be used to generate abuse, faked or misleading content in

#   the news or on social media; to impersonate others; or to automate the production of spam/phishing

#   content [54]. Advanced language models may also lead to the automation of various jobs in the

#   coming decades [16].


print(embeddings.search("Tell me about semantic search", 1)[0]["text"])
# Output:
#   Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.

#   

#   Get started with the following examples.

#   

#   |Notebook|Description||

#   |---|---|---|

#   |[Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |Overview of the functionality provided by txtai||

#   |[Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |Embed images and text into the same space for search||

#   |[Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |Question matching with semantic search||

#   |[Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |Explore topics, data connectivity and run network analysis||

#   

#   ### LLM Orchestration

#   Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).

#   

#   See below to learn more.

#   

#   |Notebook|Description||

#   |---|---|---|

#   |[Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |Build model prompts and connect tasks together with workflows||

#   |[Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |Integrate llama.cpp, LiteLLM and custom generation frameworks||

#   |[Build knowledge graphs with LLMs](https://github.


"""
Note how both answers give more than enough information for a LLM to answer the question.
"""

"""
# Wrapping up

This notebook covered how to build a retrieval system for RAG with txtai. Chunking and retrieval are key pieces of a RAG system, arguably the most important. With the commoditization of LLMs, it's going to be more and more important on how data is presented to LLMs. When given concise information, LLMs can take it from there!

"""



================================================
FILE: examples/74_OpenAI_Compatible_API.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# OpenAI Compatible API

[txtai](https://github.com/neuml/txtai) is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.

`txtai` has long been able host a [FastAPI based service](https://neuml.github.io/txtai/api/). There are clients for [Python](https://github.com/neuml/txtai.py), [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs), [Go](https://github.com/neuml/txtai.go).

The API service also supports hosting OpenAI-compatible API endpoints. A standard OpenAI client can then be used to connect to a `txtai` service. This enables quickly trying `txtai` with a familiar-to-use client. It's also a way to do local/offline development testing using the OpenAI client.

This notebook will walk through comprehensive examples.
"""

"""
# Start API service

For this notebook, we'll run `txtai` through Docker.

Save the following to `/tmp/config/config.yml`.

## config.yml
```yaml
# Enable OpenAI compat endpoint
openai: True

# Load Wikipedia Embeddings index
cloud:
  provider: huggingface-hub
  container: neuml/txtai-wikipedia

# LLM instance
llm:
  path: llava-hf/llava-interleave-qwen-0.5b-hf

# RAG pipeline configuration
rag:
  path: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
  output: flatten
  system: You are a friendly assistant. You answer questions from users.
  template: |
    Answer the following question using only the context below. Only include information
    specifically discussed.

    question: {question}
    context: {context}

# Text to Speech
texttospeech:
  path: neuml/kokoro-fp16-onnx

# Transcription
transcription:
  path: distil-whisper/distil-large-v3
```

Start Docker service.

```
docker run -it -p 8000:8000 -v /tmp/config:/config -e CONFIG=/config/config.yml \
--entrypoint uvicorn neuml/txtai-gpu --host 0.0.0.0 txtai.api:app
```

Alternatively, txtai can be directly installed and run as follows:

```
pip install txtai[all] autoawq autoawq-kernels
CONFIG=/tmp/config/config.yml uvicorn "txtai.api:app"
```

The API has token-based authorization built-in. [Read more on that here](https://neuml.github.io/txtai/api/customization/#dependencies).
"""

"""
# Run a text chat completion

The first example will run a text chat completion. The model is a RAG pipeline - this is more sophisticated than just a simple LLM call!

Agents, pipelines and workflows can all be run through this interface!
"""

from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="api-key",
)

response = client.chat.completions.create(
    messages=[{
        "role": "user",
        "content": "Tell me about the iPhone",
    }],
    model="rag",
    stream=True
)

for chunk in response:
    if chunk.choices:
        print(chunk.choices[0].delta.content, end="")

# Output:
#   The iPhone is a line of smartphones designed and marketed by Apple Inc. that uses Apple's iOS mobile operating system. The first-generation iPhone was announced by former Apple CEO Steve Jobs on January 9, 2007. 

#   

#   Since then, Apple has annually released new iPhone models and iOS updates. The most recent models being the iPhone 16 and 16 Plus, and the higher-end iPhone 16 Pro and 16 Pro Max. 

#   

#   More than 2.3 billion iPhones have been sold as of January 1, 2024, making Apple the largest vendor of mobile phones in 2023.

"""
As mentioned above, the model supports much of what's available in `txtai`. For example, let's run a chat completion that runs an embeddings search.
"""

response = client.chat.completions.create(
    messages=[{
        "role": "user",
        "content": "Tell me about the iPhone",
    }],
    model="embeddings",
)

print(response.choices[0].message.content)
# Output:
#   The iPhone is a line of smartphones designed and marketed by Apple Inc. that uses Apple's iOS mobile operating system. The first-generation iPhone was announced by former Apple CEO Steve Jobs on January 9, 2007. Since then, Apple has annually released new iPhone models and iOS updates. iPhone naming has followed various patterns throughout its history.


"""
# Vision models

Any supported `txtai` LLM can be run through the chat completion API. Let's run an example that describes an image.
"""

response = client.chat.completions.create(
    model="llm",
    messages = [{
        "role": "user",
        "content": [
            {"type": "text", "text": "What is in this image?"},
            {"type": "image_url", "image_url": {
                "url": "https://raw.githubusercontent.com/neuml/txtai/master/logo.png",
            }}
        ]}
    ]
)
print(response.choices[0].message.content)

# Output:
#   The image shows a logo with the text "Txtai" in blue and green colors. The


"""
# Embeddings API

Next let's generate embeddings. 
"""

for x in client.embeddings.create(input="This is a test", model="vectors").data:
    print(x)
# Output:
#   Embedding(embedding=[-0.01969079300761223, 0.024085085839033127, 0.0043829963542521, -0.027423616498708725, 0.040405914187431335, 0.017446696758270264, 0.028464825823903084, 0.000792442646343261, -0.03107883222401142, -0.024745089933276176, -0.013542148284614086, 0.039981111884117126, -0.01401221938431263, -0.011294773779809475, -0.04346214607357979, 0.015698621049523354, 0.03775031119585037, -0.009020405821502209, 0.046784739941358566, -0.017400527372956276, -0.0670166090130806, -0.05122058466076851, 0.027725063264369965, -0.023947732523083687, -0.044582683593034744, 0.04960233345627785, 0.029517438262701035, 0.05424104258418083, -0.06027599796652794, -0.035852570086717606, 0.01336587406694889, -0.008941668085753918, 0.00014064145216252655, -0.05230511724948883, -0.02150369994342327, 0.04969678074121475, -0.05967864394187927, -0.029450856149196625, -0.01113089732825756, -0.01256561279296875, -0.012282170355319977, 0.03466389700770378, -0.005313237197697163, -0.037443146109580994, -0.04366842657327652, -0.019057273864746094, -0.04015717655420303, 0.050483088940382004, -0.011932676658034325, -0.026569517329335213, -0.048395730555057526, 0.021978085860610008, 0.03273716941475868, -0.009176520630717278, -0.05367470160126686, 0.01982428878545761, -0.00373812741599977, 0.009933742694556713, 0.044389136135578156, -0.06162404641509056, 0.03372818976640701, 0.006638737395405769, 0.029836857691407204, 0.014663859270513058, 0.04531872272491455, 0.03151382878422737, -0.007935297675430775, -0.02053055912256241, -0.06477595120668411, -0.017908524721860886, -0.014721200801432133, -0.0072138686664402485, -0.03244556114077568, -0.018965184688568115, 0.04862097278237343, -0.02961636148393154, 0.005204972345381975, 0.015699708834290504, 0.05033862590789795, -0.017976371571421623, -0.05143386870622635, -0.014295309782028198, -0.018152274191379547, 0.04641849547624588, 0.007279090117663145, -0.0060980431735515594, -0.04208022356033325, 0.05402654781937599, 0.0001357585861114785, 0.044958386570215225, -0.03261513262987137, 0.02126067876815796, 0.020893605425953865, -0.007570710498839617, -0.015284491702914238, -0.011333705857396126, -0.006006874144077301, 0.03481211140751839, 0.04163122922182083, -0.0683935135602951, -0.030256368219852448, -0.024272358044981956, 0.04630593582987785, -0.05253031477332115, 0.011599390767514706, -0.034757863730192184, 0.0033751465380191803, -0.03200560435652733, -0.04386962205171585, 0.015501669608056545, -0.01703309454023838, -0.029905665665864944, -0.03208091855049133, -0.027883553877472878, 0.007325653452426195, 0.03735042363405228, 0.08069189637899399, -0.044986918568611145, 0.030896944925189018, 0.017477652058005333, -0.0063758366741240025, 0.02287706732749939, 0.016398170962929726, 0.01946086250245571, 0.012854589149355888, 0.04439576715230942, 0.04581235349178314, -0.0008034319616854191, -0.03105296567082405, -0.024504775181412697, 0.023659462109208107, 0.04492054134607315, -0.025883017107844353, -0.002515115775167942, 0.052770763635635376, 0.009667950682342052, -0.022283289581537247, -0.07817766815423965, 0.03883073106408119, -0.04804662615060806, 0.011968107894062996, -0.03163604810833931, 0.030380938202142715, -0.022775596007704735, 0.03142687678337097, -0.11540865898132324, -0.062065351754426956, 0.003252241527661681, -0.016604064032435417, 0.046795569360256195, -0.01973356492817402, 0.005612187087535858, 0.04902602732181549, -0.029760321602225304, -0.0006560107576660812, 0.02137850970029831, 0.021465344354510307, -0.030499190092086792, -0.013952907174825668, 0.015388991683721542, -0.004734670277684927, -0.02678225375711918, 0.056917935609817505, -0.0031489196699112654, -0.000562859873753041, 0.08021821081638336, 0.045039497315883636, 0.051955677568912506, -0.06851264089345932, -0.0202163215726614, -0.020257024094462395, 0.009915929287672043, 0.027132542803883553, -0.039319392293691635, -0.06750114262104034, 0.00721193291246891, 0.011379252187907696, -0.00012379158579278737, 0.021098755300045013, -0.017165066674351692, -0.06655416637659073, 0.03575438633561134, -0.0449126660823822, 0.024580610916018486, 0.0027450474444776773, -0.07029049843549728, -0.0058233728632330894, -0.0031324869487434626, -0.022562572732567787, -0.002092051785439253, -0.01972377672791481, -0.014447340741753578, 0.02001781575381756, -0.04224644973874092, 0.08794320374727249, -0.05012425035238266, -0.03000028431415558, -0.006967171560972929, -0.0206689964979887, 0.042854372411966324, 0.018307263031601906, 0.04896565154194832, 0.025682201609015465, -0.013927857391536236, -0.026135331019759178, 0.05985535308718681, -0.022972915321588516, -0.06837267428636551, 0.03858938440680504, 0.01297465804964304, -0.01869095303118229, -0.014788917265832424, 0.05812034010887146, -0.005296449176967144, -0.03188127279281616, -0.014335273765027523, 0.029694614931941032, -0.006149643566459417, 0.0199541375041008, -0.04401557520031929, 0.08680693805217743, 0.02373044192790985, -0.05719068646430969, 0.00026498522493056953, -0.047968123108148575, 0.05128588527441025, 0.08984201401472092, 0.018948959186673164, -0.019343748688697815, -0.02114059403538704, -0.000319077109452337, -0.0483400821685791, 0.02235756441950798, -0.04526951164007187, -0.016685402020812035, 0.04920167103409767, 0.0009292830363847315, 0.0066963727585971355, 0.06434790045022964, -0.07675006985664368, 0.025055741891264915, 0.039694759994745255, -0.04413995519280434, 0.053703855723142624, 0.022806784138083458, -0.02683648094534874, 0.04088520258665085, -0.02505207061767578, 0.038970883935689926, -0.011933756060898304, 0.017762111499905586, -0.052576545625925064, -0.02732933685183525, 0.024120833724737167, -0.011316879652440548, -0.04519795626401901, 0.012005027383565903, 0.016074027866125107, -0.019522851333022118, 0.07912492007017136, -0.010790158063173294, 0.003584112972021103, -0.018683504313230515, -0.03872854635119438, -0.0293426550924778, -0.028616394847631454, 0.0034447587095201015, 0.008824280463159084, 0.0267381239682436, -0.014405295252799988, 0.01340708788484335, 0.022090492770075798, 0.041456740349531174, 0.01306570041924715, 0.012696513906121254, -0.05636722221970558, 0.05526677146553993, 0.014159836806356907, -0.05075988918542862, -0.03631533309817314, 0.04115152731537819, 0.06140957027673721, -0.06539256870746613, -0.01610933430492878, 0.08129005879163742, -0.054096464067697525, 0.021539339795708656, -0.009134260006248951, 0.04177645593881607, 0.026524635031819344, 0.016892578452825546, 0.037963252514600754, -0.06906059384346008, 0.050708942115306854, 0.06792867928743362, -0.0004703162703663111, 0.018694648519158363, -0.031178174540400505, -0.03567223250865936, -0.035771071910858154, 0.05392008647322655, 0.06253240257501602, -0.020289720967411995, 0.034436099231243134, 0.03414503112435341, 0.0034774248488247395, -0.04452746734023094, -0.03509671986103058, -0.10872040688991547, 0.016063231974840164, 0.047865595668554306, -0.04542273283004761, 0.014507413841784, 0.0009427995537407696, -0.0031647789292037487, -0.0013884446816518903, -0.045522164553403854, 0.031990133225917816, -0.07940599322319031, -0.0021216440945863724, -0.003062204457819462, 0.0284376610070467, 0.038331907242536545, -0.021678920835256577, 0.010201317258179188, -0.01604599319398403, 0.06507452577352524, 0.0687805563211441, 0.05626540631055832, -0.035019401460886, 0.013606756925582886, 0.01355750672519207, -0.0009656146867200732, 0.008775751106441021, -0.023357177153229713, -0.027274709194898605, -0.030927035957574844, -0.014168186113238335, -0.0025208715815097094, -0.06382670253515244, 0.0016783965984359384, 0.03997219353914261, -0.011281637474894524, -0.0564236082136631, 0.0001946773991221562, -0.044997114688158035, 0.006665860302746296, -0.02552937902510166, -0.0387411043047905, -0.007421125657856464, -0.018388714641332626, 0.04417712241411209, -0.03386503830552101, -0.015952911227941513, 0.0044018859043717384, -0.03185226395726204, 0.03936305642127991, -0.0018688770942389965, -0.04392078518867493, 0.02990303561091423, -0.0194404199719429, 0.05901814624667168, -0.021767310798168182, 0.032181400805711746, 0.015370846726000309, 0.031207047402858734, -0.016042204573750496, -0.016823984682559967, -0.005706059746444225, -0.03331942856311798, 0.011479238979518414, -0.043793581426143646, 0.032494351267814636, -0.06793207675218582, 0.05236655846238136, -0.031655143946409225, 0.01929832063615322, -0.0250355564057827, -0.03658934682607651, 0.04857027530670166, -0.06623365730047226, -0.04268127307295799, -0.04363507777452469, 0.044615332037210464, -0.00559930969029665, -0.03717941418290138, 0.028203044086694717, 0.00480041466653347, 0.009005775675177574, -0.01836307905614376, 0.054084815084934235, -0.017307721078395844, 0.048483166843652725, 0.023009151220321655, -7.859049219405279e-05, 0.030783794820308685, 0.043127138167619705, 0.005765000823885202, 0.008811963722109795, -0.05386245995759964, 0.004587933421134949, -0.005802399478852749, 0.0050554038025438786, 0.03453978896141052, -0.012859856709837914, -0.01060124859213829, -0.013389998115599155, -0.04355772212147713, 0.016539031639695168, -0.05041985213756561, -0.0248723067343235, 0.08495471626520157, 0.055736441165208817, -0.019743982702493668, -0.04003654792904854, 0.05553850531578064, 0.009581065736711025, -0.020963093265891075, 0.03220677375793457, -0.012795533053576946, 0.052986159920692444, -0.05288834869861603, 0.053567126393318176, 0.01575312204658985, 0.05197490379214287, -0.012308254837989807, -0.004616653546690941, 0.005736787803471088, -0.010011504404246807, 0.010513711720705032, -0.054142292588949203, -0.06452780216932297, -0.06130351126194, 0.002477638190612197, -0.022184111177921295, -0.000995964859612286, -0.05435270443558693, 0.0074448655359447, -0.023539019748568535, -0.031608957797288895, 0.0064430260099470615, -0.030367573723196983, 0.015771696344017982, -0.014180796220898628, -0.04425235465168953, 0.06702947616577148, -0.000456854235380888, 0.010592995211482048, -0.026347137987613678, -0.03434554859995842, 0.01162006612867117, -0.00362666929140687, 0.03504545986652374, 0.002880056854337454, -0.008586738258600235, -0.0005600558361038566, -0.01934652030467987, 0.05669917166233063, -0.00034789182245731354, -0.01825639232993126, -0.012466290034353733, -0.03704797849059105, -0.002550555858761072, -0.022397562861442566, 0.020881881937384605, -0.013832776807248592, 0.027578793466091156, 0.045279063284397125, -0.000525087583810091, -0.047328196465969086, -0.007053776178508997, -0.0021893021184951067, 0.0286997202783823, 0.02384152263402939, 0.006024117581546307, 0.013520568609237671, 0.026852741837501526, 0.04367787763476372, -0.02344651333987713, -0.041360042989254, -0.027980612590909004, -0.014400728978216648, -0.0577680841088295, 0.05705561116337776, -0.00984896719455719, 0.010015214793384075, 0.0763126090168953, 0.07034917175769806, 0.011689933016896248, -0.04705473780632019, -0.028127267956733704, -0.028715649619698524, 0.00838626641780138, -0.09287010133266449, -0.05999135598540306, -0.03459229692816734, -0.03452807664871216, 0.029350021854043007, 9.120464028455899e-07, 0.006535083521157503, 0.029187319800257683, -0.06986693292856216, -0.02206997573375702, -0.05103607848286629, -0.024477669969201088, 0.020876919850707054, 0.045642390847206116, 0.04098346829414368, -0.01810697466135025, -0.018912270665168762, -0.013277142308652401, 0.0213322751224041, -0.026938313618302345, -0.05354780703783035, -0.016160599887371063, 0.0029611149802803993, -0.02684030868113041, 0.04515037313103676, 0.02446618117392063, -0.02725314162671566, -0.024469705298542976, 0.021647747606039047, 0.002507369965314865, -0.04194789007306099, 0.017087087035179138, 0.0518130287528038, 0.05085260793566704, -0.07700842618942261, 0.0056351562961936, 0.060032691806554794, 0.006674149073660374, -0.05446042865514755, -0.04615267738699913, 0.024369537830352783, 0.0271424762904644, -0.012347695417702198, 0.060294460505247116, -0.016132934018969536, 0.017447318881750107, 0.05670442432165146, 0.0015670768916606903, 0.0686553418636322, -0.026241859421133995, -0.015325505286455154, -0.01770787686109543, 0.02104933187365532, -0.046040672808885574, 0.025931548327207565, 0.038434647023677826, 0.026901748031377792, 0.009451834484934807, -0.036178428679704666, -0.017309701070189476, 0.025584906339645386, 0.061249326914548874, -0.09514082968235016, 0.00902795884758234, 0.04994441568851471, -0.0038626997265964746, -0.011658617295324802, 0.008113766089081764, -0.014046317897737026, -0.011056281626224518, 0.04113991931080818, -0.033263616263866425, -0.06105482578277588, -0.031283922493457794, -0.007570411544293165, -0.0032288332004100084, 0.0004964112886227667, -0.002208895515650511, 0.024621492251753807, 0.02076159231364727, 0.009394104592502117, -0.05932564660906792, 0.020849449560046196, 0.03622383624315262, -0.05445172265172005, -0.01577809825539589, -0.016015635803341866, -0.034171875566244125, 0.07400329411029816, 0.06425172090530396, 0.013667335733771324, -0.027485249564051628, -0.06779397279024124, 0.011731340549886227, -0.021519936621189117, 0.05336484685540199, 0.09234699606895447, -0.025581158697605133, 0.04105791822075844, 0.0033194669522345066, 0.009193984791636467, 0.01257328037172556, 0.009872003458440304, 0.013460072688758373, -0.011782833375036716, 0.056569892913103104, 0.011185556650161743, -0.001791957183741033, 0.02985113114118576, 0.03551100194454193, 0.0525372251868248, 0.009313385002315044, 0.029556646943092346, 0.010092461481690407, -0.04109922796487808, -0.09827771782875061, 0.07528837770223618, 0.018835244700312614, -0.02083747275173664, -0.04701956734061241, -0.0014823883539065719, -0.003127161879092455, -0.03790943697094917, -0.05166167765855789, 0.015131598338484764, -0.005340536590665579, -0.014197085052728653, 0.05665569752454758, 0.006526419427245855, -0.02720179408788681, 0.00903793890029192, 0.07492761313915253, -0.04322653263807297, -0.05390876159071922, 0.03704892471432686, -0.027291180565953255, -0.0566035658121109, 0.015606636181473732, 0.027065519243478775, -0.0017480048118159175, 0.027853772044181824, -0.006371915340423584, 0.030529864132404327, 0.018552660942077637, 0.034608323127031326, 0.018036214634776115, 0.03474709764122963, 0.010607247240841389, 0.008939945138990879, 0.005929744802415371, -0.025183523073792458, -0.0025635838974267244, -0.0645676925778389, 0.0062942770309746265, 0.043695416301488876, 0.011311094276607037, 0.006157045252621174, -0.0021617324091494083, 0.03866882994771004, -0.058823224157094955, 0.06246255710721016, 0.0071550337597727776, 0.0022470480762422085, -0.008880370296537876, 0.03494860604405403, 0.038959626108407974, 0.04550785943865776, 0.030317384749650955, -0.00612551299855113, 0.08027740567922592, 0.0028502782806754112, -0.008108875714242458, -0.029123658314347267, -0.012007949873805046, -0.014279266819357872, -0.02980010211467743, 0.02040782757103443, 0.06390708684921265, -0.0006256934138946235, -0.03723321482539177, -0.013057096861302853, 0.04114076867699623, -0.017182866111397743, -0.05549640208482742, 0.02064032293856144, 0.01683172956109047, -0.008635859936475754, 0.03218064829707146, 0.04564550518989563, -0.021377939730882645, 0.021940747275948524, 0.020410453900694847, 0.017982320860028267, 0.02150171808898449, 0.05921953544020653, -0.042486630380153656, -0.017924992367625237, -0.0114266537129879, -0.02765769325196743, 0.02116318792104721, -0.0008785029058344662, 0.00839359499514103, 0.007519723381847143, -0.07929962873458862, 0.01306573860347271, 0.00335461413487792, -0.013990496285259724, 0.00019492211868055165, -0.017358528450131416, -0.03889889642596245, -0.008545472286641598, 0.01378809567540884, 0.06300467997789383, 0.05205303058028221, 0.029774265363812447, 0.05180739611387253, -0.04484200477600098, -0.03888325020670891, -0.056330904364585876, 0.004683728329837322, 0.016883134841918945, -0.03816996142268181, 0.01605170965194702, 0.0022271168418228626, 0.0010828975355252624, 0.038834843784570694, 0.019416887313127518, 0.00031489337561652064, 0.05024728924036026, -0.05813521891832352, -0.006695288233458996, 0.042213670909404755, -0.012247920036315918, 0.028528228402137756, -0.02632697857916355, -0.05482589080929756, 0.00981950294226408, 0.02605678141117096, 0.06638345867395401, -0.018992368131875992, 0.04858163744211197, -0.014409814961254597, -0.0310173612087965, -0.05839765444397926, 0.08313969522714615, 0.05511852726340294, 0.047723494470119476, -0.033163223415613174, -0.040427759289741516, 0.011779758147895336, -0.05743969976902008, -0.021088508889079094, -0.018184570595622063, 0.022849485278129578, -0.010282794013619423, -0.010582848452031612, -0.038172293454408646, -0.02383989654481411, -0.047329485416412354, -0.02541566826403141, 0.027357304468750954, -0.06858660280704498, -0.06362185627222061, -0.0027012284845113754, -0.035492997616529465, -0.06344638019800186, 0.03718043491244316, 0.012817914597690105, 0.018238751217722893, -0.007895039394497871, 0.042976900935173035, -0.06253521889448166, 0.02173938974738121, 0.01422695629298687, 0.06118226796388626], index=0, object='embedding')


"""
This uses the vector model associated with the current embeddings database.
"""

"""
# Text to Speech (TTS)

This API can do more than just work with text. Let's generate speech. 
"""

from IPython.display import Audio, display

with client.audio.speech.with_streaming_response.create(
    model="neuml/kokoro-fp16-onnx",
    input="txtai is an all-in-one embeddings database for semantic search, LLM orchestration and semantic workflows",
    voice="bm_lewis",
) as response:
    response.stream_to_file(file="out.mp3")

display(Audio("out.mp3"))
# Output:
#   <IPython.lib.display.Audio object>

"""
# Transcription

The generated speech can also be transcribed back to text.
"""

f = open("out.mp3", "rb")
client.audio.transcriptions.create(
    model="whisper",
    file=f
).text
# Output:
#   "Text AI is an all in one embedding's database for semantic search, LLM orchestration and semantic workflows."

"""
# JavaScript client

Given that this is an OpenAI-compatible API, other OpenAI clients are also supported. Let's try a few examples with the JavaScript client.

Install via `npm install openai`
"""

%%writefile chat.js

import OpenAI from "openai";

const openai = new OpenAI({
    baseURL: "http://localhost:8000/v1",
    apiKey: "api-key"
});

async function main() {
    const stream = await openai.chat.completions.create({
        model: "rag",
        messages: [{ role: "user", content: "Tell me about the iPhone" }],
        stream: true,
    });

    for await (const chunk of stream) {
        process.stdout.write(chunk.choices[0]?.delta?.content || "");
    }
}

main();
# Output:
#   Overwriting chat.js


!node --no-warnings chat.js
# Output:
#   The iPhone is a line of smartphones designed and marketed by Apple Inc. that uses Apple's iOS mobile operating system. The first-generation iPhone was announced by former Apple CEO Steve Jobs on January 9, 2007. As of January 1, 2024, more than 2.3 billion iPhones have been sold, making Apple the largest vendor of mobile phones in 2023.

"""
As we can see, this is the same output as we had earlier with the Python client.

Let's try generating speech.
"""

%%writefile speech.js

import fs from "fs";
import path from "path";
import OpenAI from "openai";

const openai = new OpenAI({
    baseURL: "http://localhost:8000/v1",
    apiKey: "api-key"
});

const speechFile = path.resolve("./speech.mp3");

async function main() {
  const mp3 = await openai.audio.speech.create({
    model: "neuml/kokoro-fp16-onnx",
    input: "txtai is an all-in-one embeddings database for semantic search, LLM orchestration and semantic workflows",
    voice: "bm_lewis",
  });

  const buffer = Buffer.from(await mp3.arrayBuffer());
  await fs.promises.writeFile(speechFile, buffer);
}

main();
# Output:
#   Overwriting speech.js


!node --no-warnings speech.js

display(Audio("speech.mp3"))
# Output:
#   <IPython.lib.display.Audio object>

"""
Speech is the same as above, as expected.
"""

%%writefile transcribe.js

import fs from "fs";
import OpenAI from "openai";

const openai = new OpenAI({
    baseURL: "http://localhost:8000/v1",
    apiKey: "api-key"
});

async function main() {
  const transcription = await openai.audio.transcriptions.create({
    file: fs.createReadStream("speech.mp3"),
    model: "whisper",
  });

  console.log(transcription.text);
}

main();
# Output:
#   Overwriting transcribe.js


!node --no-warnings transcribe.js
# Output:
#   Text AI is an all in one embedding's database for semantic search, LLM orchestration and semantic workflows.


"""
# Wrapping up

This notebook covered how to setup an OpenAI-compatible API endpoint for txtai. It enables quickly trying `txtai` with a familiar-to-use client. It's also a way to do local/offline development testing using the OpenAI client. Just another way to make it easier to use `txtai`!
"""



================================================
FILE: examples/75_Medical_RAG_Research_with_txtai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Medical RAG Research with txtai

[txtai](https://github.com/neuml/txtai) is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

Large Language Models (LLMs) have captured the public's attention with their impressive capabilities. The Generative AI era has reached a fever pitch with some predicting the coming rise of superintelligence.

LLMs are far from perfect though and we're still a ways away from true AI. One big challenge is with hallucinations. Hallucinations is the term for when an LLM generates output that is factually incorrect. The alarming part of this is that on a cursory glance, it actually sounds like factual content. The default behavior of LLMs is to produce plausible answers even when no plausible answer exists. LLMs are not great at saying I don't know.

Retrieval Augmented Generation (RAG) helps reduce the risk of hallucinations by limiting the context in which a LLM can generate answers. This is typically done with a search query that hydrates a prompt with a relevant context. RAG has been one of the most practical use cases of the Generative AI era.

This notebook will demonstrate how to build a Medical RAG Research process with txtai.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai

"""
# Medical Dataset

For this example, we'll use a [PubMed subset of article metadata for H5N1](https://huggingface.co/datasets/NeuML/pubmed-h5n1). This dataset was created using [`paperetl`](https://github.com/neuml/paperetl), an open-source library for parsing medical and scientific papers.

[PubMed](https://pubmed.ncbi.nlm.nih.gov/) has over 38 million article abstracts as of June 2025. `paperetl` supports loading the full dataset with all 38 million articles or just a smaller subset. The dataset link above has more details on how this can be changed for different codes and keywords. This link also has information on how the article abstracts can be loaded in addition to the metadata.
"""

from datasets import load_dataset
from txtai import Embeddings

ds = load_dataset("neuml/pubmed-h5n1", split="train")

"""
Next, we'll build a `txtai` embeddings index with the articles. We'll use a vector embeddings model that specializes in vectorizing medical papers: [PubMedBERT Embeddings](https://huggingface.co/NeuML/pubmedbert-base-embeddings). 
"""

embeddings = Embeddings(path="neuml/pubmedbert-base-embeddings", content=True, columns={"text": "title"})
embeddings.index(x for x in ds if x["title"])

embeddings.count()
# Output:
#   7865

"""
# RAG Pipeline

There are a number of [prior examples](https://neuml.github.io/txtai/examples/#llm) on how to run RAG with `txtai`. The [RAG pipeline](https://neuml.github.io/txtai/pipeline/text/rag/) takes two main parameters, an embeddings database and an LLM. The embeddings database is the one just created above. For this example, we'll use a [simple local LLM with 600M parameters](https://huggingface.co/Qwen/Qwen3-0.6B).

Substitute your own embeddings database to change the knowledge base. `txtai` supports running local LLMs via [transformers](https://github.com/huggingface/transformers) or [llama.cpp](https://github.com/abetlen/llama-cpp-python). It also supports a wide variety of LLMs via [LiteLLM](https://github.com/BerriAI/litellm). For example, setting the 2nd RAG pipeline parameter below to `gpt-4o` along with the appropriate environment variables with access keys switches to a hosted LLM. See [this documentation page](https://neuml.github.io/txtai/pipeline/text/llm/) for more on this.
"""

from txtai import RAG

# Prompt templates
system = "You are a friendly medical assistant that answers questions"
template = """
Answer the following question using the provided context.

Question:
{question}

Context:
{context}
"""

# Create RAG pipeline
rag = RAG(embeddings, "Qwen/Qwen3-0.6B", system=system, template=template, output="flatten")
# Output:
#   Device set to use cuda:0


"""
# RAG Queries

Now that the pipeline is setup, let's run a query.
"""

print(rag("Tell me about H5N1"))
# Output:
#   <think>

#   Okay, let's see. The user is asking about H5N1. The context provided starts with "Why tell me now?" and then goes into facts about H5N1. The first sentence mentions that people and healthcare providers are weighing in on pandemic messages. Then it says H5N1 is avian influenza, a potential pandemic.

#   

#   Wait, but the user's question is about H5N1. The context doesn't go into specifics about what H5N1 is, but it does state that it's avian influenza. So I need to make sure I answer based on that. The answer should be concise, maybe mention that H5N1 is avian flu and it's a potential pandemic. Also, note that people are weighing in on messages. But I need to check if there's any more information. The context ends there. So the answer should be straightforward.

#   </think>

#   

#   H5N1 influenza viruses are a type of avian influenza, a potential pandemic influenza virus that could cause widespread illness and death. While the context highlights the importance of public health and preparedness, it does not provide more specific details about its characteristics or risks.


"""
Notice that this LLM outputs a thinking or reasoning section then the answer.

Let's review the context to validate this answer is derived from the knowledge base.
"""

embeddings.search("Tell me about H5N1", limit=10)
# Output:
#   [{'id': '16775537',

#     'text': '"Why tell me now?" the public and healthcare providers weigh in on pandemic influenza messages.',

#     'score': 0.7156285643577576},

#    {'id': '22308474',

#     'text': 'H5N1 influenza viruses: facts, not fear.',

#     'score': 0.658343493938446},

#    {'id': '16440117',

#     'text': 'Avian influenza--a pandemic waiting to happen?',

#     'score': 0.5827972888946533},

#    {'id': '20667302',

#     'text': 'The influenza A(H5N1) epidemic at six and a half years: 500 notified human cases and more to come.',

#     'score': 0.5593500137329102},

#    {'id': '18936262',

#     'text': 'What Australians know and believe about bird flu: results of a population telephone survey.',

#     'score': 0.5568690299987793},

#    {'id': '30349811',

#     'text': 'Back to the Future: Lessons Learned From the 1918 Influenza Pandemic.',

#     'score': 0.5540266036987305},

#    {'id': '17276785',

#     'text': 'Pandemic influenza: what infection control professionals should know.',

#     'score': 0.5519200563430786},

#    {'id': '16681227',

#     'text': 'A pandemic flu: not if, but when. SARS was the wake-up call we slept through.',

#     'score': 0.5518345832824707},

#    {'id': '22402712',

#     'text': 'Ferretting out the facts behind the H5N1 controversy.',

#     'score': 0.5508109331130981},

#    {'id': '25546511',

#     'text': "One-way trip: influenza virus' adaptation to gallinaceous poultry may limit its pandemic potential.",

#     'score': 0.5494509339332581}]

"""
The answer is doing a good job being based on the context above. Also keep in mind this is a small 600M parameter model, which is even more impressive.

Let's try another query.
"""

print(rag("What locations have had H5N1 outbreaks?"))
# Output:
#   <think>

#   Okay, let's see. The user is asking about the locations that have had H5N1 outbreaks, and the provided context mentions a few places: Indonesia and Bangladesh. The context also has a title about a decade of avian influenza in Bangladesh and mentions "H5N1." 

#   

#   Wait, the user's question is in English, so I need to make sure I'm interpreting the context correctly. The context includes two sentences: one about a decade in Bangladesh and another about H5N1. The user is probably looking for specific locations where H5N1 has been reported. 

#   

#   Looking at the context again, it says "Human avian influenza in Indonesia" and "A Decade of Avian Influenza in Bangladesh: Where Are We Now? Are we ready for pandemic influenza H5N1?" So the outbreaks are in Indonesia and Bangladesh. 

#   

#   I should confirm that there are no other mentions of other locations. The context doesn't provide more information beyond those two countries. Therefore, the answer should list Indonesia and Bangladesh as the locations with H5N1 outbreaks.

#   </think>

#   

#   The locations with H5N1 outbreaks are Indonesia and Bangladesh.


embeddings.search("What locations have had H5N1 outbreaks?", limit=10)
# Output:
#   [{'id': '21706937',

#     'text': 'Human avian influenza in Indonesia: are they really clustered?',

#     'score': 0.6269429326057434},

#    {'id': '31514405',

#     'text': 'A Decade of Avian Influenza in Bangladesh: Where Are We Now?',

#     'score': 0.5972536206245422},

#    {'id': '15889987',

#     'text': 'Are we ready for pandemic influenza H5N1?',

#     'score': 0.5863772630691528},

#    {'id': '17717543',

#     'text': 'Commentary: From scarcity to abundance: pandemic vaccines and other agents for "have not" countries.',

#     'score': 0.5844159126281738},

#    {'id': '22491771',

#     'text': 'Two years after pandemic influenza A/2009/H1N1: what have we learned?',

#     'score': 0.5812581777572632},

#    {'id': '39666804',

#     'text': "Why hasn't the bird flu pandemic started?",

#     'score': 0.5738048553466797},

#    {'id': '23402131',

#     'text': 'Where do avian influenza viruses meet in the Americas?',

#     'score': 0.5638074278831482},

#    {'id': '20667302',

#     'text': 'The influenza A(H5N1) epidemic at six and a half years: 500 notified human cases and more to come.',

#     'score': 0.560465395450592},

#    {'id': '17338983',

#     'text': 'Human avian influenza: how ready are we?',

#     'score': 0.555113673210144},

#    {'id': '24518630',

#     'text': 'Recognizing true H5N1 infections in humans during confirmed outbreaks.',

#     'score': 0.5501888990402222}]

"""
Once again the answer is based on the context which mentions the two countries in the answer. The context also discusses the Americas but it doesn't have as strong of language connecting H5N1 outbreaks to the location.
"""

"""
# Add citations

The last item we'll cover is citations. One of the most important aspects of a RAG process is being able to ensure the answer is based on reality. There are a number of ways to do this but in this example, we'll ask the LLM to perform this step.
"""

# Prompt templates
system = "You are a friendly medical assistant that answers questions"
template = """
Answer the following question using the provided context.

After the answer, write a citation section with ALL the original article ids used for the answer.

Question:
{question}

Context:
{context}
"""

def context(question):
    context = []
    for x in embeddings.search(question, limit=10):
        context.append(f"ARTICLE ID: {x['id']}, TEXT: {x['text']}")

    return context

# Create RAG pipeline
rag = RAG(embeddings, "Qwen/Qwen3-0.6B", system=system, template=template, output="flatten")
# Output:
#   Device set to use cuda:0


question = "What is H5N1?"
print(rag(question, context(question), maxlength=2048, stripthink=True))
# Output:
#   H5N1 is a type of avian influenza virus.  

#   

#   **Citation Section:**  

#   - ARTICLE ID: 22010536, TEXT: Is avian influenza virus A(H5N1) a real threat to human health?


"""
As expected, the answer adds a citation section. Also note that the RAG pipeline stripped the thinking section from the result.
"""

"""
# Wrapping up

This notebook covered how to build a Medical RAG Research process with `txtai`. It also covered how to modify this logic to add in your own knowledge base or use a more sophisticated LLM.

With an important space such as the medical domain, it's vital to ensure that answers are derived from reliable knowledge. This notebook shows how to add that reliability via RAG. But as with anything in an important domain, there should be a human in the loop and answers shouldn't be blindly relied upon. 
"""



================================================
FILE: examples/76_Whats_new_in_txtai_9_0.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ğŸ’¡ What's new in txtai 9.0

[txtai](https://github.com/neuml/txtai) is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

The 9.0 release adds first class support for sparse vector models (i.e. [SPLADE](https://en.wikipedia.org/wiki/Learned_sparse_retrieval)), late interaction models (i.e. [ColBERT](https://huggingface.co/colbert-ir/colbertv2.0)), fixed dimensional encoding (i.e. [MUVERA](https://arxiv.org/abs/2405.19504)) and reranking pipelines âœ¨ 

The embeddings framework was overhauled to seamlessly support both sparse and dense vector models. Previously, sparse vector support was limited to keyword/term indexes. Now learned sparse retrieval models such as SPLADE are supported. These models can help improve the accuracy of retrieval/search operations, which also improves RAG and Agents.

Support for late interaction models, such as ColBERT, were also added to the embeddings framework. Unlike traditional vector models that pool outputs into single vector outputs, late interaction models produce multiple vectors. These models are paired with the MUVERA algorithm to transform multiple vectors into fixed dimensional single vectors for search.

LLMs are quickly converging to produce similar outputs for similar inputs and becoming standard commodities. The retrieval or context layer makes or breaks projects. This is known as putting the R in RAG!

**Standard upgrade disclaimer below**

While everything is backwards compatible, it's prudent to backup production indexes before upgrading and test before deploying.
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[ann,vectors]

"""
# Sparse vector indexes

The first major change added with this release is `learned sparse retrieval` (aka sparse vector indexes) models. This effort was multi-faceted in that it required both changes to how vectors were generated as well as how they are stored.

`txtai` uses approximate nearest neighbor (ANN) search for it's vector search operations. The default library is [Faiss](https://github.com/facebookresearch/faiss). There is support for other libraries but in all cases the existing ANN backends only supported dense (i.e. NumPy) vectors.

There aren't many options out there for sparse ANN search that supports `txtai` requirements, so IVFSparse was introduced. IVFSparse is an Inverted file (IVF) index with flat vector file storage and sparse array support. There is also support for storing sparse vectors in Postgres via [pgvector](https://github.com/pgvector/pgvector).

Let's see it in action.

"""

from txtai import Embeddings

# Works with a list, dataset or generator
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create an embeddings
embeddings = Embeddings(sparse=True, content=True)
embeddings.index(data)
embeddings.search("North America", 10)
# Output:
#   [{'id': '0',

#     'text': 'US tops 5 million confirmed virus cases',

#     'score': 0.019873601198196412},

#    {'id': '1',

#     'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",

#     'score': 0.018737798929214476}]

"""
# Late interaction models

Late interaction models encode data into multi-vector outputs. In other words, multiple input tokens map to multiple output vectors. Then at search time, the maximum similarity algorithm is used to find the best matches between the corpus and a query. This algorithm has achieved excellent results on retrieval benchmarks such as [MTEB](https://github.com/embeddings-benchmark/mteb).

The downside of this approach is that it produces multiple vectors as opposed a single vector for each input. For example, if a text element tokenizes to many input tokens, there will be many output vectors vs a single one as with standard pooled vector approaches.

Starting with the 9.0 release, late interaction models are supported with embeddings instances. Late interaction vectors will be transformed into fixed dimensional vectors using the MUVERA algorithm. See below. 
"""

from txtai import Embeddings

# Works with a list, dataset or generator
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create an embeddings
embeddings = Embeddings(path="colbert-ir/colbertv2.0", content=True)
embeddings.index(data)
embeddings.search("North America", 10)
# Output:
#   [{'id': '0',

#     'text': 'US tops 5 million confirmed virus cases',

#     'score': 0.04216160625219345},

#    {'id': '1',

#     'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",

#     'score': 0.029944246634840965},

#    {'id': '3',

#     'text': 'The National Park Service warns against sacrificing slower friends in a bear attack',

#     'score': 0.015931561589241028}]

"""
# Reranking pipeline

Another major new component in this release is the Reranker pipeline. This pipeline takes an embeddings instance, a similarity instance and uses the similarity instance to rerank outputs. This is a key component of the MUVERA paper - using the standard vector index to retrieve candidates then reranking the outputs using the late interaction model.
"""

from txtai import Embeddings
from txtai.pipeline import Reranker, Similarity

# Works with a list, dataset or generator
data = [
  "US tops 5 million confirmed virus cases",
  "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
  "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
  "The National Park Service warns against sacrificing slower friends in a bear attack",
  "Maine man wins $1M from $25 lottery ticket",
  "Make huge profits without work, earn up to $100,000 a day"
]

# Create an embeddings
embeddings = Embeddings(path="colbert-ir/colbertv2.0", content=True)
embeddings.index(data)

similarity = Similarity(path="colbert-ir/colbertv2.0", lateencode=True)

ranker = Reranker(embeddings, similarity)
ranker("North America")
# Output:
#   [{'id': '1',

#     'text': "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",

#     'score': 0.3324427008628845},

#    {'id': '0',

#     'text': 'US tops 5 million confirmed virus cases',

#     'score': 0.24423550069332123},

#    {'id': '3',

#     'text': 'The National Park Service warns against sacrificing slower friends in a bear attack',

#     'score': 0.16353240609169006}]

"""
Notice that while the outputs are the same, the scoring and order is different.

Let's try a more interesting example.
"""

from txtai import Embeddings
from txtai.pipeline import Reranker, Similarity

# Create an embeddings
embeddings = Embeddings()
embeddings.load(provider="huggingface-hub", container="neuml/txtai-wikipedia")

similarity = Similarity(path="colbert-ir/colbertv2.0", lateencode=True)

ranker = Reranker(embeddings, similarity)
ranker("Tell me about ChatGPT")
# Output:
#   [{'id': 'ChatGPT',

#     'text': 'ChatGPT is a generative artificial intelligence chatbot developed by OpenAI and released on November 30, 2022. It uses large language models (LLMs) such as GPT-4o as well as other multimodal models to create human-like responses in text, speech, and images. It has access to features such as searching the web, using apps, and running programs. It is credited with accelerating the AI boom, an ongoing period of rapid investment in and public attention to the field of artificial intelligence (AI). Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.',

#     'score': 0.6639302968978882},

#    {'id': 'ChatGPT Search',

#     'text': 'ChatGPT Search (originally SearchGPT) is a search engine developed by OpenAI. It combines traditional search engine features with generative pretrained transformers (GPT) to generate responses, including citations to external websites.',

#     'score': 0.6477508544921875},

#    {'id': 'ChatGPT in education',

#     'text': 'The usage of ChatGPT in education has sparked considerable debate and exploration. ChatGPT is a chatbot based on large language models (LLMs) that was released by OpenAI in November 2022.',

#     'score': 0.5918337106704712}]

"""
# Wrapping up

This notebook gave a quick overview of txtai 9.0. Updated documentation and more examples will be forthcoming. There is much to cover and much to build on!

See the following links for more information.

- [9.0 Release on GitHub](https://github.com/neuml/txtai/releases/tag/v9.0.0)
- [Documentation site](https://neuml.github.io/txtai)
"""



================================================
FILE: examples/77_GraphRAG_with_Wikipedia_and_GPT_OSS.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# GraphRAG with Wikipedia and GPT OSS

[txtai](https://github.com/neuml/txtai) is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

Retrieval Augmented Generation (RAG) is one of the most popular techniques in the AI space today. RAG takes a user request, retrieves the best matching content and then plugs that context into an LLM prompt to generate an answer. When otherwise not mentioned, most assume the context is generated using a vector database query. But there is no rule that says context can't be generated with other methods. It could be a simple web query, SQL query, text index search or other traditional search.

We also often hear the term GraphRAG. GraphRAG means different things to different people. Here we're going to build an example that uses `txtai`, [wikipedia](https://huggingface.co/datasets/NeuML/wikipedia-20250620) and [gpt-oss](https://huggingface.co/openai/gpt-oss-20b) to research a specific topic with graphs. `txtai` has a built-in graph component that automatically generates a graph network over the data loaded into an embeddings database. We'll use a pre-built embeddings database hosted on the Hugging Face Hub, [txtai-wikipedia-slim](https://hf.co/neuml/txtai-wikipedia-slim).
"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[graph,pipeline-llm]

"""
# Load txtai-wikipedia-slim

Next, we'll load the embeddings database. This database is the top 100K most viewed Wikipedia articles with both a dense vector index and graph network enabled.
"""

from txtai import Embeddings

embeddings = Embeddings().load(provider="huggingface-hub", container="neuml/txtai-wikipedia-slim")

"""
# Build context with a graph query

The `txtai` graph component supports the [openCypher](https://opencypher.org/) query language via the [GrandCypher](https://github.com/aplbrain/grand-cypher) library.

openCypher is a language for expressive and efficient data querying of a property graph. In this example, we'll traverse the embeddings database graph looking for paths between nodes similar to `chatgpt` and `anthropic`.
"""

g = embeddings.search("""
MATCH P=(A)-[]->(B)
WHERE SIMILAR(A, 'chatgpt') AND SIMILAR(B, 'anthropic')
RETURN P
LIMIT 10
""", graph=True)

"""
The query above is an extremely powerful combination of an vector similarity node search and a graph traversal query that walks the paths between nodes. It's much more expressive than simply saying find nodes similar to each of the concepts independently. It can be considered a `deep graph search`.
"""

"""
# Plot the context network

Let's show the context as a graph plot!
"""

import matplotlib.pyplot as plt
import networkx as nx

def plot(graph):
    labels = {x: f"{graph.attribute(x, 'id')}" for x in graph.scan()}
    colors = ["#D32F2F", "#0277bd", "#7e57c2", "#757575"]

    results = embeddings.batchsimilarity(labels.values(), ["Anthropic Claude", "Google Gemini", "OpenAI GPT"])
    colors = [colors[x[0][0]] for x in results]

    options = {
        "node_size": 1000,
        "node_color": colors,
        "edge_color": "#454545",
        "font_color": "#efefef",
        "font_size": 10,
        "alpha": 1.0,
    }

    fig, ax = plt.subplots(figsize=(20, 9))
    pos = nx.spring_layout(graph.backend, seed=0, k=0.9, iterations=50)
    nx.draw_networkx(graph.backend, pos=pos, labels=labels, **options)
    ax.set_facecolor("#303030")
    ax.axis("off")
    fig.set_facecolor("#303030")

    plt.show()

plot(g)
# Output:
#   <Figure size 2000x900 with 1 Axes>

"""
# Print the context as text

Let's further inspect the graph nodes.
"""

context = ""
for x in g.scan():
    uid = g.attribute(x, "id")

    context += f"- id: {uid}\n"
    context += f"  url: https://en.wikipedia.org/wiki/{uid.replace(' ', '_')}\n"
    context += f"  text: {g.attribute(x, 'text')}\n"
    context += f"  links: {[g.attribute(n, 'id') for n in g.edges(x)]}\n"

print(context)
# Output:
#   - id: ChatGPT

#     url: https://en.wikipedia.org/wiki/ChatGPT

#     text: ChatGPT is a generative artificial intelligence chatbot developed by OpenAI and released on November 30, 2022. It uses large language models (LLMs) such as GPT-4o as well as other multimodal models to create human-like responses in text, speech, and images. It has access to features such as searching the web, using apps, and running programs. It is credited with accelerating the AI boom, an ongoing period of rapid investment in and public attention to the field of artificial intelligence (AI). Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.

#     links: ['GPT-4', 'GPT-4.5', 'OpenAI', 'Gemini (chatbot)', 'GPT-3', 'GPT-4.1', 'Gemini (language model)', 'Anthropic', 'Claude (language model)']

#   - id: GPT-4

#     url: https://en.wikipedia.org/wiki/GPT-4

#     text: Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model trained and created by OpenAI and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus until being replaced in 2025, via OpenAI's API, and via the free chatbot Microsoft Copilot.

#     links: ['ChatGPT', 'GPT-3', 'GPT-4.5', 'GPT-4.1', 'OpenAI', 'Gemini (chatbot)', 'Gemini (language model)', 'Claude (language model)']

#   - id: GPT-4.5

#     url: https://en.wikipedia.org/wiki/GPT-4.5

#     text: GPT-4.5 (codenamed "Orion") is a large language model developed by OpenAI as part of the GPT series. Officially released on February 27, 2025, GPT-4.5 is available to users subscribed to the ChatGPT Plus and Pro plans across web, mobile, and desktop platforms. Access is also provided through the OpenAI API and the OpenAI Developer Playground, but the company plans to phase out API access to the model in July.

#     links: ['GPT-4.1', 'GPT-4', 'ChatGPT', 'GPT-3', 'OpenAI', 'Claude (language model)', 'Gemini (language model)', 'Anthropic', 'Gemini (chatbot)']

#   - id: OpenAI

#     url: https://en.wikipedia.org/wiki/OpenAI

#     text: OpenAI, Inc. is an American artificial intelligence (AI) organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop "safe and beneficial" artificial general intelligence (AGI), which it defines as "highly autonomous systems that outperform humans at most economically valuable work". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.

#     links: ['ChatGPT', 'GPT-4', 'GPT-3', 'GPT-4.5', 'Anthropic', 'GPT-4.1', 'Gemini (chatbot)', 'Gemini (language model)']

#   - id: Gemini (chatbot)

#     url: https://en.wikipedia.org/wiki/Gemini_(chatbot)

#     text: Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 in response to the rise of OpenAI's ChatGPT. It was previously based on the LaMDA and PaLM LLMs.

#     links: ['Gemini (language model)', 'ChatGPT', 'GPT-4', 'Anthropic', 'OpenAI', 'GPT-4.5']

#   - id: GPT-3

#     url: https://en.wikipedia.org/wiki/GPT-3

#     text: Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.

#     links: ['GPT-4', 'GPT-4.1', 'ChatGPT', 'OpenAI', 'GPT-4.5', 'Claude (language model)', 'Gemini (language model)']

#   - id: GPT-4.1

#     url: https://en.wikipedia.org/wiki/GPT-4.1

#     text: GPT-4.1 is a large language model within OpenAI's GPT series. It was released on April 14, 2025. GPT-4.1 can be accessed through the OpenAI API or the OpenAI Developer Playground. Three different models were simultaneously released: GPT-4.1, GPT-4.1 mini, and GPT-4.1 nano.

#     links: ['GPT-4.5', 'GPT-4', 'GPT-3', 'ChatGPT', 'OpenAI', 'Gemini (language model)', 'Claude (language model)']

#   - id: Gemini (language model)

#     url: https://en.wikipedia.org/wiki/Gemini_(language_model)

#     text: Gemini is a family of multimodal large language models (LLMs) developed by Google DeepMind, and the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name. In March 2025, Gemini 2.5 Pro Experimental was rated as highly competitive.

#     links: ['Gemini (chatbot)', 'GPT-4', 'ChatGPT', 'GPT-4.5', 'GPT-4.1', 'GPT-3', 'OpenAI', 'Anthropic']

#   - id: Anthropic

#     url: https://en.wikipedia.org/wiki/Anthropic

#     text: Anthropic PBC is an American artificial intelligence (AI) startup company founded in 2021. Anthropic has developed a family of large language models (LLMs) named Claude as a competitor to OpenAI's ChatGPT and Google's Gemini. According to the company, it researches and develops AI to "study their safety properties at the technological frontier" and use this research to deploy safe models for the public.

#     links: ['Claude (language model)', 'OpenAI', 'ChatGPT', 'Gemini (chatbot)', 'GPT-4.5', 'Gemini (language model)']

#   - id: Claude (language model)

#     url: https://en.wikipedia.org/wiki/Claude_(language_model)

#     text: Claude is a family of large language models developed by Anthropic. The first model was released in March 2023.

#     links: ['Anthropic', 'GPT-3', 'GPT-4.5', 'GPT-4', 'ChatGPT', 'GPT-4.1']

#   


"""
# GraphRAG

Now that we have our graph context, we'll plug that into an LLM prompt.
"""

from txtai import LLM

llm = LLM("unsloth/gpt-oss-20b-GGUF/gpt-oss-20b-Q4_K_M.gguf", n_ctx=20000)

from IPython.display import display, Markdown

out = llm(f"""
Analyze the following context and write an article about it
{context}
""", defaultrole="user", maxlength=20000, stripthink=True)

display(Markdown(out))
# Output:
#   <IPython.core.display.Markdown object>

"""
# Wrapping up

There we have it, GraphRAG in a very straightforward and easy-to-understand manner. The best ideas often are the simple ones!


"""



================================================
FILE: examples/78_Accessing_Low_Level_Vector_APIs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Accessing Low Level Vector APIs

[txtai](https://github.com/neuml/txtai) is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.

The primary interface to build vector databases with `txtai` is through [Embeddings instances](https://neuml.github.io/txtai/embeddings/). `txtai` also supports accessing all of it's features through lower level APIs. 

Let's dive in.

"""

"""
# Install dependencies

Install `txtai` and all dependencies.
"""

%%capture
!pip install git+https://github.com/neuml/txtai#egg=txtai[ann] gguf

"""
# Load a dataset

We'll use a [subset](https://huggingface.co/datasets/m-a-p/FineFineWeb-test) of the [FineFineWeb dataset](https://huggingface.co/datasets/m-a-p/FineFineWeb). This dataset is a domain-labeled version of the general purpose [FineWeb dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb). 
"""

from datasets import load_dataset

ds = load_dataset("m-a-p/FineFineWeb-test", split="train")

"""
# Building an Embeddings database

Before going into the low-level API, let's recap how we build an Embeddings database.
"""

from txtai import Embeddings

embeddings = Embeddings()
embeddings.index(ds["text"][:10000])
for uid, score in embeddings.search("nasa", 1):
    print(score, ds[uid]["text"][:100])
# Output:
#   0.6012564897537231 The National Aeronautics and Space Administration (NASA) is the United Statesâ€™ civil space program. 


"""
This simple example abstracts the heavy lifting behind the `Embeddings` interface. Behind the scenes, it defaults to vectorizing text using [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). Vectors are stored in a [Faiss index](https://github.com/facebookresearch/faiss).

The first 10K records are vectorized and stored in the vector index. Then at query time, the query is vectorized and a vector similarity search is run.

While the `Embeddings` interface is convenient, it's also possible to access lower level APIs. 
"""

"""
# Vectors Interface

First, let's vectorize our data using the low level APIs. We'll use the default Hugging Face vectorizer available in `txtai`.
"""

from txtai.ann import ANNFactory
from txtai.vectors import VectorsFactory

vectors = VectorsFactory.create({"path": "sentence-transformers/all-MiniLM-L6-v2"})
data = vectors.vectorize(ds["text"])

"""
# ANN Interface

Now that we have a NumPy array of vectors, let's store them in an Approximate Neighest Neighbor (ANN) backend. Recall earlier, we used the default Faiss interface. For this example, we're going to use the [PyTorch ANN](https://neuml.github.io/txtai/embeddings/configuration/ann/#torch). This will allow us to use new features that are available as of `txtai` 9.1.
"""

ann = ANNFactory.create({
    "backend": "torch",
    "torch": {
        "safetensors": True,
    }
})
ann.index(data)
ann.save("vectors.safetensors")

"""
This ANN builds a Torch tensor with the vectors and stores them in a [Safetensors](https://github.com/huggingface/safetensors) file.

The code below shows how the file is simply a standard Safetensors file.
"""

from safetensors import safe_open

def tensorinfo():
    memory = 0
    with safe_open("vectors.safetensors", framework="np") as f:
        for key in f.keys():
            array = f.get_tensor(key)
            print(key, array.shape)
            memory += array.nbytes

    print(f"Memory = {memory / 1024 / 1024:.2f} MB")

tensorinfo()
# Output:
#   data (1411868, 384)

#   Memory = 2068.17 MB


"""
# Vector search

Now let's show how these low-level APIs can be used to implement vector search.
"""

import textwrap

def search(text):
    result = ann.search(vectors.vectorize([text]), 1)
    index, score = result[0][0]
    print(textwrap.fill(ds[index]["text"], width=150), "\n\n", score)

search("How far is earth from mars?")
# Output:
#   The answer to your question, that how many miles is it from earth to mars, is very easy to know. Because of huge satellites which are being sent to

#   mars in search of life from many countries, we have discovered a lot about mars. According to experts, earth and mars reaches to their closest points

#   in every 26 months. This situation is considered as opposition of mars as the location of sun and mars in totally opposite to each other in relation

#   to earth. When this opposition takes place, the planet is visible with a red tint in the sky from earth. And this also gives mars a name, i.e. the red

#   planet. Mars is also the fourth planet from sun, which is located between Jupiter and earth. Its distance from sun is not only opposite but is also

#   much further away, than that of the earth and sun. The distance between the sun and mars is said to be 140 million miles. Mars can reach about 128

#   million miles closer to the sun whereas it can even travel around 154 million miles away from it. The assumed distance between mars and earth is said

#   to be between 40 to 225 million miles. The distance between these two planets keeps on changing throughout the year because of the elliptical path in

#   which all the planets rotate. As the distance between mars, sun and earth is so much high, it takes a Martian year, for mars to go around the sun. The

#   Martian period includes a time of around 687 earth days. This means that, it takes more than 2 years for the mars to reach its initial rotation point.

#   If we talk about one Martian day, it is the total time which is taken by a planet to spin around once. This day usually lasts longer than our regular

#   earth days. So this was the actual reason which states the distance between earth and mars. 

#   

#    0.7060051560401917


"""
# Torch 4-bit quantization

`txtai` 9.1 adds a new feature: 4-bit vector quantization. This means that instead of using 32-bit floats for each vector dimension, this method uses 4 bits. This reduces memory usage to ~12-13% of the original size.
"""

ann = ANNFactory.create({
    "backend": "torch",
    "torch": {
        "safetensors": True,
        "quantize": {
            "type": "nf4"
        }
    }
})
ann.index(data)
ann.save("vectors.safetensors")

tensorinfo()
# Output:
#   absmax (8471208,)

#   code (16,)

#   data (271078656, 1)

#   shape (2,)

#   Memory = 290.84 MB


"""
Note how the unquantized vectors took 2068.17 MB and this only takes 290.84 MB! With quantization and ever growing GPUs, this opens the possibility of pinning your entire vector database in GPU memory!

For example, let's extrapolate this dataset to 100M rows.

```
(290.84 MB / 1,411,868) * 100,000,000 = 20,599.7 MB
```

An entire 100M row dataset could fit into a single RTX 3090 / 4090 consumer GPU!

Let's confirm search still works the same.
"""

search("How far is earth from mars?")
# Output:
#   The answer to your question, that how many miles is it from earth to mars, is very easy to know. Because of huge satellites which are being sent to

#   mars in search of life from many countries, we have discovered a lot about mars. According to experts, earth and mars reaches to their closest points

#   in every 26 months. This situation is considered as opposition of mars as the location of sun and mars in totally opposite to each other in relation

#   to earth. When this opposition takes place, the planet is visible with a red tint in the sky from earth. And this also gives mars a name, i.e. the red

#   planet. Mars is also the fourth planet from sun, which is located between Jupiter and earth. Its distance from sun is not only opposite but is also

#   much further away, than that of the earth and sun. The distance between the sun and mars is said to be 140 million miles. Mars can reach about 128

#   million miles closer to the sun whereas it can even travel around 154 million miles away from it. The assumed distance between mars and earth is said

#   to be between 40 to 225 million miles. The distance between these two planets keeps on changing throughout the year because of the elliptical path in

#   which all the planets rotate. As the distance between mars, sun and earth is so much high, it takes a Martian year, for mars to go around the sun. The

#   Martian period includes a time of around 687 earth days. This means that, it takes more than 2 years for the mars to reach its initial rotation point.

#   If we talk about one Martian day, it is the total time which is taken by a planet to spin around once. This day usually lasts longer than our regular

#   earth days. So this was the actual reason which states the distance between earth and mars. 

#   

#    0.6982609033584595


"""
Same result. Note the score is slightly different but this is expected.
"""

"""
# GGUF Support

`txtai` 9.1 also adds support for [GGML](https://github.com/ggml-org/ggml) / [GGUF](https://huggingface.co/docs/hub/en/gguf) popularized by [llama.cpp](https://github.com/ggml-org/llama.cpp).
"""

ann = ANNFactory.create({
    "backend": "ggml",
    "ggml": {
        "quantize": "Q4_0"
    }
})
ann.index(data)
ann.save("vectors.gguf")

"""
Now let's check out the generated file using the [gguf](https://github.com/ggml-org/llama.cpp/tree/master/gguf-py) package provided by llama.cpp.
"""

from gguf.gguf_reader import GGUFReader

reader = GGUFReader("vectors.gguf")

# List all tensors
info = "{:<30} | {:<15} | {:<12} | {}"
print(info.format("Tensor Name", "Shape", "Size", "Quantization"))
print("-" * 80)
for tensor in reader.tensors:
    shape = "x".join(map(str, tensor.shape))
    size = f"{tensor.n_elements / 2 / 1024 / 1024:.2f} MB"
    quantization = tensor.tensor_type.name
    print(info.format(tensor.name, shape, size, quantization))
# Output:
#   Tensor Name                    | Shape           | Size         | Quantization

#   --------------------------------------------------------------------------------

#   data                           | 384x1411868     | 258.52 MB    | Q4_0


"""
And search like we did with Torch.
"""

search("How far is earth from mars?")
# Output:
#   The answer to your question, that how many miles is it from earth to mars, is very easy to know. Because of huge satellites which are being sent to

#   mars in search of life from many countries, we have discovered a lot about mars. According to experts, earth and mars reaches to their closest points

#   in every 26 months. This situation is considered as opposition of mars as the location of sun and mars in totally opposite to each other in relation

#   to earth. When this opposition takes place, the planet is visible with a red tint in the sky from earth. And this also gives mars a name, i.e. the red

#   planet. Mars is also the fourth planet from sun, which is located between Jupiter and earth. Its distance from sun is not only opposite but is also

#   much further away, than that of the earth and sun. The distance between the sun and mars is said to be 140 million miles. Mars can reach about 128

#   million miles closer to the sun whereas it can even travel around 154 million miles away from it. The assumed distance between mars and earth is said

#   to be between 40 to 225 million miles. The distance between these two planets keeps on changing throughout the year because of the elliptical path in

#   which all the planets rotate. As the distance between mars, sun and earth is so much high, it takes a Martian year, for mars to go around the sun. The

#   Martian period includes a time of around 687 earth days. This means that, it takes more than 2 years for the mars to reach its initial rotation point.

#   If we talk about one Martian day, it is the total time which is taken by a planet to spin around once. This day usually lasts longer than our regular

#   earth days. So this was the actual reason which states the distance between earth and mars. 

#   

#    0.7043964862823486


"""
# Wrapping up

While the `Embeddings` interface is the preferred way to build vector databases with `txtai`, it's entirely possible to also build with the low level APIs!
"""



================================================
FILE: examples/article.py
================================================
"""
Application that builds a summary of an article.

Requires streamlit to be installed.
  pip install streamlit
"""

import os

import streamlit as st

from txtai.pipeline import Summary, Textractor
from txtai.workflow import UrlTask, Task, Workflow


class Application:
    """
    Main application.
    """

    def __init__(self):
        """
        Creates a new application.
        """

        textract = Textractor(paragraphs=True, minlength=100, join=True)
        summary = Summary("sshleifer/distilbart-cnn-12-6")

        self.workflow = Workflow([UrlTask(textract), Task(summary)])

    def run(self):
        """
        Runs a Streamlit application.
        """

        st.title("Article Summary")
        st.markdown("This application builds a summary of an article.")

        url = st.text_input("URL")
        if url:
            # Run workflow and get summary
            summary = list(self.workflow([url]))[0]

            # Write results
            st.write(summary)
            st.markdown("*Source: " + url + "*")


@st.cache(allow_output_mutation=True)
def create():
    """
    Creates and caches a Streamlit application.

    Returns:
        Application
    """

    return Application()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = create()
    app.run()



================================================
FILE: examples/baseball.py
================================================
"""
Baseball statistics application with txtai and Streamlit.

Install txtai and streamlit (>= 1.23) to run:
  pip install txtai streamlit
"""

import datetime
import math
import os
import random

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

from txtai.embeddings import Embeddings


class Stats:
    """
    Base stats class. Contains methods for loading, indexing and searching baseball stats.
    """

    def __init__(self):
        """
        Creates a new Stats instance.
        """

        # Load columns
        self.columns = self.loadcolumns()

        # Load stats data
        self.stats = self.load()

        # Load names
        self.names = self.loadnames()

        # Build index
        self.vectors, self.data, self.embeddings = self.index()

    def loadcolumns(self):
        """
        Returns a list of data columns.

        Returns:
            list of columns
        """

        raise NotImplementedError

    def load(self):
        """
        Loads and returns raw stats.

        Returns:
            stats
        """

        raise NotImplementedError

    def metric(self):
        """
        Primary metric column.

        Returns:
            metric column name
        """

        raise NotImplementedError

    def vector(self, row):
        """
        Build a vector for input row.

        Args:
            row: input row

        Returns:
            row vector
        """

        raise NotImplementedError

    def loadnames(self):
        """
        Loads a name - player id dictionary.

        Returns:
            {player name: player id}
        """

        # Get unique names
        names = {}
        rows = self.stats.sort_values(by=self.metric(), ascending=False)[["nameFirst", "nameLast", "playerID"]].drop_duplicates().reset_index()
        for x, row in rows.iterrows():
            # Name key
            key = f"{row['nameFirst']} {row['nameLast']}"
            key += f" ({row['playerID']})" if key in names else ""

            if key not in names:
                # Scale scores of top n players
                exponent = 2 if ((len(rows) - x) / len(rows)) >= 0.95 else 1

                # score = num seasons ^ exponent
                score = math.pow(len(self.stats[self.stats["playerID"] == row["playerID"]]), exponent)

                # Save name key - values pair
                names[key] = (row["playerID"], score)

        return names

    def index(self):
        """
        Builds an embeddings index to stats data. Returns vectors, input data and embeddings index.

        Returns:
            vectors, data, embeddings
        """

        # Build data dictionary
        vectors = {f'{row["yearID"]}{row["playerID"]}': self.transform(row) for _, row in self.stats.iterrows()}
        data = {f'{row["yearID"]}{row["playerID"]}': dict(row) for _, row in self.stats.iterrows()}

        embeddings = Embeddings(
            {
                "transform": self.transform,
            }
        )

        embeddings.index((uid, vectors[uid], None) for uid in vectors)

        return vectors, data, embeddings

    def metrics(self, name):
        """
        Looks up a player's active years, best statistical year and key metrics.

        Args:
            name: player name

        Returns:
            active, best, metrics
        """

        if name in self.names:
            # Get player stats
            stats = self.stats[self.stats["playerID"] == self.names[name][0]]

            # Build key metrics
            metrics = stats[["yearID", self.metric()]]

            # Get best year, sort by primary metric
            best = int(stats.sort_values(by=self.metric(), ascending=False)["yearID"].iloc[0])

            # Get years active, best year, along with metric trends
            return metrics["yearID"].tolist(), best, metrics

        return range(1871, datetime.datetime.today().year), 1950, None

    def search(self, name=None, year=None, row=None, limit=10):
        """
        Runs an embeddings search. This method takes either a player-year or stats row as input.

        Args:
            name: player name to search
            year: year to search
            row: row of stats to search
            limit: max results to return

        Returns:
            list of results
        """

        if row:
            query = self.vector(row)
        else:
            # Lookup player key and build vector id
            name = self.names.get(name)
            query = f"{year}{name[0] if name else name}"
            query = self.vectors.get(query)

        results, ids = [], set()
        if query is not None:
            for uid, _ in self.embeddings.search(query, limit * 5):
                # Only add unique players
                if uid[4:] not in ids:
                    result = self.data[uid].copy()
                    result["link"] = f'https://www.baseball-reference.com/players/{result["nameLast"].lower()[0]}/{result["bbrefID"]}.shtml'
                    results.append(result)
                    ids.add(uid[4:])

                    if len(ids) >= limit:
                        break

        return results

    def transform(self, row):
        """
        Transforms a stats row into a vector.

        Args:
            row: stats row

        Returns:
            vector
        """

        if isinstance(row, np.ndarray):
            return row

        return np.array([0.0 if not row[x] or np.isnan(row[x]) else row[x] for x in self.columns])


class Batting(Stats):
    """
    Batting stats.
    """

    def loadcolumns(self):
        return [
            "birthMonth",
            "yearID",
            "age",
            "height",
            "weight",
            "G",
            "AB",
            "R",
            "H",
            "1B",
            "2B",
            "3B",
            "HR",
            "RBI",
            "SB",
            "CS",
            "BB",
            "SO",
            "IBB",
            "HBP",
            "SH",
            "SF",
            "GIDP",
            "POS",
            "AVG",
            "OBP",
            "TB",
            "SLG",
            "OPS",
            "OPS+",
        ]

    def load(self):
        # Retrieve raw data from GitHub
        players = pd.read_csv("https://raw.githubusercontent.com/chadwickbureau/baseballdatabank/master/core/People.csv")
        batting = pd.read_csv("https://raw.githubusercontent.com/chadwickbureau/baseballdatabank/master/core/Batting.csv")
        fielding = pd.read_csv("https://raw.githubusercontent.com/chadwickbureau/baseballdatabank/master/core/Fielding.csv")

        # Merge player data in
        batting = pd.merge(players, batting, how="inner", on=["playerID"])

        # Require player to have at least 350 plate appearances.
        batting = batting[((batting["AB"] + batting["BB"]) >= 350) & (batting["stint"] == 1)]

        # Derive primary player positions
        positions = self.positions(fielding)

        # Calculated columns
        batting["age"] = batting["yearID"] - batting["birthYear"]
        batting["POS"] = batting.apply(lambda row: self.position(positions, row), axis=1)
        batting["AVG"] = batting["H"] / batting["AB"]
        batting["OBP"] = (batting["H"] + batting["BB"]) / (batting["AB"] + batting["BB"])
        batting["1B"] = batting["H"] - batting["2B"] - batting["3B"] - batting["HR"]
        batting["TB"] = batting["1B"] + 2 * batting["2B"] + 3 * batting["3B"] + 4 * batting["HR"]
        batting["SLG"] = batting["TB"] / batting["AB"]
        batting["OPS"] = batting["OBP"] + batting["SLG"]
        batting["OPS+"] = 100 + (batting["OPS"] - batting["OPS"].mean()) * 100

        return batting

    def metric(self):
        return "OPS+"

    def vector(self, row):
        row["TB"] = row["1B"] + 2 * row["2B"] + 3 * row["3B"] + 4 * row["HR"]
        row["AVG"] = row["H"] / row["AB"]
        row["OBP"] = (row["H"] + row["BB"]) / (row["AB"] + row["BB"])
        row["SLG"] = row["TB"] / row["AB"]
        row["OPS"] = row["OBP"] + row["SLG"]
        row["OPS+"] = 100 + (row["OPS"] - self.stats["OPS"].mean()) * 100

        return self.transform(row)

    def positions(self, fielding):
        """
        Derives primary positions for players.

        Args:
            fielding: fielding data

        Returns:
            {player id: (position, number of games)}
        """

        positions = {}
        for _, row in fielding.iterrows():
            uid = f'{row["yearID"]}{row["playerID"]}'
            position = row["POS"] if row["POS"] else 0
            if position == "P":
                position = 1
            elif position == "C":
                position = 2
            elif position == "1B":
                position = 3
            elif position == "2B":
                position = 4
            elif position == "3B":
                position = 5
            elif position == "SS":
                position = 6
            elif position == "OF":
                position = 7

            # Save position if not set or player played more at this position
            if uid not in positions or positions[uid][1] < row["G"]:
                positions[uid] = (position, row["G"])

        return positions

    def position(self, positions, row):
        """
        Looks up primary position for player row.

        Arg:
            positions: all player positions
            row: player row

        Returns:
            primary player positions
        """

        uid = f'{row["yearID"]}{row["playerID"]}'
        return positions[uid][0] if uid in positions else 0


class Pitching(Stats):
    """
    Pitching stats.
    """

    def loadcolumns(self):
        return [
            "birthMonth",
            "yearID",
            "age",
            "height",
            "weight",
            "W",
            "L",
            "G",
            "GS",
            "CG",
            "SHO",
            "SV",
            "IPouts",
            "H",
            "ER",
            "HR",
            "BB",
            "SO",
            "BAOpp",
            "ERA",
            "IBB",
            "WP",
            "HBP",
            "BK",
            "BFP",
            "GF",
            "R",
            "SH",
            "SF",
            "GIDP",
            "WHIP",
            "WADJ",
        ]

    def load(self):
        # Retrieve raw data from GitHub
        players = pd.read_csv("https://raw.githubusercontent.com/chadwickbureau/baseballdatabank/master/core/People.csv")
        pitching = pd.read_csv("https://raw.githubusercontent.com/chadwickbureau/baseballdatabank/master/core/Pitching.csv")

        # Merge player data in
        pitching = pd.merge(players, pitching, how="inner", on=["playerID"])

        # Require player to have 20 appearances
        pitching = pitching[(pitching["G"] >= 20) & (pitching["stint"] == 1)]

        # Calculated columns
        pitching["age"] = pitching["yearID"] - pitching["birthYear"]
        pitching["WHIP"] = (pitching["BB"] + pitching["H"]) / (pitching["IPouts"] / 3)
        pitching["WADJ"] = (pitching["W"] + pitching["SV"]) / (pitching["ERA"] + pitching["WHIP"])

        return pitching

    def metric(self):
        return "WADJ"

    def vector(self, row):
        row["WHIP"] = (row["BB"] + row["H"]) / (row["IPouts"] / 3) if row["IPouts"] else None
        row["WADJ"] = (row["W"] + row["SV"]) / (row["ERA"] + row["WHIP"]) if row["ERA"] and row["WHIP"] else None

        return self.transform(row)


class Application:
    """
    Main application.
    """

    def __init__(self):
        """
        Creates a new application.
        """

        # Batting stats
        self.batting = Batting()

        # Pitching stats
        self.pitching = Pitching()

    def run(self):
        """
        Runs a Streamlit application.
        """

        st.title("âš¾ Baseball Statistics")
        st.markdown(
            """
            This application finds the best matching historical players using vector search with [txtai](https://github.com/neuml/txtai).
            Raw data is from the [Baseball Databank](https://github.com/chadwickbureau/baseballdatabank) GitHub project. Read [this
            article](https://medium.com/neuml/explore-baseball-history-with-vector-search-5778d98d6846) for more details.
        """
        )

        player, search = st.tabs(["Player", "Search"])

        # Player tab
        with player:
            self.player()

        # Search
        with search:
            self.search()

    def player(self):
        """
        Player tab.
        """

        st.markdown("Match by player-season. Each player search defaults to the best season sorted by OPS or Wins Adjusted.")

        # Get parameters
        params = self.params()

        # Category and stats
        category = self.category(params.get("category"), "category")
        stats = self.batting if category == "Batting" else self.pitching

        # Player name
        name = self.name(stats.names, params.get("name"))

        # Player metrics
        active, best, metrics = stats.metrics(name)

        # Player year
        year = self.year(active, params.get("year"), best)

        # Display metrics chart
        if len(active) > 1:
            self.chart(category, metrics)

        # Run search
        results = stats.search(name, year)

        # Display results
        self.table(results, ["link", "nameFirst", "nameLast", "teamID"] + stats.columns[1:])

        # Save parameters
        st.experimental_set_query_params(category=category, name=name, year=year)

    def search(self):
        """
        Stats search tab.
        """

        st.markdown("Find players with similar statistics.")

        stats, category = None, self.category("Batting", "searchcategory")
        with st.form("search"):
            if category == "Batting":
                stats, columns = self.batting, self.batting.columns[:-6]
            elif category == "Pitching":
                stats, columns = self.pitching, self.pitching.columns[:-2]

            # Enter stats with data editor
            inputs = st.data_editor(pd.DataFrame([dict((column, None) for column in columns)]), hide_index=True).astype(float)

            submitted = st.form_submit_button("Search")
            if submitted:
                # Run search
                results = stats.search(row=inputs.to_dict(orient="records")[0])

                # Display table
                self.table(results, ["link", "nameFirst", "nameLast", "teamID"] + stats.columns[1:])

    def params(self):
        """
        Get application parameters. This method combines URL parameters with session parameters.

        Returns:
            parameters
        """

        # Get parameters
        params = st.experimental_get_query_params()
        params = {x: params[x][0] for x in params}

        # Sync parameters with session state
        if all(x in st.session_state for x in ["category", "name", "year"]):
            # Copy session year if category and name are unchanged
            params["year"] = str(st.session_state["year"]) if all(params.get(x) == st.session_state[x] for x in ["category", "name"]) else None

            # Copy category and name from session state
            params["category"] = st.session_state["category"]
            params["name"] = st.session_state["name"]

        return params

    def category(self, category, key):
        """
        Builds category input widget.

        Args:
            category: category parameter
            key: widget key

        Returns:
            category component
        """

        # List of stat categories
        categories = ["Batting", "Pitching"]

        # Get category parameter, default if not available or valid
        default = categories.index(category) if category and category in categories else 0

        # Radio box component
        return st.radio("Stat", categories, index=default, horizontal=True, key=key)

    def name(self, names, name):
        """
        Builds name input widget.

        Args:
            names: list of all allowable names

        Returns:
            name component
        """

        # Get name parameter, default to random weighted value if not valid
        name = name if name and name in names else random.choices(list(names.keys()), weights=[names[x][1] for x in names])[0]

        # Sort names for display
        names = sorted(names)

        # Select box component
        return st.selectbox("Name", names, names.index(name), key="name")

    def year(self, years, year, best):
        """
        Builds year input widget.

        Args:
            years: active years for a player
            year: year parameter
            best: default to best year if year is invalid

        Returns:
            year component
        """

        # Get year parameter, default if not available or valid
        year = int(year) if year and year.isdigit() and int(year) in years else best

        # Slider component
        return int(st.select_slider("Year", years, year, key="year") if len(years) > 1 else years[0])

    def chart(self, category, metrics):
        """
        Displays a metric chart.

        Args:
            category: Batting or Pitching
            metrics: player metrics to plot
        """

        # Key metric
        metric = self.batting.metric() if category == "Batting" else self.pitching.metric()

        # Cast year to string
        metrics["yearID"] = metrics["yearID"].astype(str)

        # Metric over years
        chart = (
            alt.Chart(metrics)
            .mark_line(interpolate="monotone", point=True, strokeWidth=2.5, opacity=0.75)
            .encode(x=alt.X("yearID", title=""), y=alt.Y(metric, scale=alt.Scale(zero=False)))
        )

        # Create metric median rule line
        rule = alt.Chart(metrics).mark_rule(color="gray", strokeDash=[3, 5], opacity=0.5).encode(y=f"median({metric})")

        # Layered chart configuration
        chart = (chart + rule).encode(y=alt.Y(title=metric)).properties(height=200).configure_axis(grid=False)

        # Draw chart
        st.altair_chart(chart + rule, theme="streamlit", use_container_width=True)

    def table(self, results, columns):
        """
        Displays a list of results as a table.

        Args:
            results: list of results
            columns: column names
        """

        if results:
            st.dataframe(
                results,
                column_order=columns,
                column_config={
                    "link": st.column_config.LinkColumn("Link", width="small"),
                    "yearID": st.column_config.NumberColumn("Year", format="%d"),
                    "nameFirst": "First",
                    "nameLast": "Last",
                    "teamID": "Team",
                    "age": "Age",
                    "weight": "Weight",
                    "height": "Height",
                },
            )
        else:
            st.write("Player-Year not found")


@st.cache_resource(show_spinner=False)
def create():
    """
    Creates and caches a Streamlit application.

    Returns:
        Application
    """

    return Application()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = create()
    app.run()



================================================
FILE: examples/benchmarks.py
================================================
"""
Runs benchmark evaluations with the BEIR dataset.

Install txtai and the following dependencies to run:
    pip install txtai pytrec_eval rank-bm25 bm25s elasticsearch psutil
"""

import argparse
import csv
import json
import os
import pickle
import sqlite3
import time

import psutil
import yaml

import numpy as np

from bm25s import BM25 as BM25Sparse
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from pytrec_eval import RelevanceEvaluator
from rank_bm25 import BM25Okapi
from tqdm.auto import tqdm

from txtai.embeddings import Embeddings
from txtai.pipeline import LLM, RAG, Similarity, Tokenizer
from txtai.scoring import ScoringFactory


class Index:
    """
    Base index definition. Defines methods to index and search a dataset.
    """

    def __init__(self, path, config, output, refresh):
        """
        Creates a new index.

        Args:
            path: path to dataset
            config: path to config file
            output: path to store index
            refresh: overwrites existing index if True, otherwise existing index is loaded
        """

        self.path = path
        self.config = config
        self.output = output
        self.refresh = refresh

        # Build and save index
        self.backend = self.index()

    def __call__(self, limit, filterscores=True):
        """
        Main evaluation logic. Loads an index, runs the dataset queries and returns the results.

        Args:
            limit: maximum results
            filterscores: if exact matches should be filtered out

        Returns:
            search results
        """

        uids, queries = self.load()

        # Run queries in batches
        offset, results = 0, {}
        for batch in self.batch(queries, 256):
            for i, r in enumerate(self.search(batch, limit + 1)):
                # Get result as list of (id, score) tuples
                r = list(r)
                r = [(x["id"], x["score"]) for x in r] if r and isinstance(r[0], dict) else r

                if filterscores:
                    r = [(uid, score) for uid, score in r if uid != uids[offset + i]][:limit]

                results[uids[offset + i]] = dict(r)

            # Increment offset
            offset += len(batch)

        return results

    def search(self, queries, limit):
        """
        Runs a search for a set of queries.

        Args:
            queries: list of queries to run
            limit: maximum results

        Returns:
            search results
        """

        return self.backend.batchsearch(queries, limit)

    def index(self):
        """
        Indexes a dataset.
        """

        raise NotImplementedError

    def rows(self):
        """
        Iterates over the dataset yielding a row at a time for indexing.
        """

        # Data file
        path = f"{self.path}/corpus.jsonl"

        # Get total count
        with open(path, encoding="utf-8") as f:
            total = sum(1 for _ in f)

        # Yield data
        with open(path, encoding="utf-8") as f:
            for line in tqdm(f, total=total):
                row = json.loads(line)
                text = f'{row["title"]}. {row["text"]}' if row.get("title") else row["text"]
                if text:
                    yield (row["_id"], text, None)

    def load(self):
        """
        Loads queries for the dataset. Returns a list of expected result ids and input queries.

        Returns:
            (result ids, input queries)
        """

        with open(f"{self.path}/queries.jsonl", encoding="utf-8") as f:
            data = [json.loads(query) for query in f]
            uids, queries = [x["_id"] for x in data], [x["text"] for x in data]

        return uids, queries

    def batch(self, data, size):
        """
        Splits data into equal sized batches.

        Args:
            data: input data
            size: batch size

        Returns:
            data split into equal size batches
        """

        return [data[x : x + size] for x in range(0, len(data), size)]

    def readconfig(self, key, default):
        """
        Reads configuration from a config file. Returns default configuration
        if config file is not found or config key isn't present.

        Args:
            key: configuration key to lookup
            default: default configuration

        Returns:
            config if found, otherwise returns default config
        """

        if self.config and os.path.exists(self.config):
            # Read configuration
            with open(self.config, "r", encoding="utf-8") as f:
                # Check for config
                config = yaml.safe_load(f)
                if key in config:
                    return config[key]

        return default


class Embed(Index):
    """
    Embeddings index using txtai.
    """

    def index(self):
        if os.path.exists(self.output) and not self.refresh:
            embeddings = Embeddings()
            embeddings.load(self.output)
        else:
            # Read configuration
            config = self.readconfig("embeddings", {"batch": 8192, "encodebatch": 128, "faiss": {"quantize": True, "sample": 0.05}})

            # Build index
            embeddings = Embeddings(config)
            embeddings.index(self.rows())
            embeddings.save(self.output)

        return embeddings


class Hybrid(Index):
    """
    Hybrid embeddings + BM25 index using txtai.
    """

    def index(self):
        if os.path.exists(self.output) and not self.refresh:
            embeddings = Embeddings()
            embeddings.load(self.output)
        else:
            # Read configuration
            config = self.readconfig(
                "hybrid",
                {
                    "batch": 8192,
                    "encodebatch": 128,
                    "faiss": {"quantize": True, "sample": 0.05},
                    "scoring": {"method": "bm25", "terms": True, "normalize": True},
                },
            )

            # Build index
            embeddings = Embeddings(config)
            embeddings.index(self.rows())
            embeddings.save(self.output)

        return embeddings


class RetrievalAugmentedGeneration(Embed):
    """
    Retrieval augmented generation (RAG) using txtai.
    """

    def __init__(self, path, config, output, refresh):
        # Parent logic
        super().__init__(path, config, output, refresh)

        # Read LLM configuration
        llm = self.readconfig("llm", {})

        # Read RAG configuration
        rag = self.readconfig("rag", {})

        # Load RAG pipeline
        self.rag = RAG(self.backend, LLM(**llm), output="reference", **rag)

    def search(self, queries, limit):
        # Set context window size to limit and run
        self.rag.context = limit
        return [[(x["reference"], 1)] for x in self.rag(queries, maxlength=4096)]


class Score(Index):
    """
    BM25 index using txtai.
    """

    def index(self):
        # Read configuration
        config = self.readconfig("scoring", {"method": "bm25", "terms": True})

        # Create scoring instance
        scoring = ScoringFactory.create(config)

        output = os.path.join(self.output, "scoring")
        if os.path.exists(output) and not self.refresh:
            scoring.load(output)
        else:
            scoring.index(self.rows())
            scoring.save(output)

        return scoring


class Similar(Index):
    """
    Search data using a similarity pipeline.
    """

    def index(self):
        # Load similarity pipeline
        model = Similarity(**self.readconfig("similar", {}))

        # Get datasets
        data = list(self.rows())
        ids = [x[0] for x in data]
        texts = [x[1] for x in data]

        # Encode texts
        data = model.encode(texts, "data")

        return (ids, data, model)

    def search(self, queries, limit):
        # Unpack backend
        ids, data, model = self.backend

        # Run model inference
        results = []
        for result in model(queries, data, limit=limit):
            results.append([(ids[x], score) for x, score in result])

        return results


class Rerank(Embed):
    """
    Embeddings index using txtai combined with a similarity pipeline
    """

    def index(self):
        # Build embeddings index
        embeddings = super().index()

        # Combine similar pipeline with embeddings
        model = Similar(self.path, self.config, self.output, self.refresh)
        return model.index() + (embeddings,)

    def search(self, queries, limit):
        # Unpack backend
        ids, data, model, embeddings = self.backend

        # Run initial query
        indices = []
        for r in embeddings.batchsearch(queries, limit * 10):
            indices.append({x: ids.index(uid) for x, (uid, _) in enumerate(r)})

        # Run model inference
        results = []
        for x, query in enumerate(queries):
            queue = data[list(indices[x].values())]
            if len(queue) > 0:
                result = model(query, queue, limit=limit)
                results.append([(ids[indices[x][i]], score) for i, score in result])

        return results


class RankBM25(Index):
    """
    BM25 index using rank-bm25.
    """

    def search(self, queries, limit):
        ids, backend = self.backend
        tokenizer, results = Tokenizer(), []
        for query in queries:
            scores = backend.get_scores(tokenizer(query))
            topn = np.argsort(scores)[::-1][:limit]
            results.append([(ids[x], scores[x]) for x in topn])

        return results

    def index(self):
        output = os.path.join(self.output, "rank")
        if os.path.exists(output) and not self.refresh:
            with open(output, "rb") as f:
                ids, model = pickle.load(f)
        else:
            # Tokenize data
            tokenizer, data = Tokenizer(), []
            for uid, text, _ in self.rows():
                data.append((uid, tokenizer(text)))

            ids = [uid for uid, _ in data]
            model = BM25Okapi([text for _, text in data])

            # Save model
            with open(output, "wb") as out:
                pickle.dump(model, out)

        return ids, model


class BM25S(Index):
    """
    BM25 as implemented by bm25s
    """

    def __init__(self, path, config, output, refresh):
        # Corpus ids
        self.ids = None

        # Parent logic
        super().__init__(path, config, output, refresh)

    def search(self, queries, limit):
        tokenizer = Tokenizer()
        results, scores = self.backend.retrieve([tokenizer(x) for x in queries], corpus=self.ids, k=limit)

        # List of queries => list of matches (id, score)
        x = []
        for a, b in zip(results, scores):
            x.append([(str(c), float(d)) for c, d in zip(a, b)])

        return x

    def index(self):
        tokenizer = Tokenizer()
        ids, texts = [], []

        for uid, text, _ in self.rows():
            ids.append(uid)
            texts.append(text)

        self.ids = ids

        if os.path.exists(self.output) and not self.refresh:
            model = BM25Sparse.load(self.output)
        else:
            model = BM25Sparse(method="lucene", k1=1.2, b=0.75)
            model.index([tokenizer(x) for x in texts], leave_progress=False)
            model.save(self.output)

        return model


class SQLiteFTS(Index):
    """
    BM25 index using SQLite's FTS extension.
    """

    def search(self, queries, limit):
        tokenizer, results = Tokenizer(), []
        for query in queries:
            query = tokenizer(query)
            query = " OR ".join([f'"{q}"' for q in query])

            self.backend.execute(
                f"SELECT id, bm25(textindex) * -1 score FROM textindex WHERE text MATCH ? ORDER BY bm25(textindex) LIMIT {limit}", [query]
            )

            results.append(list(self.backend))

        return results

    def index(self):
        if os.path.exists(self.output) and not self.refresh:
            # Load existing database
            connection = sqlite3.connect(self.output)
        else:
            # Delete existing database
            if os.path.exists(self.output):
                os.remove(self.output)

            # Create new database
            connection = sqlite3.connect(self.output)

            # Tokenize data
            tokenizer, data = Tokenizer(), []
            for uid, text, _ in self.rows():
                data.append((uid, " ".join(tokenizer(text))))

            # Create table
            connection.execute("CREATE VIRTUAL TABLE textindex using fts5(id, text)")

            # Load data and build index
            connection.executemany("INSERT INTO textindex VALUES (?, ?)", data)

            connection.commit()

        return connection.cursor()


class Elastic(Index):
    """
    BM25 index using Elasticsearch.
    """

    def search(self, queries, limit):
        # Generate bulk queries
        request = []
        for query in queries:
            req_head = {"index": "textindex", "search_type": "dfs_query_then_fetch"}
            req_body = {
                "_source": False,
                "query": {"multi_match": {"query": query, "type": "best_fields", "fields": ["text"], "tie_breaker": 0.5}},
                "size": limit,
            }
            request.extend([req_head, req_body])

        # Run ES query
        response = self.backend.msearch(body=request, request_timeout=600)

        # Read responses
        results = []
        for resp in response["responses"]:
            result = resp["hits"]["hits"]
            results.append([(r["_id"], r["_score"]) for r in result])

        return results

    def index(self):
        es = Elasticsearch("http://localhost:9200")

        # Delete existing index
        # pylint: disable=W0702
        try:
            es.indices.delete(index="textindex")
        except:
            pass

        bulk(es, ({"_index": "textindex", "_id": uid, "text": text} for uid, text, _ in self.rows()))
        es.indices.refresh(index="textindex")

        return es


def relevance(path):
    """
    Loads relevance data for evaluation.

    Args:
        path: path to dataset test file

    Returns:
        relevance data
    """

    rel = {}
    with open(f"{path}/qrels/test.tsv", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter="\t", quoting=csv.QUOTE_MINIMAL)
        next(reader)

        for row in reader:
            queryid, corpusid, score = row[0], row[1], int(row[2])
            if queryid not in rel:
                rel[queryid] = {corpusid: score}
            else:
                rel[queryid][corpusid] = score

    return rel


def create(method, path, config, output, refresh):
    """
    Creates a new index.

    Args:
        method: indexing method
        path: path to dataset
        config: path to config file
        output: path to store index
        refresh: overwrites existing index if True, otherwise existing index is loaded

    Returns:
        Index
    """

    if method == "hybrid":
        return Hybrid(path, config, output, refresh)
    if method == "rag":
        return RetrievalAugmentedGeneration(path, config, output, refresh)
    if method == "scoring":
        return Score(path, config, output, refresh)
    if method == "rank":
        return RankBM25(path, config, output, refresh)
    if method == "bm25s":
        return BM25S(path, config, output, refresh)
    if method == "sqlite":
        return SQLiteFTS(path, config, output, refresh)
    if method == "es":
        return Elastic(path, config, output, refresh)
    if method == "similar":
        return Similar(path, config, output, refresh)
    if method == "rerank":
        return Rerank(path, config, output, refresh)

    # Default
    return Embed(path, config, output, refresh)


def compute(results):
    """
    Computes metrics using the results from an evaluation run.

    Args:
        results: evaluation results

    Returns:
        metrics
    """

    metrics = {}
    for r in results:
        for metric in results[r]:
            if metric not in metrics:
                metrics[metric] = []

            metrics[metric].append(results[r][metric])

    return {metric: round(np.mean(values), 5) for metric, values in metrics.items()}


def evaluate(methods, path, args):
    """
    Runs an evaluation.

    Args:
        methods: list of indexing methods to test
        path: path to dataset
        args: command line arguments

    Returns:
        {calculated performance metrics}
    """

    print(f"------ {os.path.basename(path)} ------")

    # Performance stats
    performance = {}

    # Calculate stats for each model type
    topk = args.topk
    evaluator = RelevanceEvaluator(relevance(path), {f"ndcg_cut.{topk}", f"map_cut.{topk}", f"recall.{topk}", f"P.{topk}"})
    for method in methods:
        # Stats for this source
        stats = {}
        performance[method] = stats

        # Create index and get results
        start = time.time()
        output = args.output if args.output else f"{path}/{method}"
        index = create(method, path, args.config, output, args.refresh)

        # Add indexing metrics
        stats["index"] = round(time.time() - start, 2)
        stats["memory"] = int(psutil.Process().memory_info().rss / (1024 * 1024))
        stats["disk"] = int(sum(d.stat().st_size for d in os.scandir(output) if d.is_file()) / 1024) if os.path.isdir(output) else 0

        print("INDEX TIME =", time.time() - start)
        print(f"MEMORY USAGE = {stats['memory']} MB")
        print(f"DISK USAGE = {stats['disk']} KB")

        start = time.time()
        results = index(topk)

        # Add search metrics
        stats["search"] = round(time.time() - start, 2)
        print("SEARCH TIME =", time.time() - start)

        # Calculate stats
        metrics = compute(evaluator.evaluate(results))

        # Add accuracy metrics
        for stat in [f"ndcg_cut_{topk}", f"map_cut_{topk}", f"recall_{topk}", f"P_{topk}"]:
            stats[stat] = metrics[stat]

        # Print model stats
        print(f"------ {method} ------")
        print(f"NDCG@{topk} =", metrics[f"ndcg_cut_{topk}"])
        print(f"MAP@{topk} =", metrics[f"map_cut_{topk}"])
        print(f"Recall@{topk} =", metrics[f"recall_{topk}"])
        print(f"P@{topk} =", metrics[f"P_{topk}"])

    print()
    return performance


def benchmarks(args):
    """
    Main benchmark execution method.

    Args:
        args: command line arguments
    """

    # Directory where BEIR datasets are stored
    directory = args.directory if args.directory else "beir"

    if args.sources and args.methods:
        sources, methods = args.sources.split(","), args.methods.split(",")
        mode = "a"
    else:
        # Default sources and methods
        sources = [
            "trec-covid",
            "nfcorpus",
            "nq",
            "hotpotqa",
            "fiqa",
            "arguana",
            "webis-touche2020",
            "quora",
            "dbpedia-entity",
            "scidocs",
            "fever",
            "climate-fever",
            "scifact",
        ]
        methods = ["embed", "hybrid", "rag", "scoring", "rank", "bm25s", "sqlite", "es", "similar", "rerank"]
        mode = "w"

    # Run and save benchmarks
    with open("benchmarks.json", mode, encoding="utf-8") as f:
        for source in sources:
            # Run evaluations
            results = evaluate(methods, f"{directory}/{source}", args)

            # Save as JSON lines output
            for method, stats in results.items():
                stats["source"] = source
                stats["method"] = method
                stats["name"] = args.name if args.name else method

                json.dump(stats, f)
                f.write("\n")


if __name__ == "__main__":
    # Command line parser
    parser = argparse.ArgumentParser(description="Benchmarks")
    parser.add_argument("-c", "--config", help="path to config file", metavar="CONFIG")
    parser.add_argument("-d", "--directory", help="root directory path with datasets", metavar="DIRECTORY")
    parser.add_argument("-m", "--methods", help="comma separated list of methods", metavar="METHODS")
    parser.add_argument("-n", "--name", help="name to assign to this run, defaults to method name", metavar="NAME")
    parser.add_argument("-o", "--output", help="index output directory path", metavar="OUTPUT")
    parser.add_argument(
        "-r",
        "--refresh",
        help="refreshes index if set, otherwise uses existing index if available",
        action="store_true",
    )
    parser.add_argument("-s", "--sources", help="comma separated list of data sources", metavar="SOURCES")
    parser.add_argument("-t", "--topk", help="top k results to use for the evaluation", metavar="TOPK", type=int, default=10)

    # Calculate benchmarks
    benchmarks(parser.parse_args())



================================================
FILE: examples/books.py
================================================
"""
Search application using Open Library book data. Requires the following steps to be run:

Install Streamlit
  pip install streamlit

Download and prepare data
  mkdir openlibrary && cd openlibrary
  wget -O works.txt.gz https://openlibrary.org/data/ol_dump_works_latest.txt.gz
  gunzip works.txt.gz
  grep "\"description\":" works.txt > filtered.txt

Build index
  python books.py openlibrary

Run application
  streamlit run books.py openlibrary
"""

import json
import os
import sqlite3
import sys

import pandas as pd
import streamlit as st

from txtai.embeddings import Embeddings


class Application:
    """
    Main application.
    """

    def __init__(self, path):
        """
        Creates a new application.

        Args:
            path: root path to data
        """

        self.path = path
        self.dbpath = os.path.join(self.path, "books")

    def rows(self, index):
        """
        Iterates over dataset yielding each row.

        Args:
            index: yields rows for embeddings indexing if True, otherwise yields database rows
        """

        with open(os.path.join(self.path, "filtered.txt"), encoding="utf-8") as infile:
            for x, row in enumerate(infile):
                if x % 1000 == 0:
                    print(f"Processed {x} rows", end="\r")

                row = row.split("\t")
                uid, data = row[1], json.loads(row[4])

                description = data["description"]
                if isinstance(description, dict):
                    description = description["value"]

                if "title" in data:
                    if index:
                        yield (uid, data["title"] + ". " + description, None)
                    else:
                        cover = f"{data['covers'][0]}" if "covers" in data and data["covers"] else None
                        yield (uid, data["title"], description, cover)

    def database(self):
        """
        Builds a SQLite database.
        """

        # Database file path
        dbfile = os.path.join(self.dbpath, "books.sqlite")

        # Delete existing file
        if os.path.exists(dbfile):
            os.remove(dbfile)

        # Create output database
        db = sqlite3.connect(dbfile)

        # Create database cursor
        cur = db.cursor()

        cur.execute("CREATE TABLE books (Id TEXT PRIMARY KEY, Title TEXT, Description TEXT, Cover TEXT)")

        for uid, title, description, cover in self.rows(False):
            cur.execute("INSERT INTO books (Id, Title, Description, Cover) VALUES (?, ?, ?, ?)", (uid, title, description, cover))

        # Finish and close database
        db.commit()
        db.close()

    def build(self):
        """
        Builds an embeddings index and database.
        """

        # Build embeddings index
        embeddings = Embeddings({"path": "sentence-transformers/msmarco-distilbert-base-v4"})
        embeddings.index(self.rows(True))
        embeddings.save(self.dbpath)

        # Build SQLite DB
        self.database()

    @st.cache(allow_output_mutation=True)
    def load(self):
        """
        Loads and caches embeddings index.

        Returns:
            embeddings index
        """

        embeddings = Embeddings()
        embeddings.load(self.dbpath)

        return embeddings

    def run(self):
        """
        Runs a Streamlit application.
        """

        # Build embeddings index
        embeddings = self.load()

        db = sqlite3.connect(os.path.join(self.dbpath, "books.sqlite"))
        cur = db.cursor()

        st.title("Book search")

        st.markdown(
            "This application builds a local txtai index using book data from [openlibrary.org](https://openlibrary.org). "
            + "Links to the Open Library pages and covers are shown in the application."
        )

        query = st.text_input("Search query:")
        if query:
            ids = [uid for uid, score in embeddings.search(query, 10) if score >= 0.5]

            results = []
            for uid in ids:
                cur.execute("SELECT Title, Description, Cover FROM books WHERE Id=?", (uid,))
                result = cur.fetchone()

                if result:
                    # Build cover image
                    cover = (
                        f"<img src='http://covers.openlibrary.org/b/id/{result[2]}-M.jpg'/>"
                        if result[2]
                        else "<img src='http://openlibrary.org/images/icons/avatar_book-lg.png'/>"
                    )

                    # Append book link
                    cover = f"<a target='_blank' href='https://openlibrary.org/{uid}'>{cover}</a>"
                    title = f"<a target='_blank' href='https://openlibrary.org/{uid}'>{result[0]}</a>"

                    results.append({"Cover": cover, "Title": title, "Description": result[1]})

            st.write(pd.DataFrame(results).to_html(escape=False, index=False), unsafe_allow_html=True)

        db.close()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Application is used both to index and search
    app = Application(sys.argv[1])

    # pylint: disable=W0212
    if st._is_running_with_streamlit:
        # Run application using existing index/db
        app.run()
    else:
        # Not running through streamlit, build database/index
        app.build()



================================================
FILE: examples/images.py
================================================
"""
Builds a similarity index for a directory of images

Requires streamlit to be installed.
  pip install streamlit
"""

import glob
import os
import sys

import streamlit as st

from PIL import Image

from txtai.embeddings import Embeddings


class Application:
    """
    Main application
    """

    def __init__(self, directory):
        """
        Creates a new application.

        Args:
            directory: directory of images
        """

        self.embeddings = self.build(directory)

    def build(self, directory):
        """
        Builds an image embeddings index.

        Args:
            directory: directory with images

        Returns:
            Embeddings index
        """

        embeddings = Embeddings({"method": "sentence-transformers", "path": "clip-ViT-B-32"})
        embeddings.index(self.images(directory))

        # Update model to support multilingual queries
        embeddings.config["path"] = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
        embeddings.model = embeddings.loadvectors()

        return embeddings

    def images(self, directory):
        """
        Generator that loops over each image in a directory.

        Args:
            directory: directory with images
        """

        for path in glob.glob(directory + "/*jpg") + glob.glob(directory + "/*png"):
            yield (path, Image.open(path), None)

    def run(self):
        """
        Runs a Streamlit application.
        """

        st.title("Image search")

        st.markdown("This application shows how images and text can be embedded into the same space to support similarity search. ")
        st.markdown(
            "[sentence-transformers](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/image-search) "
            + "recently added support for the [OpenAI CLIP model](https://github.com/openai/CLIP). This model embeds text and images into "
            + "the same space, enabling image similarity search. txtai can directly utilize these models."
        )

        query = st.text_input("Search query:")
        if query:
            index, _ = self.embeddings.search(query, 1)[0]
            st.image(Image.open(index))


@st.cache(allow_output_mutation=True)
def create(directory):
    """
    Creates and caches a Streamlit application.

    Args:
        directory: directory of images to index

    Returns:
        Application
    """

    return Application(directory)


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = create(sys.argv[1])
    app.run()



================================================
FILE: examples/similarity.py
================================================
"""
Basic similarity search example. Used in the original txtai demo.

Requires streamlit to be installed.
  pip install streamlit
"""

import os

import streamlit as st

from txtai.embeddings import Embeddings


class Application:
    """
    Main application.
    """

    def __init__(self):
        """
        Creates a new application.
        """

        # Create embeddings model, backed by sentence-transformers & transformers
        self.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

    def run(self):
        """
        Runs a Streamlit application.
        """

        st.title("Similarity Search")
        st.markdown("This application runs a basic similarity search that identifies the best matching row for a query.")

        data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        data = st.text_area("Data", value="\n".join(data))
        query = st.text_input("Query")

        data = data.split("\n")

        if query:
            # Get index of best section that best matches query
            uid = self.embeddings.similarity(query, data)[0][0]
            st.write(data[uid])


@st.cache(allow_output_mutation=True)
def create():
    """
    Creates and caches a Streamlit application.

    Returns:
        Application
    """

    return Application()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = create()
    app.run()



================================================
FILE: examples/wiki.py
================================================
"""
Application that queries Wikipedia API and summarizes the top result.

Requires streamlit to be installed.
  pip install streamlit
"""

import os
import urllib.parse

import requests
import streamlit as st

from txtai.pipeline import Summary


class Application:
    """
    Main application.
    """

    SEARCH_TEMPLATE = "https://en.wikipedia.org/w/api.php?action=opensearch&search=%s&limit=1&namespace=0&format=json"
    CONTENT_TEMPLATE = "https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles=%s"

    def __init__(self):
        """
        Creates a new application.
        """

        self.summary = Summary("sshleifer/distilbart-cnn-12-6")

    def run(self):
        """
        Runs a Streamlit application.
        """

        st.title("Wikipedia")
        st.markdown("This application queries the Wikipedia API and summarizes the top result.")

        query = st.text_input("Query")

        if query:
            query = urllib.parse.quote_plus(query)
            data = requests.get(Application.SEARCH_TEMPLATE % query).json()
            if data and data[1]:
                page = urllib.parse.quote_plus(data[1][0])
                content = requests.get(Application.CONTENT_TEMPLATE % page).json()
                content = list(content["query"]["pages"].values())[0]["extract"]

                st.write(self.summary(content))
                st.markdown("*Source: " + data[3][0] + "*")
            else:
                st.markdown("*No results found*")


@st.cache(allow_output_mutation=True)
def create():
    """
    Creates and caches a Streamlit application.

    Returns:
        Application
    """

    return Application()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = create()
    app.run()



================================================
FILE: examples/workflows.py
================================================
"""
Build txtai workflows.

Requires streamlit to be installed.
  pip install streamlit
"""

import contextlib
import copy
import os
import re
import tempfile
import threading
import time

import uvicorn
import yaml

import pandas as pd
import streamlit as st

import txtai.api.application
import txtai.app


class Server(uvicorn.Server):
    """
    Threaded uvicorn server used to bring up an API service.
    """

    def __init__(self, application=None, host="127.0.0.1", port=8000, log_level="info"):
        """
        Initialize server configuration.
        """

        config = uvicorn.Config(application, host=host, port=port, log_level=log_level)
        super().__init__(config)

    def install_signal_handlers(self):
        """
        Signal handlers no-op.
        """

    @contextlib.contextmanager
    def service(self):
        """
        Runs threaded server service.
        """

        # pylint: disable=W0201
        thread = threading.Thread(target=self.run)
        thread.start()
        try:
            while not self.started:
                time.sleep(1e-3)
            yield

        finally:
            self.should_exit = True
            thread.join()


class Process:
    """
    Container for an active Workflow process instance.
    """

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def get(name, config):
        """
        Lookup or creates a new workflow process instance.

        Args:
            name: workflow name
            config: application configuration

        Returns:
            Process
        """

        process = Process()

        # Build workflow
        with st.spinner("Building workflow...."):
            process.build(name, config)

        return process

    def __init__(self):
        """
        Creates a new Process.
        """

        # Application handle
        self.application = None

        # Workflow name
        self.name = None

        # Workflow data
        self.data = None

    def build(self, name, config):
        """
        Builds an application.

        Args:
            name: workflow name
            config: application configuration
        """

        # Create application
        self.application = txtai.app.Application(config)

        # Workflow name
        self.name = name

    def run(self, data):
        """
        Runs a workflow using data as input.

        Args:
            data: input data
        """

        if data and self.application:
            # Build tuples for embedding index
            if self.application.embeddings:
                data = [(x, element, None) for x, element in enumerate(data)]

            # Process workflow
            with st.spinner("Running workflow...."):
                results = []
                for result in self.application.workflow(self.name, data):
                    # Store result
                    results.append(result)

                    # Write result if this isn't an indexing workflow
                    if not self.application.embeddings:
                        st.write(result)

                # Store workflow results
                self.data = results

    def search(self, query):
        """
        Runs a search.

        Args:
            query: input query
        """

        if self.application and query:
            st.markdown(
                """
            <style>
            table td:nth-child(1) {
                display: none
            }
            table th:nth-child(1) {
                display: none
            }
            table {text-align: left !important}
            </style>
            """,
                unsafe_allow_html=True,
            )

            results = []
            for result in self.application.search(query, 5):
                # Text is only present when content is stored
                if "text" not in result:
                    uid, score = result["id"], result["score"]
                    results.append({"text": self.find(uid), "score": f"{score:.2}"})
                else:
                    if "id" in result and "text" in result:
                        result["text"] = self.content(result.pop("id"), result["text"])
                    if "score" in result and result["score"]:
                        result["score"] = f'{result["score"]:.2}'

                    results.append(result)

            df = pd.DataFrame(results)
            st.write(df.to_html(escape=False), unsafe_allow_html=True)

    def find(self, key):
        """
        Lookup record from cached data by uid key.

        Args:
            key: id to search for

        Returns:
            text for matching id
        """

        # Lookup text by id
        text = [text for uid, text, _ in self.data if uid == key][0]
        return self.content(key, text)

    def content(self, uid, text):
        """
        Builds a content reference for uid and text.

        Args:
            uid: record id
            text: record text

        Returns:
            content
        """

        if uid and isinstance(uid, str) and uid.lower().startswith("http"):
            return f"<a href='{uid}' rel='noopener noreferrer' target='blank'>{text}</a>"

        return text


class Application:
    """
    Main application.
    """

    def load(self, components):
        """
        Load an existing workflow file.

        Args:
            components: list of components to load

        Returns:
            (names of components loaded, workflow config, file changed)
        """

        workflow = st.file_uploader("Load workflow", type=["yml"])
        if workflow:
            # Detect file upload change
            upload = workflow.name != self.state("path")
            st.session_state["path"] = workflow.name

            workflow = yaml.safe_load(workflow)

            st.markdown("---")

            # Get tasks for first workflow
            tasks = list(workflow["workflow"].values())[0]["tasks"]
            selected = []

            for task in tasks:
                name = task.get("action", task.get("task"))
                if name in components:
                    selected.append(name)
                elif name in ["index", "upsert"]:
                    selected.append("embeddings")

            return (selected, workflow, upload)

        return (None, None, None)

    def state(self, key):
        """
        Lookup a session state variable.

        Args:
            key: variable key

        Returns:
            variable value
        """

        if key in st.session_state:
            return st.session_state[key]

        return None

    def appsetting(self, workflow, name):
        """
        Looks up an application configuration setting.

        Args:
            workflow: workflow configuration
            name: setting name

        Returns:
            app setting value
        """

        if workflow:
            config = workflow.get("app")
            if config:
                return config.get(name)

        return None

    def setting(self, config, name, default=None):
        """
        Looks up a component configuration setting.

        Args:
            config: component configuration
            name: setting name
            default: default setting value

        Returns:
            setting value
        """

        return config.get(name, default) if config else default

    def text(self, label, component, config, name, default=None):
        """
        Create a new text input field.

        Args:
            label: field label
            component: component name
            config: component configuration
            name: setting name
            default: default setting value

        Returns:
            text input field value
        """

        default = self.setting(config, name, default)
        if not default:
            default = ""
        elif isinstance(default, list):
            default = ",".join(default)
        elif isinstance(default, dict):
            default = ",".join(default.keys())

        return st.text_input(label, value=default, key=component + name)

    def number(self, label, component, config, name, default=None):
        """
        Creates a new numeric input field.

        Args:
            label: field label
            component: component name
            config: component configuration
            name: setting name
            default: default setting value

        Returns:
            numeric value
        """

        value = self.text(label, component, config, name, default)
        return int(value) if value else None

    def boolean(self, label, component, config, name, default=False):
        """
        Creates a new checkbox field.

        Args:
            label: field label
            component: component name
            config: component configuration
            name: setting name
            default: default setting value

        Returns:
            boolean value
        """

        default = self.setting(config, name, default)
        return st.checkbox(label, value=default, key=component + name)

    def select(self, label, component, config, name, options, default=0):
        """
        Creates a new select box field.

        Args:
            label: field label
            component: component name
            config: component configuration
            name: setting name
            options: list of dropdown options
            default: default setting value

        Returns:
            boolean value
        """

        index = self.setting(config, name)
        index = [x for x, option in enumerate(options) if option == default]

        # Derive default index
        default = index[0] if index else default

        return st.selectbox(label, options, index=default, key=component + name)

    def split(self, text):
        """
        Splits text on commas and returns a list.

        Args:
            text: input text

        Returns:
            list
        """

        return [x.strip() for x in text.split(",")]

    def options(self, component, workflow, index):
        """
        Extracts component settings into a component configuration dict.

        Args:
            component: component type
            workflow: existing workflow, can be None
            index: task index

        Returns:
            dict with component settings
        """

        # pylint: disable=R0912, R0915
        options = {"type": component}

        st.markdown("---")

        # Lookup component configuration
        #   - Runtime components have config defined within tasks
        #   - Pipeline components have config defined at workflow root
        config = None
        if workflow:
            if component in ["service", "translation"]:
                # Service config is found in tasks section
                tasks = list(workflow["workflow"].values())[0]["tasks"]
                tasks = [task for task in tasks if task.get("task") == component or task.get("action") == component]
                if tasks:
                    config = tasks[0]
            else:
                config = workflow.get(component)

        if component == "embeddings":
            st.markdown(f"**{index + 1}.) Embeddings Index**  \n*Index workflow output*")
            options["index"] = self.text("Embeddings storage path", component, config, "index")
            options["path"] = self.text("Embeddings model path", component, config, "path", "sentence-transformers/nli-mpnet-base-v2")
            options["upsert"] = self.boolean("Upsert", component, config, "upsert")
            options["content"] = self.boolean("Content", component, config, "content")

        elif component in ("segmentation", "textractor"):
            if component == "segmentation":
                st.markdown(f"**{index + 1}.) Segment**  \n*Split text into semantic units*")
            else:
                st.markdown(f"**{index + 1}.) Textract**  \n*Extract text from documents*")

            options["sentences"] = self.boolean("Split sentences", component, config, "sentences")
            options["lines"] = self.boolean("Split lines", component, config, "lines")
            options["paragraphs"] = self.boolean("Split paragraphs", component, config, "paragraphs")
            options["join"] = self.boolean("Join tokenized", component, config, "join")
            options["minlength"] = self.number("Min section length", component, config, "minlength")

        elif component == "service":
            st.markdown(f"**{index + 1}.) Service**  \n*Extract data from an API*")
            options["url"] = self.text("URL", component, config, "url")
            options["method"] = self.select("Method", component, config, "method", ["get", "post"], 0)
            options["params"] = self.text("URL parameters", component, config, "params")
            options["batch"] = self.boolean("Run as batch", component, config, "batch", True)
            options["extract"] = self.text("Subsection(s) to extract", component, config, "extract")

            if options["params"]:
                options["params"] = {key: None for key in self.split(options["params"])}
            if options["extract"]:
                options["extract"] = self.split(options["extract"])

        elif component == "summary":
            st.markdown(f"**{index + 1}.) Summary**  \n*Abstractive text summarization*")
            options["path"] = self.text("Model", component, config, "path", "sshleifer/distilbart-cnn-12-6")
            options["minlength"] = self.number("Min length", component, config, "minlength")
            options["maxlength"] = self.number("Max length", component, config, "maxlength")

        elif component == "tabular":
            st.markdown(f"**{index + 1}.) Tabular**  \n*Split tabular data into rows and columns*")
            options["idcolumn"] = self.text("Id columns", component, config, "idcolumn")
            options["textcolumns"] = self.text("Text columns", component, config, "textcolumns")
            options["content"] = self.text("Content", component, config, "content")

            if options["textcolumns"]:
                options["textcolumns"] = self.split(options["textcolumns"])

            if options["content"]:
                options["content"] = self.split(options["content"])
                if len(options["content"]) == 1 and options["content"][0] == "1":
                    options["content"] = options["content"][0]

        elif component == "transcription":
            st.markdown(f"**{index + 1}.) Transcribe**  \n*Transcribe audio to text*")
            options["path"] = self.text("Model", component, config, "path", "facebook/wav2vec2-base-960h")

        elif component == "translation":
            st.markdown(f"**{index + 1}.) Translate**  \n*Machine translation*")
            options["target"] = self.text("Target language code", component, config, "args", "en")

        return options

    def config(self, components):
        """
        Builds configuration for components

        Args:
            components: list of components to add to configuration

        Returns:
            (workflow name, configuration)
        """

        data = {}
        tasks = []
        name = None

        for component in components:
            component = dict(component)
            name = wtype = component.pop("type")

            if wtype == "embeddings":
                index = component.pop("index")
                upsert = component.pop("upsert")

                data[wtype] = component
                data["writable"] = True

                if index:
                    data["path"] = index

                name = "index"
                tasks.append({"action": "upsert" if upsert else "index"})

            elif wtype == "segmentation":
                data[wtype] = component
                tasks.append({"action": wtype})

            elif wtype == "service":
                config = {**component}
                config["task"] = wtype
                tasks.append(config)

            elif wtype == "summary":
                data[wtype] = {"path": component.pop("path")}
                tasks.append({"action": wtype})

            elif wtype == "tabular":
                data[wtype] = component
                tasks.append({"action": wtype})

            elif wtype == "textractor":
                data[wtype] = component
                tasks.append({"action": wtype, "task": "url"})

            elif wtype == "transcription":
                data[wtype] = {"path": component.pop("path")}
                tasks.append({"action": wtype, "task": "url"})

            elif wtype == "translation":
                data[wtype] = {}
                tasks.append({"action": wtype, "args": list(component.values())})

        # Add in workflow
        data["workflow"] = {name: {"tasks": tasks}}

        # Return workflow name and application configuration
        return (name, data)

    def api(self, config):
        """
        Starts an internal uvicorn server to host an API service for the current workflow.

        Args:
            config: workflow configuration as YAML string
        """

        # Generate workflow file
        workflow = os.path.join(tempfile.gettempdir(), "workflow.yml")
        with open(workflow, "w", encoding="utf-8") as f:
            f.write(config)

        os.environ["CONFIG"] = workflow
        txtai.api.application.start()
        server = Server(txtai.api.application.app)
        with server.service():
            uid = 0
            while True:
                stop = st.empty()
                click = stop.button("stop", key=uid)
                if not click:
                    time.sleep(5)
                    uid += 1
                stop.empty()

    def inputs(self, selected, workflow):
        """
        Generate process input fields.

        Args:
            selected: list of selected components
            workflow: workflow configuration

        Returns:
            True if inputs changed, False otherwise
        """

        change, query = False, None
        with st.expander("Data", expanded="embeddings" not in selected):
            default = self.appsetting(workflow, "data")
            default = default if default else ""

            data = st.text_area("Input", height=10, value=default)

            if selected and data and data != self.state("data"):
                change = True

            # Save data and workflow state
            st.session_state["data"] = data

        if "embeddings" in selected:
            default = self.appsetting(workflow, "query")
            default = default if default else ""

            # Set query and limit
            query = st.text_input("Query", value=default)

            if selected and query and query != self.state("query"):
                change = True

        # Save query state
        st.session_state["query"] = query

        return change or self.state("api") or self.state("download")

    def data(self):
        """
        Gets input data.

        Returns:
            input data
        """

        data = self.state("data")

        # Split on newlines if urls detected, allows a list of urls to be processed
        if re.match(r"^(http|https|file):\/\/", data):
            return [x for x in data.split("\n") if x]

        return [data]

    def process(self, components, index):
        """
        Processes the current application action.

        Args:
            components: workflow components
            index: True if this is an indexing workflow
        """

        # Generate application configuration
        name, config = self.config(components)

        # Get workflow process
        process = Process.get(name, copy.deepcopy(config))

        # Run workflow process
        process.run(self.data())

        # Run search
        if index:
            process.search(self.state("query"))

        return name, config

    def run(self):
        """
        Runs Streamlit application.
        """

        build = False
        with st.sidebar:
            st.image("https://github.com/neuml/txtai/raw/master/logo.png", width=256)
            st.markdown("# Workflow builder  \n*Build and apply workflows to data*  ")
            st.markdown("---")

            # Component configuration
            labels = {"segmentation": "segment", "textractor": "textract", "transcription": "transcribe", "translation": "translate"}
            components = ["embeddings", "segmentation", "service", "summary", "tabular", "textractor", "transcription", "translation"]

            selected, workflow, upload = self.load(components)
            selected = st.multiselect("Select components", components, default=selected, format_func=lambda text: labels.get(text, text))

            if selected:
                st.markdown(
                    """
                <style>
                [data-testid="stForm"] {
                    border: 0;
                    padding: 0;
                }
                </style>
                """,
                    unsafe_allow_html=True,
                )

                with st.form("workflow"):
                    # Get selected options
                    components = [self.options(component, workflow, x) for x, component in enumerate(selected)]
                    st.markdown("---")

                    # Build or re-build workflow when build button clicked or new workflow loaded
                    build = st.form_submit_button("Build", help="Build the workflow and run within this application")

        # Generate input fields
        inputs = self.inputs(selected, workflow)

        # Only execute if build button clicked, new workflow uploaded or inputs changed
        if build or upload or inputs:
            # Process current action
            name, config = self.process(components, "embeddings" in selected)

            with st.sidebar:
                with st.expander("Other Actions", expanded=True):
                    col1, col2 = st.columns(2)

                    # Add state information to configuration and export to YAML string
                    config = config.copy()
                    config.update({"app": {"data": self.state("data"), "query": self.state("query")}})
                    config = yaml.dump(config)

                    api = col1.button("API", key="api", help="Start an API instance within this application")
                    if api:
                        with st.spinner(f"Running workflow '{name}' via API service, click stop to terminate"):
                            self.api(config)

                    col2.download_button("Export", config, file_name="workflow.yml", key="download", help="Export the API workflow as YAML")


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # Create and run application
    app = Application()
    app.run()



================================================
FILE: src/python/txtai/__init__.py
================================================
"""
Base imports
"""

import logging

# Top-level imports
from .agent import Agent
from .app import Application
from .embeddings import Embeddings
from .pipeline import LLM, RAG
from .workflow import Workflow

# Configure logging per standard Python library recommendations
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())



================================================
FILE: src/python/txtai/version.py
================================================
"""
Version strings
"""

# Current version tag
__version__ = "9.2.0"



================================================
FILE: src/python/txtai/agent/__init__.py
================================================
"""
Agent imports
"""

# Conditional import
try:
    from .base import Agent
    from .factory import ProcessFactory
    from .model import PipelineModel
    from .tool import *
except ImportError:
    from .placeholder import Agent



================================================
FILE: src/python/txtai/agent/base.py
================================================
"""
Agent module
"""

from .factory import ProcessFactory


class Agent:
    """
    An agent automatically creates workflows to answer multi-faceted user requests. Agents iteratively prompt and/or interface with tools to
    step through a process and ultimately come to an answer for a request.

    Agents excel at complex tasks where multiple tools and/or methods are required. They incorporate a level of randomness similar to different
    people working on the same task. When the request is simple and/or there is a rule-based process, other methods such as RAG and Workflows
    should be explored.
    """

    def __init__(self, **kwargs):
        """
        Creates a new Agent.

        Args:
            kwargs: arguments to pass to the underlying Agent backend and LLM pipeline instance
        """

        # Ensure backwards compatibility
        if "max_iterations" in kwargs:
            kwargs["max_steps"] = kwargs.pop("max_iterations")

        # Create agent process runner
        self.process = ProcessFactory.create(kwargs)

        # Tools dictionary
        self.tools = self.process.tools

    def __call__(self, text, maxlength=8192, stream=False, **kwargs):
        """
        Runs an agent loop.

        Args:
            text: instructions to run
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            kwargs: additional keyword arguments

        Returns:
            result
        """

        # Process parameters
        self.process.model.parameters(maxlength)

        # Run agent loop
        return self.process.run(text, stream=stream, **kwargs)



================================================
FILE: src/python/txtai/agent/factory.py
================================================
"""
Factory module
"""

from smolagents import CodeAgent, ToolCallingAgent

from .model import PipelineModel
from .tool import ToolFactory


class ProcessFactory:
    """
    Methods to create agent processes.
    """

    @staticmethod
    def create(config):
        """
        Create an agent process runner. The agent process runner takes a list of tools and an LLM
        and executes an agent process flow.

        Args:
            config: agent configuration

        Returns:
            agent process runner
        """

        constructor = ToolCallingAgent
        method = config.pop("method", None)
        if method == "code":
            constructor = CodeAgent

        # Create model backed by LLM pipeline
        model = config.pop("model", config.pop("llm", None))
        model = PipelineModel(**model) if isinstance(model, dict) else PipelineModel(model)

        # Create the agent process
        return constructor(tools=ToolFactory.create(config), model=model, **config)



================================================
FILE: src/python/txtai/agent/model.py
================================================
"""
Model module
"""

import re

from enum import Enum

from smolagents import ChatMessage, Model, get_clean_message_list, tool_role_conversions
from smolagents.models import get_tool_call_from_text, remove_stop_sequences

from ..pipeline import LLM


class PipelineModel(Model):
    """
    Model backed by a LLM pipeline.
    """

    def __init__(self, path=None, method=None, **kwargs):
        """
        Creates a new LLM model.

        Args:
            path: model path or instance
            method: llm model framework, infers from path if not provided
            kwargs: model keyword arguments
        """

        self.llm = path if isinstance(path, LLM) else LLM(path, method, **kwargs)
        self.maxlength = 8192

        # Set base class parameters
        self.model_id = self.llm.generator.path

        # Call parent constructor
        super().__init__(flatten_messages_as_text=not self.llm.isvision(), **kwargs)

    # pylint: disable=W0613
    def generate(self, messages, stop_sequences=None, response_format=None, tools_to_call_from=None, **kwargs):
        """
        Runs LLM inference. This method signature must match the smolagents specification.

        Args:
            messages: list of messages to run
            stop_sequences: optional list of stop sequences
            response_format: response format to use in the model's response.
            tools_to_call_from: list of tools that the model can use to generate responses.
            kwargs: additional keyword arguments

        Returns:
            result
        """

        # Get clean message list
        messages = self.clean(messages)

        # Get LLM output
        response = self.llm(messages, maxlength=self.maxlength, stop=stop_sequences, **kwargs)

        # Remove stop sequences from LLM output
        if stop_sequences is not None:
            response = remove_stop_sequences(response, stop_sequences)

        # Load response into a chat message
        message = ChatMessage(role="assistant", content=response)

        # Extract first tool action, if necessary
        if tools_to_call_from:
            message.tool_calls = [
                get_tool_call_from_text(
                    re.sub(r".*?Action:(.*?\n\}).*", r"\1", response, flags=re.DOTALL), self.tool_name_key, self.tool_arguments_key
                )
            ]

        return message

    def parameters(self, maxlength):
        """
        Set LLM inference parameters.

        Args:
            maxlength: maximum sequence length
        """

        self.maxlength = maxlength

    def clean(self, messages):
        """
        Gets a clean message list.

        Args:
            messages: input messages

        Returns:
            clean messages
        """

        # Get clean message list
        messages = get_clean_message_list(messages, role_conversions=tool_role_conversions, flatten_messages_as_text=self.flatten_messages_as_text)

        # Ensure all roles are strings and not enums for compability across LLM frameworks
        for message in messages:
            if "role" in message:
                message["role"] = message["role"].value if isinstance(message["role"], Enum) else message["role"]

        return messages



================================================
FILE: src/python/txtai/agent/placeholder.py
================================================
"""
Placeholder module
"""


class Agent:
    """
    Agent placeholder stub for when smolagents isn't installed
    """

    def __init__(self, *args, **kwargs):
        """
        Raises an exception that smolagents isn't installed.
        """

        raise ImportError('smolagents is not available - install "agent" extra to enable')



================================================
FILE: src/python/txtai/agent/tool/__init__.py
================================================
"""
Tool imports
"""

from .embeddings import EmbeddingsTool
from .factory import ToolFactory
from .function import FunctionTool



================================================
FILE: src/python/txtai/agent/tool/embeddings.py
================================================
"""
Embeddings module
"""

from smolagents import Tool

from ...embeddings import Embeddings


class EmbeddingsTool(Tool):
    """
    Tool to execute an Embeddings search.
    """

    def __init__(self, config):
        """
        Creates a new EmbeddingsTool.

        Args:
            config: embeddings tool configuration
        """

        # Tool parameters
        self.name = config["name"]
        self.description = f"""{config['description']}. Results are returned as a list of dict elements.
Each result has keys 'id', 'text', 'score'."""

        # Input and output descriptions
        self.inputs = {"query": {"type": "string", "description": "The search query to perform."}}
        self.output_type = "any"

        # Load embeddings instance
        self.embeddings = self.load(config)

        # Validate parameters and initialize tool
        super().__init__()

    # pylint: disable=W0221
    def forward(self, query):
        """
        Runs a search.

        Args:
            query: input query

        Returns:
            search results
        """

        return self.embeddings.search(query, 5)

    def load(self, config):
        """
        Loads an embeddings instance from config.

        Args:
            config: embeddings tool configuration

        Returns:
            Embeddings
        """

        if "target" in config:
            return config["target"]

        embeddings = Embeddings()
        embeddings.load(**config)

        return embeddings



================================================
FILE: src/python/txtai/agent/tool/factory.py
================================================
"""
Factory module
"""

import inspect

from types import FunctionType, MethodType

import mcpadapt.core

from mcpadapt.smolagents_adapter import SmolAgentsAdapter
from smolagents import PythonInterpreterTool, Tool, tool as CreateTool, VisitWebpageTool, WebSearchTool
from transformers.utils import chat_template_utils, TypeHintParsingException

from ...embeddings import Embeddings
from .embeddings import EmbeddingsTool
from .function import FunctionTool


class ToolFactory:
    """
    Methods to create tools.
    """

    # Default toolkit
    DEFAULTS = {"python": PythonInterpreterTool(), "websearch": WebSearchTool(), "webview": VisitWebpageTool()}

    @staticmethod
    def create(config):
        """
        Creates a new list of tools. This method iterates of the `tools` configuration option and creates a Tool instance
        for each entry. This supports the following:

          - Tool instance
          - Dictionary with `name`, `description`, `inputs`, `output` and `target` function configuration
          - String with a tool alias name

        Returns:
            list of tools
        """

        tools = []
        for tool in config.pop("tools", []):
            # Create tool from function and it's documentation
            if not isinstance(tool, Tool) and (isinstance(tool, (FunctionType, MethodType)) or hasattr(tool, "__call__")):
                tool = ToolFactory.createtool(tool)

            # Create tool from input dictionary
            elif isinstance(tool, dict):
                # Get target function
                target = tool.get("target")

                # Create tool from input dictionary
                tool = (
                    EmbeddingsTool(tool)
                    if isinstance(target, Embeddings) or any(x in tool for x in ["container", "path"])
                    else ToolFactory.createtool(target, tool)
                )

            # Get default tool, if applicable
            elif isinstance(tool, str) and tool in ToolFactory.DEFAULTS:
                tool = ToolFactory.DEFAULTS[tool]

            # Support importing MCP tool collections
            elif isinstance(tool, str) and tool.startswith("http"):
                tools.extend(mcpadapt.core.MCPAdapt({"url": tool}, SmolAgentsAdapter()).tools())
                tool = None

            # Add tool
            if tool:
                tools.append(tool)

        return tools

    @staticmethod
    def createtool(target, config=None):
        """
        Creates a new Tool.

        Args:
            target: target object or function
            config: optional tool configuration

        Returns:
            Tool
        """

        try:
            # Try to create using CreateTool function - this fails when no annotations are available
            return CreateTool(target)
        except (TypeHintParsingException, TypeError):
            return ToolFactory.fromdocs(target, config if config else {})

    @staticmethod
    def fromdocs(target, config):
        """
        Creates a tool from method documentation.

        Args:
            target: target object or function
            config: tool configuration

        Returns:
            Tool
        """

        # Get function name and target - use target if it's a function or method, else use target.__call__
        name = target.__name__ if isinstance(target, (FunctionType, MethodType)) or not hasattr(target, "__call__") else target.__class__.__name__
        target = target if isinstance(target, (FunctionType, MethodType)) or not hasattr(target, "__call__") else target.__call__

        # Extract target documentation
        doc = inspect.getdoc(target)
        description, parameters, _ = chat_template_utils.parse_google_format_docstring(doc.strip()) if doc else (None, {}, None)

        # Get list of required parameters
        signature = inspect.signature(target)
        inputs = {}
        for pname, param in signature.parameters.items():
            if param.default == inspect.Parameter.empty and pname in parameters:
                inputs[pname] = {"type": "any", "description": parameters[pname]}

        # Create function tool
        return FunctionTool(
            {
                "name": config.get("name", name.lower()),
                "description": config.get("description", description),
                "inputs": config.get("inputs", inputs),
                "target": config.get("target", target),
            }
        )



================================================
FILE: src/python/txtai/agent/tool/function.py
================================================
"""
Function imports
"""

from smolagents import Tool


class FunctionTool(Tool):
    """
    Creates a FunctionTool. A FunctionTool takes descriptive configuration and injects it along with a target function
    into an LLM prompt.
    """

    # pylint: disable=W0231
    def __init__(self, config):
        """
        Creates a FunctionTool.

        Args:
            config: `name`, `description`, `inputs`, `output` and `target` configuration
        """

        # Tool parameters
        self.name = config["name"]
        self.description = config["description"]
        self.inputs = config["inputs"]
        self.output_type = config.get("output", config.get("output_type", "any"))
        self.target = config["target"]

        # pylint: disable=C0103
        # Skip forward signature validation
        self.skip_forward_signature_validation = True

        # Validate parameters and initialize tool
        super().__init__()

    def forward(self, *args, **kwargs):
        """
        Runs target function.

        Args:
            args: positional args
            kwargs: keyword args

        Returns:
            result
        """

        return self.target(*args, **kwargs)



================================================
FILE: src/python/txtai/ann/__init__.py
================================================
"""
ANN imports
"""

from .base import ANN
from .dense import *
from .sparse import *



================================================
FILE: src/python/txtai/ann/base.py
================================================
"""
ANN (Approximate Nearest Neighbor) module
"""

import datetime
import platform

from ..version import __version__


class ANN:
    """
    Base class for ANN instances. This class builds vector indexes to support similarity search.
    The built-in ANN backends store ids and vectors. Content storage is supported via database instances.
    """

    def __init__(self, config):
        """
        Creates a new ANN.

        Args:
            config: index configuration parameters
        """

        # ANN index
        self.backend = None

        # ANN configuration
        self.config = config

    def load(self, path):
        """
        Loads an ANN at path.

        Args:
            path: path to load ann index
        """

        raise NotImplementedError

    def index(self, embeddings):
        """
        Builds an ANN index.

        Args:
            embeddings: embeddings array
        """

        raise NotImplementedError

    def append(self, embeddings):
        """
        Append elements to an existing index.

        Args:
            embeddings: embeddings array
        """

        raise NotImplementedError

    def delete(self, ids):
        """
        Deletes elements from existing index.

        Args:
            ids: ids to delete
        """

        raise NotImplementedError

    def search(self, queries, limit):
        """
        Searches ANN index for query. Returns topn results.

        Args:
            queries: queries array
            limit: maximum results

        Returns:
            query results
        """

        raise NotImplementedError

    def count(self):
        """
        Number of elements in the ANN index.

        Returns:
            count
        """

        raise NotImplementedError

    def save(self, path):
        """
        Saves an ANN index at path.

        Args:
            path: path to save ann index
        """

        raise NotImplementedError

    def close(self):
        """
        Closes this ANN.
        """

        self.backend = None

    def setting(self, name, default=None):
        """
        Looks up backend specific setting.

        Args:
            name: setting name
            default: default value when setting not found

        Returns:
            setting value
        """

        # Get the backend-specific config object
        backend = self.config.get(self.config["backend"])

        # Get setting value, set default value if not found
        setting = backend.get(name) if backend else None
        return setting if setting or (backend and name in backend) else default

    def metadata(self, settings=None):
        """
        Adds index build metadata.

        Args:
            settings: index build settings
        """

        # ISO 8601 timestamp
        create = datetime.datetime.now(datetime.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

        # Set build metadata if this is not an update
        if settings:
            self.config["build"] = {
                "create": create,
                "python": platform.python_version(),
                "settings": settings,
                "system": f"{platform.system()} ({platform.machine()})",
                "txtai": __version__,
            }

        # Set last update date
        self.config["update"] = create



================================================
FILE: src/python/txtai/ann/dense/__init__.py
================================================
"""
Dense ANN imports
"""

from .annoy import Annoy
from .factory import ANNFactory
from .faiss import Faiss
from .hnsw import HNSW
from .numpy import NumPy
from .pgvector import PGVector
from .torch import Torch



================================================
FILE: src/python/txtai/ann/dense/annoy.py
================================================
"""
Annoy module
"""

# Conditional import
try:
    from annoy import AnnoyIndex

    ANNOY = True
except ImportError:
    ANNOY = False

from ..base import ANN


# pylint: disable=W0223
class Annoy(ANN):
    """
    Builds an ANN index using the Annoy library.
    """

    def __init__(self, config):
        super().__init__(config)

        if not ANNOY:
            raise ImportError('Annoy is not available - install "ann" extra to enable')

    def load(self, path):
        # Load index
        self.backend = AnnoyIndex(self.config["dimensions"], self.config["metric"])
        self.backend.load(path)

    def index(self, embeddings):
        # Inner product is equal to cosine similarity on normalized vectors
        self.config["metric"] = "dot"

        # Create index
        self.backend = AnnoyIndex(self.config["dimensions"], self.config["metric"])

        # Add items - position in embeddings is used as the id
        for x in range(embeddings.shape[0]):
            self.backend.add_item(x, embeddings[x])

        # Build index
        ntrees = self.setting("ntrees", 10)
        self.backend.build(ntrees)

        # Add index build metadata
        self.metadata({"ntrees": ntrees})

    def search(self, queries, limit):
        # Lookup search k setting
        searchk = self.setting("searchk", -1)

        # Annoy doesn't have a built in batch query method
        results = []
        for query in queries:
            # Run the query
            ids, scores = self.backend.get_nns_by_vector(query, n=limit, search_k=searchk, include_distances=True)

            # Map results to [(id, score)]
            results.append(list(zip(ids, scores)))

        return results

    def count(self):
        # Number of items in index
        return self.backend.get_n_items()

    def save(self, path):
        # Write index
        self.backend.save(path)



================================================
FILE: src/python/txtai/ann/dense/factory.py
================================================
"""
Factory module
"""

from ...util import Resolver

from .annoy import Annoy
from .faiss import Faiss
from .ggml import GGML
from .hnsw import HNSW
from .numpy import NumPy
from .pgvector import PGVector
from .sqlite import SQLite
from .torch import Torch


class ANNFactory:
    """
    Methods to create ANN indexes.
    """

    @staticmethod
    def create(config):
        """
        Create an ANN.

        Args:
            config: index configuration parameters

        Returns:
            ANN
        """

        # ANN instance
        ann = None
        backend = config.get("backend", "faiss")

        # Create ANN instance
        if backend == "annoy":
            ann = Annoy(config)
        elif backend == "faiss":
            ann = Faiss(config)
        elif backend == "hnsw":
            ann = HNSW(config)
        elif backend == "ggml":
            ann = GGML(config)
        elif backend == "numpy":
            ann = NumPy(config)
        elif backend == "pgvector":
            ann = PGVector(config)
        elif backend == "sqlite":
            ann = SQLite(config)
        elif backend == "torch":
            ann = Torch(config)
        else:
            ann = ANNFactory.resolve(backend, config)

        # Store config back
        config["backend"] = backend

        return ann

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: index configuration parameters

        Returns:
            ANN
        """

        try:
            return Resolver()(backend)(config)
        except Exception as e:
            raise ImportError(f"Unable to resolve ann backend: '{backend}'") from e



================================================
FILE: src/python/txtai/ann/dense/faiss.py
================================================
"""
Faiss module
"""

import math
import platform

import numpy as np

from faiss import omp_set_num_threads
from faiss import index_factory, IO_FLAG_MMAP, METRIC_INNER_PRODUCT, read_index, write_index
from faiss import index_binary_factory, read_index_binary, write_index_binary, IndexBinaryIDMap

from ..base import ANN

if platform.system() == "Darwin":
    # Workaround for a Faiss issue causing segmentation faults on macOS. See txtai FAQ for more.
    omp_set_num_threads(1)


class Faiss(ANN):
    """
    Builds an ANN index using the Faiss library.
    """

    def __init__(self, config):
        super().__init__(config)

        # Scalar quantization
        quantize = self.config.get("quantize")
        self.qbits = quantize if quantize and isinstance(quantize, int) and not isinstance(quantize, bool) else None

    def load(self, path):
        # Get read function
        readindex = read_index_binary if self.qbits else read_index

        # Load index
        self.backend = readindex(path, IO_FLAG_MMAP if self.setting("mmap") is True else 0)

    def index(self, embeddings):
        # Compute model training size
        train, sample = embeddings, self.setting("sample")
        if sample:
            # Get sample for training
            rng = np.random.default_rng(0)
            indices = sorted(rng.choice(train.shape[0], int(sample * train.shape[0]), replace=False, shuffle=False))
            train = train[indices]

        # Configure embeddings index. Inner product is equal to cosine similarity on normalized vectors.
        params = self.configure(embeddings.shape[0], train.shape[0])

        # Create index
        self.backend = self.create(embeddings, params)

        # Train model
        self.backend.train(train)

        # Add embeddings - position in embeddings is used as the id
        self.backend.add_with_ids(embeddings, np.arange(embeddings.shape[0], dtype=np.int64))

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata({"components": params})

    def append(self, embeddings):
        new = embeddings.shape[0]

        # Append new ids - position in embeddings + existing offset is used as the id
        self.backend.add_with_ids(embeddings, np.arange(self.config["offset"], self.config["offset"] + new, dtype=np.int64))

        # Update id offset and index metadata
        self.config["offset"] += new
        self.metadata()

    def delete(self, ids):
        # Remove specified ids
        self.backend.remove_ids(np.array(ids, dtype=np.int64))

    def search(self, queries, limit):
        # Set nprobe and nflip search parameters
        self.backend.nprobe = self.nprobe()
        self.backend.nflip = self.setting("nflip", self.backend.nprobe)

        # Run the query
        scores, ids = self.backend.search(queries, limit)

        # Map results to [(id, score)]
        results = []
        for x, score in enumerate(scores):
            # Transform scores and add results
            results.append(list(zip(ids[x].tolist(), self.scores(score))))

        return results

    def count(self):
        return self.backend.ntotal

    def save(self, path):
        # Get write function
        writeindex = write_index_binary if self.qbits else write_index

        # Write index
        writeindex(self.backend, path)

    def configure(self, count, train):
        """
        Configures settings for a new index.

        Args:
            count: initial number of embeddings rows
            train: number of rows selected for model training

        Returns:
            user-specified or generated components setting
        """

        # Lookup components setting
        components = self.setting("components")

        if components:
            # Format and return components string
            return self.components(components, train)

        # Derive quantization. Prefer backend-specific setting. Fallback to root-level parameter.
        quantize = self.setting("quantize", self.config.get("quantize"))
        quantize = 8 if isinstance(quantize, bool) else quantize

        # Get storage setting
        storage = f"SQ{quantize}" if quantize else "Flat"

        # Small index, use storage directly with IDMap
        if count <= 5000:
            return "BFlat" if self.qbits else f"IDMap,{storage}"

        x = self.cells(train)
        components = f"BIVF{x}" if self.qbits else f"IVF{x},{storage}"

        return components

    def create(self, embeddings, params):
        """
        Creates a new index.

        Args:
            embeddings: embeddings to index
            params: index parameters

        Returns:
            new index
        """

        # Create binary index
        if self.qbits:
            index = index_binary_factory(embeddings.shape[1] * 8, params)

            # Wrap with BinaryIDMap, if necessary
            if any(x in params for x in ["BFlat", "BHNSW"]):
                index = IndexBinaryIDMap(index)

            return index

        # Create standard float index
        return index_factory(embeddings.shape[1], params, METRIC_INNER_PRODUCT)

    def cells(self, count):
        """
        Calculates the number of IVF cells for an IVF index.

        Args:
            count: number of embeddings rows

        Returns:
            number of IVF cells
        """

        # Calculate number of IVF cells where x = min(4 * sqrt(embeddings count), embeddings count / 39)
        # Faiss requires at least 39 points per cluster
        return max(min(round(4 * math.sqrt(count)), int(count / 39)), 1)

    def components(self, components, train):
        """
        Formats a components string. This method automatically calculates the optimal number of IVF cells, if omitted.

        Args:
            components: input components string
            train: number of rows selected for model training

        Returns:
            formatted components string
        """

        # Optimal number of IVF cells
        x = self.cells(train)

        # Add number of IVF cells, if missing
        components = [f"IVF{x}" if component == "IVF" else component for component in components.split(",")]

        # Return components string
        return ",".join(components)

    def nprobe(self):
        """
        Gets or derives the nprobe search parameter.

        Returns:
            nprobe setting
        """

        # Get size of embeddings index
        count = self.count()

        default = 6 if count <= 5000 else round(self.cells(count) / 16)
        return self.setting("nprobe", default)

    def scores(self, scores):
        """
        Calculates the index score from the input score. This method returns the hamming score
        (1.0 - (hamming distance / total number of bits)) for binary indexes and the input
        scores otherwise.

        Args:
            scores: input scores

        Returns:
            index scores
        """

        # Calculate hamming score, bound between 0.0 - 1.0
        if self.qbits:
            return np.clip(1.0 - (scores / (self.config["dimensions"] * 8)), 0.0, 1.0).tolist()

        # Standard scoring
        return scores.tolist()



================================================
FILE: src/python/txtai/ann/dense/ggml.py
================================================
"""
GGML module
"""

import ctypes
import os

import numpy as np

# Conditional import
try:
    import ggml
    from ggml import utils

    LIBGGML = True
except ImportError:
    LIBGGML = False

from ..base import ANN


class GGML(ANN):
    """
    Builds an ANN index backed by GGML.
    """

    def __init__(self, config):
        super().__init__(config)

        if not LIBGGML:
            raise ImportError('GGML is not available - install "ann" extra to enable')

    def load(self, path):
        # Create GGML Tensors
        self.backend = GGMLTensors(self.setting("gpu", True), self.setting("querysize", 64), self.setting("quantize"))

        # Load existing GGUF file
        self.backend.load(path)

    def index(self, embeddings):
        # Create GGML Tensors
        self.backend = GGMLTensors(self.setting("gpu", True), self.setting("querysize", 64), self.setting("quantize"))

        # Add embeddings data
        self.backend.index(embeddings)

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata(self.settings())

    def append(self, embeddings):
        # Append embeddings to existing tensors
        self.backend.append(embeddings)

        # Update id offset and index metadata
        self.config["offset"] += embeddings.shape[0]
        self.metadata()

    def delete(self, ids):
        self.backend.delete(ids)

    def search(self, queries, limit):
        scores = self.backend.search(queries)

        # Get topn ids
        ids = np.argsort(-scores)[:, :limit]

        # Map results to [(id, score)]
        results = []
        for x, score in enumerate(scores):
            # Add results
            results.append(list(zip(ids[x].tolist(), score[ids[x]].tolist())))

        return results

    def count(self):
        return self.backend.count()

    def save(self, path):
        self.backend.save(path)

    def close(self):
        # Cleanup resources before setting backend to None
        if self.backend:
            self.backend.close()

        # Parent logic
        super().close()

    def settings(self):
        """
        Returns settings for this instance.

        Returns:
            dict
        """

        return {"ggml": ggml.__version__}


class GGMLTensors:
    """
    Interface to read and write GGML tensor data.
    """

    def __init__(self, gpu, querysize, quantize):
        """
        Creates a new GGMLTensors.

        Args:
            gpu: if GPU should be used
            querysize: query buffer size
            quantize: data quantization setting
        """

        # Settings
        self.gpu, self.querysize, self.quantize = gpu, querysize, quantize

        # GGML parameters
        self.context, self.backend = None, None
        self.buffer, self.queries, self.data, self.deletes = None, None, None, []
        self.allocator, self.graph, self.output = None, None, None

    def __del__(self):
        """
        Ensure resources are cleaned up.
        """

        # Cleanup resources
        self.close()

    def load(self, path):
        """
        Loads GGML tensors from a GGUF file at path.

        Args:
            path: path to GGUF file
        """

        # Initialize GGML objects
        self.context = self.createcontext()
        self.backend = self.createbackend()
        self.allocator = self.createallocator(self.backend)

        # Temporary context for GGUF
        context = ctypes.c_void_p()

        # Cast as ggml_context**
        params = ggml.gguf_init_params(ctx=ctypes.pointer(context), no_alloc=False)

        # Load GGUF file
        gguf = ggml.gguf_init_from_file(path.encode("utf-8"), params)

        # Load tensors from GGUF file
        self.loadtensors(context)

        # Create graph operation
        self.graph, self.output = self.creategraph()

        # Cleanup temporary resources
        ggml.gguf_free(gguf)
        ggml.ggml_free(context)

    def index(self, embeddings):
        """
        Indexes embeddings as GGML tensors.

        Args:
            embeddings: embeddings array
        """

        # Initialize GGML objects
        self.context = self.createcontext()
        self.backend = self.createbackend()
        self.allocator = self.createallocator(self.backend)

        # Create query buffer and data tensors
        self.createtensors(embeddings)

        # Create graph operation
        self.graph, self.output = self.creategraph()

    def append(self, embeddings):
        """
        Appends embeddings to GGML tensors.

        Args:
            embeddings: embeddings array
        """

        # Initialize GGML objects
        context = self.createcontext()
        backend = self.createbackend()
        allocator = self.createallocator(backend)

        # Merge embeddings tensors
        buffer, queries, data = self.mergetensors(context, backend, embeddings)
        deletes = self.deletes

        # Free existing objects
        self.close()

        # Store new objects
        self.context, self.backend, self.allocator = (context, backend, allocator)
        self.buffer, self.queries, self.data, self.deletes = (buffer, queries, data, deletes)

        # Create graph operation
        self.graph, self.output = self.creategraph()

    def delete(self, ids):
        """
        Delete ids from tensors.

        Args:
            ids: ids to delete
        """

        shape = utils.get_shape(self.data)

        # Filter any index greater than size of array
        ids = [x for x in ids if x < shape[1]]
        self.deletes.extend(ids)

    def search(self, queries):
        """
        Searches GGML tensors for the best query matches.

        Args:
            queries: queries array

        Returns:
            query results
        """

        # Process queries up to the query buffer size batches
        batches = []
        for batch in self.chunk(queries):
            # Copy queries to buffer
            ggml.ggml_backend_tensor_set(
                self.queries,
                ctypes.cast(batch.ctypes.data, ctypes.c_void_p),
                0,
                batch.nbytes,
            )

            # Run matrix multiplication operation
            ggml.ggml_backend_graph_compute(self.backend, self.graph)

            # Get size of embeddings data
            size = utils.get_shape(self.data)[1]

            # Get and return results
            results = np.zeros((batch.shape[0], size), dtype=np.float32)
            ggml.ggml_backend_tensor_get(self.output, ctypes.cast(results.ctypes.data, ctypes.c_void_p), 0, results.nbytes)

            # Clear deleted rows and add results
            results[:, self.deletes] = 0
            batches.append(results)

        # Combine batches and return as single result
        return np.concatenate(batches, axis=0)

    def count(self):
        """
        Number of elements in this GGML tensors.

        Returns:
            count
        """

        return utils.get_shape(self.data)[1] - len(self.deletes) if self.data else 0

    def save(self, path):
        """
        Saves GGML tensors as GGUF to path.

        Args:
            path: path to save
        """

        # Temporary buffer
        buffer = None

        # Init and save data tensor
        gguf = ggml.gguf_init_empty()

        # Add the data tensor
        ggml.ggml_set_name(self.data, b"data")
        ggml.gguf_add_tensor(gguf, self.data)

        # Optionally create and add the deletes tensor
        if self.deletes:
            deletes = np.array(self.deletes, dtype=np.int64)
            tensor = ggml.ggml_new_tensor_1d(self.context, ggml.GGML_TYPE_I64, deletes.shape[0])
            buffer = ggml.ggml_backend_alloc_ctx_tensors(self.context, self.backend)

            ggml.ggml_backend_tensor_set(
                tensor,
                ctypes.cast(deletes.ctypes.data, ctypes.c_void_p),
                0,
                deletes.nbytes,
            )
            ggml.ggml_set_name(tensor, b"deletes")
            ggml.gguf_add_tensor(gguf, tensor)

        # Write file and free resources
        ggml.gguf_write_to_file(gguf, path.encode("utf-8"), False)
        ggml.gguf_free(gguf)

        if buffer:
            ggml.ggml_backend_buffer_free(buffer)

    def close(self):
        """
        Closes this instance and frees resources.
        """

        if self.buffer:
            ggml.ggml_backend_buffer_free(self.buffer)
            self.buffer, self.queries, self.data, self.deletes = None, None, None, []

        if self.allocator:
            ggml.ggml_gallocr_free(self.allocator)
            self.allocator, self.graph = None, None

        if self.backend:
            ggml.ggml_backend_free(self.backend)
            self.backend = None

        if self.context:
            # Free quantization memory
            ggml.ggml_quantize_free()

            # Free context
            ggml.ggml_free(self.context)
            self.context = None

    def createcontext(self):
        """
        Creates a new GGML context.

        Returns:
            context
        """

        # Base tensor storage
        size = ggml.ggml_tensor_overhead() * 100

        # Graph storage
        size += ggml.ggml_tensor_overhead() * ggml.GGML_DEFAULT_GRAPH_SIZE + ggml.ggml_graph_overhead()

        # Create GGML context
        params = ggml.ggml_init_params(mem_size=size, no_alloc=True)
        context = ggml.ggml_init(params)

        return context

    def createbackend(self):
        """
        Creates a new GGML backend.

        Returns:
            backend
        """

        # Attempt to create an accelerated backend
        backend = ggml.ggml_backend_init_by_type(ggml.GGML_BACKEND_DEVICE_TYPE_GPU, None) if self.gpu else None

        # Fall back to CPU backend
        if not backend:
            backend = ggml.ggml_backend_cpu_init()
            ggml.ggml_backend_cpu_set_n_threads(backend, os.cpu_count())

        return backend

    def createallocator(self, backend):
        """
        Creates a new GGML allocator.

        Args:
            backend: backend device

        Returns:
            allocator
        """

        return ggml.ggml_gallocr_new(ggml.ggml_backend_get_default_buffer_type(backend))

    def createtensors(self, data):
        """
        Creates query and data tensors.

        Args:
            data: embeddings data
        """

        # Derive embeddings data tensor type
        tensortype = self.tensortype(data)

        # Queries
        self.queries = ggml.ggml_new_tensor_2d(self.context, ggml.GGML_TYPE_F32, data.shape[1], self.querysize)

        # Embeddings data
        self.data = ggml.ggml_new_tensor_2d(self.context, tensortype, data.shape[1], data.shape[0])

        # Create buffer
        self.buffer = ggml.ggml_backend_alloc_ctx_tensors(self.context, self.backend)

        # Copy embeddings data
        self.copy(data, self.data, tensortype, 0)

    def loadtensors(self, context):
        """
        Loads existing tensors from context.

        Args:
            context: ggml context
        """

        # Load data tensor
        data = ggml.ggml_get_tensor(context, b"data")
        if data:
            # Queries
            shape = utils.get_shape(data)
            self.queries = ggml.ggml_new_tensor_2d(self.context, ggml.GGML_TYPE_F32, shape[0], self.querysize)

            # Embeddings data
            self.data = ggml.ggml_dup_tensor(self.context, data)

            # Create buffer
            self.buffer = ggml.ggml_backend_alloc_ctx_tensors(self.context, self.backend)

            # Copy tensor data to backend
            ggml.ggml_backend_tensor_set(self.data, ggml.ggml_get_data(data), 0, ggml.ggml_nbytes(data))

        # Load deletes tensor
        data = ggml.ggml_get_tensor(context, b"deletes")
        if data:
            # Convert to a NumPy array
            shape = utils.get_shape(data)
            deletes = np.ctypeslib.as_array(ctypes.cast(ggml.ggml_get_data(data), ctypes.POINTER(ctypes.c_int64)), (shape[0],))
            self.deletes = deletes.tolist()

    def mergetensors(self, context, backend, data):
        """
        Merges new embeddings data.

        Args:
            context: new context
            backend: new backend
            data: embeddings data

        Returns:
            buffer, queries, data
        """

        # Derive embeddings data tensor type
        tensortype = self.tensortype(data)

        # Queries
        queries = ggml.ggml_new_tensor_2d(context, ggml.GGML_TYPE_F32, data.shape[1], self.querysize)

        # Embeddings data with space for both existing and new data
        shape = utils.get_shape(self.data)
        merge = ggml.ggml_new_tensor_2d(context, tensortype, data.shape[1], data.shape[0] + shape[1])

        # Create new buffer
        buffer = ggml.ggml_backend_alloc_ctx_tensors(context, backend)

        # Copy existing embeddings data
        self.copy(self.data, merge, tensortype, 0)

        # Copy new embeddings data
        self.copy(data, merge, tensortype, ggml.ggml_nbytes(self.data))

        return buffer, queries, merge

    def creategraph(self):
        """
        Creates a new GGML graph.

        Returns:
            graph
        """

        # Create matrix multiply graph operation
        graph = ggml.ggml_new_graph(self.context)

        # Graph operation
        output = ggml.ggml_mul_mat(self.context, self.data, self.queries)

        # Setup and allocate graph storage
        ggml.ggml_build_forward_expand(graph, output)
        ggml.ggml_gallocr_alloc_graph(self.allocator, graph)

        return graph, output

    def tensortype(self, data):
        """
        Gets the best matching tensor type for input data.

        Args:
            data: embeddings data

        Returns:
            best matching GGML data type
        """

        # Read tensor type
        tensortype = self.quantize
        tensortype = "Q8_0" if isinstance(tensortype, bool) else f"Q{int(tensortype)}_0" if isinstance(tensortype, int) else tensortype
        tensortype = tensortype.upper() if tensortype else "F32"

        # Validate tensor type
        if not hasattr(ggml, f"GGML_TYPE_{tensortype}"):
            raise ValueError(f"Invalid tensor type {tensortype}")

        # Get tensor type
        tensortype = getattr(ggml, f"GGML_TYPE_{tensortype}")

        # Validate quantization block size
        blocksize = ggml.ggml_blck_size(tensortype)
        if data.shape[1] % blocksize != 0:
            raise ValueError(
                f'Invalid quantization configuration "{self.quantize}" with {data.shape[1]} dimensions. Must be a multiple of {blocksize}.'
            )

        return tensortype

    def copy(self, inputs, outputs, tensortype, offset):
        """
        Copies input data to backend. Quantizes to desired tensor type, if necessary.

        Args:
            inputs: input tensor
            outputs: output tensor
            tensortype: desired tensor type
            offset: data offset index for storage into outputs
        """

        if not isinstance(inputs, np.ndarray):
            # GGML tensor
            work, size = ggml.ggml_get_data(inputs), ggml.ggml_nbytes(inputs)
        elif tensortype == ggml.GGML_TYPE_F32:
            # No quantization needed
            work, size = inputs.ctypes.data, inputs.nbytes
        else:
            # Work array will be garbage collected by Python
            work = (ctypes.c_float * inputs.shape[0] * inputs.shape[1])()

            # Quantize vector data
            size = ggml.ggml_quantize_chunk(
                tensortype, ctypes.cast(inputs.ctypes.data, ctypes.POINTER(ctypes.c_float)), work, 0, inputs.shape[0], inputs.shape[1], None
            )

        # Copy data to tensor
        ggml.ggml_backend_tensor_set(outputs, work, offset, size)

    def chunk(self, queries):
        """
        Splits quries into separate batch sizes specified by size.

        Args:
            queries: queries

        Returns:
            list of evenly sized batches with the last batch having the remaining elements
        """

        return [queries[x : x + self.querysize] for x in range(0, len(queries), self.querysize)]



================================================
FILE: src/python/txtai/ann/dense/hnsw.py
================================================
"""
HNSW module
"""

import numpy as np

# Conditional import
try:
    # pylint: disable=E0611
    from hnswlib import Index

    HNSWLIB = True
except ImportError:
    HNSWLIB = False

from ..base import ANN


class HNSW(ANN):
    """
    Builds an ANN index using the hnswlib library.
    """

    def __init__(self, config):
        super().__init__(config)

        if not HNSWLIB:
            raise ImportError('HNSW is not available - install "ann" extra to enable')

    def load(self, path):
        # Load index
        self.backend = Index(dim=self.config["dimensions"], space=self.config["metric"])
        self.backend.load_index(path)

    def index(self, embeddings):
        # Inner product is equal to cosine similarity on normalized vectors
        self.config["metric"] = "ip"

        # Lookup index settings
        efconstruction = self.setting("efconstruction", 200)
        m = self.setting("m", 16)
        seed = self.setting("randomseed", 100)

        # Create index
        self.backend = Index(dim=self.config["dimensions"], space=self.config["metric"])
        self.backend.init_index(max_elements=embeddings.shape[0], ef_construction=efconstruction, M=m, random_seed=seed)

        # Add items - position in embeddings is used as the id
        self.backend.add_items(embeddings, np.arange(embeddings.shape[0], dtype=np.int64))

        # Add id offset, delete counter and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.config["deletes"] = 0
        self.metadata({"efconstruction": efconstruction, "m": m, "seed": seed})

    def append(self, embeddings):
        new = embeddings.shape[0]

        # Resize index
        self.backend.resize_index(self.config["offset"] + new)

        # Append new ids - position in embeddings + existing offset is used as the id
        self.backend.add_items(embeddings, np.arange(self.config["offset"], self.config["offset"] + new, dtype=np.int64))

        # Update id offset and index metadata
        self.config["offset"] += new
        self.metadata()

    def delete(self, ids):
        # Mark elements as deleted to omit from search results
        for uid in ids:
            try:
                self.backend.mark_deleted(uid)
                self.config["deletes"] += 1
            except RuntimeError:
                # Ignore label not found error
                continue

    def search(self, queries, limit):
        # Set ef query param
        ef = self.setting("efsearch")
        if ef:
            self.backend.set_ef(ef)

        # Run the query
        ids, distances = self.backend.knn_query(queries, k=limit)

        # Map results to [(id, score)]
        results = []
        for x, distance in enumerate(distances):
            # Convert distances to similarity scores
            scores = [1 - d for d in distance.tolist()]

            # Build (id, score) tuples, convert np.int64 to python int
            results.append(list(zip(ids[x].tolist(), scores)))

        return results

    def count(self):
        return self.backend.get_current_count() - self.config["deletes"]

    def save(self, path):
        # Write index
        self.backend.save_index(path)



================================================
FILE: src/python/txtai/ann/dense/numpy.py
================================================
"""
NumPy module
"""

import numpy as np

from safetensors import safe_open
from safetensors.numpy import save_file

from ...serialize import SerializeFactory

from ..base import ANN


class NumPy(ANN):
    """
    Builds an ANN index backed by a NumPy array.
    """

    def __init__(self, config):
        super().__init__(config)

        # Array function definitions
        self.all, self.cat, self.dot, self.zeros = np.all, np.concatenate, np.dot, np.zeros
        self.argsort, self.xor, self.clip = np.argsort, np.bitwise_xor, np.clip

        # Scalar quantization
        quantize = self.config.get("quantize")
        self.qbits = quantize if quantize and isinstance(quantize, int) and not isinstance(quantize, bool) else None

    def load(self, path):
        # Load array from file
        try:
            if self.setting("safetensors"):
                data = self.loadsafetensors(path).get("data")
            else:
                data = np.load(path, allow_pickle=False)

            self.backend = self.tensor(data)
        except ValueError:
            # Backwards compatible support for previously pickled data
            self.backend = self.tensor(SerializeFactory.create("pickle").load(path))

    def index(self, embeddings):
        # Create index
        self.backend = self.tensor(embeddings)

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata(self.settings())

    def append(self, embeddings):
        # Append new data to array
        self.backend = self.cat((self.backend, self.tensor(embeddings)), axis=0)

        # Update id offset and index metadata
        self.config["offset"] += embeddings.shape[0]
        self.metadata()

    def delete(self, ids):
        # Filter any index greater than size of array
        ids = [x for x in ids if x < self.backend.shape[0]]

        # Clear specified ids
        self.backend[ids] = self.tensor(self.zeros((len(ids), self.backend.shape[1])))

    def search(self, queries, limit):
        if self.qbits:
            # Calculate hamming score for integer vectors
            scores = self.hammingscore(queries)
        else:
            # Dot product on normalized vectors is equal to cosine similarity
            scores = self.dot(self.tensor(queries), self.backend.T)

        # Get topn ids
        ids = self.argsort(-scores)[:, :limit]

        # Map results to [(id, score)]
        results = []
        for x, score in enumerate(scores):
            # Add results
            results.append(list(zip(ids[x].tolist(), score[ids[x]].tolist())))

        return results

    def count(self):
        # Get count of non-zero rows (ignores deleted rows)
        return self.backend[~self.all(self.backend == 0, axis=1)].shape[0]

    def save(self, path):
        # Save array to file. Use stream to prevent ".npy" suffix being added.
        if self.setting("safetensors"):
            self.savesafetensors({"data": self.numpy(self.backend)}, path)
        else:
            with open(path, "wb") as handle:
                np.save(handle, self.numpy(self.backend), allow_pickle=False)

    def tensor(self, array):
        """
        Handles backend-specific code such as loading to a GPU device.

        Args:
            array: data array

        Returns:
            array with backend-specific logic applied
        """

        return array

    def numpy(self, array):
        """
        Handles backend-specific code to convert an array to numpy

        Args:
            array: data array

        Returns:
            numpy array
        """

        return array

    def totype(self, array, dtype):
        """
        Casts array to dtype.

        Args:
            array: input array
            dtype: dtype

        Returns:
            array cast as dtype
        """

        return np.int64(array) if dtype == np.int64 else array

    def settings(self):
        """
        Returns settings for this array.

        Returns:
            dict
        """

        return {"numpy": np.__version__}

    def loadsafetensors(self, path):
        """
        Loads data from a safetensors file.

        Args:
            path: path to safetensors file

        Returns:
            dict with metadata + tensors
        """

        # Merge metadata and tensors into single dictionary
        with safe_open(path, framework="np") as f:
            return {**(f.metadata() if f.metadata() else {}), **{k: f.get_tensor(k) for k in f.keys()}}

    def savesafetensors(self, data, path, metadata=None):
        """
        Saves data and metadata to a safetensors file.

        Args:
            data: tensors to save
            path: output file
            metadata: additional metadata to save
        """

        save_file(data, path, metadata)

    def hammingscore(self, queries):
        """
        Calculates a hamming distance score.

        This is defined as:

            score = 1.0 - (hamming distance / total number of bits)

        Args:
            queries: queries array

        Returns:
            scores
        """

        # Build table of number of bits for each distinct uint8 value
        table = 1 << np.arange(8)
        table = self.tensor(np.array([np.count_nonzero(x & table) for x in np.arange(256)]))

        # Number of different bits
        delta = self.xor(self.tensor(queries[:, None]), self.backend)

        # Cast to long array
        delta = self.totype(delta, np.int64)

        # Calculate score as 1.0 - percentage of different bits
        # Bound score from 0 to 1
        return self.clip(1.0 - (table[delta].sum(axis=2) / (self.config["dimensions"] * 8)), 0.0, 1.0)



================================================
FILE: src/python/txtai/ann/dense/pgvector.py
================================================
"""
PGVector module
"""

import os

import numpy as np

# Conditional import
try:
    from pgvector.sqlalchemy import BIT, HALFVEC, VECTOR

    from sqlalchemy import create_engine, delete, func, text, Column, Index, Integer, MetaData, StaticPool, Table
    from sqlalchemy.orm import Session
    from sqlalchemy.schema import CreateSchema

    PGVECTOR = True
except ImportError:
    PGVECTOR = False

from ..base import ANN


# pylint: disable=R0904
class PGVector(ANN):
    """
    Builds an ANN index backed by a Postgres database.
    """

    def __init__(self, config):
        super().__init__(config)

        if not PGVECTOR:
            raise ImportError('PGVector is not available - install "ann" extra to enable')

        # Database connection
        self.engine, self.database, self.connection, self.table = None, None, None, None

        # Scalar quantization
        quantize = self.config.get("quantize")
        self.qbits = quantize if quantize and isinstance(quantize, int) and not isinstance(quantize, bool) else None

    def load(self, path):
        # Initialize tables
        self.initialize()

    def index(self, embeddings):
        # Initialize tables
        self.initialize(recreate=True)

        # Prepare embeddings and insert rows
        self.database.execute(self.table.insert(), [{"indexid": x, "embedding": self.prepare(row)} for x, row in enumerate(embeddings)])

        # Create index
        self.createindex()

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata(self.settings())

    def append(self, embeddings):
        # Prepare embeddings and insert rows
        self.database.execute(
            self.table.insert(), [{"indexid": x + self.config["offset"], "embedding": self.prepare(row)} for x, row in enumerate(embeddings)]
        )

        # Update id offset and index metadata
        self.config["offset"] += embeddings.shape[0]
        self.metadata()

    def delete(self, ids):
        self.database.execute(delete(self.table).where(self.table.c["indexid"].in_(ids)))

    def search(self, queries, limit):
        results = []
        for query in queries:
            # Run query
            query = self.database.query(self.table.c["indexid"], self.query(query)).order_by("score").limit(limit)

            # Calculate and collect scores
            results.append([(indexid, self.score(score)) for indexid, score in query])

        return results

    def count(self):
        # pylint: disable=E1102
        return self.database.query(func.count(self.table.c["indexid"])).scalar()

    def save(self, path):
        # Commit session and connection
        self.database.commit()
        self.connection.commit()

    def close(self):
        # Parent logic
        super().close()

        # Close database connection
        if self.database:
            self.database.close()
            self.engine.dispose()

    def initialize(self, recreate=False):
        """
        Initializes a new database session.

        Args:
            recreate: Recreates the database tables if True
        """

        # Connect to database
        self.connect()

        # Set the database schema
        self.schema()

        # Table name
        table = self.setting("table", self.defaulttable())

        # Create vectors table object
        self.table = Table(table, MetaData(), Column("indexid", Integer, primary_key=True, autoincrement=False), Column("embedding", self.column()))

        # Drop table, if necessary
        if recreate:
            self.table.drop(self.connection, checkfirst=True)

        # Create table, if necessary
        self.table.create(self.connection, checkfirst=True)

    def createindex(self):
        """
        Creates a index with the current settings.
        """

        # Table name
        table = self.setting("table", self.defaulttable())

        # Create ANN index - inner product is equal to cosine similarity on normalized vectors
        index = Index(
            f"{table}-index",
            self.table.c["embedding"],
            postgresql_using="hnsw",
            postgresql_with=self.settings(),
            postgresql_ops={"embedding": self.operation()},
        )

        # Create or recreate index
        index.drop(self.connection, checkfirst=True)
        index.create(self.connection, checkfirst=True)

    def connect(self):
        """
        Establishes a database connection. Cleans up any existing database connection first.
        """

        # Close existing connection
        if self.database:
            self.close()

        # Create engine
        self.engine = create_engine(self.url(), poolclass=StaticPool, echo=False)
        self.connection = self.engine.connect()

        # Start database session
        self.database = Session(self.connection)

        # Initialize pgvector extension
        self.sqldialect(text("CREATE EXTENSION IF NOT EXISTS vector"))

    def schema(self):
        """
        Sets the database schema, if available.
        """

        # Set default schema, if necessary
        schema = self.setting("schema")
        if schema:
            with self.engine.begin():
                self.sqldialect(CreateSchema(schema, if_not_exists=True))

            self.sqldialect(text("SET search_path TO :schema,public"), {"schema": schema})

    def settings(self):
        """
        Returns settings for this index.

        Returns:
            dict
        """

        return {"m": self.setting("m", 16), "ef_construction": self.setting("efconstruction", 200)}

    def sqldialect(self, sql, parameters=None):
        """
        Executes a SQL statement based on the current SQL dialect.

        Args:
            sql: SQL to execute
            parameters: optional bind parameters
        """

        args = (sql, parameters) if self.engine.dialect.name == "postgresql" else (text("SELECT 1"),)
        self.database.execute(*args)

    def defaulttable(self):
        """
        Returns the default table name.

        Returns:
            default table name
        """

        return "vectors"

    def url(self):
        """
        Reads the database url parameter.

        Returns:
            database url
        """

        return self.setting("url", os.environ.get("ANN_URL"))

    def column(self):
        """
        Gets embedding column for the current settings.

        Returns:
            embedding column definition
        """

        if self.qbits:
            # If quantization is set, always return BIT vectors
            return BIT(self.config["dimensions"] * 8)

        if self.setting("precision") == "half":
            # 16-bit HALF precision vectors
            return HALFVEC(self.config["dimensions"])

        # Default is full 32-bit FULL precision vectors
        return VECTOR(self.config["dimensions"])

    def operation(self):
        """
        Gets the index operation for the current settings.

        Returns:
            index operation
        """

        if self.qbits:
            # If quantization is set, always return BIT vectors
            return "bit_hamming_ops"

        if self.setting("precision") == "half":
            # 16-bit HALF precision vectors
            return "halfvec_ip_ops"

        # Default is full 32-bit FULL precision vectors
        return "vector_ip_ops"

    def prepare(self, data):
        """
        Prepares data for the embeddings column. This method returns a bit string for bit vectors and
        the input data unmodified for float vectors.

        Args:
            data: input data

        Returns:
            data ready for the embeddings column
        """

        # Transform to a bit string when vector quantization is enabled
        if self.qbits:
            return "".join(np.where(np.unpackbits(data), "1", "0"))

        # Return original data
        return data

    def query(self, query):
        """
        Creates a query statement from an input query. This method uses hamming distance for bit vectors and
        the max_inner_product for float vectors.

        Args:
            query: input query

        Returns:
            query statement
        """

        # Prepare query embeddings
        query = self.prepare(query)

        # Bit vector query
        if self.qbits:
            return self.table.c["embedding"].hamming_distance(query).label("score")

        # Float vector query
        return self.table.c["embedding"].max_inner_product(query).label("score")

    def score(self, score):
        """
        Calculates the index score from the input score. This method returns the hamming score
        (1.0 - (hamming distance / total number of bits)) for bit vectors and the -score for
        float vectors.

        Args:
            score: input score

        Returns:
            index score
        """

        # Calculate hamming score as 1.0 - (hamming distance / total number of bits)
        # Bound score from 0 to 1
        if self.qbits:
            return min(max(0.0, 1.0 - (score / (self.config["dimensions"] * 8))), 1.0)

        # pgvector returns negative inner product since Postgres only supports ASC order index scans on operators
        return -score



================================================
FILE: src/python/txtai/ann/dense/sqlite.py
================================================
"""
SQLite module
"""

import os
import sqlite3

# Conditional import
try:
    import sqlite_vec

    SQLITEVEC = True
except ImportError:
    SQLITEVEC = False

from ..base import ANN


class SQLite(ANN):
    """
    Builds an ANN index backed by a SQLite database.
    """

    def __init__(self, config):
        super().__init__(config)

        if not SQLITEVEC:
            raise ImportError('sqlite-vec is not available - install "ann" extra to enable')

        # Database parameters
        self.connection, self.cursor, self.path = None, None, ""

        # Quantization setting
        self.quantize = self.setting("quantize")
        self.quantize = 8 if isinstance(self.quantize, bool) else int(self.quantize) if self.quantize else None

    def load(self, path):
        self.path = path

    def index(self, embeddings):
        # Initialize tables
        self.initialize(recreate=True)

        # Add vectors
        self.database().executemany(self.insertsql(), enumerate(embeddings))

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata(self.settings())

    def append(self, embeddings):
        self.database().executemany(self.insertsql(), [(x + self.config["offset"], row) for x, row in enumerate(embeddings)])

        self.config["offset"] += embeddings.shape[0]
        self.metadata()

    def delete(self, ids):
        self.database().executemany(self.deletesql(), [(x,) for x in ids])

    def search(self, queries, limit):
        results = []
        for query in queries:
            # Execute query
            self.database().execute(self.searchsql(), [query, limit])

            # Add query results
            results.append(list(self.database()))

        return results

    def count(self):
        self.database().execute(self.countsql())
        return self.cursor.fetchone()[0]

    def save(self, path):
        # Temporary database
        if not self.path:
            # Save temporary database
            self.connection.commit()

            # Copy data from current to new
            connection = self.copy(path)

            # Close temporary database
            self.connection.close()

            # Point connection to new connection
            self.connection = connection
            self.cursor = self.connection.cursor()
            self.path = path

        # Paths are equal, commit changes
        elif self.path == path:
            self.connection.commit()

        # New path is different from current path, copy data and continue using current connection
        else:
            self.copy(path).close()

    def close(self):
        # Parent logic
        super().close()

        # Close database connection
        if self.connection:
            self.connection.close()
            self.connection = None

    def initialize(self, recreate=False):
        """
        Initializes a new database session.

        Args:
            recreate: Recreates the database tables if True
        """

        # Create table
        self.database().execute(self.tablesql())

        # Clear data
        if recreate:
            self.database().execute(self.tosql("DELETE FROM {table}"))

    def settings(self):
        """
        Returns settings for this index.

        Returns:
            dict
        """

        sqlite, sqlitevec = self.database().execute("SELECT sqlite_version(), vec_version()").fetchone()

        return {"sqlite": sqlite, "sqlite-vec": sqlitevec}

    def database(self):
        """
        Gets the current database cursor. Creates a new connection
        if there isn't one.

        Returns:
            cursor
        """

        if not self.connection:
            self.connection = self.connect(self.path)
            self.cursor = self.connection.cursor()

        return self.cursor

    def connect(self, path):
        """
        Creates a new database connection.

        Args:
            path: path to database file

        Returns:
            database connection
        """

        # Create connection
        connection = sqlite3.connect(path, check_same_thread=False)

        # Load sqlite-vec extension
        connection.enable_load_extension(True)
        sqlite_vec.load(connection)
        connection.enable_load_extension(False)

        # Return connection and cursor
        return connection

    def copy(self, path):
        """
        Copies content from the current database into target.

        Args:
            path: target database path

        Returns:
            new database connection
        """

        # Delete existing file, if necessary
        if os.path.exists(path):
            os.remove(path)

        # Create new connection
        connection = self.connect(path)

        if self.connection.in_transaction:
            # Initialize connection
            connection.execute(self.tablesql())

            # The backup call will hang if there are uncommitted changes, need to copy over
            # with iterdump (which is much slower)
            for sql in self.connection.iterdump():
                if self.tosql('insert into "{table}"') in sql.lower():
                    connection.execute(sql)
        else:
            # Database is up to date, can do a more efficient copy with SQLite C API
            self.connection.backup(connection)

        return connection

    def tablesql(self):
        """
        Builds a CREATE table statement for table.

        Returns:
            CREATE TABLE
        """

        # Binary quantization
        if self.quantize == 1:
            embedding = f"embedding BIT[{self.config['dimensions']}]"

        # INT8 quantization
        elif self.quantize == 8:
            embedding = f"embedding INT8[{self.config['dimensions']}] distance=cosine"

        # Standard FLOAT32
        else:
            embedding = f"embedding FLOAT[{self.config['dimensions']}] distance=cosine"

        # Return CREATE TABLE sql
        return self.tosql(("CREATE VIRTUAL TABLE IF NOT EXISTS {table} USING vec0" "(indexid INTEGER PRIMARY KEY, " f"{embedding})"))

    def insertsql(self):
        """
        Creates an INSERT SQL statement.

        Returns:
            INSERT
        """

        return self.tosql(f"INSERT INTO {{table}}(indexid, embedding) VALUES (?, {self.embeddingsql()})")

    def deletesql(self):
        """
        Creates a DELETE SQL statement.

        Returns:
            DELETE
        """

        return self.tosql("DELETE FROM {table} WHERE indexid = ?")

    def searchsql(self):
        """
        Creates a SELECT SQL statement for search.

        Returns:
            SELECT
        """

        return self.tosql(("SELECT indexid, 1 - distance FROM {table} " f"WHERE embedding MATCH {self.embeddingsql()} AND k = ? ORDER BY distance"))

    def countsql(self):
        """
        Creates a SELECT COUNT statement.

        Returns:
            SELECT COUNT
        """

        return self.tosql("SELECT count(indexid) FROM {table}")

    def embeddingsql(self):
        """
        Creates an embeddings column SQL snippet.

        Returns:
            embeddings column SQL
        """

        # Binary quantization
        if self.quantize == 1:
            embedding = "vec_quantize_binary(?)"

        # INT8 quantization
        elif self.quantize == 8:
            embedding = "vec_quantize_int8(?, 'unit')"

        # Standard FLOAT32
        else:
            embedding = "?"

        return embedding

    def tosql(self, sql):
        """
        Creates a SQL statement substituting in the configured table name.

        Args:
            sql: SQL statement with a {table} parameter

        Returns:
            fully resolved SQL statement
        """

        table = self.setting("table", "vectors")
        return sql.format(table=table)



================================================
FILE: src/python/txtai/ann/dense/torch.py
================================================
"""
PyTorch module
"""

import numpy as np
import torch

try:
    from bitsandbytes import matmul_4bit
    from bitsandbytes.functional import (
        QuantState,
        int8_vectorwise_quant,
        int8_vectorwise_dequant,
        int8_linear_matmul,
        int8_mm_dequant,
        quantize_4bit,
        dequantize_4bit,
    )

    BNB = True
except ImportError:
    BNB = False

from .numpy import NumPy


class Torch(NumPy):
    """
    Builds an ANN index backed by a PyTorch array.
    """

    def __init__(self, config):
        super().__init__(config)

        # Define array functions
        self.all, self.cat, self.dot, self.zeros = torch.all, torch.cat, torch.mm, torch.zeros
        self.argsort, self.xor, self.clip = torch.argsort, torch.bitwise_xor, torch.clip

        # Quantization parameters
        self.qstate, self.qdeleted = None, 0

        # Initialize quantization
        settings = self.qsettings()
        if settings:
            if not BNB:
                raise ImportError('bitsandbytes is not available - install "ann" extra to enable')

            if settings.get("type") == "int8":
                # Matrix multiply for 8 bit vectors
                self.dot = self.matmul8bit
            else:
                # Matrix multiply for 4 bit vectors
                self.dot = self.matmul4bit

            # Require safetensors storage
            self.config[self.config["backend"]]["safetensors"] = True

    def index(self, embeddings):
        with QuantizeContext(self):
            super().index(embeddings)

    def append(self, embeddings):
        with QuantizeContext(self):
            super().append(embeddings)

    def delete(self, ids):
        with QuantizeContext(self):
            super().delete(ids)

            # Calculate deleted for quantized data, if necessary
            if self.qstate:
                self.qdeleted = self.qstate.shape[0] - super().count()

    def count(self):
        return self.qstate.shape[0] - self.qdeleted if self.qstate else super().count()

    def tensor(self, array):
        # Convert array to Tensor
        if isinstance(array, np.ndarray):
            array = torch.from_numpy(array)

        # Load to GPU device, if available
        return array.cuda() if torch.cuda.is_available() else array

    def numpy(self, array):
        return array.cpu().numpy()

    def totype(self, array, dtype):
        return array.long() if dtype == np.int64 else array

    def settings(self):
        return {"torch": torch.__version__}

    def loadsafetensors(self, path):
        data = super().loadsafetensors(path)

        # Load quantization settings
        if self.qsettings():
            self.qstate = QuantState(
                absmax=self.tensor(data["absmax"]),
                shape=torch.Size(data["shape"].tolist()),
                code=self.tensor(data["code"]) if "code" in data else None,
                blocksize=int(data["blocksize"]) if "blocksize" in data else None,
                quant_type=data["quant_type"],
                dtype=getattr(torch, data["dtype"]),
            )
            self.qdeleted = int(data["qdeleted"])

        return data

    def savesafetensors(self, data, path, metadata=None):
        # Save quantization settings
        if self.qstate:
            # Required elements
            data["absmax"] = self.qstate.absmax.cpu().numpy()
            data["shape"] = np.array(list(self.qstate.shape))

            metadata = {
                "quant_type": str(self.qstate.quant_type),
                "dtype": str(self.qstate.dtype).rsplit(".", maxsplit=1)[-1],
                "qdeleted": str(self.qdeleted),
            }

            # Add optional elements
            if self.qstate.code is not None:
                data["code"] = self.qstate.code.cpu().numpy()

            if self.qstate.blocksize:
                metadata["blocksize"] = str(self.qstate.blocksize)

        super().savesafetensors(data, path, metadata)

    def quantize(self):
        """
        Quantizes data if quantization if supported and enabled.
        """

        # Get quantization settings and quantize
        settings = self.qsettings()
        if settings:
            if settings.get("type") == "int8":
                # Get current backend config
                shape, dtype = self.backend.shape, self.backend.dtype

                # 8-bit quantization
                self.backend, absmax, _ = int8_vectorwise_quant(self.backend.half())
                self.qstate = QuantState(absmax=absmax, shape=shape, quant_type=settings["type"], dtype=dtype)
            else:
                # 4-bit quantization
                self.backend, self.qstate = quantize_4bit(
                    self.backend, blocksize=settings.get("blocksize", 64), quant_type=settings.get("type", "nf4")
                )

    def dequantize(self):
        """
        Dequantizes data if quantization is supported and enabled.
        """

        # Dequantize using current quantization state
        if self.qstate:
            if self.qstate.quant_type == "int8":
                # 8-bit quantization
                self.backend = int8_vectorwise_dequant(self.backend, self.qstate.absmax)
            else:
                # 4-bit quantization
                self.backend = dequantize_4bit(self.backend, self.qstate)

    def qsettings(self):
        """
        Parse quantization settings. Only read parameters if CUDA is available.

        Returns:
            {quantization settings}
        """

        quantize = self.setting("quantize")
        return {"quantize": True} if quantize and isinstance(quantize, bool) else quantize

    def matmul8bit(self, query, data):
        """
        8-bit integer matrix multiplication.

        Args:
            query: query matrix
            data: data matrix

        Returns:
            query @ data
        """

        # Matrix multiplication method requires transposing data matrix
        query, absmax, _ = int8_vectorwise_quant(query.half())
        return int8_mm_dequant(int8_linear_matmul(query, data.T), absmax, self.qstate.absmax).float()

    def matmul4bit(self, query, data):
        """
        4-bit float matrix multiplication.

        Args:
            query: query matrix
            data: data matrix

        Returns:
            query @ data
        """

        # Matrix multiplication method transposes data already
        return matmul_4bit(query, data, self.qstate)


class QuantizeContext:
    """
    Quantization context. Facilitates modifications to quantized tensors.
    """

    def __init__(self, ann):
        self.ann = ann

    def __enter__(self):
        self.ann.dequantize()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.ann.quantize()



================================================
FILE: src/python/txtai/ann/sparse/__init__.py
================================================
"""
Sparse ANN imports
"""

from .factory import SparseANNFactory
from .ivfsparse import IVFSparse
from .pgsparse import PGSparse



================================================
FILE: src/python/txtai/ann/sparse/factory.py
================================================
"""
Factory module
"""

from ...util import Resolver

from .ivfsparse import IVFSparse
from .pgsparse import PGSparse


class SparseANNFactory:
    """
    Methods to create Sparse ANN indexes.
    """

    @staticmethod
    def create(config):
        """
        Create an Sparse ANN.

        Args:
            config: index configuration parameters

        Returns:
            Sparse ANN
        """

        # ANN instance
        ann = None
        backend = config.get("backend", "ivfsparse")

        # Create ANN instance
        if backend == "ivfsparse":
            ann = IVFSparse(config)
        elif backend == "pgsparse":
            ann = PGSparse(config)
        else:
            ann = SparseANNFactory.resolve(backend, config)

        # Store config back
        config["backend"] = backend

        return ann

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: index configuration parameters

        Returns:
            ANN
        """

        try:
            return Resolver()(backend)(config)
        except Exception as e:
            raise ImportError(f"Unable to resolve sparse ann backend: '{backend}'") from e



================================================
FILE: src/python/txtai/ann/sparse/ivfsparse.py
================================================
"""
IVFSparse module
"""

import math
import os

from multiprocessing.pool import ThreadPool

import numpy as np

# Conditional import
try:
    from scipy.sparse import csr_matrix, vstack
    from scipy.sparse.linalg import norm
    from sklearn.cluster import MiniBatchKMeans
    from sklearn.metrics import pairwise_distances_argmin_min
    from sklearn.utils.extmath import safe_sparse_dot

    IVFSPARSE = True
except ImportError:
    IVFSPARSE = False

from ...serialize import SerializeFactory
from ...util import SparseArray
from ..base import ANN


class IVFSparse(ANN):
    """
    Inverted file (IVF) index with flat vector file storage and sparse array support.

    IVFSparse builds an IVF index and enables approximate nearest neighbor (ANN) search.

    This index is modeled after Faiss and supports many of the same parameters.

    See this link for more: https://github.com/facebookresearch/faiss/wiki/Faster-search
    """

    def __init__(self, config):
        super().__init__(config)

        if not IVFSPARSE:
            raise ImportError('IVFSparse is not available - install "ann" extra to enable')

        # Cluster centroids, if computed
        self.centroids = None

        # Cluster id mapping
        self.ids = None

        # Cluster data blocks - can be a single block with no computed centroids
        self.blocks = None

        # Deleted ids
        self.deletes = None

    def index(self, embeddings):
        # Compute model training size
        train, sample = embeddings, self.setting("sample")
        if sample:
            # Get sample for training
            rng = np.random.default_rng(0)
            indices = sorted(rng.choice(train.shape[0], int(sample * train.shape[0]), replace=False, shuffle=False))
            train = train[indices]

        # Get number of clusters. Note that final number of clusters could be lower due to filtering duplicate centroids
        # and pruning of small clusters
        clusters = self.nlist(embeddings.shape[0], train.shape[0])

        # Build cluster centroids if approximate search is enabled
        # A single cluster performs exact search
        self.centroids = self.build(train, clusters) if clusters > 1 else None

        # Sort into clusters
        ids = self.aggregate(embeddings)

        # Prune small clusters (less than minpoints parameter) and rebuild
        indices = sorted(k for k, v in ids.items() if len(v) >= self.minpoints())
        if len(indices) > 0 and len(ids) > 1 and len(indices) != len(ids.keys()):
            self.centroids = self.centroids[indices]
            ids = self.aggregate(embeddings)

        # Sort clusters by id
        self.ids = dict(sorted(ids.items(), key=lambda x: x[0]))

        # Create cluster data blocks
        self.blocks = {k: embeddings[v] for k, v in self.ids.items()}

        # Calculate block max summary vectors and use as centroids
        self.centroids = vstack([csr_matrix(x.max(axis=0)) for x in self.blocks.values()]) if self.centroids is not None else None

        # Initialize deletes
        self.deletes = []

        # Add id offset and index build metadata
        self.config["offset"] = embeddings.shape[0]
        self.metadata({"clusters": len(self.blocks)})

    def append(self, embeddings):
        # Get offset
        offset = self.size()

        # Sort into clusters and merge
        for cluster, ids in self.aggregate(embeddings).items():
            # Add new ids
            self.ids[cluster].extend([x + offset for x in ids])

            # Add new data
            self.blocks[cluster] = vstack([self.blocks[cluster], embeddings[ids]])

        # Update id offset and index metadata
        self.config["offset"] += embeddings.shape[0]
        self.metadata()

    def delete(self, ids):
        # Set index ids as deleted
        self.deletes.extend(ids)

    def search(self, queries, limit):
        results = []

        # Calculate number of threads using a thread batch size of 32
        threads = queries.shape[0] // 32
        threads = min(max(threads, 1), os.cpu_count())

        # Approximate search
        blockids = self.topn(queries, self.centroids, self.nprobe())[0] if self.centroids is not None else None

        # This method is able to run as multiple threads due to a number of numpy/scipy method calls that drop the GIL.
        results = []
        with ThreadPool(threads) as pool:
            for result in pool.starmap(self.scan, [(x, limit, blockids[i] if blockids is not None else None) for i, x in enumerate(queries)]):
                results.append(result)

        return results

    def count(self):
        return self.size() - len(self.deletes)

    def load(self, path):
        # Create streaming serializer and limit read size to a byte at a time to ensure
        # only msgpack data is consumed
        serializer = SerializeFactory.create("msgpack", streaming=True, read_size=1)

        with open(path, "rb") as f:
            # Read header
            unpacker = serializer.loadstream(f)
            header = next(unpacker)

            # Read cluster centroids, if available
            self.centroids = SparseArray().load(f) if header["centroids"] else None

            # Read cluster ids
            self.ids = dict(next(unpacker))

            # Read cluster data blocks
            self.blocks = {}
            for key in self.ids:
                self.blocks[key] = SparseArray().load(f)

            # Read deletes
            self.deletes = next(unpacker)

    def save(self, path):
        # IVFSparse storage format:
        #    - header msgpack
        #    - centroids sparse array (optional based on header parameters)
        #    - cluster ids msgpack
        #    - cluster data blocks list of sparse arrays
        #    - deletes msgpack

        # Create message pack serializer
        serializer = SerializeFactory.create("msgpack")

        with open(path, "wb") as f:
            # Write header
            serializer.savestream({"centroids": self.centroids is not None, "count": self.count(), "blocks": len(self.blocks)}, f)

            # Write cluster centroids, if available
            if self.centroids is not None:
                SparseArray().save(f, self.centroids)

            # Write cluster id mapping
            serializer.savestream(list(self.ids.items()), f)

            # Write cluster data blocks
            for block in self.blocks.values():
                SparseArray().save(f, block)

            # Write deletes
            serializer.savestream(self.deletes, f)

    def build(self, train, clusters):
        """
        Builds a k-means cluster to calculate centroid points for aggregating data blocks.

        Args:
            train: training data
            clusters: number of clusters to create

        Returns:
            cluster centroids
        """

        # Select top n most important features that contribute to L2 vector norm
        indices = np.argsort(-norm(train, axis=0))[: self.setting("nfeatures", 25)]
        data = train[:, indices]
        data = train

        # Cluster data using k-means
        kmeans = MiniBatchKMeans(n_clusters=clusters, random_state=0, n_init=5).fit(data)

        # Find closest points to each cluster center and use those as centroids
        indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, data, metric="l2")

        # Filter out duplicate centroids and return cluster centroids
        return train[np.unique(indices)]

    def aggregate(self, data):
        """
        Aggregates input data array into clusters. This method sorts each data element into the
        cluster with the highest L2 similarity centroid.

        Args:
            data: input data

        Returns:
            {cluster, ids}
        """

        # Exact search when only a single cluster
        if self.centroids is None:
            return {0: list(range(data.shape[0]))}

        # Map data to closest centroids
        indices, _ = pairwise_distances_argmin_min(data, self.centroids, metric="l2")

        # Sort into clusters
        ids = {}
        for x, cluster in enumerate(indices.tolist()):
            if cluster not in ids:
                ids[cluster] = []

            # Save id
            ids[cluster].append(x)

        return ids

    def topn(self, queries, data, limit, deletes=None):
        """
        Gets the top n most similar data elements for query.

        Args:
            queries: queries array
            data: data array
            limit: top n
            deletes: optional list of deletes to filter from results

        Returns:
            list of matching (indices, scores)
        """

        # Dot product similarity
        scores = safe_sparse_dot(queries, data.T, dense_output=True)

        # Clear deletes
        if deletes is not None:
            scores[:, deletes] = 0

        # Get top n matching indices and scores
        indices = np.argpartition(-scores, limit if limit < scores.shape[0] else scores.shape[0] - 1)[:, :limit]
        scores = np.take_along_axis(scores, indices, axis=1)

        return indices, scores

    def scan(self, query, limit, blockids):
        """
        Scans a list of blocks for top n ids that match query.

        Args:
            query: input query
            limit top n
            blockids: block ids to scan

        Returns:
            list of (id, scores)
        """

        if self.centroids is not None:
            # Stack into single ids list
            ids = np.concatenate([self.ids[x] for x in blockids if x in self.ids])

            # Stack data rows
            data = vstack([self.blocks[x] for x in blockids if x in self.blocks])
        else:
            # Exact search
            ids, data = np.array(self.ids[0]), self.blocks[0]

        # Get deletes
        deletes = np.argwhere(np.isin(ids, self.deletes)).ravel()

        # Calculate similarity
        indices, scores = self.topn(query, data, limit, deletes)
        indices, scores = indices[0], scores[0]

        # Map data ids and return
        return list(zip(ids[indices].tolist(), scores.tolist()))

    def nlist(self, count, train):
        """
        Calculates the number of clusters for this IVFSparse index. Note that the final number of clusters
        could be lower as duplicate cluster centroids are filtered out.

        Args:
            count: initial dataset size
            train: number of rows used to train

        Returns:
            number of clusters
        """

        # Get data size
        default = 1 if count <= 5000 else self.cells(train)

        # Number of clusters to create
        return self.setting("nlist", default)

    def nprobe(self):
        """
        Gets or derives the nprobe search parameter.

        Returns:
            nprobe setting
        """

        # Get size of embeddings index
        size = self.size()

        default = 6 if size <= 5000 else self.cells(size) // 16
        return self.setting("nprobe", default)

    def cells(self, count):
        """
        Calculates the number of IVF cells for an IVFSparse index.

        Args:
            count: number of rows

        Returns:
            number of IVF cells
        """

        # Calculate number of IVF cells where x = min(4 * sqrt(count), count / minpoints)
        return max(min(round(4 * math.sqrt(count)), int(count / self.minpoints())), 1)

    def size(self):
        """
        Gets the total size of this index including deletes.

        Returns:
            size
        """

        return sum(len(x) for x in self.ids.values())

    def minpoints(self):
        """
        Gets the minimum number of points per cluster.

        Returns:
            minimum points per cluster
        """

        # Minimum number of points per cluster
        # Match faiss default that requires at least 39 points per clusters
        return self.setting("minpoints", 39)



================================================
FILE: src/python/txtai/ann/sparse/pgsparse.py
================================================
"""
PGSparse module
"""

import os

import numpy as np

# Conditional import
try:
    from pgvector import SparseVector
    from pgvector.sqlalchemy import SPARSEVEC

    PGSPARSE = True
except ImportError:
    PGSPARSE = False

from ..dense import PGVector


class PGSparse(PGVector):
    """
    Builds a Sparse ANN index backed by a Postgres database.
    """

    def __init__(self, config):
        if not PGSPARSE:
            raise ImportError('PGSparse is not available - install "ann" extra to enable')

        super().__init__(config)

        # Quantization not supported
        self.qbits = None

    def defaulttable(self):
        return "svectors"

    def url(self):
        return self.setting("url", os.environ.get("SCORING_URL", os.environ.get("ANN_URL")))

    def column(self):
        return SPARSEVEC(self.config["dimensions"])

    def operation(self):
        return "sparsevec_ip_ops"

    def prepare(self, data):
        # pgvector only allows 1000 non-zero values for sparse vectors
        # Trim to top 1000 values, if necessary
        if data.count_nonzero() > 1000:
            value = -np.sort(-data[0, :].data)[1000]
            data.data = np.where(data.data > value, data.data, 0)
            data.eliminate_zeros()

        # Wrap as sparse vector
        return SparseVector(data)



================================================
FILE: src/python/txtai/api/__init__.py
================================================
"""
API imports
"""

# Conditional import
try:
    from .authorization import Authorization
    from .application import app, start
    from .base import API
    from .cluster import Cluster
    from .extension import Extension
    from .factory import APIFactory
    from .responses import *
    from .routers import *
    from .route import EncodingAPIRoute
except ImportError as missing:
    # pylint: disable=W0707
    raise ImportError('API is not available - install "api" extra to enable') from missing



================================================
FILE: src/python/txtai/api/application.py
================================================
"""
FastAPI application module
"""

import inspect
import os
import sys

from fastapi import APIRouter, Depends, FastAPI
from fastapi_mcp import FastApiMCP
from httpx import AsyncClient

from .authorization import Authorization
from .base import API
from .factory import APIFactory

from ..app import Application


def get():
    """
    Returns global API instance.

    Returns:
        API instance
    """

    return INSTANCE


def create():
    """
    Creates a FastAPI instance.
    """

    # Application dependencies
    dependencies = []

    # Default implementation of token authorization
    token = os.environ.get("TOKEN")
    if token:
        dependencies.append(Depends(Authorization(token)))

    # Add custom dependencies
    deps = os.environ.get("DEPENDENCIES")
    if deps:
        for dep in deps.split(","):
            # Create and add dependency
            dep = APIFactory.get(dep.strip())()
            dependencies.append(Depends(dep))

    # Create FastAPI application
    return FastAPI(lifespan=lifespan, dependencies=dependencies if dependencies else None)


def apirouters():
    """
    Lists available APIRouters.

    Returns:
        {router name: router}
    """

    # Get handle to api module
    api = sys.modules[".".join(__name__.split(".")[:-1])]

    available = {}
    for name, rclass in inspect.getmembers(api, inspect.ismodule):
        if hasattr(rclass, "router") and isinstance(rclass.router, APIRouter):
            available[name.lower()] = rclass.router

    return available


def lifespan(application):
    """
    FastAPI lifespan event handler.

    Args:
        application: FastAPI application to initialize
    """

    # pylint: disable=W0603
    global INSTANCE

    # Load YAML settings
    config = Application.read(os.environ.get("CONFIG"))

    # Instantiate API instance
    api = os.environ.get("API_CLASS")
    INSTANCE = APIFactory.create(config, api) if api else API(config)

    # Get all known routers
    routers = apirouters()

    # Conditionally add routes based on configuration
    for name, router in routers.items():
        if name in config:
            application.include_router(router)

    # Special case for embeddings clusters
    if "cluster" in config and "embeddings" not in config:
        application.include_router(routers["embeddings"])

    # Special case to add similarity instance for embeddings
    if "embeddings" in config and "similarity" not in config:
        application.include_router(routers["similarity"])

    # Execute extensions if present
    extensions = os.environ.get("EXTENSIONS")
    if extensions:
        for extension in extensions.split(","):
            # Create instance and execute extension
            extension = APIFactory.get(extension.strip())()
            extension(application)

    # Add Model Context Protocol (MCP) service, if applicable
    if config.get("mcp"):
        mcp = FastApiMCP(application, http_client=AsyncClient(timeout=100))
        mcp.mount()

    yield


def start():
    """
    Runs application lifespan handler.
    """

    list(lifespan(app))


# FastAPI instance txtai API instances
app, INSTANCE = create(), None



================================================
FILE: src/python/txtai/api/authorization.py
================================================
"""
Authorization module
"""

import hashlib
import os

from fastapi import Header, HTTPException


class Authorization:
    """
    Basic token authorization.
    """

    def __init__(self, token=None):
        """
        Creates a new Authorization instance.

        Args:
            token: SHA-256 hash of token to check
        """

        self.token = token if token else os.environ.get("TOKEN")

    def __call__(self, authorization: str = Header(default=None)):
        """
        Validates authorization header is present and equal to current token.

        Args:
            authorization: authorization header
        """

        if not authorization or self.token != self.digest(authorization):
            raise HTTPException(status_code=401, detail="Invalid Authorization Token")

    def digest(self, authorization):
        """
        Computes a SHA-256 hash for input authorization token.

        Args:
            authorization: authorization header

        Returns:
            SHA-256 hash of authorization token
        """

        # Replace Bearer prefix
        prefix = "Bearer "
        token = authorization[len(prefix) :] if authorization.startswith(prefix) else authorization

        # Compute SHA-256 hash
        return hashlib.sha256(token.encode("utf-8")).hexdigest()



================================================
FILE: src/python/txtai/api/base.py
================================================
"""
API module
"""

import json

from .cluster import Cluster

from ..app import Application


class API(Application):
    """
    Base API template. The API is an extended txtai application, adding the ability to cluster API instances together.

    Downstream applications can extend this base template to add/modify functionality.
    """

    def __init__(self, config, loaddata=True):
        super().__init__(config, loaddata)

        # Embeddings cluster
        self.cluster = None
        if self.config.get("cluster"):
            self.cluster = Cluster(self.config["cluster"])

    # pylint: disable=W0221
    def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False, request=None):
        # When search is invoked via the API, limit is set from the request
        # When search is invoked directly, limit is set using the method parameter
        limit = self.limit(request.query_params.get("limit") if request and hasattr(request, "query_params") else limit)
        weights = self.weights(request.query_params.get("weights") if request and hasattr(request, "query_params") else weights)
        index = request.query_params.get("index") if request and hasattr(request, "query_params") else index
        parameters = request.query_params.get("parameters") if request and hasattr(request, "query_params") else parameters
        graph = request.query_params.get("graph") if request and hasattr(request, "query_params") else graph

        # Decode parameters
        parameters = json.loads(parameters) if parameters and isinstance(parameters, str) else parameters

        if self.cluster:
            return self.cluster.search(query, limit, weights, index, parameters, graph)

        return super().search(query, limit, weights, index, parameters, graph)

    def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):
        if self.cluster:
            return self.cluster.batchsearch(queries, self.limit(limit), weights, index, parameters, graph)

        return super().batchsearch(queries, limit, weights, index, parameters, graph)

    def add(self, documents):
        """
        Adds a batch of documents for indexing.

        Downstream applications can override this method to also store full documents in an external system.

        Args:
            documents: list of {id: value, text: value}

        Returns:
            unmodified input documents
        """

        if self.cluster:
            self.cluster.add(documents)
        else:
            super().add(documents)

        return documents

    def index(self):
        """
        Builds an embeddings index for previously batched documents.
        """

        if self.cluster:
            self.cluster.index()
        else:
            super().index()

    def upsert(self):
        """
        Runs an embeddings upsert operation for previously batched documents.
        """

        if self.cluster:
            self.cluster.upsert()
        else:
            super().upsert()

    def delete(self, ids):
        """
        Deletes from an embeddings index. Returns list of ids deleted.

        Args:
            ids: list of ids to delete

        Returns:
            ids deleted
        """

        if self.cluster:
            return self.cluster.delete(ids)

        return super().delete(ids)

    def reindex(self, config, function=None):
        """
        Recreates this embeddings index using config. This method only works if document content storage is enabled.

        Args:
            config: new config
            function: optional function to prepare content for indexing
        """

        if self.cluster:
            self.cluster.reindex(config, function)
        else:
            super().reindex(config, function)

    def count(self):
        """
        Total number of elements in this embeddings index.

        Returns:
            number of elements in embeddings index
        """

        if self.cluster:
            return self.cluster.count()

        return super().count()

    def limit(self, limit):
        """
        Parses the number of results to return from the request. Allows range of 1-250, with a default of 10.

        Args:
            limit: limit parameter

        Returns:
            bounded limit
        """

        # Return between 1 and 250 results, defaults to 10
        return max(1, min(250, int(limit) if limit else 10))

    def weights(self, weights):
        """
        Parses the weights parameter from the request.

        Args:
            weights: weights parameter

        Returns:
            weights
        """

        return float(weights) if weights else weights



================================================
FILE: src/python/txtai/api/cluster.py
================================================
"""
Cluster module
"""

import asyncio
import json
import random
import urllib.parse
import zlib

import aiohttp

from ..database.sql import Aggregate


class Cluster:
    """
    Aggregates multiple embeddings shards into a single logical embeddings instance.
    """

    # pylint: disable=W0231
    def __init__(self, config=None):
        """
        Creates a new Cluster.

        Args:
            config: cluster configuration
        """

        # Configuration
        self.config = config

        # Embeddings shard urls
        self.shards = None
        if "shards" in self.config:
            self.shards = self.config["shards"]

        # Query aggregator
        self.aggregate = Aggregate()

    def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input query. This method will run either an index search
        or an index + database search depending on if a database is available.

        Args:
            query: input query
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: dict of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of {id: value, score: value} for index search, list of dict for an index + database search
        """

        # Build URL
        action = f"search?query={urllib.parse.quote_plus(query)}"
        if limit:
            action += f"&limit={limit}"
        if weights:
            action += f"&weights={weights}"
        if index:
            action += f"&index={index}"
        if parameters:
            action += f"&parameters={json.dumps(parameters) if isinstance(parameters, dict) else parameters}"
        if graph is not None:
            action += f"&graph={graph}"

        # Run query and flatten results into single results list
        results = []
        for result in self.execute("get", action):
            results.extend(result)

        # Combine aggregate functions and sort
        results = self.aggregate(query, results)

        # Limit results
        return results[: (limit if limit else 10)]

    def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input queries. This method will run either an index search
        or an index + database search depending on if a database is available.

        Args:
            queries: input queries
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: list of dicts of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of {id: value, score: value} per query for index search, list of dict per query for an index + database search
        """

        # POST parameters
        params = {"queries": queries}
        if limit:
            params["limit"] = limit
        if weights:
            params["weights"] = weights
        if index:
            params["index"] = index
        if parameters:
            params["parameters"] = parameters
        if graph is not None:
            params["graph"] = graph

        # Run query
        batch = self.execute("post", "batchsearch", [params] * len(self.shards))

        # Combine results per query
        results = []
        for x, query in enumerate(queries):
            result = []
            for section in batch:
                result.extend(section[x])

            # Aggregate, sort and limit results
            results.append(self.aggregate(query, result)[: (limit if limit else 10)])

        return results

    def add(self, documents):
        """
        Adds a batch of documents for indexing.

        Args:
            documents: list of {id: value, text: value}
        """

        self.execute("post", "add", self.shard(documents))

    def index(self):
        """
        Builds an embeddings index for previously batched documents.
        """

        self.execute("get", "index")

    def upsert(self):
        """
        Runs an embeddings upsert operation for previously batched documents.
        """

        self.execute("get", "upsert")

    def delete(self, ids):
        """
        Deletes from an embeddings cluster. Returns list of ids deleted.

        Args:
            ids: list of ids to delete

        Returns:
            ids deleted
        """

        return [uid for ids in self.execute("post", "delete", [ids] * len(self.shards)) for uid in ids]

    def reindex(self, config, function=None):
        """
        Recreates this embeddings index using config. This method only works if document content storage is enabled.

        Args:
            config: new config
            function: optional function to prepare content for indexing
        """

        self.execute("post", "reindex", [{"config": config, "function": function}] * len(self.shards))

    def count(self):
        """
        Total number of elements in this embeddings cluster.

        Returns:
            number of elements in embeddings cluster
        """

        return sum(self.execute("get", "count"))

    def shard(self, documents):
        """
        Splits documents into equal sized shards.

        Args:
            documents: input documents

        Returns:
            list of evenly sized shards with the last shard having the remaining elements
        """

        shards = [[] for _ in range(len(self.shards))]
        for document in documents:
            uid = document.get("id") if isinstance(document, dict) else document
            if uid and isinstance(uid, str):
                # Quick int hash of string to help derive shard id
                uid = zlib.adler32(uid.encode("utf-8"))
            elif uid is None:
                # Get random shard id when uid isn't set
                uid = random.randint(0, len(shards) - 1)

            shards[uid % len(self.shards)].append(document)

        return shards

    def execute(self, method, action, data=None):
        """
        Executes a HTTP action asynchronously.

        Args:
            method: get or post
            action: url action to perform
            data: post parameters

        Returns:
            json results if any
        """

        # Get urls
        urls = [f"{shard}/{action}" for shard in self.shards]
        close = False

        # Use existing loop if available, otherwise create one
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            close = True

        try:
            return loop.run_until_complete(self.run(urls, method, data))
        finally:
            # Close loop if it was created in this method
            if close:
                loop.close()

    async def run(self, urls, method, data):
        """
        Runs an async action.

        Args:
            urls: run against this list of urls
            method: get or post
            data: list of data for each url or None

        Returns:
            json results if any
        """

        async with aiohttp.ClientSession(raise_for_status=True) as session:
            tasks = []

            for x, url in enumerate(urls):
                if method == "post":
                    if not data or data[x]:
                        tasks.append(asyncio.ensure_future(self.post(session, url, data[x] if data else None)))
                else:
                    tasks.append(asyncio.ensure_future(self.get(session, url)))

            return await asyncio.gather(*tasks)

    async def get(self, session, url):
        """
        Runs an async HTTP GET request.

        Args:
            session: ClientSession
            url: url

        Returns:
            json results if any
        """

        async with session.get(url) as resp:
            return await resp.json()

    async def post(self, session, url, data):
        """
        Runs an async HTTP POST request.

        Args:
            session: ClientSession
            url: url
            data: data to POST

        Returns:
            json results if any
        """

        async with session.post(url, json=data) as resp:
            return await resp.json()



================================================
FILE: src/python/txtai/api/extension.py
================================================
"""
Extension module
"""


class Extension:
    """
    Defines an API extension. API extensions can expose custom pipelines or other custom logic.
    """

    def __call__(self, app):
        """
        Hook to register custom routing logic and/or modify the FastAPI instance.

        Args:
            app: FastAPI application instance
        """

        return



================================================
FILE: src/python/txtai/api/factory.py
================================================
"""
API factory module
"""

from ..util import Resolver


class APIFactory:
    """
    API factory. Creates new API instances.
    """

    @staticmethod
    def get(api):
        """
        Gets a new instance of api class.

        Args:
            api: API instance class

        Returns:
            API
        """

        return Resolver()(api)

    @staticmethod
    def create(config, api):
        """
        Creates a new API instance.

        Args:
            config: API configuration
            api: API instance class

        Returns:
            API instance
        """

        return APIFactory.get(api)(config)



================================================
FILE: src/python/txtai/api/route.py
================================================
"""
Route module
"""

from fastapi.routing import APIRoute, get_request_handler

from .responses import ResponseFactory


class EncodingAPIRoute(APIRoute):
    """
    Extended APIRoute that encodes responses based on HTTP Accept header.
    """

    def get_route_handler(self):
        """
        Resolves a response class based on the HTTP Accept header.

        Returns:
            route handler function
        """

        async def handler(request):
            route = get_request_handler(
                dependant=self.dependant,
                body_field=self.body_field,
                status_code=self.status_code,
                response_class=ResponseFactory.create(request),
                response_field=self.secure_cloned_response_field,
                response_model_include=self.response_model_include,
                response_model_exclude=self.response_model_exclude,
                response_model_by_alias=self.response_model_by_alias,
                response_model_exclude_unset=self.response_model_exclude_unset,
                response_model_exclude_defaults=self.response_model_exclude_defaults,
                response_model_exclude_none=self.response_model_exclude_none,
                dependency_overrides_provider=self.dependency_overrides_provider,
            )

            return await route(request)

        return handler



================================================
FILE: src/python/txtai/api/responses/__init__.py
================================================
"""
Responses imports
"""

from .factory import ResponseFactory
from .json import JSONEncoder, JSONResponse
from .messagepack import MessagePackResponse



================================================
FILE: src/python/txtai/api/responses/factory.py
================================================
"""
Factory module
"""

from .json import JSONResponse
from .messagepack import MessagePackResponse


class ResponseFactory:
    """
    Methods to create Response classes.
    """

    @staticmethod
    def create(request):
        """
        Gets a response class for request using the Accept header.

        Args:
            request: request

        Returns:
            response class
        """

        # Get Accept header
        accept = request.headers.get("Accept")

        # Get response class
        return MessagePackResponse if accept == MessagePackResponse.media_type else JSONResponse



================================================
FILE: src/python/txtai/api/responses/json.py
================================================
"""
JSON module
"""

import base64
import json

from io import BytesIO
from typing import Any

import fastapi.responses

from PIL.Image import Image


class JSONEncoder(json.JSONEncoder):
    """
    Extended JSONEncoder that serializes images and byte streams as base64 encoded text.
    """

    def default(self, o):
        # Convert Image to BytesIO
        if isinstance(o, Image):
            buffered = BytesIO()
            o.save(buffered, format=o.format, quality="keep")
            o = buffered

        # Unpack bytes from BytesIO
        if isinstance(o, BytesIO):
            o = o.getvalue()

        # Base64 encode bytes instances
        if isinstance(o, bytes):
            return base64.b64encode(o).decode("utf-8")

        # Default handler
        return super().default(o)


class JSONResponse(fastapi.responses.JSONResponse):
    """
    Extended JSONResponse that serializes images and byte streams as base64 encoded text.
    """

    def render(self, content: Any) -> bytes:
        """
        Renders content to the response.

        Args:
            content: input content

        Returns:
            rendered content as bytes
        """

        return json.dumps(content, ensure_ascii=False, allow_nan=False, indent=None, separators=(",", ":"), cls=JSONEncoder).encode("utf-8")



================================================
FILE: src/python/txtai/api/responses/messagepack.py
================================================
"""
MessagePack module
"""

from io import BytesIO
from typing import Any

import msgpack

from fastapi import Response
from PIL.Image import Image


class MessagePackResponse(Response):
    """
    Encodes responses with MessagePack.
    """

    media_type = "application/msgpack"

    def render(self, content: Any) -> bytes:
        """
        Renders content to the response.

        Args:
            content: input content

        Returns:
            rendered content as bytes
        """

        return msgpack.packb(content, default=MessagePackEncoder())


class MessagePackEncoder:
    """
    Extended MessagePack encoder that converts images to bytes.
    """

    def __call__(self, o):
        # Convert Image to bytes
        if isinstance(o, Image):
            buffered = BytesIO()
            o.save(buffered, format=o.format, quality="keep")
            o = buffered

        # Get bytes from BytesIO
        if isinstance(o, BytesIO):
            o = o.getvalue()

        return o



================================================
FILE: src/python/txtai/api/routers/__init__.py
================================================
"""
Router imports
"""

from . import agent
from . import caption
from . import embeddings
from . import entity
from . import extractor
from . import labels
from . import llm
from . import objects
from . import openai
from . import rag
from . import reranker
from . import segmentation
from . import similarity
from . import summary
from . import tabular
from . import textractor
from . import texttospeech
from . import transcription
from . import translation
from . import workflow
from . import upload



================================================
FILE: src/python/txtai/api/routers/agent.py
================================================
"""
Defines API paths for agent endpoints.
"""

from typing import Optional

from fastapi import APIRouter, Body
from fastapi.responses import StreamingResponse

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/agent")
def agent(name: str = Body(...), text: str = Body(...), maxlength: Optional[int] = Body(default=None), stream: Optional[bool] = Body(default=None)):
    """
    Executes a named agent for input text.

    Args:
        name: agent name
        text: instructions to run
        maxlength: maximum sequence length
        stream: stream response if True, defaults to False

    Returns:
        response text
    """

    # Build keyword arguments
    kwargs = {key: value for key, value in [("stream", stream), ("maxlength", maxlength)] if value}

    # Run agent
    result = application.get().agent(name, text, **kwargs)

    # Handle both standard and streaming responses
    return StreamingResponse(result) if stream else result



================================================
FILE: src/python/txtai/api/routers/caption.py
================================================
"""
Defines API paths for caption endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/caption")
def caption(file: str):
    """
    Builds captions for images.

    Args:
        file: file to process

    Returns:
        list of captions
    """

    return application.get().pipeline("caption", (file,))


@router.post("/batchcaption")
def batchcaption(files: List[str] = Body(...)):
    """
    Builds captions for images.

    Args:
        files: list of files to process

    Returns:
        list of captions
    """

    return application.get().pipeline("caption", (files,))



================================================
FILE: src/python/txtai/api/routers/embeddings.py
================================================
"""
Defines API paths for embeddings endpoints.
"""

from io import BytesIO
from typing import List, Optional

import PIL

from fastapi import APIRouter, Body, File, Form, HTTPException, Request, UploadFile
from fastapi.encoders import jsonable_encoder

from .. import application
from ..responses import ResponseFactory
from ..route import EncodingAPIRoute

from ...app import ReadOnlyError
from ...graph import Graph

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/search")
def search(query: str, request: Request):
    """
    Finds documents most similar to the input query. This method will run either an index search
    or an index + database search depending on if a database is available.

    Args:
        query: input query
        request: FastAPI request

    Returns:
        list of {id: value, score: value} for index search, list of dict for an index + database search
    """

    # Execute search
    results = application.get().search(query, request=request)

    # Encode using standard FastAPI encoder but skip certain classes
    results = jsonable_encoder(
        results, custom_encoder={bytes: lambda x: x, BytesIO: lambda x: x, PIL.Image.Image: lambda x: x, Graph: lambda x: x.savedict()}
    )

    # Return raw response to prevent duplicate encoding
    response = ResponseFactory.create(request)
    return response(results)


# pylint: disable=W0621
@router.post("/batchsearch")
def batchsearch(
    request: Request,
    queries: List[str] = Body(...),
    limit: int = Body(default=None),
    weights: float = Body(default=None),
    index: str = Body(default=None),
    parameters: List[dict] = Body(default=None),
    graph: bool = Body(default=False),
):
    """
    Finds documents most similar to the input queries. This method will run either an index search
    or an index + database search depending on if a database is available.

    Args:
        queries: input queries
        limit: maximum results
        weights: hybrid score weights, if applicable
        index: index name, if applicable
        parameters: list of dicts of named parameters to bind to placeholders
        graph: return graph results if True

    Returns:
        list of {id: value, score: value} per query for index search, list of dict per query for an index + database search
    """

    # Execute search
    results = application.get().batchsearch(queries, limit, weights, index, parameters, graph)

    # Encode using standard FastAPI encoder but skip certain classes
    results = jsonable_encoder(
        results, custom_encoder={bytes: lambda x: x, BytesIO: lambda x: x, PIL.Image.Image: lambda x: x, Graph: lambda x: x.savedict()}
    )

    # Return raw response to prevent duplicate encoding
    response = ResponseFactory.create(request)
    return response(results)


@router.post("/add")
def add(documents: List[dict] = Body(...)):
    """
    Adds a batch of documents for indexing.

    Args:
        documents: list of {id: value, text: value, tags: value}
    """

    try:
        application.get().add(documents)
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.post("/addobject")
def addobject(data: List[bytes] = File(), uid: List[str] = Form(default=None), field: str = Form(default=None)):
    """
    Adds a batch of binary documents for indexing.

    Args:
        data: list of binary objects
        uid: list of corresponding ids
        field: optional object field name
    """

    if uid and len(data) != len(uid):
        raise HTTPException(status_code=422, detail="Length of data and document lists must match")

    try:
        # Add objects
        application.get().addobject(data, uid, field)
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.post("/addimage")
def addimage(data: List[UploadFile] = File(), uid: List[str] = Form(), field: str = Form(default=None)):
    """
    Adds a batch of images for indexing.

    Args:
        data: list of images
        uid: list of corresponding ids
        field: optional object field name
    """

    if uid and len(data) != len(uid):
        raise HTTPException(status_code=422, detail="Length of data and uid lists must match")

    try:
        # Add images
        application.get().addobject([PIL.Image.open(content.file) for content in data], uid, field)
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.get("/index")
def index():
    """
    Builds an embeddings index for previously batched documents.
    """

    try:
        application.get().index()
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.get("/upsert")
def upsert():
    """
    Runs an embeddings upsert operation for previously batched documents.
    """

    try:
        application.get().upsert()
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.post("/delete")
def delete(ids: List = Body(...)):
    """
    Deletes from an embeddings index. Returns list of ids deleted.

    Args:
        ids: list of ids to delete

    Returns:
        ids deleted
    """

    try:
        return application.get().delete(ids)
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.post("/reindex")
def reindex(config: dict = Body(...), function: str = Body(default=None)):
    """
    Recreates this embeddings index using config. This method only works if document content storage is enabled.

    Args:
        config: new config
        function: optional function to prepare content for indexing
    """

    try:
        application.get().reindex(config, function)
    except ReadOnlyError as e:
        raise HTTPException(status_code=403, detail=e.args[0]) from e


@router.get("/count")
def count():
    """
    Total number of elements in this embeddings index.

    Returns:
        number of elements in embeddings index
    """

    return application.get().count()


@router.post("/explain")
def explain(query: str = Body(...), texts: List[str] = Body(default=None), limit: int = Body(default=None)):
    """
    Explains the importance of each input token in text for a query.

    Args:
        query: query text
        texts: list of text

    Returns:
        list of dict where a higher scores represents higher importance relative to the query
    """

    return application.get().explain(query, texts, limit)


@router.post("/batchexplain")
def batchexplain(queries: List[str] = Body(...), texts: List[str] = Body(default=None), limit: int = Body(default=None)):
    """
    Explains the importance of each input token in text for a query.

    Args:
        query: query text
        texts: list of text

    Returns:
        list of dict where a higher scores represents higher importance relative to the query
    """

    return application.get().batchexplain(queries, texts, limit)


@router.get("/transform")
def transform(text: str, category: Optional[str] = None, index: Optional[str] = None):
    """
    Transforms text into an embeddings array.

    Args:
        text: input text
        category: category for instruction-based embeddings
        index: index name, if applicable

    Returns:
        embeddings array
    """

    return application.get().transform(text, category, index)


@router.post("/batchtransform")
def batchtransform(texts: List[str] = Body(...), category: Optional[str] = None, index: Optional[str] = None):
    """
    Transforms list of text into embeddings arrays.

    Args:
        texts: list of text
        category: category for instruction-based embeddings
        index: index name, if applicable

    Returns:
        embeddings arrays
    """

    return application.get().batchtransform(texts, category, index)



================================================
FILE: src/python/txtai/api/routers/entity.py
================================================
"""
Defines API paths for entity endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/entity")
def entity(text: str):
    """
    Applies a token classifier to text.

    Args:
        text: input text

    Returns:
        list of (entity, entity type, score) per text element
    """

    return application.get().pipeline("entity", (text,))


@router.post("/batchentity")
def batchentity(texts: List[str] = Body(...)):
    """
    Applies a token classifier to text.

    Args:
        texts: list of text

    Returns:
        list of (entity, entity type, score) per text element
    """

    return application.get().pipeline("entity", (texts,))



================================================
FILE: src/python/txtai/api/routers/extractor.py
================================================
"""
Defines API paths for extractor endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/extract")
def extract(queue: List[dict] = Body(...), texts: Optional[List[str]] = Body(default=None)):
    """
    Extracts answers to input questions.

    Args:
        queue: list of {name: value, query: value, question: value, snippet: value}
        texts: optional list of text

    Returns:
        list of {name: value, answer: value}
    """

    return application.get().extract(queue, texts)



================================================
FILE: src/python/txtai/api/routers/labels.py
================================================
"""
Defines API paths for labels endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/label")
def label(text: str = Body(...), labels: List[str] = Body(...)):
    """
    Applies a zero shot classifier to text using a list of labels. Returns a list of
    {id: value, score: value} sorted by highest score, where id is the index in labels.

    Args:
        text: input text
        labels: list of labels

    Returns:
        list of {id: value, score: value} per text element
    """

    return application.get().label(text, labels)


@router.post("/batchlabel")
def batchlabel(texts: List[str] = Body(...), labels: List[str] = Body(...)):
    """
    Applies a zero shot classifier to list of text using a list of labels. Returns a list of
    {id: value, score: value} sorted by highest score, where id is the index in labels per
    text element.

    Args:
        texts: list of text
        labels: list of labels

    Returns:
        list of {id: value score: value} per text element
    """

    return application.get().label(texts, labels)



================================================
FILE: src/python/txtai/api/routers/llm.py
================================================
"""
Defines API paths for llm endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body
from fastapi.responses import StreamingResponse

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/llm")
def llm(text: str, maxlength: Optional[int] = None, stream: Optional[bool] = False, stripthink: Optional[bool] = False):
    """
    Runs a LLM pipeline for the input text.

    Args:
        text: input text
        maxlength: optional response max length
        stream: streams response if True

    Returns:
        response text
    """

    # Build keyword arguments
    params = [("maxlength", maxlength), ("stream", stream), ("stripthink", stripthink)]
    kwargs = {key: value for key, value in params if value}

    # Run pipeline
    result = application.get().pipeline("llm", text, **kwargs)

    # Handle both standard and streaming responses
    return StreamingResponse(result) if stream else result


@router.post("/batchllm")
def batchllm(
    texts: List[str] = Body(...),
    maxlength: Optional[int] = Body(default=None),
    stream: Optional[bool] = Body(default=False),
    stripthink: Optional[bool] = Body(default=False),
):
    """
    Runs a LLM pipeline for the input texts.

    Args:
        texts: input texts
        maxlength: optional response max length
        stream: streams response if True

    Returns:
        response texts
    """

    # Build keyword arguments
    params = [("maxlength", maxlength), ("stream", stream), ("stripthink", stripthink)]
    kwargs = {key: value for key, value in params if value}

    # Run pipeline
    result = application.get().pipeline("llm", texts, **kwargs)

    # Handle both standard and streaming responses
    return StreamingResponse(result) if stream else result



================================================
FILE: src/python/txtai/api/routers/objects.py
================================================
"""
Defines API paths for objects endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/objects")
def objects(file: str):
    """
    Applies object detection/image classification models to images.

    Args:
        file: file to process

    Returns:
        list of (label, score) elements
    """

    return application.get().pipeline("objects", (file,))


@router.post("/batchobjects")
def batchobjects(files: List[str] = Body(...)):
    """
    Applies object detection/image classification models to images.

    Args:
        files: list of files to process

    Returns:
        list of (label, score) elements
    """

    return application.get().pipeline("objects", (files,))



================================================
FILE: src/python/txtai/api/routers/openai.py
================================================
"""
Defines an OpenAI-compatible API endpoint for txtai.

See the following specification for more information:
https://github.com/openai/openai-openapi
"""

import uuid
import json
import time

from typing import List, Optional, Union

from fastapi import APIRouter, Body, Form, UploadFile
from fastapi.responses import Response, StreamingResponse

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


# pylint: disable=W0622
@router.post("/v1/chat/completions")
def chat(
    messages: List[dict] = Body(...),
    model: str = Body(...),
    max_completion_tokens: Optional[int] = Body(default=None),
    stream: Optional[bool] = Body(default=False),
):
    """
    Runs a chat completion request.

    Args:
        messages: list of messages [{"role": role, "content": content}]
        model: agent name, workflow name, pipeline name or embeddings
        max_completion_tokens: sets the max length to generate
        stream: streams response if True

    Returns:
        chat completion
    """

    # Build keyword arguments
    kwargs = {key: value for key, value in [("stream", stream), ("maxlength", max_completion_tokens)] if value}

    # Get first message
    message = messages[0]["content"]

    # Agent
    if model in application.get().agents:
        result = application.get().agent(model, message, **kwargs)

    # Embeddings search
    elif model == "embeddings":
        result = application.get().search(message, 1, **kwargs)[0]["text"]

    # Pipeline
    elif model in application.get().pipelines and model != "llm":
        result = application.get().pipeline(model, message, **kwargs)

    # Workflow
    elif model in application.get().workflows:
        result = list(application.get().workflow(model, [message], **kwargs))[0]

    # Default to running all messages through default LLM
    else:
        result = application.get().pipeline("llm", messages, **kwargs)

    # Write response
    return StreamingResponse(StreamingChatResponse()(model, result)) if stream else ChatResponse()(model, result)


@router.post("/v1/embeddings")
def embeddings(input: Union[str, List[str]] = Body(...), model: str = Body(...)):
    """
    Creates an embeddings vector for the input text.

    Args:
        input: text|list
        model: model name

    Returns:
        list of embeddings vectors
    """

    # Convert to embeddings
    result = application.get().batchtransform([input] if isinstance(input, str) else input)

    # Build and return response
    data = []
    for index, embedding in enumerate(result):
        data.append({"object": "embedding", "embedding": embedding, "index": index})

    return {"object": "list", "data": data, "model": model}


@router.post("/v1/audio/speech")
def speech(input: str = Body(...), voice: str = Body(...), response_format: Optional[str] = Body(default="mp3")):
    """
    Generates speech for the input text.

    Args:
        input: input text
        voice: speaker name
        response_format: audio encoding format, defaults to mp3

    Returns:
        audio data
    """

    # Convert to audio
    audio = application.get().pipeline("texttospeech", input, speaker=voice, encoding=response_format)

    # Write audio
    return Response(audio)


@router.post("/v1/audio/transcriptions")
def transcribe(file: UploadFile, language: Optional[str] = Form(default=None), response_format: Optional[str] = Form(default="json")):
    """
    Transcribes audio to text.

    Args:
        file: audio input file
        language: language of input audio
        response_format: output format (json or text)

    Returns:
        transcribed text
    """

    # Transcribe
    text = application.get().pipeline("transcription", file.file, language=language, task="transcribe")
    return text if response_format == "text" else {"text": text}


@router.post("/v1/audio/translations")
def translate(
    file: UploadFile,
    response_format: Optional[str] = Form(default="json"),
):
    """
    Translates audio to English.

    Args:
        file: audio input file
        response_format: output format (json or text)

    Returns:
        translated text
    """

    # Transcribe and translate to English
    text = application.get().pipeline("transcription", file.file, language="English", task="translate")
    return text if response_format == "text" else {"text": text}


class ChatResponse:
    """
    Returns a chat response object.
    """

    def __call__(self, model, result):
        return {
            "id": str(uuid.uuid4()),
            "object": "chat.completion",
            "created": int(time.time() * 1000),
            "model": model,
            "choices": [{"id": 0, "message": {"role": "assistant", "content": result}, "finish_reason": "stop"}],
        }


class StreamingChatResponse:
    """
    Returns a streaming chat response object.
    """

    def __call__(self, model, result):
        for chunk in result:
            yield "data: " + json.dumps(
                {
                    "id": str(uuid.uuid4()),
                    "object": "chat.completion.chunk",
                    "created": int(time.time() * 1000),
                    "model": model,
                    "choices": [{"id": 0, "delta": {"content": chunk}}],
                }
            ) + "\n\n"

        yield "data: [DONE]\n\n"



================================================
FILE: src/python/txtai/api/routers/rag.py
================================================
"""
Defines API paths for rag endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body
from fastapi.responses import StreamingResponse

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/rag")
def rag(query: str, maxlength: Optional[int] = None, stream: Optional[bool] = False, stripthink: Optional[bool] = False):
    """
    Runs a RAG pipeline for the input query.

    Args:
        query: input RAG query
        maxlength: optional response max length
        stream: streams response if True

    Returns:
        answer
    """

    # Build keyword arguments
    params = [("maxlength", maxlength), ("stream", stream), ("stripthink", stripthink)]
    kwargs = {key: value for key, value in params if value}

    # Run pipeline
    result = application.get().pipeline("rag", query, **kwargs)

    # Handle both standard and streaming responses
    return StreamingResponse(result) if stream else result


@router.post("/batchrag")
def batchrag(
    queries: List[str] = Body(...),
    maxlength: Optional[int] = Body(default=None),
    stream: Optional[bool] = Body(default=False),
    stripthink: Optional[bool] = Body(default=False),
):
    """
    Runs a RAG pipeline for the input queries.

    Args:
        queries: input RAG queries
        maxlength: optional response max length
        stream: streams response if True

    Returns:
        answers
    """

    # Build keyword arguments
    params = [("maxlength", maxlength), ("stream", stream), ("stripthink", stripthink)]
    kwargs = {key: value for key, value in params if value}

    # Run pipeline
    result = application.get().pipeline("rag", queries, **kwargs)

    # Handle both standard and streaming responses
    return StreamingResponse(result) if stream else result



================================================
FILE: src/python/txtai/api/routers/reranker.py
================================================
"""
Defines API paths for reranking endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/rerank")
def rerank(query: str, limit: Optional[int] = 3, factor: Optional[int] = 10):
    """
    Queries an embeddings database and reranks the results with a similarity pipeline.

    Args:
        query: query text
        limit: maximum results
        factor: factor to multiply limit by for the initial embeddings search

    Returns:
        query results
    """

    return application.get().pipeline("reranker", (query, limit, factor))


@router.post("/batchrerank")
def batchrerank(queries: List[str] = Body(...), limit: Optional[int] = Body(default=3), factor: Optional[int] = Body(default=10)):
    """
    Queries an embeddings database and reranks the results with a similarity pipeline.

    Args:
        queries: list of queries
        limit: maximum results
        factor: factor to multiply limit by for the initial embeddings search

    Returns:
        query results
    """

    return application.get().pipeline("reranker", (queries, limit, factor))



================================================
FILE: src/python/txtai/api/routers/segmentation.py
================================================
"""
Defines API paths for segmentation endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/segment")
def segment(text: str):
    """
    Segments text into semantic units.

    Args:
        text: input text

    Returns:
        segmented text
    """

    return application.get().pipeline("segmentation", (text,))


@router.post("/batchsegment")
def batchsegment(texts: List[str] = Body(...)):
    """
    Segments text into semantic units.

    Args:
        texts: list of texts to segment

    Returns:
        list of segmented text
    """

    return application.get().pipeline("segmentation", (texts,))



================================================
FILE: src/python/txtai/api/routers/similarity.py
================================================
"""
Defines API paths for similarity endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/similarity")
def similarity(query: str = Body(...), texts: List[str] = Body(...)):
    """
    Computes the similarity between query and list of text. Returns a list of
    {id: value, score: value} sorted by highest score, where id is the index
    in texts.

    Args:
        query: query text
        texts: list of text

    Returns:
        list of {id: value, score: value}
    """

    return application.get().similarity(query, texts)


@router.post("/batchsimilarity")
def batchsimilarity(queries: List[str] = Body(...), texts: List[str] = Body(...)):
    """
    Computes the similarity between list of queries and list of text. Returns a list
    of {id: value, score: value} sorted by highest score per query, where id is the
    index in texts.

    Args:
        queries: queries text
        texts: list of text

    Returns:
        list of {id: value, score: value} per query
    """

    return application.get().batchsimilarity(queries, texts)



================================================
FILE: src/python/txtai/api/routers/summary.py
================================================
"""
Defines API paths for summary endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/summary")
def summary(text: str, minlength: Optional[int] = None, maxlength: Optional[int] = None):
    """
    Runs a summarization model against a block of text.

    Args:
        text: text to summarize
        minlength: minimum length for summary
        maxlength: maximum length for summary

    Returns:
        summary text
    """

    return application.get().pipeline("summary", (text, minlength, maxlength))


@router.post("/batchsummary")
def batchsummary(texts: List[str] = Body(...), minlength: Optional[int] = Body(default=None), maxlength: Optional[int] = Body(default=None)):
    """
    Runs a summarization model against a block of text.

    Args:
        texts: list of text to summarize
        minlength: minimum length for summary
        maxlength: maximum length for summary

    Returns:
        list of summary text
    """

    return application.get().pipeline("summary", (texts, minlength, maxlength))



================================================
FILE: src/python/txtai/api/routers/tabular.py
================================================
"""
Defines API paths for tabular endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/tabular")
def tabular(file: str):
    """
    Splits tabular data into rows and columns.

    Args:
        file: file to process

    Returns:
        list of (id, text, tag) elements
    """

    return application.get().pipeline("tabular", (file,))


@router.post("/batchtabular")
def batchtabular(files: List[str] = Body(...)):
    """
    Splits tabular data into rows and columns.

    Args:
        files: list of files to process

    Returns:
        list of (id, text, tag) elements
    """

    return application.get().pipeline("tabular", (files,))



================================================
FILE: src/python/txtai/api/routers/textractor.py
================================================
"""
Defines API paths for textractor endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/textract")
def textract(file: str):
    """
    Extracts text from a file at path.

    Args:
        file: file to extract text

    Returns:
        extracted text
    """

    return application.get().pipeline("textractor", (file,))


@router.post("/batchtextract")
def batchtextract(files: List[str] = Body(...)):
    """
    Extracts text from a file at path.

    Args:
        files: list of files to extract text

    Returns:
        list of extracted text
    """

    return application.get().pipeline("textractor", (files,))



================================================
FILE: src/python/txtai/api/routers/texttospeech.py
================================================
"""
Defines API paths for TTS endpoints
"""

from typing import Optional

from fastapi import APIRouter, Response

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/texttospeech")
def texttospeech(text: str, speaker: Optional[str] = None, encoding: Optional[str] = "mp3"):
    """
    Generates speech from text.

    Args:
        text: text
        speaker: speaker id, defaults to 1
        encoding: optional audio encoding format

    Returns:
        Audio data
    """

    # Convert to audio
    audio = application.get().pipeline("texttospeech", text, speaker=speaker, encoding=encoding)

    # Write audio
    return Response(audio, headers={"Content-Disposition": f"attachment;filename=speech.{encoding.lower()}"})



================================================
FILE: src/python/txtai/api/routers/transcription.py
================================================
"""
Defines API paths for transcription endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/transcribe")
def transcribe(file: str):
    """
    Transcribes audio files to text.

    Args:
        file: file to transcribe

    Returns:
        transcribed text
    """

    return application.get().pipeline("transcription", (file,))


@router.post("/batchtranscribe")
def batchtranscribe(files: List[str] = Body(...)):
    """
    Transcribes audio files to text.

    Args:
        files: list of files to transcribe

    Returns:
        list of transcribed text
    """

    return application.get().pipeline("transcription", (files,))



================================================
FILE: src/python/txtai/api/routers/translation.py
================================================
"""
Defines API paths for translation endpoints.
"""

from typing import List, Optional

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.get("/translate")
def translate(text: str, target: Optional[str] = "en", source: Optional[str] = None):
    """
    Translates text from source language into target language.

    Args:
        text: text to translate
        target: target language code, defaults to "en"
        source: source language code, detects language if not provided

    Returns:
        translated text
    """

    return application.get().pipeline("translation", (text, target, source))


@router.post("/batchtranslate")
def batchtranslate(texts: List[str] = Body(...), target: Optional[str] = Body(default="en"), source: Optional[str] = Body(default=None)):
    """
    Translates text from source language into target language.

    Args:
        texts: list of text to translate
        target: target language code, defaults to "en"
        source: source language code, detects language if not provided

    Returns:
        list of translated text
    """

    return application.get().pipeline("translation", (texts, target, source))



================================================
FILE: src/python/txtai/api/routers/upload.py
================================================
"""
Defines API paths for upload endpoints.
"""

import shutil
import tempfile

from typing import List

from fastapi import APIRouter, File, Form, UploadFile

from ..route import EncodingAPIRoute


router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/upload")
def upload(files: List[UploadFile] = File(), suffix: str = Form(default=None)):
    """
    Uploads files for local server processing.

    Args:
        data: list of files to upload

    Returns:
        list of server paths
    """

    paths = []
    for f in files:
        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=suffix) as tmp:
            shutil.copyfileobj(f.file, tmp)
            paths.append(tmp.name)

    return paths



================================================
FILE: src/python/txtai/api/routers/workflow.py
================================================
"""
Defines API paths for workflow endpoints.
"""

from typing import List

from fastapi import APIRouter, Body

from .. import application
from ..route import EncodingAPIRoute

router = APIRouter(route_class=EncodingAPIRoute)


@router.post("/workflow")
def workflow(name: str = Body(...), elements: List = Body(...)):
    """
    Executes a named workflow using elements as input.

    Args:
        name: workflow name
        elements: list of elements to run through workflow

    Returns:
        list of processed elements
    """

    return application.get().workflow(name, elements)



================================================
FILE: src/python/txtai/app/__init__.py
================================================
"""
App imports
"""

from .base import Application, ReadOnlyError



================================================
FILE: src/python/txtai/app/base.py
================================================
"""
Application module
"""

import os

from multiprocessing.pool import ThreadPool
from threading import RLock

import yaml

from ..agent import Agent
from ..embeddings import Documents, Embeddings
from ..pipeline import PipelineFactory
from ..workflow import WorkflowFactory


# pylint: disable=R0904
class Application:
    """
    Builds YAML-configured txtai applications.
    """

    @staticmethod
    def read(data):
        """
        Reads a YAML configuration file.

        Args:
            data: input data

        Returns:
            yaml
        """

        if isinstance(data, str):
            if os.path.exists(data):
                # Read yaml from file
                with open(data, "r", encoding="utf-8") as f:
                    # Read configuration
                    return yaml.safe_load(f)

            # Attempt to read yaml from input
            data = yaml.safe_load(data)
            if not isinstance(data, str):
                return data

            # File not found and input is not yaml, raise error
            raise FileNotFoundError(f"Unable to load file '{data}'")

        # Return unmodified
        return data

    def __init__(self, config, loaddata=True):
        """
        Creates an Application instance, which encapsulates embeddings, pipelines and workflows.

        Args:
            config: index configuration
            loaddata: If True (default), load existing index data, if available. Otherwise, only load models.
        """

        # Initialize member variables
        self.config, self.documents, self.embeddings = Application.read(config), None, None

        # Write lock - allows only a single thread to update embeddings
        self.lock = RLock()

        # ThreadPool - runs scheduled workflows
        self.pool = None

        # Create pipelines
        self.createpipelines()

        # Create workflows
        self.createworkflows()

        # Create agents
        self.createagents()

        # Create embeddings index
        self.indexes(loaddata)

    def __del__(self):
        """
        Close threadpool when this object is garbage collected.
        """

        if hasattr(self, "pool") and self.pool:
            self.pool.close()
            self.pool = None

    def createpipelines(self):
        """
        Create pipelines.
        """

        # Pipeline definitions
        self.pipelines = {}

        # Default pipelines
        pipelines = list(PipelineFactory.list().keys())

        # Add custom pipelines
        for key in self.config:
            if "." in key:
                pipelines.append(key)

        # Move dependent pipelines to end of list
        dependent = ["similarity", "extractor", "rag", "reranker"]
        pipelines = sorted(pipelines, key=lambda x: dependent.index(x) + 1 if x in dependent else 0)

        # Create pipelines
        for pipeline in pipelines:
            if pipeline in self.config:
                config = self.config[pipeline] if self.config[pipeline] else {}

                # Add application reference, if requested
                if "application" in config:
                    config["application"] = self

                # Custom pipeline parameters
                if pipeline in ["extractor", "rag"]:
                    if "similarity" not in config:
                        # Add placeholder, will be set to embeddings index once initialized
                        config["similarity"] = None

                    # Resolve reference pipelines
                    if config.get("similarity") in self.pipelines:
                        config["similarity"] = self.pipelines[config["similarity"]]

                    if config.get("path") in self.pipelines:
                        config["path"] = self.pipelines[config["path"]]

                elif pipeline == "similarity" and "path" not in config and "labels" in self.pipelines:
                    config["model"] = self.pipelines["labels"]

                elif pipeline == "reranker":
                    config["embeddings"] = None
                    config["similarity"] = self.pipelines["similarity"]

                self.pipelines[pipeline] = PipelineFactory.create(config, pipeline)

    def createworkflows(self):
        """
        Create workflows.
        """

        # Workflow definitions
        self.workflows = {}

        # Create workflows
        if "workflow" in self.config:
            for workflow, config in self.config["workflow"].items():
                # Create copy of config
                config = config.copy()

                # Resolve callable functions
                config["tasks"] = [self.resolvetask(task) for task in config["tasks"]]

                # Resolve stream functions
                if "stream" in config:
                    config["stream"] = self.resolvetask(config["stream"])

                # Get scheduler config
                schedule = config.pop("schedule", None)

                # Create workflow
                self.workflows[workflow] = WorkflowFactory.create(config, workflow)

                # Schedule job if necessary
                if schedule:
                    # Create pool if necessary
                    if not self.pool:
                        self.pool = ThreadPool()

                    self.pool.apply_async(self.workflows[workflow].schedule, kwds=schedule)

    def createagents(self):
        """
        Create agents.
        """

        # Agent definitions
        self.agents = {}

        # Create agents
        if "agent" in self.config:
            for agent, config in self.config["agent"].items():
                # Create copy of config
                config = config.copy()

                # Resolve LLM
                config["llm"] = self.function("llm")

                # Resolve tools
                for tool in config.get("tools", []):
                    if isinstance(tool, dict) and "target" in tool:
                        tool["target"] = self.function(tool["target"])

                # Create agent
                self.agents[agent] = Agent(**config)

    def indexes(self, loaddata):
        """
        Initialize an embeddings index.

        Args:
            loaddata: If True (default), load existing index data, if available. Otherwise, only load models.
        """

        # Get embeddings configuration
        config = self.config.get("embeddings")
        if config:
            # Resolve application functions in embeddings config
            config = self.resolveconfig(config.copy())

        # Load embeddings index if loaddata and index exists
        if loaddata and Embeddings().exists(self.config.get("path"), self.config.get("cloud")):
            # Initialize empty embeddings
            self.embeddings = Embeddings()

            # Pass path and cloud settings. Set application functions as config overrides.
            self.embeddings.load(
                self.config.get("path"),
                self.config.get("cloud"),
                {key: config[key] for key in ["functions", "transform"] if key in config} if config else None,
            )

        elif "embeddings" in self.config:
            # Create new embeddings with config
            self.embeddings = Embeddings(config)

        # If an extractor pipeline is defined and the similarity attribute is None, set to embeddings index
        for key in ["extractor", "rag"]:
            pipeline = self.pipelines.get(key)
            config = self.config.get(key)

            if pipeline and config is not None and config["similarity"] is None:
                pipeline.similarity = self.embeddings

        # Attach embeddings to reranker
        if "reranker" in self.pipelines:
            self.pipelines["reranker"].embeddings = self.embeddings

    def resolvetask(self, task):
        """
        Resolves callable functions for a task.

        Args:
            task: input task config
        """

        # Check for task shorthand syntax
        task = {"action": task} if isinstance(task, (str, list)) else task

        if "action" in task:
            action = task["action"]
            values = [action] if not isinstance(action, list) else action

            actions = []
            for a in values:
                if a in ["index", "upsert"]:
                    # Add queue action to buffer documents to index
                    actions.append(self.add)

                    # Override and disable unpacking for indexing actions
                    task["unpack"] = False

                    # Add finalize to trigger indexing
                    task["finalize"] = self.upsert if a == "upsert" else self.index
                elif a == "search":
                    actions.append(self.batchsearch)
                elif a == "transform":
                    # Transform vectors
                    actions.append(self.batchtransform)

                    # Override and disable one-to-many transformations
                    task["onetomany"] = False
                else:
                    # Resolve action to callable function
                    actions.append(self.function(a))

            # Save resolved action(s)
            task["action"] = actions[0] if not isinstance(action, list) else actions

        # Resolve initializer
        if "initialize" in task and isinstance(task["initialize"], str):
            task["initialize"] = self.function(task["initialize"])

        # Resolve finalizer
        if "finalize" in task and isinstance(task["finalize"], str):
            task["finalize"] = self.function(task["finalize"])

        return task

    def resolveconfig(self, config):
        """
        Resolves callable functions stored in embeddings configuration.

        Args:
            config: embeddings config

        Returns:
            resolved config
        """

        if "functions" in config:
            # Resolve callable functions
            functions = []
            for fn in config["functions"]:
                original = fn
                try:
                    if isinstance(fn, dict):
                        fn = fn.copy()
                        fn["function"] = self.function(fn["function"])
                    else:
                        fn = self.function(fn)

                # pylint: disable=W0703
                except Exception:
                    # Not a resolvable function, pipeline or workflow - further resolution will happen in embeddings
                    fn = original

                functions.append(fn)

            config["functions"] = functions

        if "transform" in config:
            # Resolve transform function
            config["transform"] = self.function(config["transform"])

        return config

    def function(self, function):
        """
        Get a handle to a callable function.

        Args:
            function: function name

        Returns:
            resolved function
        """

        # Check if function is a pipeline
        if function in self.pipelines:
            return self.pipelines[function]

        # Check if function is a workflow
        if function in self.workflows:
            return self.workflows[function]

        # Attempt to resolve action as a callable function
        return PipelineFactory.create({}, function)

    def search(self, query, limit=10, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input query. This method will run either an index search
        or an index + database search depending on if a database is available.

        Args:
            query: input query
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: dict of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of {id: value, score: value} for index search, list of dict for an index + database search
        """

        if self.embeddings:
            with self.lock:
                results = self.embeddings.search(query, limit, weights, index, parameters, graph)

            # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries.
            return results if graph else [{"id": r[0], "score": float(r[1])} if isinstance(r, tuple) else r for r in results]

        return None

    def batchsearch(self, queries, limit=10, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input queries. This method will run either an index search
        or an index + database search depending on if a database is available.

        Args:
            queries: input queries
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: list of dicts of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of {id: value, score: value} per query for index search, list of dict per query for an index + database search
        """

        if self.embeddings:
            with self.lock:
                search = self.embeddings.batchsearch(queries, limit, weights, index, parameters, graph)

            results = []
            for result in search:
                # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries.
                results.append(result if graph else [{"id": r[0], "score": float(r[1])} if isinstance(r, tuple) else r for r in result])
            return results

        return None

    def add(self, documents):
        """
        Adds a batch of documents for indexing.

        Args:
            documents: list of {id: value, data: value, tags: value}

        Returns:
            unmodified input documents
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to add documents to a read-only index (writable != True)")

        if self.embeddings:
            with self.lock:
                # Create documents file if not already open
                if not self.documents:
                    self.documents = Documents()

                # Add documents
                self.documents.add(list(documents))

        # Return unmodified input documents
        return documents

    def addobject(self, data, uid, field):
        """
        Helper method that builds a batch of object documents.

        Args:
            data: object content
            uid: optional list of corresponding uids
            field: optional field to set

        Returns:
            documents
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to add documents to a read-only index (writable != True)")

        documents = []
        for x, content in enumerate(data):
            if field:
                row = {"id": uid[x], field: content} if uid else {field: content}
            elif uid:
                row = (uid[x], content)
            else:
                row = content

            documents.append(row)

        return self.add(documents)

    def index(self):
        """
        Builds an embeddings index for previously batched documents.
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to index a read-only index (writable != True)")

        if self.embeddings and self.documents:
            with self.lock:
                # Reset index
                self.indexes(False)

                # Build scoring index if term weighting is enabled
                if self.embeddings.isweighted():
                    self.embeddings.score(self.documents)

                # Build embeddings index
                self.embeddings.index(self.documents)

                # Save index if path available, otherwise this is an memory-only index
                if self.config.get("path"):
                    self.embeddings.save(self.config["path"], self.config.get("cloud"))

                # Reset document stream
                self.documents.close()
                self.documents = None

    def upsert(self):
        """
        Runs an embeddings upsert operation for previously batched documents.
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to upsert a read-only index (writable != True)")

        if self.embeddings and self.documents:
            with self.lock:
                # Run upsert
                self.embeddings.upsert(self.documents)

                # Save index if path available, otherwise this is an memory-only index
                if self.config.get("path"):
                    self.embeddings.save(self.config["path"], self.config.get("cloud"))

                # Reset document stream
                self.documents.close()
                self.documents = None

    def delete(self, ids):
        """
        Deletes from an embeddings index. Returns list of ids deleted.

        Args:
            ids: list of ids to delete

        Returns:
            ids deleted
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to delete from a read-only index (writable != True)")

        if self.embeddings:
            with self.lock:
                # Run delete operation
                deleted = self.embeddings.delete(ids)

                # Save index if path available, otherwise this is an memory-only index
                if self.config.get("path"):
                    self.embeddings.save(self.config["path"], self.config.get("cloud"))

                # Return deleted ids
                return deleted

        return None

    def reindex(self, config, function=None):
        """
        Recreates embeddings index using config. This method only works if document content storage is enabled.

        Args:
            config: new config
            function: optional function to prepare content for indexing
        """

        # Raise error if index is not writable
        if not self.config.get("writable"):
            raise ReadOnlyError("Attempting to reindex a read-only index (writable != True)")

        if self.embeddings:
            with self.lock:
                # Resolve function, if necessary
                function = self.function(function) if function and isinstance(function, str) else function

                # Reindex
                self.embeddings.reindex(config, function)

                # Save index if path available, otherwise this is an memory-only index
                if self.config.get("path"):
                    self.embeddings.save(self.config["path"], self.config.get("cloud"))

    def count(self):
        """
        Total number of elements in this embeddings index.

        Returns:
            number of elements in embeddings index
        """

        if self.embeddings:
            return self.embeddings.count()

        return None

    def similarity(self, query, texts):
        """
        Computes the similarity between query and list of text. Returns a list of
        {id: value, score: value} sorted by highest score, where id is the index
        in texts.

        Args:
            query: query text
            texts: list of text

        Returns:
            list of {id: value, score: value}
        """

        # Use similarity instance if available otherwise fall back to embeddings model
        if "similarity" in self.pipelines:
            return [{"id": uid, "score": float(score)} for uid, score in self.pipelines["similarity"](query, texts)]
        if self.embeddings:
            return [{"id": uid, "score": float(score)} for uid, score in self.embeddings.similarity(query, texts)]

        return None

    def batchsimilarity(self, queries, texts):
        """
        Computes the similarity between list of queries and list of text. Returns a list
        of {id: value, score: value} sorted by highest score per query, where id is the
        index in texts.

        Args:
            queries: queries text
            texts: list of text

        Returns:
            list of {id: value, score: value} per query
        """

        # Use similarity instance if available otherwise fall back to embeddings model
        if "similarity" in self.pipelines:
            return [[{"id": uid, "score": float(score)} for uid, score in r] for r in self.pipelines["similarity"](queries, texts)]
        if self.embeddings:
            return [[{"id": uid, "score": float(score)} for uid, score in r] for r in self.embeddings.batchsimilarity(queries, texts)]

        return None

    def explain(self, query, texts=None, limit=10):
        """
        Explains the importance of each input token in text for a query.

        Args:
            query: query text
            texts: optional list of text, otherwise runs search query
            limit: optional limit if texts is None

        Returns:
            list of dict per input text where a higher token scores represents higher importance relative to the query
        """

        if self.embeddings:
            with self.lock:
                return self.embeddings.explain(query, texts, limit)

        return None

    def batchexplain(self, queries, texts=None, limit=10):
        """
        Explains the importance of each input token in text for a list of queries.

        Args:
            query: queries text
            texts: optional list of text, otherwise runs search queries
            limit: optional limit if texts is None

        Returns:
            list of dict per input text per query where a higher token scores represents higher importance relative to the query
        """

        if self.embeddings:
            with self.lock:
                return self.embeddings.batchexplain(queries, texts, limit)

        return None

    def transform(self, text, category=None, index=None):
        """
        Transforms text into embeddings arrays.

        Args:
            text: input text
            category: category for instruction-based embeddings
            index: index name, if applicable

        Returns:
            embeddings array
        """

        if self.embeddings:
            return [float(x) for x in self.embeddings.transform(text, category, index)]

        return None

    def batchtransform(self, texts, category=None, index=None):
        """
        Transforms list of text into embeddings arrays.

        Args:
            texts: list of text
            category: category for instruction-based embeddings
            index: index name, if applicable

        Returns:
            embeddings arrays
        """

        if self.embeddings:
            return [[float(x) for x in result] for result in self.embeddings.batchtransform(texts, category, index)]

        return None

    def extract(self, queue, texts=None):
        """
        Extracts answers to input questions.

        Args:
            queue: list of {name: value, query: value, question: value, snippet: value}
            texts: optional list of text

        Returns:
            list of {name: value, answer: value}
        """

        if self.embeddings and "extractor" in self.pipelines:
            # Get extractor instance
            extractor = self.pipelines["extractor"]

            # Run extractor and return results as dicts
            return extractor(queue, texts)

        return None

    def label(self, text, labels):
        """
        Applies a zero shot classifier to text using a list of labels. Returns a list of
        {id: value, score: value} sorted by highest score, where id is the index in labels.

        Args:
            text: text|list
            labels: list of labels

        Returns:
            list of {id: value, score: value} per text element
        """

        if "labels" in self.pipelines:
            # Text is a string
            if isinstance(text, str):
                return [{"id": uid, "score": float(score)} for uid, score in self.pipelines["labels"](text, labels)]

            # Text is a list
            return [[{"id": uid, "score": float(score)} for uid, score in result] for result in self.pipelines["labels"](text, labels)]

        return None

    def pipeline(self, name, *args, **kwargs):
        """
        Generic pipeline execution method.

        Args:
            name: pipeline name
            args: pipeline positional arguments
            kwargs: pipeline keyword arguments

        Returns:
            pipeline results
        """

        # Backwards compatible with previous pipeline function arguments
        args = args[0] if args and len(args) == 1 and isinstance(args[0], tuple) else args

        if name in self.pipelines:
            return self.pipelines[name](*args, **kwargs)

        return None

    def workflow(self, name, elements):
        """
        Executes a workflow.

        Args:
            name: workflow name
            elements: elements to process

        Returns:
            processed elements
        """

        if hasattr(elements, "__len__") and hasattr(elements, "__getitem__"):
            # Convert to tuples and return as a list since input is sized
            elements = [tuple(element) if isinstance(element, list) else element for element in elements]
        else:
            # Convert to tuples and return as a generator since input is not sized
            elements = (tuple(element) if isinstance(element, list) else element for element in elements)

        # Execute workflow
        return self.workflows[name](elements)

    def agent(self, name, *args, **kwargs):
        """
        Executes an agent.

        Args:
            name: agent name
            args: agent positional arguments
            kwargs: agent keyword arguments
        """

        if name in self.agents:
            return self.agents[name](*args, **kwargs)

        return None

    def wait(self):
        """
        Closes threadpool and waits for completion.
        """

        if self.pool:
            self.pool.close()
            self.pool.join()
            self.pool = None


class ReadOnlyError(Exception):
    """
    Error raised when trying to modify a read-only index
    """



================================================
FILE: src/python/txtai/archive/__init__.py
================================================
"""
Archive imports
"""

from .base import Archive
from .compress import Compress
from .factory import ArchiveFactory
from .tar import Tar
from .zip import Zip



================================================
FILE: src/python/txtai/archive/base.py
================================================
"""
Archive module
"""

import os

from tempfile import TemporaryDirectory

from .tar import Tar
from .zip import Zip


class Archive:
    """
    Base class for archive instances.
    """

    def __init__(self, directory=None):
        """
        Creates a new archive instance.

        Args:
            directory: directory to use as working directory, defaults to a temporary directory
        """

        self.directory = directory

    def isarchive(self, path):
        """
        Checks if path is an archive file based on the extension.

        Args:
            path: path to check

        Returns:
            True if the path ends with an archive extension, False otherwise
        """

        return path and any(path.lower().endswith(extension) for extension in [".tar.bz2", ".tar.gz", ".tar.xz", ".zip"])

    def path(self):
        """
        Gets the current working directory for this archive instance.

        Returns:
            archive working directory
        """

        # Default to a temporary directory. All files created in this directory will be deleted
        # when this archive instance goes out of scope.
        if not self.directory:
            # pylint: disable=R1732
            self.directory = TemporaryDirectory()

        return self.directory.name if isinstance(self.directory, TemporaryDirectory) else self.directory

    def load(self, path, compression=None):
        """
        Extracts file at path to archive working directory.

        Args:
            path: path to archive file
            compression: compression format, infers from path if not provided
        """

        # Unpack compressed file
        compress = self.create(path, compression)
        compress.unpack(path, self.path())

    def save(self, path, compression=None):
        """
        Archives files in archive working directory to file at path.

        Args:
            path: path to archive file
            compression: compression format, infers from path if not provided
        """

        # Create output directory, if necessary
        output = os.path.dirname(path)
        if output:
            os.makedirs(output, exist_ok=True)

        # Pack into compressed file
        compress = self.create(path, compression)
        compress.pack(self.path(), path)

    def create(self, path, compression):
        """
        Method to construct a Compress instance.

        Args:
            path: file path
            compression: compression format, infers using file extension if not provided

        Returns:
            Compress
        """

        # Infer compression format from path if not provided
        compression = compression if compression else path.lower().split(".")[-1]

        # Create compression instance
        return Zip() if compression == "zip" else Tar()



================================================
FILE: src/python/txtai/archive/compress.py
================================================
"""
Compress module
"""

import os


class Compress:
    """
    Base class for Compress instances.
    """

    def pack(self, path, output):
        """
        Compresses files in directory path to file output.

        Args:
            path: input directory path
            output: output file
        """

        raise NotImplementedError

    def unpack(self, path, output):
        """
        Extracts all files in path to output.

        Args:
            path: input file path
            output: output directory
        """

        raise NotImplementedError

    def validate(self, directory, path):
        """
        Validates path is under directory.

        Args:
            directory: base directory
            path: path to validate

        Returns:
            True if path is under directory, False otherwise
        """

        directory = os.path.abspath(directory)
        path = os.path.abspath(path)
        prefix = os.path.commonprefix([directory, path])

        return prefix == directory



================================================
FILE: src/python/txtai/archive/factory.py
================================================
"""
Factory module
"""

from .base import Archive


class ArchiveFactory:
    """
    Methods to create Archive instances.
    """

    @staticmethod
    def create(directory=None):
        """
        Create a new Archive instance.

        Args:
            directory: optional default working directory, otherwise uses a temporary directory

        Returns:
            Archive
        """

        return Archive(directory)



================================================
FILE: src/python/txtai/archive/tar.py
================================================
"""
Tar module
"""

import os
import tarfile

from .compress import Compress


class Tar(Compress):
    """
    Tar compression
    """

    def pack(self, path, output):
        # Infer compression type
        compression = self.compression(output)

        with tarfile.open(output, f"w:{compression}" if compression else "w") as tar:
            tar.add(path, arcname=".")

    def unpack(self, path, output):
        # Infer compression type
        compression = self.compression(path)

        with tarfile.open(path, f"r:{compression}" if compression else "r") as tar:
            # Validate paths
            for member in tar.getmembers():
                fullpath = os.path.join(path, member.name)

                # Reject paths outside of base directory and links
                if not self.validate(path, fullpath) or member.issym() or member.islnk():
                    raise IOError(f"Invalid tar entry: {member.name}{'->' + member.linkname if member.linkname else ''}")

            # Unpack data. Apply default data filter to only allow basic TAR features.
            kwargs = {"filter": "data"} if hasattr(tarfile, "data_filter") else {}
            tar.extractall(output, **kwargs)

    def compression(self, path):
        """
        Gets compression type for path.

        Args:
            path: path to file

        Returns:
            compression type
        """

        # Infer compression type from last path component. Limit to supported types.
        compression = path.lower().split(".")[-1]
        return compression if compression in ("bz2", "gz", "xz") else None



================================================
FILE: src/python/txtai/archive/zip.py
================================================
"""
Zip module
"""

import os

from zipfile import ZipFile, ZIP_DEFLATED

from .compress import Compress


class Zip(Compress):
    """
    Zip compression
    """

    def pack(self, path, output):
        with ZipFile(output, "w", ZIP_DEFLATED) as zfile:
            for root, _, files in sorted(os.walk(path)):
                for f in files:
                    # Generate archive name with relative path, if necessary
                    name = os.path.join(os.path.relpath(root, path), f)

                    # Write file to zip
                    zfile.write(os.path.join(root, f), arcname=name)

    def unpack(self, path, output):
        with ZipFile(path, "r") as zfile:
            # Validate paths if directory specified
            for fullpath in zfile.namelist():
                fullpath = os.path.join(path, fullpath)
                if os.path.dirname(fullpath) and not self.validate(path, fullpath):
                    raise IOError(f"Invalid zip entry: {fullpath}")

            # Unpack data
            zfile.extractall(output)



================================================
FILE: src/python/txtai/cloud/__init__.py
================================================
"""
Cloud imports
"""

from .base import Cloud
from .factory import CloudFactory
from .hub import HuggingFaceHub
from .storage import ObjectStorage



================================================
FILE: src/python/txtai/cloud/base.py
================================================
"""
Cloud module
"""

import os

from ..archive import ArchiveFactory


class Cloud:
    """
    Base class for cloud providers. Cloud providers sync content between local and remote storage.
    """

    def __init__(self, config):
        """
        Creates a new cloud connection.

        Args:
            config: cloud configuration
        """

        self.config = config

    def exists(self, path=None):
        """
        Checks if path exists in cloud. If path is None, this method checks if the container exists.

        Args:
            path: path to check

        Returns:
            True if path or container exists, False otherwise
        """

        return self.metadata(path) is not None

    def metadata(self, path=None):
        """
        Returns metadata for path from cloud. If path is None, this method returns metadata
        for container.

        Args:
            path: retrieve metadata for this path

        Returns:
            path or container metadata if available, otherwise returns None
        """

        raise NotImplementedError

    def load(self, path=None):
        """
        Retrieves content from cloud and stores locally. If path is empty, this method retrieves
        all content in the container.

        Args:
            path: path to retrieve

        Returns:
            local path which can be different than input path
        """

        raise NotImplementedError

    def save(self, path):
        """
        Sends local content stored in path to cloud.

        Args:
            path: local path to sync
        """

        raise NotImplementedError

    def isarchive(self, path):
        """
        Check if path is an archive file.

        Args:
            path: path to check

        Returns:
            True if path ends with an archive extension, false otherwise
        """

        return ArchiveFactory.create().isarchive(path)

    def listfiles(self, path):
        """
        Lists files in path. If path is a file, this method returns a single element list
        containing path.

        Args:
            path: path to list

        Returns:
            List of files
        """

        # List all files if path is a directory
        if os.path.isdir(path):
            return [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]

        # Path is a file
        return [path]



================================================
FILE: src/python/txtai/cloud/factory.py
================================================
"""
Factory module
"""

from ..util import Resolver

from .hub import HuggingFaceHub
from .storage import ObjectStorage, LIBCLOUD


class CloudFactory:
    """
    Methods to create Cloud instances.
    """

    @staticmethod
    def create(config):
        """
        Creates a Cloud instance.

        Args:
            config: cloud configuration

        Returns:
            Cloud
        """

        # Cloud instance
        cloud = None

        provider = config.get("provider", "")

        # Hugging Face Hub
        if provider.lower() == "huggingface-hub":
            cloud = HuggingFaceHub(config)

        # Cloud object storage
        elif ObjectStorage.isprovider(provider):
            cloud = ObjectStorage(config)

        # External provider
        elif provider:
            cloud = CloudFactory.resolve(provider, config)

        return cloud

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom cloud backend.

        Args:
            backend: backend class
            config: configuration parameters

        Returns:
            Cloud
        """

        try:
            return Resolver()(backend)(config)

        except Exception as e:
            # Failure message
            message = f'Unable to resolve cloud backend: "{backend}".'

            # Append message if LIBCLOUD is not installed
            message += ' Cloud storage is not available - install "cloud" extra to enable' if not LIBCLOUD else ""

            raise ImportError(message) from e



================================================
FILE: src/python/txtai/cloud/hub.py
================================================
"""
Hugging Face Hub module
"""

import os
import tempfile

import huggingface_hub

from huggingface_hub.utils import RepositoryNotFoundError

from .base import Cloud


class HuggingFaceHub(Cloud):
    """
    Hugging Face Hub cloud provider.
    """

    def metadata(self, path=None):
        try:
            # If this is an archive file path, get file metadata
            if self.isarchive(path):
                url = huggingface_hub.hf_hub_url(
                    repo_id=self.config["container"], filename=os.path.basename(path), revision=self.config.get("revision")
                )

                return huggingface_hub.get_hf_file_metadata(url=url, token=self.config.get("token"))

            # Otherwise return repository metadata
            return huggingface_hub.model_info(repo_id=self.config["container"], revision=self.config.get("revision"), token=self.config.get("token"))

        except RepositoryNotFoundError:
            return None

    def load(self, path=None):
        # Download archvie file and return local path
        if self.isarchive(path):
            return huggingface_hub.hf_hub_download(
                repo_id=self.config["container"],
                filename=os.path.basename(path),
                revision=self.config.get("revision"),
                cache_dir=self.config.get("cache"),
                token=self.config.get("token"),
            )

        # Download repository and return cached path
        return huggingface_hub.snapshot_download(
            repo_id=self.config["container"], revision=self.config.get("revision"), cache_dir=self.config.get("cache"), token=self.config.get("token")
        )

    def save(self, path):
        # Get or create repository
        huggingface_hub.create_repo(
            repo_id=self.config["container"], token=self.config.get("token"), private=self.config.get("private", True), exist_ok=True
        )

        # Enable lfs-tracking of embeddings index files
        self.lfstrack()

        # Upload files
        for f in self.listfiles(path):
            huggingface_hub.upload_file(
                repo_id=self.config["container"],
                revision=self.config.get("revision"),
                token=self.config.get("token"),
                path_or_fileobj=f,
                path_in_repo=os.path.basename(f),
            )

    def lfstrack(self):
        """
        Adds lfs-tracking of embeddings index files. This method adds tracking for documents and embeddings to .gitattributes.
        """

        # Get and read .gitattributes file
        path = huggingface_hub.hf_hub_download(
            repo_id=self.config["container"], filename=os.path.basename(".gitattributes"), token=self.config.get("token")
        )

        with open(path, "r", encoding="utf-8") as f:
            content = f.read()

        # Check if index files are lfs-tracked. Update .gitattributes, if necessary.
        if "embeddings " not in content:
            # Add documents and embeddings to lfs tracking
            content += "documents filter=lfs diff=lfs merge=lfs -text\n"
            content += "embeddings filter=lfs diff=lfs merge=lfs -text\n"

            # pylint: disable=R1732
            with tempfile.NamedTemporaryFile(mode="w", delete=False) as tmp:
                tmp.write(content)
                attributes = tmp.name

            # Upload file
            huggingface_hub.upload_file(
                repo_id=self.config["container"], token=self.config.get("token"), path_or_fileobj=attributes, path_in_repo=os.path.basename(path)
            )

            # Remove temporary file
            os.remove(attributes)



================================================
FILE: src/python/txtai/cloud/storage.py
================================================
"""
Object storage module
"""

import os

# Conditional import
try:
    from libcloud.storage.providers import get_driver, DRIVERS
    from libcloud.storage.types import ContainerDoesNotExistError, ObjectDoesNotExistError

    LIBCLOUD = True
except ImportError:
    LIBCLOUD, DRIVERS = False, None


from .base import Cloud


class ObjectStorage(Cloud):
    """
    Object storage cloud provider backed by Apache libcloud.
    """

    @staticmethod
    def isprovider(provider):
        """
        Checks if this provider is an object storage provider.

        Args:
            provider: provider name

        Returns:
            True if this is an object storage provider
        """

        return LIBCLOUD and provider and provider.lower() in [x.lower() for x in DRIVERS]

    def __init__(self, config):
        super().__init__(config)

        if not LIBCLOUD:
            raise ImportError('Cloud object storage is not available - install "cloud" extra to enable')

        # Get driver for provider
        driver = get_driver(config["provider"])

        # Get client connection
        self.client = driver(
            config.get("key", os.environ.get("ACCESS_KEY")),
            config.get("secret", os.environ.get("ACCESS_SECRET")),
            **{field: config.get(field) for field in ["host", "port", "region", "token"] if config.get(field)},
        )

    def metadata(self, path=None):
        try:
            # If this is an archive path, check if file exists
            if self.isarchive(path):
                return self.client.get_object(self.config["container"], self.objectname(path))

            # Otherwise check if container exists
            return self.client.get_container(self.config["container"])
        except (ContainerDoesNotExistError, ObjectDoesNotExistError):
            return None

    def load(self, path=None):
        # Download archive file
        if self.isarchive(path):
            obj = self.client.get_object(self.config["container"], self.objectname(path))

            # Create local directory, if necessary
            directory = os.path.dirname(path)
            if directory:
                os.makedirs(directory, exist_ok=True)

            obj.download(path, overwrite_existing=True)

        # Download files in container. Optionally filter with a provided prefix.
        else:
            container = self.client.get_container(self.config["container"])
            for obj in container.list_objects(prefix=self.config.get("prefix")):
                # Derive local path and directory
                localpath = os.path.join(path, obj.name)
                directory = os.path.dirname(localpath)

                # Create local directory, if necessary
                os.makedirs(directory, exist_ok=True)

                # Download file locally
                obj.download(localpath, overwrite_existing=True)

        return path

    def save(self, path):
        # Get or create container
        try:
            container = self.client.get_container(self.config["container"])
        except ContainerDoesNotExistError:
            container = self.client.create_container(self.config["container"])

        # Upload files
        for f in self.listfiles(path):
            with open(f, "rb") as iterator:
                self.client.upload_object_via_stream(iterator=iterator, container=container, object_name=self.objectname(f))

    def objectname(self, name):
        """
        Derives an object name. This method checks if a prefix configuration parameter is present and combines
        it with the input name parameter.

        Args:
            name: input name

        Returns:
            object name
        """

        # Get base name
        name = os.path.basename(name)

        # Get optional prefix/folder
        prefix = self.config.get("prefix")

        # Prepend prefix, if applicable
        return f"{prefix}/{name}" if prefix else name



================================================
FILE: src/python/txtai/console/__init__.py
================================================
"""
Console imports
"""

from .base import Console



================================================
FILE: src/python/txtai/console/__main__.py
================================================
"""
Main module.
"""

import sys

from .base import Console


def main(path=None):
    """
    Console execution loop.

    Args:
        path: model path
    """

    Console(path).cmdloop()


if __name__ == "__main__":
    main(sys.argv[1] if len(sys.argv) > 1 else None)



================================================
FILE: src/python/txtai/console/base.py
================================================
"""
Console module
"""

import os
import shlex

from cmd import Cmd

# Conditional import
try:
    from rich import box
    from rich.console import Console as RichConsole
    from rich.table import Table

    RICH = True
except ImportError:
    RICH = False

from txtai.app import Application
from txtai.embeddings import Embeddings


class Console(Cmd):
    """
    txtai console.
    """

    def __init__(self, path=None):
        """
        Creates a new command line console.

        Args:
            path: path to initial configuration, if any
        """

        super().__init__()

        if not RICH:
            raise ImportError('Console is not available - install "console" extra to enable')

        self.prompt = ">>> "

        # Rich console
        self.console = RichConsole()

        # App parameters
        self.app = None
        self.path = path

        # Parameters
        self.vhighlight = None
        self.vlimit = None

    def preloop(self):
        """
        Loads initial configuration.
        """

        self.console.print("txtai console", style="#03a9f4")

        # Load default path
        if self.path:
            self.load(self.path)

    def default(self, line):
        """
        Default event loop.

        Args:
            line: command line
        """

        # pylint: disable=W0703
        try:
            command = line.lower()
            if command.startswith(".config"):
                self.config()
            elif command.startswith(".highlight"):
                self.highlight(command)
            elif command.startswith(".limit"):
                self.limit(command)
            elif command.startswith(".load"):
                command = self.split(line)
                self.path = command[1]
                self.load(self.path)
            elif command.startswith(".workflow"):
                self.workflow(line)
            else:
                # Search is default action
                self.search(line)
        except Exception:
            self.console.print_exception()

    def config(self):
        """
        Processes .config command.
        """

        self.console.print(self.app.config)

    def highlight(self, command):
        """
        Processes .highlight command.

        Args:
            command: command line
        """

        _, action = self.split(command, "#ffff00")
        self.vhighlight = action
        self.console.print(f"Set highlight to {self.vhighlight}")

    def limit(self, command):
        """
        Processes .limit command.

        Args:
            command: command line
        """

        _, action = self.split(command, 10)
        self.vlimit = int(action)
        self.console.print(f"Set limit to {self.vlimit}")

    def load(self, path):
        """
        Processes .load command.

        Args:
            path: path to configuration
        """

        if self.isyaml(path):
            self.console.print(f"Loading application {path}")
            self.app = Application(path)
        else:
            self.console.print(f"Loading index {path}")

            # Load embeddings index
            self.app = Embeddings()
            self.app.load(path)

    def search(self, query):
        """
        Runs a search query.

        Args:
            query: query to run
        """

        if self.vhighlight:
            results = self.app.explain(query, limit=self.vlimit)
        else:
            results = self.app.search(query, limit=self.vlimit)

        columns, table = {}, Table(box=box.SQUARE, style="#03a9f4")

        # Build column list
        result = results[0]
        if isinstance(result, tuple):
            columns = dict.fromkeys(["id", "score"])
        else:
            columns = dict(result)

        # Add columns to table
        columns = list(x for x in columns if x != "tokens")
        for column in columns:
            table.add_column(column)

        # Add rows to table
        for result in results:
            if isinstance(result, tuple):
                table.add_row(*(self.render(result, None, x) for x in result))
            else:
                table.add_row(*(self.render(result, column, result.get(column)) for column in columns))

        # Print table to console
        self.console.print(table)

    def workflow(self, command):
        """
        Processes .workflow command.

        Args:
            command: command line
        """

        command = shlex.split(command)
        if isinstance(self.app, Application):
            self.console.print(list(self.app.workflow(command[1], command[2:])))

    def isyaml(self, path):
        """
        Checks if file at path is a valid YAML file.

        Args:
            path: file to check

        Returns:
            True if file is valid YAML, False otherwise
        """

        if os.path.exists(path) and os.path.isfile(path):
            try:
                return Application.read(path)
            # pylint: disable=W0702
            except:
                pass

        return False

    def split(self, command, default=None):
        """
        Splits command by whitespace.

        Args:
            command: command line
            default: default command action

        Returns:
            command action
        """

        values = command.split(" ", 1)
        return values if len(values) > 1 else (command, default)

    def render(self, result, column, value):
        """
        Renders a search result column value.

        Args:
            result: result row
            column: column name
            value: column value
        """

        if isinstance(value, float):
            return f"{value:.4f}"

        # Explain highlighting
        if column == "text" and "tokens" in result:
            spans = []
            for token, score in result["tokens"]:
                color = None
                if score >= 0.02:
                    color = f"b {self.vhighlight}"

                spans.append((token, score, color))

            if result["score"] >= 0.05 and not [color for _, _, color in spans if color]:
                mscore = max(score for _, score, _ in spans)
                spans = [(token, score, f"b {self.vhighlight}" if score == mscore else color) for token, score, color in spans]

            output = ""
            for token, _, color in spans:
                if color:
                    output += f"[{color}]{token}[/{color}] "
                else:
                    output += f"{token} "

            return output

        return str(value)



================================================
FILE: src/python/txtai/data/__init__.py
================================================
"""
Data imports
"""

from .base import Data
from .labels import Labels
from .questions import Questions
from .sequences import Sequences
from .texts import Texts
from .tokens import Tokens



================================================
FILE: src/python/txtai/data/base.py
================================================
"""
Data module
"""

from .tokens import Tokens


class Data:
    """
    Base data tokenization class.
    """

    def __init__(self, tokenizer, columns, maxlength):
        """
        Creates new base instance for tokenizing data.

        Args:
            tokenizer: model tokenizer
            columns: column names
            maxlength: maximum sequence length
        """

        self.tokenizer = tokenizer
        self.columns = columns
        self.maxlength = maxlength

    def __call__(self, train, validation, workers):
        """
        Tokenizes training and validation data and returns processed datasets.

        Args:
            train: training data
            validation: validation data
            workers: number of concurrent tokenizers when processing datasets, only main process used when set to None

        Returns:
            (train, validation)
        """

        return (self.prepare(train, self.process, workers), self.prepare(validation, self.process, workers) if validation else None)

    def prepare(self, data, fn, workers):
        """
        Prepares and tokenizes data for training.

        Args:
            data: input data
            fn: tokenize processing function to apply
            workers: number of concurrent tokenizers when processing datasets, only main process used when set to None

        Returns:
            tokens
        """

        if hasattr(data, "map"):
            # Hugging Face dataset
            tokens = data.map(fn, batched=True, num_proc=workers, remove_columns=data.column_names)
        else:
            # Re-orient data into columns for efficient batch tokenization
            columns = {}
            if hasattr(data, "columns"):
                # Polars/pandas DataFrame
                for column in data.columns:
                    columns[column] = list(data[column])
            else:
                # Iterable dicts
                for row in data:
                    for column in row.keys():
                        if column not in columns:
                            columns[column] = []

                        columns[column].append(row[column])

            # Process column-oriented data
            tokens = Tokens(fn(columns))

        return tokens

    def labels(self, data):
        """
        Extracts a list of unique labels from data.

        Args:
            data: input data

        Returns:
            list of unique labels
        """

        # Last column is label
        column = self.columns[-1]

        # Return length of labels if it's an array
        length = self.length(data[column][0] if hasattr(data, "columns") else data[0][column])
        if length:
            return length

        if hasattr(data, "map"):
            # Hugging Face dataset
            labels = sorted(data.unique(self.columns[-1]))
        elif hasattr(data, "columns"):
            # Polars/pandas DataFrame
            labels = sorted(data[self.columns[-1]].unique())
        else:
            # Iterable dicts
            labels = sorted({row[self.columns[-1]] for row in data})

        # Labels are single numeric values per entry
        #   - Consider a regression task if at least one label isn't an integer
        #   - Otherwise use number of labels for a classification task
        return 1 if [x for x in labels if float(x) != int(x)] else len(labels)

    def process(self, data):
        """
        Tokenizes batch of input data

        Args:
            data: input data batch

        Returns:
            tokenized data
        """

        return data

    def length(self, value):
        """
        Returns the length of value if value has a len function defined. Otherwise,
        None is returned.

        Args:
            value: value to check

        Returns:
            length of value if available, otherwise returns None
        """

        return len(value) if hasattr(value, "__len__") else None



================================================
FILE: src/python/txtai/data/labels.py
================================================
"""
Labels module
"""

from .base import Data


class Labels(Data):
    """
    Tokenizes text-classification datasets as input for training text-classification models.
    """

    def __init__(self, tokenizer, columns, maxlength):
        """
        Creates a new instance for tokenizing Labels training data.

        Args:
            tokenizer: model tokenizer
            columns: tuple of columns to use for text/label
            maxlength: maximum sequence length
        """

        super().__init__(tokenizer, columns, maxlength)

        # Standardize columns
        if not self.columns:
            self.columns = ("text", None, "label")
        elif len(columns) < 3:
            self.columns = (self.columns[0], None, self.columns[-1])

    def process(self, data):
        # Column keys
        text1, text2, label = self.columns

        # Tokenizer inputs can be single string or string pair, depending on task
        text = (data[text1], data[text2]) if text2 else (data[text1],)

        # Tokenize text and add label
        inputs = self.tokenizer(*text, max_length=self.maxlength, padding=True, truncation=True)
        inputs[label] = data[label]

        return inputs



================================================
FILE: src/python/txtai/data/questions.py
================================================
"""
Questions module
"""

from .base import Data


class Questions(Data):
    """
    Tokenizes question-answering datasets as input for training question-answering models.
    """

    def __init__(self, tokenizer, columns, maxlength, stride):
        """
        Creates a new instance for tokenizing Questions training data.

        Args:
            tokenizer: model tokenizer
            columns: tuple of columns to use for question/context/answer
            maxlength: maximum sequence length
            stride: chunk size for splitting data for QA tasks
        """

        super().__init__(tokenizer, columns, maxlength)

        if not self.columns:
            self.columns = ("question", "context", "answers")

        self.question, self.context, self.answer = self.columns
        self.stride = stride
        self.rpad = tokenizer.padding_side == "right"

    def process(self, data):
        # Tokenize data
        tokenized = self.tokenize(data)

        # Get mapping of overflowing tokens and answer offsets
        samples = tokenized.pop("overflow_to_sample_mapping")
        offsets = tokenized.pop("offset_mapping")

        # Start/end positions
        tokenized["start_positions"] = []
        tokenized["end_positions"] = []

        for x, offset in enumerate(offsets):
            # Label NO ANSWER with CLS token
            inputids = tokenized["input_ids"][x]
            clstoken = inputids.index(self.tokenizer.cls_token_id)

            # Sequence ids
            sequences = tokenized.sequence_ids(x)

            # Get and format answer
            answers = self.answers(data, samples[x])

            # If no answers are given, set cls token as answer.
            if len(answers["answer_start"]) == 0:
                tokenized["start_positions"].append(clstoken)
                tokenized["end_positions"].append(clstoken)
            else:
                # Start/end character index of the answer in the text.
                startchar = answers["answer_start"][0]
                endchar = startchar + len(answers["text"][0])

                # Start token index of the current span in the text.
                start = 0
                while sequences[start] != (1 if self.rpad else 0):
                    start += 1

                # End token index of the current span in the text.
                end = len(inputids) - 1
                while sequences[end] != (1 if self.rpad else 0):
                    end -= 1

                # Map start character and end character to matching token index
                while start < len(offset) and offset[start][0] <= startchar:
                    start += 1
                tokenized["start_positions"].append(start - 1)

                while offset[end][1] >= endchar:
                    end -= 1
                tokenized["end_positions"].append(end + 1)

        return tokenized

    def tokenize(self, data):
        """
        Tokenizes batch of data

        Args:
            data: input data batch

        Returns:
            tokenized data
        """

        # Trim question whitespace
        data[self.question] = [x.lstrip() for x in data[self.question]]

        # Tokenize records
        return self.tokenizer(
            data[self.question if self.rpad else self.context],
            data[self.context if self.rpad else self.question],
            truncation="only_second" if self.rpad else "only_first",
            max_length=self.maxlength,
            stride=self.stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding=True,
        )

    def answers(self, data, index):
        """
        Gets and formats an answer.

        Args:
            data: input examples
            index: answer index to retrieve

        Returns:
            answers dict
        """

        # Answer mappings
        answers = data[self.answer][index]
        context = data[self.context][index]

        # Handle mapping string answers to dict
        if not isinstance(answers, dict):
            if not answers:
                answers = {"text": [], "answer_start": []}
            else:
                answers = {"text": [answers], "answer_start": [context.index(answers)]}

        return answers



================================================
FILE: src/python/txtai/data/sequences.py
================================================
"""
Sequences module
"""

from .base import Data


class Sequences(Data):
    """
    Tokenizes sequence-sequence datasets as input for training sequence-sequence models
    """

    def __init__(self, tokenizer, columns, maxlength, prefix):
        """
        Creates a new instance for tokenizing Sequences training data.

        Args:
            tokenizer: model tokenizer
            columns: tuple of columns to use for text/label
            maxlength: maximum sequence length
            prefix: source prefix
        """

        super().__init__(tokenizer, columns, maxlength)

        # Standardize columns
        if not self.columns:
            self.columns = ("source", "target")

        # Save source prefix
        self.prefix = prefix

    def process(self, data):
        # Column keys
        source, target = self.columns

        # Tokenize source
        source = [self.prefix + x if self.prefix else x for x in data[source]]
        inputs = self.tokenizer(source, max_length=self.maxlength, padding=False, truncation=True)

        # Tokenize target
        with self.tokenizer.as_target_tokenizer():
            targets = self.tokenizer(data[target], max_length=self.maxlength, padding=False, truncation=True)

        # Combine inputs
        inputs["labels"] = targets["input_ids"]

        return inputs



================================================
FILE: src/python/txtai/data/texts.py
================================================
"""
Texts module
"""

from itertools import chain

from .base import Data


class Texts(Data):
    """
    Tokenizes text datasets as input for training language models.
    """

    def __init__(self, tokenizer, columns, maxlength):
        """
        Creates a new instance for tokenizing Texts training data.

        Args:
            tokenizer: model tokenizer
            columns: tuple of columns to use for text
            maxlength: maximum sequence length
        """

        super().__init__(tokenizer, columns, maxlength)

        # Standardize columns
        if not self.columns:
            self.columns = ("text", None)

    def process(self, data):
        # Column keys
        text1, text2 = self.columns

        # Tokenizer inputs can be single string or string pair, depending on task
        text = (data[text1], data[text2]) if text2 else (data[text1],)

        # Tokenize text and add label
        inputs = self.tokenizer(*text, return_special_tokens_mask=True)

        # Concat and return tokenized inputs
        return self.concat(inputs)

    def concat(self, inputs):
        """
        Concatenates tokenized text into chunks of maxlength.

        Args:
            inputs: tokenized input

        Returns:
            Chunks of tokenized text each with a size of maxlength
        """

        # Concatenate tokenized text
        concat = {k: list(chain(*inputs[k])) for k in inputs.keys()}

        # Calculate total length
        length = len(concat[list(inputs.keys())[0]])

        # Ensure total is multiple of maxlength, drop last incomplete chunk
        if length >= self.maxlength:
            length = (length // self.maxlength) * self.maxlength

        # Split into chunks of maxlength
        result = {k: [v[x : x + self.maxlength] for x in range(0, length, self.maxlength)] for k, v in concat.items()}

        return result



================================================
FILE: src/python/txtai/data/tokens.py
================================================
"""
Tokens module
"""

import torch


class Tokens(torch.utils.data.Dataset):
    """
    Default dataset used to hold tokenized data.
    """

    def __init__(self, columns):
        self.data = []

        # Map column-oriented data to rows
        for column in columns:
            for x, value in enumerate(columns[column]):
                if len(self.data) <= x:
                    self.data.append({})

                self.data[x][column] = value

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index]



================================================
FILE: src/python/txtai/database/__init__.py
================================================
"""
Database imports
"""

from .base import Database
from .client import Client
from .duckdb import DuckDB
from .embedded import Embedded
from .encoder import *
from .factory import DatabaseFactory
from .rdbms import RDBMS
from .schema import *
from .sqlite import SQLite
from .sql import *



================================================
FILE: src/python/txtai/database/base.py
================================================
"""
Database module
"""

import logging
import types

from .encoder import EncoderFactory
from .sql import SQL, SQLError, Token

# Logging configuration
logger = logging.getLogger(__name__)


class Database:
    """
    Base class for database instances. This class encapsulates a content database used for
    storing field content as dicts and objects. The database instance works in conjuction
    with a vector index to execute SQL-driven similarity search.
    """

    def __init__(self, config):
        """
        Creates a new Database.

        Args:
            config: database configuration
        """

        # Initialize configuration
        self.configure(config)

    def load(self, path):
        """
        Loads a database path.

        Args:
            path: database url
        """

        raise NotImplementedError

    def insert(self, documents, index=0):
        """
        Inserts documents into the database.

        Args:
            documents: list of documents to save
            index: indexid offset, used for internal ids
        """

        raise NotImplementedError

    def delete(self, ids):
        """
        Deletes documents from database.

        Args:
            ids: ids to delete
        """

        raise NotImplementedError

    def reindex(self, config):
        """
        Reindexes internal database content and streams results back. This method must renumber indexids
        sequentially as deletes could have caused indexid gaps.

        Args:
            config: new configuration
        """

        raise NotImplementedError

    def save(self, path):
        """
        Saves a database at path.

        Args:
            path: path to write database
        """

        raise NotImplementedError

    def close(self):
        """
        Closes this database.
        """

        raise NotImplementedError

    def ids(self, ids):
        """
        Retrieves the internal indexids for a list of ids. Multiple indexids may be present for an id in cases
        where data is segmented.

        Args:
            ids: list of document ids

        Returns:
            list of (indexid, id)
        """

        raise NotImplementedError

    def count(self):
        """
        Retrieves the count of this database instance.

        Returns:
            total database count
        """

        raise NotImplementedError

    def search(self, query, similarity=None, limit=None, parameters=None, indexids=False):
        """
        Runs a search against the database. Supports the following methods:

            1. Standard similarity query. This mode retrieves content for the ids in the similarity results
            2. Similarity query as SQL. This mode will combine similarity results and database results into
               a single result set. Similarity queries are set via the SIMILAR() function.
            3. SQL with no similarity query. This mode runs a SQL query and retrieves the results without similarity queries.

        Example queries:
            "natural language processing" - standard similarity only query
            "select * from txtai where similar('natural language processing')" - similarity query as SQL
            "select * from txtai where similar('nlp') and entry > '2021-01-01'" - similarity query with additional SQL clauses
            "select id, text, score from txtai where similar('nlp')" - similarity query with additional SQL column selections
            "select * from txtai where entry > '2021-01-01' - database only query

        Args:
            query: input query
            similarity: similarity results as [(indexid, score)]
            limit: maximum number of results to return
            parameters: dict of named parameters to bind to placeholders

        Returns:
            query results as a list of dicts
            list of ([indexid, score]) if indexids is True
        """

        # Parse query if necessary
        if isinstance(query, str):
            query = self.parse(query)

        # Add in similar results
        where = query.get("where")

        if "select" in query and similarity:
            for x in range(len(similarity)):
                token = f"{Token.SIMILAR_TOKEN}{x}"
                if where and token in where:
                    where = where.replace(token, self.embed(similarity, x))

        elif similarity:
            # Not a SQL query, load similarity results, if any
            where = self.embed(similarity, 0)

        # Save where
        query["where"] = where

        # Run query
        return self.query(query, limit, parameters, indexids)

    def parse(self, query):
        """
        Parses a query into query components.

        Args:
            query: input query

        Returns:
            dict of parsed query components
        """

        return self.sql(query)

    def resolve(self, name, alias=None):
        """
        Resolves a query column name with the database column name. This method also builds alias expressions
        if alias is set.

        Args:
            name: query column name
            alias: alias name, defaults to None

        Returns:
            database column name
        """

        raise NotImplementedError

    def embed(self, similarity, batch):
        """
        Embeds similarity query results into a database query.

        Args:
            similarity: similarity results as [(indexid, score)]
            batch: batch id
        """

        raise NotImplementedError

    def query(self, query, limit, parameters, indexids):
        """
        Executes query against database.

        Args:
            query: input query
            limit: maximum number of results to return
            parameters: dict of named parameters to bind to placeholders
            indexids: results are returned as [(indexid, score)] regardless of select clause parameters if True

        Returns:
            query results
        """

        raise NotImplementedError

    def configure(self, config):
        """
        Initialize configuration.

        Args:
            config: configuration
        """

        # Database configuration
        self.config = config

        # SQL parser
        self.sql = SQL(self)

        # Load objects encoder
        encoder = self.config.get("objects")
        self.encoder = EncoderFactory.create(encoder) if encoder else None

        # Transform columns
        columns = config.get("columns", {})
        self.text = columns.get("text", "text")
        self.object = columns.get("object", "object")

        # Custom functions and expressions
        self.functions, self.expressions = None, None

        # Load custom functions
        self.registerfunctions(self.config)

        # Load custom expressions
        self.registerexpressions(self.config)

    def registerfunctions(self, config):
        """
        Register custom functions. This method stores the function details for underlying
        database implementations to handle.

        Args:
            config: database configuration
        """

        inputs = config.get("functions") if config else None
        if inputs:
            functions = []
            for fn in inputs:
                name, argcount = None, -1

                # Optional function configuration
                if isinstance(fn, dict):
                    name, argcount, fn = fn.get("name"), fn.get("argcount", -1), fn["function"]

                # Determine if this is a callable object or a function
                if not isinstance(fn, types.FunctionType) and hasattr(fn, "__call__"):
                    name = name if name else fn.__class__.__name__.lower()
                    fn = fn.__call__
                else:
                    name = name if name else fn.__name__.lower()

                # Store function details
                functions.append((name, argcount, fn))

            # pylint: disable=W0201
            self.functions = functions

    def registerexpressions(self, config):
        """
        Register custom expressions. This method parses and resolves expressions for later use in SQL queries.

        Args:
            config: database configuration
        """

        inputs = config.get("expressions") if config else None
        if inputs:
            expressions = {}
            for entry in inputs:
                name = entry.get("name")
                expression = entry.get("expression")
                if name and expression:
                    expressions[name] = self.sql.snippet(expression)

            # pylint: disable=W0201
            self.expressions = expressions

    def execute(self, function, *args):
        """
        Executes a user query. This method has common error handling logic.

        Args:
            function: database execute function
            args: function arguments

        Returns:
            result of function(args)
        """

        try:
            # Debug log SQL
            logger.debug(" ".join(["%s"] * len(args)), *args)

            return function(*args)
        except Exception as e:
            raise SQLError(e) from None

    def setting(self, name, default=None):
        """
        Looks up database specific setting.

        Args:
            name: setting name
            default: default value when setting not found

        Returns:
            setting value
        """

        # Get the database-specific config object
        database = self.config.get(self.config["content"])

        # Get setting value, set default value if not found
        setting = database.get(name) if database else None
        return setting if setting else default



================================================
FILE: src/python/txtai/database/client.py
================================================
"""
Client module
"""

import os
import time

# Conditional import
try:
    from sqlalchemy import StaticPool, Text, cast, create_engine, insert, text as textsql
    from sqlalchemy.orm import Session, aliased
    from sqlalchemy.schema import CreateSchema

    from .schema import Base, Batch, Document, Object, Section, SectionBase, Score

    ORM = True
except ImportError:
    ORM = False

from .rdbms import RDBMS


class Client(RDBMS):
    """
    Database client instance. This class connects to an external database using SQLAlchemy. It supports any database
    that is supported by SQLAlchemy (PostgreSQL, MariaDB, etc) and has JSON support.
    """

    def __init__(self, config):
        """
        Creates a new Database.

        Args:
            config: database configuration parameters
        """

        super().__init__(config)

        if not ORM:
            raise ImportError('SQLAlchemy is not available - install "database" extra to enable')

        # SQLAlchemy parameters
        self.engine, self.dbconnection = None, None

    def save(self, path):
        # Commit session and database connection
        super().save(path)

        if self.dbconnection:
            self.dbconnection.commit()

    def close(self):
        super().close()

        # Dispose of engine, which also closes dbconnection
        if self.engine:
            self.engine.dispose()

    def reindexstart(self):
        # Working table name
        name = f"rebuild{round(time.time() * 1000)}"

        # Create working table metadata
        type("Rebuild", (SectionBase,), {"__tablename__": name})
        Base.metadata.tables[name].create(self.dbconnection)

        return name

    def reindexend(self, name):
        # Remove table object from metadata
        Base.metadata.remove(Base.metadata.tables[name])

    def jsonprefix(self):
        # JSON column prefix
        return "cast("

    def jsoncolumn(self, name):
        # Alias documents table
        d = aliased(Document, name="d")

        # Build JSON column expression for column
        return str(cast(d.data[name].as_string(), Text).compile(dialect=self.engine.dialect, compile_kwargs={"literal_binds": True}))

    def createtables(self):
        # Create tables
        Base.metadata.create_all(self.dbconnection, checkfirst=True)

        # Clear existing data - table schema is created upon connecting to database
        for table in ["sections", "documents", "objects"]:
            self.cursor.execute(f"DELETE FROM {table}")

    def finalize(self):
        # Flush cached objects
        self.connection.flush()

    def insertdocument(self, uid, data, tags, entry):
        self.connection.add(Document(id=uid, data=data, tags=tags, entry=entry))

    def insertobject(self, uid, data, tags, entry):
        self.connection.add(Object(id=uid, object=data, tags=tags, entry=entry))

    def insertsection(self, index, uid, text, tags, entry):
        # Save text section
        self.connection.add(Section(indexid=index, id=uid, text=text, tags=tags, entry=entry))

    def createbatch(self):
        # Create temporary batch table, if necessary
        Base.metadata.tables["batch"].create(self.dbconnection, checkfirst=True)

    def insertbatch(self, indexids, ids, batch):
        if indexids:
            self.connection.execute(insert(Batch), [{"indexid": i, "batch": batch} for i in indexids])
        if ids:
            self.connection.execute(insert(Batch), [{"id": str(uid), "batch": batch} for uid in ids])

    def createscores(self):
        # Create temporary scores table, if necessary
        Base.metadata.tables["scores"].create(self.dbconnection, checkfirst=True)

    def insertscores(self, scores):
        # Average scores by id
        if scores:
            self.connection.execute(insert(Score), [{"indexid": i, "score": sum(s) / len(s)} for i, s in scores.items()])

    def connect(self, path=None):
        # Connection URL
        content = self.config.get("content")

        # Read ENV variable, if necessary
        content = os.environ.get("CLIENT_URL") if content == "client" else content

        # Create engine using database URL
        self.engine = create_engine(content, poolclass=StaticPool, echo=False, json_serializer=lambda x: x)
        self.dbconnection = self.engine.connect()

        # Create database session
        database = Session(self.dbconnection)

        # Set default schema, if necessary
        schema = self.config.get("schema")
        if schema:
            with self.engine.begin():
                self.sqldialect(database, CreateSchema(schema, if_not_exists=True))

            self.sqldialect(database, textsql("SET search_path TO :schema"), {"schema": schema})

        return database

    def getcursor(self):
        return Cursor(self.connection)

    def rows(self):
        return self.cursor

    def addfunctions(self):
        return

    def sqldialect(self, database, sql, parameters=None):
        """
        Executes a SQL statement based on the current SQL dialect.

        Args:
            database: current database
            sql: SQL to execute
            parameters: optional bind parameters
        """

        args = (sql, parameters) if self.engine.dialect.name == "postgresql" else (textsql("SELECT 1"),)
        database.execute(*args)


class Cursor:
    """
    Implements basic compatibility with the Python DB-API.
    """

    def __init__(self, connection):
        self.connection = connection
        self.result = None

    def __iter__(self):
        return self.result

    def execute(self, statement, parameters=None):
        """
        Executes statement.

        Args:
            statement: statement to execute
            parameters: optional dictionary with bind parameters
        """

        if isinstance(statement, str):
            statement = textsql(statement)

        self.result = self.connection.execute(statement, parameters)

    def fetchall(self):
        """
        Fetches all rows from the current result.

        Returns:
            all rows from current result
        """

        return self.result.all() if self.result else None

    def fetchone(self):
        """
        Fetches first row from current result.

        Returns:
            first row from current result
        """

        return self.result.first() if self.result else None

    @property
    def description(self):
        """
        Returns columns for current result.

        Returns:
            list of columns
        """

        return [(key,) for key in self.result.keys()] if self.result else None



================================================
FILE: src/python/txtai/database/duckdb.py
================================================
"""
DuckDB module
"""

import os
import re

from tempfile import TemporaryDirectory

# Conditional import
try:
    import duckdb

    DUCKDB = True
except ImportError:
    DUCKDB = False

from .embedded import Embedded
from .schema import Statement


class DuckDB(Embedded):
    """
    Database instance backed by DuckDB.
    """

    # Delete single document and object
    DELETE_DOCUMENT = "DELETE FROM documents WHERE id = ?"
    DELETE_OBJECT = "DELETE FROM objects WHERE id = ?"

    def __init__(self, config):
        super().__init__(config)

        if not DUCKDB:
            raise ImportError('DuckDB is not available - install "database" extra to enable')

    def execute(self, function, *args):
        # Call parent method with DuckDB compatible arguments
        return super().execute(function, *self.formatargs(args))

    def insertdocument(self, uid, data, tags, entry):
        # Delete existing document
        self.cursor.execute(DuckDB.DELETE_DOCUMENT, [uid])

        # Call parent method
        super().insertdocument(uid, data, tags, entry)

    def insertobject(self, uid, data, tags, entry):
        # Delete existing object
        self.cursor.execute(DuckDB.DELETE_OBJECT, [uid])

        # Call parent method
        super().insertobject(uid, data, tags, entry)

    def connect(self, path=":memory:"):
        # Create connection and start a transaction
        # pylint: disable=I1101
        connection = duckdb.connect(path)
        connection.begin()

        return connection

    def getcursor(self):
        return self.connection

    def jsonprefix(self):
        # Return json column prefix
        return "json_extract_string(data"

    def jsoncolumn(self, name):
        # Generate json column using json_extract function
        return f"json_extract_string(data, '$.{name}')"

    def rows(self):
        # Iteratively retrieve and yield rows
        batch = 256
        rows = self.cursor.fetchmany(batch)
        while rows:
            yield from rows
            rows = self.cursor.fetchmany(batch)

    def addfunctions(self):
        # DuckDB doesn't currently support scalar functions
        return

    def copy(self, path):
        # Delete existing file, if necessary
        if os.path.exists(path):
            os.remove(path)

        # Create database connection
        # pylint: disable=I1101
        connection = duckdb.connect(path)

        # List of tables
        tables = ["documents", "objects", "sections"]

        with TemporaryDirectory() as directory:
            # Export existing tables
            for table in tables:
                self.connection.execute(f"COPY {table} TO '{directory}/{table}.parquet' (FORMAT parquet)")

            # Create initial schema
            for schema in [Statement.CREATE_DOCUMENTS, Statement.CREATE_OBJECTS, Statement.CREATE_SECTIONS % "sections"]:
                connection.execute(schema)

            # Import tables into new schema
            for table in tables:
                connection.execute(f"COPY {table} FROM '{directory}/{table}.parquet' (FORMAT parquet)")

            # Create indexes and sync data to database file
            connection.execute(Statement.CREATE_SECTIONS_INDEX)
            connection.execute("CHECKPOINT")

        # Start transaction
        connection.begin()

        return connection

    def formatargs(self, args):
        """
        DuckDB doesn't support named parameters. This method replaces named parameters with question marks
        and makes parameters a list.

        Args:
            args: input arguments

        Returns:
            DuckDB compatible args
        """

        if args and len(args) > 1:
            # Unpack query args
            query, parameters = args

            # Iterate over parameters
            #   - Replace named parameters with ?'s
            #   - Build list of value with position indexes
            params = []
            for key, value in parameters.items():
                pattern = rf"\:{key}(?=\s|$)"
                match = re.search(pattern, query)
                if match:
                    query = re.sub(pattern, "?", query, count=1)
                    params.append((match.start(), value))

            # Repack query and parameter list
            args = (query, [value for _, value in sorted(params, key=lambda x: x[0])])

        return args



================================================
FILE: src/python/txtai/database/embedded.py
================================================
"""
Embedded module
"""

from .rdbms import RDBMS


class Embedded(RDBMS):
    """
    Base class for embedded relational databases. An embedded relational database stores all content in a local file.
    """

    def __init__(self, config):
        """
        Creates a new Database.

        Args:
            config: database configuration parameters
        """

        super().__init__(config)

        # Path to database file
        self.path = None

    def load(self, path):
        # Call parent logic
        super().load(path)

        # Store path reference
        self.path = path

    def save(self, path):
        # Temporary database
        if not self.path:
            # Save temporary database
            self.connection.commit()

            # Copy data from current to new
            connection = self.copy(path)

            # Close temporary database
            self.connection.close()

            # Point connection to new connection
            self.session(connection=connection)
            self.path = path

        # Paths are equal, commit changes
        elif self.path == path:
            self.connection.commit()

        # New path is different from current path, copy data and continue using current connection
        else:
            self.copy(path).close()

    def jsonprefix(self):
        # Return json column prefix
        return "json_extract(data"

    def jsoncolumn(self, name):
        # Generate json column using json_extract function
        return f"json_extract(data, '$.{name}')"

    def copy(self, path):
        """
        Copies the current database into path.

        Args:
            path: path to write database

        Returns:
            new connection with data copied over
        """

        raise NotImplementedError



================================================
FILE: src/python/txtai/database/factory.py
================================================
"""
Factory module
"""

from urllib.parse import urlparse

from ..util import Resolver

from .client import Client
from .duckdb import DuckDB
from .sqlite import SQLite


class DatabaseFactory:
    """
    Methods to create document databases.
    """

    @staticmethod
    def create(config):
        """
        Create a Database.

        Args:
            config: database configuration parameters

        Returns:
            Database
        """

        # Database instance
        database = None

        # Enables document database
        content = config.get("content")

        # Standardize content name
        if content is True:
            content = "sqlite"

        # Create document database instance
        if content == "duckdb":
            database = DuckDB(config)
        elif content == "sqlite":
            database = SQLite(config)
        elif content:
            # Check if content is a URL
            url = urlparse(content)
            if content == "client" or url.scheme:
                # Connect to database server URL
                database = Client(config)
            else:
                # Resolve custom database if content is not a URL
                database = DatabaseFactory.resolve(content, config)

        # Store config back
        config["content"] = content

        return database

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: index configuration parameters

        Returns:
            Database
        """

        try:
            return Resolver()(backend)(config)
        except Exception as e:
            raise ImportError(f"Unable to resolve database backend: '{backend}'") from e



================================================
FILE: src/python/txtai/database/rdbms.py
================================================
"""
RDBMS module
"""

import datetime
import json

from .base import Database
from .schema import Statement


# pylint: disable=R0904
class RDBMS(Database):
    """
    Base relational database class. A relational database uses SQL to insert, update, delete and select from a
    database instance.
    """

    def __init__(self, config):
        """
        Creates a new Database.

        Args:
            config: database configuration parameters
        """

        super().__init__(config)

        # Database connection
        self.connection = None
        self.cursor = None

    def load(self, path):
        # Load an existing database. Thread locking must be handled externally.
        self.session(path)

    def insert(self, documents, index=0):
        # Initialize connection if not open
        self.initialize()

        # Get entry date
        entry = datetime.datetime.now(datetime.timezone.utc)

        # Insert documents
        for uid, document, tags in documents:
            if isinstance(document, dict):
                # Insert document and use return value for sections table
                document = self.loaddocument(uid, document, tags, entry)

            if document is not None:
                if isinstance(document, list):
                    # Join tokens to text
                    document = " ".join(document)
                elif not isinstance(document, str):
                    # If object support is enabled, save object
                    self.loadobject(uid, document, tags, entry)

                    # Clear section text for objects, even when objects aren't inserted
                    document = None

                # Save text section
                self.loadsection(index, uid, document, tags, entry)
                index += 1

        # Post processing logic
        self.finalize()

    def delete(self, ids):
        if self.connection:
            # Batch ids
            self.batch(ids=ids)

            # Delete all documents, objects and sections by id
            self.cursor.execute(Statement.DELETE_DOCUMENTS)
            self.cursor.execute(Statement.DELETE_OBJECTS)
            self.cursor.execute(Statement.DELETE_SECTIONS)

    def reindex(self, config):
        if self.connection:
            # Set new configuration
            self.configure(config)

            # Resolve text column
            select = self.resolve(self.text)

            # Initialize reindex operation
            name = self.reindexstart()

            # Copy data over
            self.cursor.execute(Statement.COPY_SECTIONS % (name, select))

            # Stream new results
            self.cursor.execute(Statement.STREAM_SECTIONS % name)
            for uid, text, data, obj, tags in self.rows():
                if not text and self.encoder and obj:
                    yield (uid, self.encoder.decode(obj), tags)
                else:
                    # Read JSON data, if provided
                    data = json.loads(data) if data and isinstance(data, str) else data

                    # Stream data if available, otherwise use section text
                    yield (uid, data if data else text, tags)

            # Swap as new table
            self.cursor.execute(Statement.DROP_SECTIONS)
            self.cursor.execute(Statement.RENAME_SECTIONS % name)

            # Finish reindex operation
            self.reindexend(name)

    def save(self, path):
        if self.connection:
            self.connection.commit()

    def close(self):
        # Close connection
        if self.connection:
            self.connection.close()

    def ids(self, ids):
        # Batch ids and run query
        self.batch(ids=ids)
        self.cursor.execute(Statement.SELECT_IDS)

        # Format and return results
        return self.cursor.fetchall()

    def count(self):
        self.cursor.execute(Statement.COUNT_IDS)
        return self.cursor.fetchone()[0]

    def resolve(self, name, alias=None):
        # Standard column names
        sections = ["indexid", "id", "tags", "entry"]
        noprefix = ["data", "object", "score", "text"]

        # Alias expression
        if alias:
            # Skip if name matches alias or alias is a standard column name
            if name == alias or alias in sections:
                return name

            # Build alias clause
            return f'{name} as "{alias}"'

        # Resolve expression
        if self.expressions and name in self.expressions:
            return self.expressions[name]

        # Name is already resolved, skip
        if name.startswith(self.jsonprefix()) or any(f"s.{s}" == name for s in sections):
            return name

        # Standard columns - need prefixes
        if name.lower() in sections:
            return f"s.{name}"

        # Standard columns - no prefixes
        if name.lower() in noprefix:
            return name

        # Other columns come from documents.data JSON
        return self.jsoncolumn(name)

    def embed(self, similarity, batch):
        # Load similarity results id batch
        self.batch(indexids=[i for i, _ in similarity[batch]], batch=batch)

        # Average and load all similarity scores with first batch
        if not batch:
            self.scores(similarity)

        # Return ids clause placeholder
        return Statement.IDS_CLAUSE % batch

    # pylint: disable=R0912
    def query(self, query, limit, parameters, indexids):
        # Extract query components
        select = query.get("select", self.defaults())
        where = query.get("where")
        groupby, having = query.get("groupby"), query.get("having")
        orderby, qlimit, offset = query.get("orderby"), query.get("limit"), query.get("offset")
        similarity = query.get("similar")

        # Select "indexid, score" when indexids is True
        if indexids:
            select = f"{self.resolve('indexid')}, {self.resolve('score')}"

        # Build query text
        query = Statement.TABLE_CLAUSE % select
        if where is not None:
            query += f" WHERE {where}"
        if groupby is not None:
            query += f" GROUP BY {groupby}"
        if having is not None:
            query += f" HAVING {having}"
        if orderby is not None:
            query += f" ORDER BY {orderby}"

        # Default ORDER BY if not provided and similarity scores are available
        if similarity and orderby is None:
            query += " ORDER BY score DESC"

        # Apply query limit
        if qlimit is not None or limit:
            query += f" LIMIT {qlimit if qlimit else limit}"

            # Apply offset
            if offset is not None:
                query += f" OFFSET {offset}"

        # Clear scores when no similar clauses present
        if not similarity:
            self.scores(None)

        # Runs a user query through execute method, which has common user query handling logic
        args = (query, parameters) if parameters else (query,)
        self.execute(self.cursor.execute, *args)

        # Retrieve column list from query
        columns = [c[0] for c in self.cursor.description]

        # Map results and return
        results = []
        for row in self.rows():
            result = {}

            # Copy columns to result. In cases with duplicate column names, find one with a value
            for x, column in enumerate(columns):
                if column not in result or result[column] is None:
                    # Decode object
                    if self.encoder and column == self.object:
                        result[column] = self.encoder.decode(row[x])
                    else:
                        result[column] = row[x]

            results.append(result)

        # Transform results, if necessary
        return [(x["indexid"], x["score"]) for x in results] if indexids else results

    def initialize(self):
        """
        Creates connection and initial database schema if no connection exists.
        """

        if not self.connection:
            # Create database session. Thread locking must be handled externally.
            self.session()

            # Create initial table schema
            self.createtables()

    def session(self, path=None, connection=None):
        """
        Starts a new database session.

        Args:
            path: path to database file
            connection: existing connection to use
        """

        # Create database connection and cursor
        self.connection = connection if connection else self.connect(path) if path else self.connect()
        self.cursor = self.getcursor()

        # Register custom functions - session scope
        self.addfunctions()

        # Create temporary tables - session scope
        self.createbatch()
        self.createscores()

    def createtables(self):
        """
        Creates the initial table schema.
        """

        self.cursor.execute(Statement.CREATE_DOCUMENTS)
        self.cursor.execute(Statement.CREATE_OBJECTS)
        self.cursor.execute(Statement.CREATE_SECTIONS % "sections")
        self.cursor.execute(Statement.CREATE_SECTIONS_INDEX)

    def finalize(self):
        """
        Post processing logic run after inserting a batch of documents. Default method is no-op.
        """

    def loaddocument(self, uid, document, tags, entry):
        """
        Applies pre-processing logic and inserts a document.

        Args:
            uid: unique id
            document: input document dictionary
            tags: document tags
            entry: generated entry date

        Returns:
            section value
        """

        # Make a copy of document before changing
        document = document.copy()

        # Get and remove object field from document
        obj = document.pop(self.object) if self.object in document else None

        # Insert document as JSON
        if document:
            self.insertdocument(uid, json.dumps(document, allow_nan=False), tags, entry)

        # If text and object are both available, load object as it won't otherwise be used
        if self.text in document and obj:
            self.loadobject(uid, obj, tags, entry)

        # Return value to use for section - use text if available otherwise use object
        return document[self.text] if self.text in document else obj

    def insertdocument(self, uid, data, tags, entry):
        """
        Inserts a document.

        Args:
            uid: unique id
            data: document data
            tags: document tags
            entry: generated entry date
        """

        self.cursor.execute(Statement.INSERT_DOCUMENT, [uid, data, tags, entry])

    def loadobject(self, uid, obj, tags, entry):
        """
        Applies pre-preprocessing logic and inserts an object.

        Args:
            uid: unique id
            obj: input object
            tags: object tags
            entry: generated entry date
        """

        # If object support is enabled, save object
        if self.encoder:
            self.insertobject(uid, self.encoder.encode(obj), tags, entry)

    def insertobject(self, uid, data, tags, entry):
        """
        Inserts an object.

        Args:
            uid: unique id
            data: encoded data
            tags: object tags
            entry: generated entry date
        """

        self.cursor.execute(Statement.INSERT_OBJECT, [uid, data, tags, entry])

    def loadsection(self, index, uid, text, tags, entry):
        """
        Applies pre-processing logic and inserts a section.

        Args:
            index: index id
            uid: unique id
            text: section text
            tags: section tags
            entry: generated entry date
        """

        self.insertsection(index, uid, text, tags, entry)

    def insertsection(self, index, uid, text, tags, entry):
        """
        Inserts a section.

        Args:
            index: index id
            uid: unique id
            text: section text
            tags: section tags
            entry: generated entry date
        """

        # Save text section
        self.cursor.execute(Statement.INSERT_SECTION, [index, uid, text, tags, entry])

    def reindexstart(self):
        """
        Starts a reindex operation.

        Returns:
            temporary working table name
        """

        # Working table name
        name = "rebuild"

        # Create new table to hold reordered sections
        self.cursor.execute(Statement.CREATE_SECTIONS % name)

        return name

    # pylint: disable=W0613
    def reindexend(self, name):
        """
        Ends a reindex operation.

        Args:
            name: working table name
        """

        self.cursor.execute(Statement.CREATE_SECTIONS_INDEX)

    def batch(self, indexids=None, ids=None, batch=None):
        """
        Loads ids to a temporary batch table for efficient query processing.

        Args:
            indexids: list of indexids
            ids: list of ids
            batch: batch index, used when statement has multiple subselects
        """

        # Delete batch when batch id is empty or for batch 0
        if not batch:
            self.cursor.execute(Statement.DELETE_BATCH)

        # Add batch
        self.insertbatch(indexids, ids, batch)

    def createbatch(self):
        """
        Creates temporary batch table.
        """

        # Create or Replace temporary batch table
        self.cursor.execute(Statement.CREATE_BATCH)

    def insertbatch(self, indexids, ids, batch):
        """
        Inserts batch of ids.
        """

        if indexids:
            self.cursor.executemany(Statement.INSERT_BATCH_INDEXID, [(i, batch) for i in indexids])
        if ids:
            self.cursor.executemany(Statement.INSERT_BATCH_ID, [(str(uid), batch) for uid in ids])

    def scores(self, similarity):
        """
        Loads a batch of similarity scores to a temporary table for efficient query processing.

        Args:
            similarity: similarity results as [(indexid, score)]
        """

        # Delete scores
        self.cursor.execute(Statement.DELETE_SCORES)

        if similarity:
            # Average scores per id, needed for multiple similar() clauses
            scores = {}
            for s in similarity:
                for i, score in s:
                    if i not in scores:
                        scores[i] = []
                    scores[i].append(score)

            # Add scores
            self.insertscores(scores)

    def createscores(self):
        """
        Creates temporary scores table.
        """

        # Create or Replace temporary scores table
        self.cursor.execute(Statement.CREATE_SCORES)

    def insertscores(self, scores):
        """
        Inserts a batch of scores.

        Args:
            scores: scores to add
        """

        # Average scores by id
        if scores:
            self.cursor.executemany(Statement.INSERT_SCORE, [(i, sum(s) / len(s)) for i, s in scores.items()])

    def defaults(self):
        """
        Returns a list of default columns when there is no select clause.

        Returns:
            list of default columns
        """

        return "s.id, text, score"

    def connect(self, path=None):
        """
        Creates a new database connection.

        Args:
            path: path to database file

        Returns:
            connection
        """

        raise NotImplementedError

    def getcursor(self):
        """
        Opens a cursor for current connection.

        Returns:
            cursor
        """

        raise NotImplementedError

    def jsonprefix(self):
        """
        Returns json column prefix to test for.

        Returns:
            dynamic column prefix
        """

        raise NotImplementedError

    def jsoncolumn(self, name):
        """
        Builds a json extract column expression for name.

        Args:
            name: column name

        Returns:
            dynamic column expression
        """

        raise NotImplementedError

    def rows(self):
        """
        Returns current cursor row iterator for last executed query.

        Args:
            cursor: cursor

        Returns:
            iterable collection of rows
        """

        raise NotImplementedError

    def addfunctions(self):
        """
        Adds custom functions in current connection.
        """

        raise NotImplementedError



================================================
FILE: src/python/txtai/database/sqlite.py
================================================
"""
SQLite module
"""

import os
import sqlite3

from .embedded import Embedded


class SQLite(Embedded):
    """
    Database instance backed by SQLite.
    """

    def connect(self, path=""):
        # Create connection
        connection = sqlite3.connect(path, check_same_thread=False)

        # Enable WAL mode, if necessary
        if self.setting("wal"):
            connection.execute("PRAGMA journal_mode=WAL")

        return connection

    def getcursor(self):
        return self.connection.cursor()

    def rows(self):
        return self.cursor

    def addfunctions(self):
        if self.connection and self.functions:
            # Enable callback tracebacks to show user-defined function errors
            sqlite3.enable_callback_tracebacks(True)

            for name, argcount, fn in self.functions:
                self.connection.create_function(name, argcount, fn)

    def copy(self, path):
        # Delete existing file, if necessary
        if os.path.exists(path):
            os.remove(path)

        # Create database. Thread locking must be handled externally.
        connection = self.connect(path)

        if self.connection.in_transaction:
            # The backup call will hang if there are uncommitted changes, need to copy over
            # with iterdump (which is much slower)
            for sql in self.connection.iterdump():
                connection.execute(sql)
        else:
            # Database is up to date, can do a more efficient copy with SQLite C API
            self.connection.backup(connection)

        return connection



================================================
FILE: src/python/txtai/database/encoder/__init__.py
================================================
"""
Encoder imports
"""

from .base import Encoder
from .factory import EncoderFactory
from .image import ImageEncoder
from .serialize import SerializeEncoder



================================================
FILE: src/python/txtai/database/encoder/base.py
================================================
"""
Encoder module
"""

from io import BytesIO


class Encoder:
    """
    Encodes and decodes object content. The base encoder works only with byte arrays. It can be extended to encode different datatypes.
    """

    def encode(self, obj):
        """
        Encodes an object to a byte array using the encoder.

        Args:
            obj: object to encode

        Returns:
            encoded object as a byte array
        """

        return obj

    def decode(self, data):
        """
        Decodes input byte array into an object using this encoder.

        Args:
            data: encoded data

        Returns:
            decoded object
        """

        return BytesIO(data) if data else None



================================================
FILE: src/python/txtai/database/encoder/factory.py
================================================
"""
Encoder factory module
"""

from ...util import Resolver

from .base import Encoder
from .serialize import SerializeEncoder


class EncoderFactory:
    """
    Encoder factory. Creates new Encoder instances.
    """

    @staticmethod
    def get(encoder):
        """
        Gets a new instance of encoder class.

        Args:
            encoder: Encoder instance class

        Returns:
            Encoder class
        """

        # Local task if no package
        if "." not in encoder:
            # Get parent package
            encoder = ".".join(__name__.split(".")[:-1]) + "." + encoder.capitalize() + "Encoder"

        return Resolver()(encoder)

    @staticmethod
    def create(encoder):
        """
        Creates a new Encoder instance.

        Args:
            encoder: Encoder instance class

        Returns:
            Encoder
        """

        # Return default encoder
        if encoder is True:
            return Encoder()

        # Supported serialization methods
        if encoder in ["messagepack", "pickle"]:
            return SerializeEncoder(encoder)

        # Get Encoder instance
        return EncoderFactory.get(encoder)()



================================================
FILE: src/python/txtai/database/encoder/image.py
================================================
"""
ImageEncoder module
"""

from io import BytesIO

# Conditional import
try:
    from PIL import Image

    PIL = True
except ImportError:
    PIL = False

from .base import Encoder


class ImageEncoder(Encoder):
    """
    Encodes and decodes Image objects as compressed binary content, using the original image's algorithm.
    """

    def __init__(self):
        """
        Creates a new ImageEncoder.
        """

        if not PIL:
            raise ImportError('ImageEncoder is not available - install "database" extra to enable')

    def encode(self, obj):
        # Create byte stream
        output = BytesIO()

        # Write image to byte stream
        obj.save(output, format=obj.format, quality="keep")

        # Return byte array
        return output.getvalue()

    def decode(self, data):
        # Return a PIL image
        return Image.open(BytesIO(data)) if data else None



================================================
FILE: src/python/txtai/database/encoder/serialize.py
================================================
"""
SerializeEncoder module
"""

from ...serialize import SerializeFactory

from .base import Encoder


class SerializeEncoder(Encoder):
    """
    Encodes and decodes objects using the internal serialize package.
    """

    def __init__(self, method):
        # Parent constructor
        super().__init__()

        # Pickle serialization
        self.serializer = SerializeFactory.create(method)

    def encode(self, obj):
        # Pickle object
        return self.serializer.savebytes(obj)

    def decode(self, data):
        # Unpickle to object
        return self.serializer.loadbytes(data)



================================================
FILE: src/python/txtai/database/schema/__init__.py
================================================
"""
Schema imports
"""

from .orm import *
from .statement import Statement



================================================
FILE: src/python/txtai/database/schema/orm.py
================================================
"""
ORM Module
"""

# Conditional import
try:
    from sqlalchemy import Column, DateTime, Float, JSON, Integer, LargeBinary, String, Text
    from sqlalchemy.orm import DeclarativeBase

    ORM = True
except ImportError:
    ORM = False


# Standard database schema using object relational mapping (ORM).
if ORM:

    def idcolumn():
        """
        Creates an id column. This method creates an unbounded text field for platforms that support it.

        Returns:
            id column definition
        """

        return String(512).with_variant(Text(), "sqlite", "postgresql")

    class Base(DeclarativeBase):
        """
        Base mapping.
        """

    class Batch(Base):
        """
        Batch temporary table mapping.
        """

        __tablename__ = "batch"
        __table_args__ = {"prefixes": ["TEMPORARY"]}

        autoid = Column(Integer, primary_key=True, autoincrement=True)
        indexid = Column(Integer)
        id = Column(idcolumn())
        batch = Column(Integer)

    class Score(Base):
        """
        Scores temporary table mapping.
        """

        __tablename__ = "scores"
        __table_args__ = {"prefixes": ["TEMPORARY"]}

        indexid = Column(Integer, primary_key=True, autoincrement=False)
        score = Column(Float)

    class Document(Base):
        """
        Documents table mapping.
        """

        __tablename__ = "documents"

        id = Column(idcolumn(), primary_key=True)
        data = Column(JSON)
        tags = Column(Text)
        entry = Column(DateTime(timezone=True))

    class Object(Base):
        """
        Objects table mapping.
        """

        __tablename__ = "objects"

        id = Column(idcolumn(), primary_key=True)
        object = Column(LargeBinary)
        tags = Column(Text)
        entry = Column(DateTime(timezone=True))

    class SectionBase(Base):
        """
        Generic sections table mapping. Allows multiple section table names for reindexing.
        """

        __abstract__ = True

        indexid = Column(Integer, primary_key=True, autoincrement=False)
        id = Column(idcolumn(), index=True)
        text = Column(Text)
        tags = Column(Text)
        entry = Column(DateTime(timezone=True))

    class Section(SectionBase):
        """
        Section table mapping.
        """

        __tablename__ = "sections"



================================================
FILE: src/python/txtai/database/schema/statement.py
================================================
"""
Statement module
"""


class Statement:
    """
    Standard database schema SQL statements.
    """

    # Temporary table for working with id batches
    CREATE_BATCH = """
        CREATE TEMP TABLE IF NOT EXISTS batch (
            indexid INTEGER,
            id TEXT,
            batch INTEGER
        )
    """

    DELETE_BATCH = "DELETE FROM batch"
    INSERT_BATCH_INDEXID = "INSERT INTO batch (indexid, batch) VALUES (?, ?)"
    INSERT_BATCH_ID = "INSERT INTO batch (id, batch) VALUES (?, ?)"

    # Temporary table for joining similarity scores
    CREATE_SCORES = """
        CREATE TEMP TABLE IF NOT EXISTS scores (
            indexid INTEGER PRIMARY KEY,
            score REAL
        )
    """

    DELETE_SCORES = "DELETE FROM scores"
    INSERT_SCORE = "INSERT INTO scores VALUES (?, ?)"

    # Documents - stores full content
    CREATE_DOCUMENTS = """
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            data JSON,
            tags TEXT,
            entry DATETIME
        )
    """

    INSERT_DOCUMENT = "INSERT OR REPLACE INTO documents VALUES (?, ?, ?, ?)"
    DELETE_DOCUMENTS = "DELETE FROM documents WHERE id IN (SELECT id FROM batch)"

    # Objects - stores binary content
    CREATE_OBJECTS = """
        CREATE TABLE IF NOT EXISTS objects (
            id TEXT PRIMARY KEY,
            object BLOB,
            tags TEXT,
            entry DATETIME
        )
    """

    INSERT_OBJECT = "INSERT OR REPLACE INTO objects VALUES (?, ?, ?, ?)"
    DELETE_OBJECTS = "DELETE FROM objects WHERE id IN (SELECT id FROM batch)"

    # Sections - stores section text
    CREATE_SECTIONS = """
        CREATE TABLE IF NOT EXISTS %s (
            indexid INTEGER PRIMARY KEY,
            id TEXT,
            text TEXT,
            tags TEXT,
            entry DATETIME
        )
    """

    CREATE_SECTIONS_INDEX = "CREATE INDEX section_id ON sections(id)"
    INSERT_SECTION = "INSERT INTO sections VALUES (?, ?, ?, ?, ?)"
    DELETE_SECTIONS = "DELETE FROM sections WHERE id IN (SELECT id FROM batch)"
    COPY_SECTIONS = (
        "INSERT INTO %s SELECT (select count(*) - 1 from sections s1 where s.indexid >= s1.indexid) indexid, "
        + "s.id, %s AS text, s.tags, s.entry FROM sections s LEFT JOIN documents d ON s.id = d.id ORDER BY indexid"
    )
    STREAM_SECTIONS = (
        "SELECT s.id, s.text, data, object, s.tags FROM %s s "
        + "LEFT JOIN documents d ON s.id = d.id "
        + "LEFT JOIN objects o ON s.id = o.id ORDER BY indexid"
    )
    DROP_SECTIONS = "DROP TABLE sections"
    RENAME_SECTIONS = "ALTER TABLE %s RENAME TO sections"

    # Queries
    SELECT_IDS = "SELECT indexid, id FROM sections WHERE id in (SELECT id FROM batch)"
    COUNT_IDS = "SELECT count(indexid) FROM sections"

    # Partial sql clauses
    TABLE_CLAUSE = (
        "SELECT %s FROM sections s "
        + "LEFT JOIN documents d ON s.id = d.id "
        + "LEFT JOIN objects o ON s.id = o.id "
        + "LEFT JOIN scores sc ON s.indexid = sc.indexid"
    )
    IDS_CLAUSE = "s.indexid in (SELECT indexid from batch WHERE batch=%s)"



================================================
FILE: src/python/txtai/database/sql/__init__.py
================================================
"""
SQL imports
"""

from .aggregate import Aggregate
from .base import SQL, SQLError
from .expression import Expression
from .token import Token



================================================
FILE: src/python/txtai/database/sql/aggregate.py
================================================
"""
Aggregate module
"""

import itertools
import operator

from .base import SQL


class Aggregate(SQL):
    """
    Aggregates partial results from queries. Partial results come from queries when working with sharded indexes.
    """

    def __init__(self, database=None):
        # Always return token lists as this method requires them
        super().__init__(database, True)

    def __call__(self, query, results):
        """
        Analyzes query results, combines aggregate function results and applies ordering.

        Args:
            query: input query
            results: query results

        Returns:
            aggregated query results
        """

        # Parse query
        query = super().__call__(query)

        # Check if this is a SQL query
        if "select" in query:
            # Get list of unique and aggregate columns. If no aggregate columns or order by found, skip
            columns = list(results[0].keys())
            aggcolumns = self.aggcolumns(columns)
            if aggcolumns or query["orderby"]:
                # Merge aggregate columns
                if aggcolumns:
                    results = self.aggregate(query, results, columns, aggcolumns)

                # Sort results and return
                return self.orderby(query, results) if query["orderby"] else self.defaultsort(results)

        # Otherwise, run default sort
        return self.defaultsort(results)

    def aggcolumns(self, columns):
        """
        Filters columns for columns that have an aggregate function call.

        Args:
            columns: list of columns

        Returns:
            list of aggregate columns
        """

        aggregates = {}
        for column in columns:
            column = column.lower()
            if column.startswith(("count(", "sum(", "total(")):
                aggregates[column] = sum
            elif column.startswith("max("):
                aggregates[column] = max
            elif column.startswith("min("):
                aggregates[column] = min
            elif column.startswith("avg("):
                aggregates[column] = lambda x: sum(x) / len(x)

        return aggregates

    def aggregate(self, query, results, columns, aggcolumns):
        """
        Merges aggregate columns in results.

        Args:
            query: input query
            results: query results
            columns: list of select columns
            aggcolumns: list of aggregate columns

        Returns:
            results with aggregates merged
        """

        # Group data, if necessary
        if query["groupby"]:
            results = self.groupby(query, results, columns)
        else:
            results = [results]

        # Compute column values
        rows = []
        for result in results:
            # Calculate/copy column values
            row = {}
            for column in columns:
                if column in aggcolumns:
                    # Calculate aggregate value
                    function = aggcolumns[column]
                    row[column] = function([r[column] for r in result])
                else:
                    # Non aggregate column value repeat, use first value
                    row[column] = result[0][column]

            # Add row using original query columns
            rows.append(row)

        return rows

    def groupby(self, query, results, columns):
        """
        Groups results using query group by clause.

        Args:
            query: input query
            results: query results
            columns: list of select columns

        Returns:
            results grouped using group by clause
        """

        groupby = [column for column in columns if column.lower() in query["groupby"]]
        if groupby:
            results = sorted(results, key=operator.itemgetter(*groupby))
            return [list(value) for _, value in itertools.groupby(results, operator.itemgetter(*groupby))]

        return [results]

    def orderby(self, query, results):
        """
        Applies an order by clause to results.

        Args:
            query: input query
            results: query results

        Returns:
            results ordered using order by clause
        """

        # Sort in reverse order
        for clause in query["orderby"][::-1]:
            # Order by columns must be selected
            reverse = False
            if clause.lower().endswith(" asc"):
                clause = clause.rsplit(" ")[0]
            elif clause.lower().endswith(" desc"):
                clause = clause.rsplit(" ")[0]
                reverse = True

            # Order by columns must be in select clause
            if clause in query["select"]:
                results = sorted(results, key=operator.itemgetter(clause), reverse=reverse)

        return results

    def defaultsort(self, results):
        """
        Default sorting algorithm for results. Sorts by score descending, if available.

        Args:
            results: query results

        Returns:
            results ordered by score descending
        """

        # Sort standard query using score column, if present
        if results and "score" in results[0]:
            return sorted(results, key=lambda x: x["score"], reverse=True)

        return results



================================================
FILE: src/python/txtai/database/sql/base.py
================================================
"""
SQL module
"""

from io import StringIO
from shlex import shlex

from .expression import Expression


class SQL:
    """
    Translates txtai SQL statements into database native queries.
    """

    # List of clauses to parse
    CLAUSES = ["select", "from", "where", "group", "having", "order", "limit", "offset"]

    def __init__(self, database=None, tolist=False):
        """
        Creates a new SQL query parser.

        Args:
            database: database instance that provides resolver callback, if any
            tolist: outputs expression lists if True, expression text otherwise, defaults to False
        """

        # Expression parser
        self.expression = Expression(database.resolve if database else self.defaultresolve, tolist)

    def __call__(self, query):
        """
        Parses an input SQL query and normalizes column names in the query clauses. This method will also embed
        similarity search placeholders into the query.

        Args:
            query: input query

        Returns:
            {clause name: clause text}
        """

        clauses = None
        if self.issql(query):
            # Ignore multiple statements
            query = query.split(";")[0]

            # Tokenize query
            tokens, positions = self.tokenize(query)

            # Alias clauses and similar queries
            aliases, similar = {}, []

            # Parse SQL clauses
            clauses = {
                "select": self.parse(tokens, positions, "select", alias=True, aliases=aliases),
                "where": self.parse(tokens, positions, "where", aliases=aliases, similar=similar),
                "groupby": self.parse(tokens, positions, "group", offset=2, aliases=aliases),
                "having": self.parse(tokens, positions, "having", aliases=aliases),
                "orderby": self.parse(tokens, positions, "order", offset=2, aliases=aliases),
                "limit": self.parse(tokens, positions, "limit", aliases=aliases),
                "offset": self.parse(tokens, positions, "offset", aliases=aliases),
            }

            # Add parsed similar queries, if any
            if similar:
                clauses["similar"] = similar

        # Return clauses, default to full query if this is not a SQL query
        return clauses if clauses else {"similar": [[query]]}

    # pylint: disable=W0613
    def defaultresolve(self, name, alias=None):
        """
        Default resolve function. Performs no processing, only returns name.

        Args:
            name: query column name
            alias: alias name, defaults to None

        Returns:
            name
        """

        return name

    def issql(self, query):
        """
        Detects if this is a SQL query.

        Args:
            query: input query

        Returns:
            True if this is a valid SQL query, False otherwise
        """

        if isinstance(query, str):
            # Reduce query to a lower-cased single line stripped of leading/trailing whitespace
            query = query.lower().strip(";").replace("\n", " ").replace("\t", " ").strip()

            # Detect if this is a valid txtai SQL statement
            return query.startswith("select ") and (" from txtai " in query or query.endswith(" from txtai"))

        return False

    def snippet(self, text):
        """
        Parses a partial SQL snippet.

        Args:
            text: SQL snippet

        Returns:
            parsed snippet
        """

        tokens, _ = self.tokenize(text)
        return self.expression(tokens)

    def tokenize(self, query):
        """
        Tokenizes SQL query into tokens.

        Args:
            query: input query

        Returns:
            (tokenized query, token positions)
        """

        # Build a simple SQL lexer
        #   - Punctuation chars are parsed as standalone tokens which helps identify operators
        #   - Add additional wordchars to prevent splitting on those values
        #   - Disable comments
        tokens = shlex(StringIO(query), punctuation_chars="=!<>+-*/%|")
        tokens.wordchars += ":@#"
        tokens.commenters = ""
        tokens = list(tokens)

        # Identify sql clause token positions
        positions = {}

        # Get position of clause keywords. For multi-term clauses, validate next token matches as well
        for x, token in enumerate(tokens):
            t = token.lower()
            if t not in positions and t in SQL.CLAUSES and (t not in ["group", "order"] or (x + 1 < len(tokens) and tokens[x + 1].lower() == "by")):
                positions[t] = x

        return (tokens, positions)

    def parse(self, tokens, positions, name, offset=1, alias=False, aliases=None, similar=None):
        """
        Runs query column name to database column name mappings for clauses. This method will also
        parse SIMILAR() function calls, extract parameters for those calls and leave a placeholder
        to be filled in with similarity results.

        Args:
            tokens: query tokens
            positions: token positions - used to locate the start of sql clauses
            name: current query clause name
            offset: how many tokens are in the clause name
            alias: True if terms in the clause should be aliased (i.e. column as alias)
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
            similar: list where parsed similar clauses should be stored

        Returns:
            formatted clause
        """

        clause = None
        if name in positions:
            # Find the next clause token
            end = [positions.get(x, len(tokens)) for x in SQL.CLAUSES[SQL.CLAUSES.index(name) + 1 :]]
            end = min(end) if end else len(tokens)

            # Start after current clause token and end before next clause or end of string
            clause = tokens[positions[name] + offset : end]

            # Parse and resolve parameters
            clause = self.expression(clause, alias, aliases, similar)

        return clause


class SQLError(Exception):
    """
    Raised for errors generated by user SQL queries
    """



================================================
FILE: src/python/txtai/database/sql/expression.py
================================================
"""
Expression module
"""

from .token import Token


class Expression:
    """
    Parses expression statements and runs a set of substitution/formatting rules.
    """

    def __init__(self, resolver, tolist):
        """
        Creates a new expression parser.

        Args:
            resolver: function to call to resolve query column names with database column names
            tolist: outputs expression lists if True, text if False
        """

        self.resolver = resolver
        self.tolist = tolist

    def __call__(self, tokens, alias=False, aliases=None, similar=None):
        """
        Parses and formats a list of tokens as follows:
            - Replaces query column names with database column names
            - Adds similar query placeholders and extracts similar function parameters
            - Rewrites expression and returns

        Args:
            tokens: input expression
            alias: if True, column aliases should be generated and added to aliases dict
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
            similar: list of similar queries, if present new similar queries are appended to this list

        Returns:
            rewritten clause
        """

        # Processes token expressions and applies a set of transformation rules
        transformed = self.process(list(tokens), alias, aliases, similar)

        # Re-write alias expression and return
        if alias and not self.tolist:
            return self.buildalias(transformed, tokens, aliases)

        # Re-write input expression and return
        return self.buildlist(transformed) if self.tolist is True else self.buildtext(transformed)

    def process(self, tokens, alias, aliases, similar):
        """
        Replaces query column names with database column names, adds similar query placeholders and
        extracts similar function parameters.

        Args:
            tokens: input expression
            alias: if True, column aliases should be generated and added to aliases dict
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
            similar: list of similar queries, if present new similar queries are appended to this list

        Returns:
            transformed tokens
        """

        # Create clause index and token iterator. Iterator skips distinct tokens.
        index, iterator = 0, ((x, token) for x, token in enumerate(tokens) if not Token.isdistinct(token))
        for x, token in iterator:
            # Check if separator, increment clause index
            if Token.isseparator(token):
                index += 1

            # Check if token is a square bracket
            elif Token.isbracket(token):
                # Resolve bracket expression
                self.bracket(iterator, tokens, x)

            # Check if token is a similar function
            elif Token.issimilar(tokens, x, similar):
                # Resolve similar expression
                self.similar(iterator, tokens, x, similar)

            # Check if token is a function
            elif Token.isfunction(tokens, x):
                # Resolve function expression
                self.function(iterator, tokens, token, aliases, similar)

            # Check for alias expression
            elif Token.isalias(tokens, x, alias):
                # Process alias expression
                self.alias(iterator, tokens, x, aliases, index)

            # Check for attribute expression
            elif Token.isattribute(tokens, x):
                # Resolve attribute expression
                self.attribute(tokens, x, aliases)

            # Check for compound expression
            elif Token.iscompound(tokens, x):
                # Resolve compound expression
                self.compound(iterator, tokens, x, aliases, similar)

        # Remove replaced tokens
        return [token for token in tokens if token]

    def buildtext(self, tokens):
        """
        Builds a new expression from tokens. This method applies a set of rules to generate whitespace between tokens.

        Args:
            tokens: input expression

        Returns:
            expression text
        """

        # Rebuild expression
        text = ""
        for token in tokens:
            # Write token with whitespace rules applied
            text += Token.wrapspace(text, token)

        # Remove any leading/trailing whitespace and return
        return text.strip()

    def buildlist(self, tokens):
        """
        Builds a new expression from tokens. This method returns a list of expression components. These components can be joined together
        on commas to form a text expression.

        Args:
            tokens: input expression

        Returns:
            expression list
        """

        parts, current, parens, brackets = [], [], 0, 0

        for token in tokens:
            # Create new part
            if token == "," and not parens and not brackets:
                parts.append(self.buildtext(current))
                current = []
            else:
                # Accumulate tokens
                if token == "(":
                    parens += 1
                elif token == ")":
                    parens -= 1
                elif token == "[":
                    brackets += 1
                elif token == "]":
                    brackets -= 1
                elif Token.issortorder(token):
                    token = f" {token}"
                current.append(token)

        # Add last part
        if current:
            parts.append(self.buildtext(current))

        return parts

    def buildalias(self, transformed, tokens, aliases):
        """
        Builds new alias text expression from transformed and input tokens.

        Args:
            transformed: transformed tokens
            tokens: original input tokens
            aliases: dict of column aliases

        Returns:
            alias text expression
        """

        # Convert tokens to expressions
        transformed = self.buildlist(transformed)
        tokens = self.buildlist(tokens)

        expression = []
        for x, token in enumerate(transformed):
            if x not in aliases.values():
                alias = tokens[x]

                # Strip leading/trailing brackets from alias name that doesn't have operators
                if not any(Token.isoperator(t) for t in alias) and alias[0] in ("[", "(") and alias[-1] in ("]", ")"):
                    alias = alias[1:-1]

                # Strip leading distinct keyword
                values = alias.split()
                if len(values) > 0 and Token.isdistinct(values[0]):
                    alias = " ".join(values[1:])

                # Resolve alias
                token = self.resolver(token, alias)

            expression.append(token)

        # Build alias text expression
        return ", ".join(expression)

    def bracket(self, iterator, tokens, x):
        """
        Consumes a [bracket] expression.

        Args:
            iterator: tokens iterator
            tokens: input tokens
            x: current position
        """

        # Function parameters
        params = []

        # Clear token from stream
        token = tokens[x]
        tokens[x] = None

        # Bracket counter (current token is an open bracket)
        brackets = 1

        # Read until token is a end bracket
        while token and (token != "]" or brackets > 0):
            x, token = next(iterator, (None, None))

            # Increase/decrease bracket counter
            if token == "[":
                brackets += 1
            elif token == "]":
                brackets -= 1

            # Accumulate tokens
            if token != "]" or brackets > 0:
                params.append(token)

            # Clear token from stream
            tokens[x] = None

        # Set last token to resolved bracket expression
        tokens[x] = self.resolve(self.buildtext(params), None)

    def similar(self, iterator, tokens, x, similar):
        """
        Substitutes a similar() function call with a placeholder that can later be used to add
        embeddings query results as a filter.

        Args:
            iterator: tokens iterator
            tokens: input tokens
            x: current position
            similar: list where similar function call parameters are stored
        """

        # Function parameters
        params = []

        # Clear token from stream
        token = tokens[x]
        tokens[x] = None

        # Read until token is a closing paren
        while token and token != ")":
            x, token = next(iterator, (None, None))
            if token and token not in ["(", ",", ")"]:
                # Strip quotes and accumulate tokens
                params.append(token.replace("'", "").replace('"', ""))

            # Clear token from stream
            tokens[x] = None

        # Add placeholder for embedding similarity results
        tokens[x] = f"{Token.SIMILAR_TOKEN}{len(similar)}"

        # Save parameters
        similar.append(params)

    def function(self, iterator, tokens, token, aliases, similar):
        """
        Resolves column names within the function's parameters.

        Args:
            iterator: tokens iterator
            tokens: input tokens
            token: current token
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
            similar: list where similar function call parameters are stored
        """

        # Consume function parameters
        while token and token != ")":
            x, token = next(iterator, (None, None))

            # Check if token is a square bracket
            if Token.isbracket(token):
                # Resolve bracket expression
                self.bracket(iterator, tokens, x)

            # Check if token is a similar function
            elif Token.issimilar(tokens, x, similar):
                # Resolve similar expression
                self.similar(iterator, tokens, x, similar)

            # Check if token is a function
            elif Token.isfunction(tokens, x):
                # Resolve function parameters that are functions
                self.function(iterator, tokens, token, aliases, similar)

            # Check for attribute expression
            elif Token.isattribute(tokens, x):
                # Resolve attributes
                self.attribute(tokens, x, aliases)

            # Check for compound expression
            elif Token.iscompound(tokens, x):
                # Resolve compound expressions
                self.compound(iterator, tokens, x, aliases, similar)

    def alias(self, iterator, tokens, x, aliases, index):
        """
        Reads an alias clause and stores it in aliases.

        Args:
            iterator: tokens iterator
            tokens: input tokens
            x: current position
            aliases: dict where aliases are stored - stores {alias: clause index}
            index: clause index, used to match aliases with columns
        """

        token = tokens[x]

        # If this is an alias token, get next token
        if token in Token.ALIAS:
            x, token = next(iterator, (None, None))

        # Consume tokens until end of stream or a separator is found. Evaluate next token to prevent consuming here.
        while x + 1 < len(tokens) and not Token.isseparator(Token.get(tokens, x + 1)):
            x, token = next(iterator, (None, None))

        # Add normalized alias and clause index
        aliases[Token.normalize(token)] = index

    def attribute(self, tokens, x, aliases):
        """
        Resolves an attribute column name.

        Args:
            tokens: input tokens
            x: current token position
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
        """

        # Resolve attribute expression
        tokens[x] = self.resolve(tokens[x], aliases)

    def compound(self, iterator, tokens, x, aliases, similar):
        """
        Resolves column names in a compound expression (left side <operator(s)> right side).

        Args:
            iterator: tokens iterator
            tokens: input tokens
            x: current token position
            aliases: dict of generated aliases, if present these tokens should NOT be resolved
            similar: list where similar function call parameters are stored
        """

        # Resolve left side (left side already had function processing applied through standard loop)
        if Token.iscolumn(tokens[x - 1]):
            tokens[x - 1] = self.resolve(tokens[x - 1], aliases)

        # Consume operator(s), handle both single and compound operators, i.e. column NOT LIKE 1
        token = tokens[x]
        while token and Token.isoperator(token):
            x, token = next(iterator, (None, None))

        # Resolve right side
        if token and Token.iscolumn(token):
            # Need to process functions since it hasn't went through the standard loop yet
            if Token.isfunction(tokens, x):
                self.function(iterator, tokens, token, aliases, similar)
            else:
                tokens[x] = self.resolve(token, aliases)

    def resolve(self, token, aliases):
        """
        Resolves this token's value if it is not an alias or a bind parameter.

        Args:
            token: token to resolve
            aliases: dict of generated aliases, if present these tokens should NOT be resolved

        Returns:
            resolved token value
        """

        # Check for alias or bind parameter
        if (aliases and Token.normalize(token) in aliases) or (token.startswith(":")):
            return token

        return self.resolver(token)



================================================
FILE: src/python/txtai/database/sql/token.py
================================================
"""
Token module
"""


class Token:
    """
    Methods to check for token type.
    """

    # Similar token replacement
    SIMILAR_TOKEN = "__SIMILAR__"

    # Default distinct token
    DISTINCT = ["distinct"]

    # Default alias token
    ALIAS = ["as"]

    # Default list of comparison operators
    OPERATORS = ["=", "!=", "<>", ">", ">=", "<", "<=", "+", "-", "*", "/", "%", "||", "not", "between", "like", "is", "null"]

    # Default list of logic separators
    LOGIC_SEPARATORS = ["and", "or"]

    # Default list of sort order operators
    SORT_ORDER = ["asc", "desc"]

    @staticmethod
    def get(tokens, x):
        """
        Gets token at position x. This method will validate position is valid within tokens.

        Args:
            tokens: input tokens
            x: position to retrieve

        Returns:
            tokens[x] if x is a valid position, None otherwise
        """

        if 0 <= x < len(tokens):
            return tokens[x]

        return None

    @staticmethod
    def isalias(tokens, x, alias):
        """
        Checks if tokens[x] is an alias keyword.

        Args:
            tokens: input tokens
            x: current position
            alias: if column alias processing is enabled

        Returns:
            True if tokens[x] is an alias token, False otherwise
        """

        prior = Token.get(tokens, x - 1)
        token = tokens[x]

        # True if prior token is not a separator, grouping token or distinct token and current token is either a column token or quoted token
        return (
            alias
            and x > 0
            and not Token.isseparator(prior)
            and not Token.isgroupstart(prior)
            and not Token.isdistinct(prior)
            and (Token.iscolumn(token) or Token.isquoted(token))
        )

    @staticmethod
    def isattribute(tokens, x):
        """
        Checks if tokens[x] is an attribute.

        Args:
            tokens: input tokens
            x: current position

        Returns:
            True if tokens[x] is an attribute, False otherwise
        """

        # True if token is a column and next token is not an operator
        return Token.iscolumn(tokens[x]) and not Token.isoperator(Token.get(tokens, x + 1))

    @staticmethod
    def isbracket(token):
        """
        Checks if token is an open bracket.

        Args:
            token: token to test

        Returns:
            True if token is an open bracket, False otherwise
        """

        # Token is a bracket
        return token == "["

    @staticmethod
    def iscolumn(token):
        """
        Checks if token is a column name.

        Args:
            token: token to test

        Returns:
            True if this token is a column name token, False otherwise
        """

        # Columns are not operators, logic separators, literals or sort order tokens
        return (
            token
            and not Token.isoperator(token)
            and not Token.islogicseparator(token)
            and not Token.isliteral(token)
            and not Token.issortorder(token)
        )

    @staticmethod
    def iscompound(tokens, x):
        """
        Checks if tokens[x] is a compound expression.

        Args:
            tokens: input tokens
            x: current position

        Returns:
            True if tokens[x] is a compound expression, False otherwise
        """

        # Compound expression is defined as: <column> <operator(s)> <column>
        return Token.isoperator(tokens[x]) and (Token.iscolumn(Token.get(tokens, x - 1)) or Token.iscolumn(Token.get(tokens, x + 1)))

    @staticmethod
    def isdistinct(token):
        """
        Checks if token is the distinct keyword.

        Args:
            token: token to test

        Returns:
            True if this token is a distinct keyword, False otherwise
        """

        # Token is the distinct keyword
        return token and token.lower() in Token.DISTINCT

    @staticmethod
    def isfunction(tokens, x):
        """
        Checks if tokens[x] is a function.

        Args:
            tokens: input tokens
            x: current position

        Returns:
            True if tokens[x] is a function, False otherwise
        """

        # True if a column token is followed by an open paren
        return Token.iscolumn(tokens[x]) and Token.get(tokens, x + 1) == "("

    @staticmethod
    def isgroupstart(token):
        """
        Checks if token is a group start token.

        Args:
            token: token to test

        Returns:
            True if token is a group start token, False otherwise
        """

        # Token is a paren
        return token == "("

    @staticmethod
    def isliteral(token):
        """
        Checks if token is a literal.

        Args:
            token: token to test

        Returns:
            True if this token is a literal, False otherwise
        """

        # Literals are wrapped in quotes, parens, wildcards or numeric.
        return token and (token.startswith(("'", '"', ",", "(", ")", "*")) or token.replace(".", "", 1).isdigit())

    @staticmethod
    def islogicseparator(token):
        """
        Checks if token is a logic separator token.

        Args:
            token: token to test

        Returns:
            True if this token is a logic separator, False otherwise
        """

        # Token is a logic separator
        return token and token.lower() in Token.LOGIC_SEPARATORS

    @staticmethod
    def isoperator(token):
        """
        Checks if token is an operator token.

        Args:
            token: token to test

        Returns:
            True if this token is an operator, False otherwise
        """

        # Token is an operator
        return token and token.lower() in Token.OPERATORS

    @staticmethod
    def isquoted(token):
        """
        Checks if token is quoted.

        Args:
            token: token to test

        Returns:
            True if this token is quoted, False otherwise
        """

        # Token is quoted
        return token.startswith(("'", '"')) and token.endswith(("'", '"'))

    @staticmethod
    def isseparator(token):
        """
        Checks if token is a separator token.

        Args:
            token to test

        Returns:
            True if this token is a separator, False otherwise
        """

        # Token is a comma
        return token == ","

    @staticmethod
    def issimilar(tokens, x, similar):
        """
        Checks if tokens[x] is a similar() function.

        Args:
            tokens: input tokens
            x: current position
            similar: list where similar function call parameters are stored, can be None in which case similar processing is skipped

        Returns:
            True if tokens[x] is a similar clause
        """

        # True if a "similar" token is followed by an open paren
        return similar is not None and tokens[x].lower() == "similar" and Token.get(tokens, x + 1) == "("

    @staticmethod
    def issortorder(token):
        """
        Checks if token is a sort order token.

        Args:
            token: token to test

        Returns:
            True if this token is a sort order operator, False otherwise
        """

        # Token is a sort order operator
        return token and token.lower() in Token.SORT_ORDER

    @staticmethod
    def normalize(token):
        """
        Applies a normalization algorithm to the input token as follows:
            - Strip single and double quotes
            - Make lowercase

        Args:
            token: input token

        Returns:
            normalized token
        """

        # Lowercase, replace and return
        return token.lower().replace("'", "").replace('"', "")

    @staticmethod
    def wrapspace(text, token):
        """
        Applies whitespace wrapping rules to token.

        Args:
            text: current text buffer
            token: token to add

        Returns:
            token with whitespace rules applied
        """

        # Wildcards have no whitespace. Need special case since * is also multiply which does have whitespace.
        if token in ["*"] and (not text or text.endswith((" ", "("))):
            return token

        # Operator whitespace
        if Token.isoperator(token) or Token.islogicseparator(token) or token.lower() in ["in"]:
            return f" {token} " if not text.endswith(" ") else f"{token} "

        # Comma whitespace
        if Token.isseparator(token):
            return f"{token} "

        # No whitespace if any of the following is True
        if not text or text.endswith((" ", "(", "[")) or token in ["(", "[", ")", "]"] or token.startswith("."):
            return token

        # Default is to add leading whitespace
        return f" {token}"



================================================
FILE: src/python/txtai/embeddings/__init__.py
================================================
"""
Embeddings imports
"""

from .base import Embeddings
from .index import *
from .search import *



================================================
FILE: src/python/txtai/embeddings/base.py
================================================
"""
Embeddings module
"""

import json
import os
import tempfile

from ..ann import ANNFactory
from ..archive import ArchiveFactory
from ..cloud import CloudFactory
from ..database import DatabaseFactory
from ..graph import GraphFactory
from ..scoring import ScoringFactory
from ..vectors import VectorsFactory

from .index import Action, Configuration, Functions, Indexes, IndexIds, Reducer, Stream, Transform
from .search import Explain, Ids, Query, Search, Terms


# pylint: disable=C0302,R0904
class Embeddings:
    """
    Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts
    will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results
    that have the same meaning, not necessarily the same keywords.
    """

    # pylint: disable=W0231
    def __init__(self, config=None, models=None, **kwargs):
        """
        Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized.

        Args:
            config: embeddings configuration
            models: models cache, used for model sharing between embeddings
            kwargs: additional configuration as keyword args
        """

        # Index configuration
        self.config = None

        # Dimensionality reduction - word vectors only
        self.reducer = None

        # Dense vector model - transforms data into similarity vectors
        self.model = None

        # Approximate nearest neighbor index
        self.ann = None

        # Index ids when content is disabled
        self.ids = None

        # Document database
        self.database = None

        # Resolvable functions
        self.functions = None

        # Graph network
        self.graph = None

        # Sparse vectors
        self.scoring = None

        # Query model
        self.query = None

        # Index archive
        self.archive = None

        # Subindexes for this embeddings instance
        self.indexes = None

        # Models cache
        self.models = models

        # Merge configuration into single dictionary
        config = {**config, **kwargs} if config and kwargs else kwargs if kwargs else config

        # Set initial configuration
        self.configure(config)

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    def score(self, documents):
        """
        Builds a term weighting scoring index. Only used by word vectors models.

        Args:
            documents: iterable of (id, data, tags), (id, data) or data
        """

        # Build scoring index for word vectors term weighting
        if self.isweighted():
            self.scoring.index(Stream(self)(documents))

    def index(self, documents, reindex=False, checkpoint=None):
        """
        Builds an embeddings index. This method overwrites an existing index.

        Args:
            documents: iterable of (id, data, tags), (id, data) or data
            reindex: if this is a reindex operation in which case database creation is skipped, defaults to False
            checkpoint: optional checkpoint directory, enables indexing restart
        """

        # Initialize index
        self.initindex(reindex)

        # Create transform and stream
        transform = Transform(self, Action.REINDEX if reindex else Action.INDEX, checkpoint)
        stream = Stream(self, Action.REINDEX if reindex else Action.INDEX)

        with tempfile.NamedTemporaryFile(mode="wb", suffix=".npy") as buffer:
            # Load documents into database and transform to vectors
            ids, dimensions, embeddings = transform(stream(documents), buffer)
            if embeddings is not None:
                # Build LSA model (if enabled). Remove principal components from embeddings.
                if self.config.get("pca"):
                    self.reducer = Reducer(embeddings, self.config["pca"])
                    self.reducer(embeddings)

                # Save index dimensions
                self.config["dimensions"] = dimensions

                # Create approximate nearest neighbor index
                self.ann = self.createann()

                # Add embeddings to the index
                self.ann.index(embeddings)

            # Save indexids-ids mapping for indexes with no database, except when this is a reindex
            if ids and not reindex and not self.database:
                self.ids = self.createids(ids)

        # Index scoring, if necessary
        # This must occur before graph index in order to be available to the graph
        if self.issparse():
            self.scoring.index()

        # Index subindexes, if necessary
        if self.indexes:
            self.indexes.index()

        # Index graph, if necessary
        if self.graph:
            self.graph.index(Search(self, indexonly=True), Ids(self), self.batchsimilarity)

    def upsert(self, documents, checkpoint=None):
        """
        Runs an embeddings upsert operation. If the index exists, new data is
        appended to the index, existing data is updated. If the index doesn't exist,
        this method runs a standard index operation.

        Args:
            documents: iterable of (id, data, tags), (id, data) or data
            checkpoint: optional checkpoint directory, enables indexing restart
        """

        # Run standard insert if index doesn't exist or it has no records
        if not self.count():
            self.index(documents, checkpoint=checkpoint)
            return

        # Create transform and stream
        transform = Transform(self, Action.UPSERT, checkpoint=checkpoint)
        stream = Stream(self, Action.UPSERT)

        with tempfile.NamedTemporaryFile(mode="wb", suffix=".npy") as buffer:
            # Load documents into database and transform to vectors
            ids, _, embeddings = transform(stream(documents), buffer)
            if embeddings is not None:
                # Remove principal components from embeddings, if necessary
                if self.reducer:
                    self.reducer(embeddings)

                # Append embeddings to the index
                self.ann.append(embeddings)

            # Save indexids-ids mapping for indexes with no database
            if ids and not self.database:
                self.ids = self.createids(self.ids + ids)

        # Scoring upsert, if necessary
        # This must occur before graph upsert in order to be available to the graph
        if self.issparse():
            self.scoring.upsert()

        # Subindexes upsert, if necessary
        if self.indexes:
            self.indexes.upsert()

        # Graph upsert, if necessary
        if self.graph:
            self.graph.upsert(Search(self, indexonly=True), Ids(self), self.batchsimilarity)

    def delete(self, ids):
        """
        Deletes from an embeddings index. Returns list of ids deleted.

        Args:
            ids: list of ids to delete

        Returns:
            list of ids deleted
        """

        # List of internal indices for each candidate id to delete
        indices = []

        # List of deleted ids
        deletes = []

        if self.database:
            # Retrieve indexid-id mappings from database
            ids = self.database.ids(ids)

            # Parse out indices and ids to delete
            indices = [i for i, _ in ids]
            deletes = sorted(set(uid for _, uid in ids))

            # Delete ids from database
            self.database.delete(deletes)
        elif self.ann or self.scoring:
            # Find existing ids
            for uid in ids:
                indices.extend([index for index, value in enumerate(self.ids) if uid == value])

            # Clear embeddings ids
            for index in indices:
                deletes.append(self.ids[index])
                self.ids[index] = None

        # Delete indices for all indexes and data stores
        if indices:
            # Delete ids from ann
            if self.isdense():
                self.ann.delete(indices)

            # Delete ids from scoring
            if self.issparse():
                self.scoring.delete(indices)

            # Delete ids from subindexes
            if self.indexes:
                self.indexes.delete(indices)

            # Delete ids from graph
            if self.graph:
                self.graph.delete(indices)

        return deletes

    def reindex(self, config=None, function=None, **kwargs):
        """
        Recreates embeddings index using config. This method only works if document content storage is enabled.

        Args:
            config: new config
            function: optional function to prepare content for indexing
            kwargs: additional configuration as keyword args
        """

        if self.database:
            # Merge configuration into single dictionary
            config = {**config, **kwargs} if config and kwargs else config if config else kwargs

            # Keep content and objects parameters to ensure database is preserved
            config["content"] = self.config["content"]
            if "objects" in self.config:
                config["objects"] = self.config["objects"]

            # Reset configuration
            self.configure(config)

            # Reset function references
            if self.functions:
                self.functions.reset()

            # Reindex
            if function:
                self.index(function(self.database.reindex(self.config)), True)
            else:
                self.index(self.database.reindex(self.config), True)

    def transform(self, document, category=None, index=None):
        """
        Transforms document into an embeddings vector.

        Args:
            documents: iterable of (id, data, tags), (id, data) or data
            category: category for instruction-based embeddings
            index: index name, if applicable

        Returns:
            embeddings vector
        """

        return self.batchtransform([document], category, index)[0]

    def batchtransform(self, documents, category=None, index=None):
        """
        Transforms documents into embeddings vectors.

        Args:
            documents: iterable of (id, data, tags), (id, data) or data
            category: category for instruction-based embeddings
            index: index name, if applicable

        Returns:
            embeddings vectors
        """

        # Initialize default parameters, if necessary
        self.defaults()

        # Get vector model
        model = self.findmodel(index)

        # Convert documents into embeddings
        embeddings = model.batchtransform(Stream(self)(documents), category)

        # Reduce the dimensionality of the embeddings. Scale the embeddings using this
        # model to reduce the noise of common but less relevant terms.
        if self.reducer:
            self.reducer(embeddings)

        return embeddings

    def count(self):
        """
        Total number of elements in this embeddings index.

        Returns:
            number of elements in this embeddings index
        """

        if self.ann:
            return self.ann.count()
        if self.scoring:
            return self.scoring.count()
        if self.database:
            return self.database.count()
        if self.ids:
            return len([uid for uid in self.ids if uid is not None])

        # Default to 0 when no suitable method found
        return 0

    def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input query. This method runs an index search, index + database search
        or a graph search, depending on the embeddings configuration and query.

        Args:
            query: input query
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: dict of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of (id, score) for index search
            list of dict for an index + database search
            graph when graph is set to True
        """

        results = self.batchsearch([query], limit, weights, index, [parameters], graph)
        return results[0] if results else results

    def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):
        """
        Finds documents most similar to the input query. This method runs an index search, index + database search
        or a graph search, depending on the embeddings configuration and query.

        Args:
            queries: input queries
            limit: maximum results
            weights: hybrid score weights, if applicable
            index: index name, if applicable
            parameters: list of dicts of named parameters to bind to placeholders
            graph: return graph results if True

        Returns:
            list of (id, score) per query for index search
            list of dict per query for an index + database search
            list of graph per query when graph is set to True
        """

        # Determine if graphs should be returned
        graph = graph if self.graph else False

        # Execute search
        results = Search(self, indexids=graph)(queries, limit, weights, index, parameters)

        # Create subgraphs using results, if necessary
        return [self.graph.filter(x) if isinstance(x, list) else x for x in results] if graph else results

    def similarity(self, query, data):
        """
        Computes the similarity between query and list of data. Returns a list of
        (id, score) sorted by highest score, where id is the index in data.

        Args:
            query: input query
            data: list of data

        Returns:
            list of (id, score)
        """

        return self.batchsimilarity([query], data)[0]

    def batchsimilarity(self, queries, data):
        """
        Computes the similarity between list of queries and list of data. Returns a list
        of (id, score) sorted by highest score per query, where id is the index in data.

        Args:
            queries: input queries
            data: list of data

        Returns:
            list of (id, score) per query
        """

        # Convert queries to embedding vectors
        queries = self.batchtransform(((None, query, None) for query in queries), "query")
        data = self.batchtransform(((None, row, None) for row in data), "data")

        # Get vector model
        model = self.findmodel()

        # Dot product on normalized vectors is equal to cosine similarity
        scores = model.dot(queries, data)

        # Add index and sort desc based on score
        return [sorted(enumerate(score), key=lambda x: x[1], reverse=True) for score in scores]

    def explain(self, query, texts=None, limit=None):
        """
        Explains the importance of each input token in text for a query. This method requires either content to be enabled
        or texts to be provided.

        Args:
            query: input query
            texts: optional list of (text|list of tokens), otherwise runs search query
            limit: optional limit if texts is None

        Returns:
            list of dict per input text where a higher token scores represents higher importance relative to the query
        """

        results = self.batchexplain([query], texts, limit)
        return results[0] if results else results

    def batchexplain(self, queries, texts=None, limit=None):
        """
        Explains the importance of each input token in text for a list of queries. This method requires either content to be enabled
        or texts to be provided.

        Args:
            queries: input queries
            texts: optional list of (text|list of tokens), otherwise runs search queries
            limit: optional limit if texts is None

        Returns:
            list of dict per input text per query where a higher token scores represents higher importance relative to the query
        """

        return Explain(self)(queries, texts, limit)

    def terms(self, query):
        """
        Extracts keyword terms from a query.

        Args:
            query: input query

        Returns:
            query reduced down to keyword terms
        """

        return self.batchterms([query])[0]

    def batchterms(self, queries):
        """
        Extracts keyword terms from a list of queries.

        Args:
            queries: list of queries

        Returns:
            list of queries reduced down to keyword term strings
        """

        return Terms(self)(queries)

    def exists(self, path=None, cloud=None, **kwargs):
        """
        Checks if an index exists at path.

        Args:
            path: input path
            cloud: cloud storage configuration
            kwargs: additional configuration as keyword args

        Returns:
            True if index exists, False otherwise
        """

        # Check if this exists in a cloud instance
        cloud = self.createcloud(cloud=cloud, **kwargs)
        if cloud:
            return cloud.exists(path)

        # Check if this is an archive file and exists
        path, apath = self.checkarchive(path)
        if apath:
            return os.path.exists(apath)

        # Return true if path has a config.json or config file with an offset set
        return path and (os.path.exists(f"{path}/config.json") or os.path.exists(f"{path}/config")) and "offset" in Configuration().load(path)

    def load(self, path=None, cloud=None, config=None, **kwargs):
        """
        Loads an existing index from path.

        Args:
            path: input path
            cloud: cloud storage configuration
            config: configuration overrides
            kwargs: additional configuration as keyword args

        Returns:
            Embeddings
        """

        # Load from cloud, if configured
        cloud = self.createcloud(cloud=cloud, **kwargs)
        if cloud:
            path = cloud.load(path)

        # Check if this is an archive file and extract
        path, apath = self.checkarchive(path)
        if apath:
            self.archive.load(apath)

        # Load index configuration
        self.config = Configuration().load(path)

        # Apply config overrides
        self.config = {**self.config, **config} if config else self.config

        # Approximate nearest neighbor index - stores dense vectors
        self.ann = self.createann()
        if self.ann:
            self.ann.load(f"{path}/embeddings")

        # Dimensionality reduction model - word vectors only
        if self.config.get("pca"):
            self.reducer = Reducer()
            self.reducer.load(f"{path}/lsa")

        # Index ids when content is disabled
        self.ids = self.createids()
        if self.ids:
            self.ids.load(f"{path}/ids")

        # Document database - stores document content
        self.database = self.createdatabase()
        if self.database:
            self.database.load(f"{path}/documents")

        # Sparse vectors - stores term sparse arrays
        self.scoring = self.createscoring()
        if self.scoring:
            self.scoring.load(f"{path}/scoring")

        # Subindexes
        self.indexes = self.createindexes()
        if self.indexes:
            self.indexes.load(f"{path}/indexes")

        # Graph network - stores relationships
        self.graph = self.creategraph()
        if self.graph:
            self.graph.load(f"{path}/graph")

        # Dense vectors - transforms data to embeddings vectors
        self.model = self.loadvectors()

        # Query model
        self.query = self.loadquery()

        return self

    def save(self, path, cloud=None, **kwargs):
        """
        Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip.
        In those cases, the index is stored as a compressed file.

        Args:
            path: output path
            cloud: cloud storage configuration
            kwargs: additional configuration as keyword args
        """

        if self.config:
            # Check if this is an archive file
            path, apath = self.checkarchive(path)

            # Create output directory, if necessary
            os.makedirs(path, exist_ok=True)

            # Save index configuration
            Configuration().save(self.config, path)

            # Save approximate nearest neighbor index
            if self.ann:
                self.ann.save(f"{path}/embeddings")

            # Save dimensionality reduction model (word vectors only)
            if self.reducer:
                self.reducer.save(f"{path}/lsa")

            # Save index ids
            if self.ids:
                self.ids.save(f"{path}/ids")

            # Save document database
            if self.database:
                self.database.save(f"{path}/documents")

            # Save scoring index
            if self.scoring:
                self.scoring.save(f"{path}/scoring")

            # Save subindexes
            if self.indexes:
                self.indexes.save(f"{path}/indexes")

            # Save graph
            if self.graph:
                self.graph.save(f"{path}/graph")

            # If this is an archive, save it
            if apath:
                self.archive.save(apath)

            # Save to cloud, if configured
            cloud = self.createcloud(cloud=cloud, **kwargs)
            if cloud:
                cloud.save(apath if apath else path)

    def close(self):
        """
        Closes this embeddings index and frees all resources.
        """

        self.config, self.archive = None, None
        self.reducer, self.query = None, None
        self.ids = None

        # Close ANN
        if self.ann:
            self.ann.close()
            self.ann = None

        # Close database
        if self.database:
            self.database.close()
            self.database, self.functions = None, None

        # Close scoring
        if self.scoring:
            self.scoring.close()
            self.scoring = None

        # Close graph
        if self.graph:
            self.graph.close()
            self.graph = None

        # Close indexes
        if self.indexes:
            self.indexes.close()
            self.indexes = None

        # Close vectors model
        if self.model:
            self.model.close()
            self.model = None

        self.models = None

    def info(self):
        """
        Prints the current embeddings index configuration.
        """

        if self.config:
            # Print configuration
            print(json.dumps(self.config, sort_keys=True, default=str, indent=2))

    def issparse(self):
        """
        Checks if this instance has an associated sparse keyword or sparse vectors scoring index.

        Returns:
            True if scoring has an associated sparse keyword/vector index, False otherwise
        """

        return self.scoring and self.scoring.issparse()

    def isdense(self):
        """
        Checks if this instance has an associated ANN instance.

        Returns:
            True if this instance has an associated ANN, False otherwise
        """

        return self.ann is not None

    def isweighted(self):
        """
        Checks if this instance has an associated scoring instance with term weighting enabled.

        Returns:
            True if term weighting is enabled, False otherwise
        """

        return self.scoring and self.scoring.isweighted()

    def findmodel(self, index=None):
        """
        Finds the primary vector model used by this instance.

        Returns:
            Vectors
        """

        return (
            self.indexes.findmodel(index)
            if index and self.indexes
            else (
                self.model
                if self.model
                else self.scoring.findmodel() if self.scoring and self.scoring.findmodel() else self.indexes.findmodel() if self.indexes else None
            )
        )

    def configure(self, config):
        """
        Sets the configuration for this embeddings index and loads config-driven models.

        Args:
            config: embeddings configuration
        """

        # Configuration
        self.config = config

        # Dimensionality reduction model
        self.reducer = None

        # Create scoring instance for word vectors term weighting
        scoring = self.config.get("scoring") if self.config else None
        self.scoring = self.createscoring() if scoring and not self.hassparse() else None

        # Dense vectors - transforms data to embeddings vectors
        self.model = self.loadvectors() if self.config else None

        # Query model
        self.query = self.loadquery() if self.config else None

    def initindex(self, reindex):
        """
        Initialize new index.

        Args:
            reindex: if this is a reindex operation in which case database creation is skipped, defaults to False
        """

        # Initialize default parameters, if necessary
        self.defaults()

        # Initialize index ids, only created when content is disabled
        self.ids = None

        # Create document database, if necessary
        if not reindex:
            self.database = self.createdatabase()

            # Reset archive since this is a new index
            self.archive = None

        # Close existing ANN, if necessary
        if self.ann:
            self.ann.close()

        # Initialize ANN, will be created after index transformations complete
        self.ann = None

        # Create scoring only if the scoring config is for a sparse index
        if self.hassparse():
            self.scoring = self.createscoring()

        # Create subindexes, if necessary
        self.indexes = self.createindexes()

        # Create graph, if necessary
        self.graph = self.creategraph()

    def defaults(self):
        """
        Apply default parameters to current configuration.

        Returns:
            configuration with default parameters set
        """

        self.config = self.config if self.config else {}

        # Expand sparse index shortcuts
        if not self.config.get("scoring") and any(self.config.get(key) for key in ["keyword", "sparse", "hybrid"]):
            self.defaultsparse()

        # Expand graph shortcuts
        if self.config.get("graph") is True:
            self.config["graph"] = {}

        # Check if default model should be loaded
        if not self.model and (self.defaultallowed() or self.config.get("dense")):
            self.config["path"] = "sentence-transformers/all-MiniLM-L6-v2"

            # Load dense vectors model
            self.model = self.loadvectors()

    def defaultsparse(self):
        """
        Logic to derive default sparse index configuration.
        """

        # Check for keyword and hybrid parameters
        method = None
        for x in ["keyword", "hybrid"]:
            value = self.config.get(x)
            if value:
                method = value if isinstance(value, str) else "bm25"

                # Enable dense index when hybrid enabled
                if x == "hybrid":
                    self.config["dense"] = True

        sparse = self.config.get("sparse", {})
        if sparse or method == "sparse":
            # Sparse vector configuration
            sparse = {"path": self.config.get("sparse")} if isinstance(sparse, str) else {} if isinstance(sparse, bool) else sparse
            sparse["path"] = sparse.get("path", "opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini")

            # Merge in sparse parameters
            self.config["scoring"] = {**{"method": "sparse"}, **sparse}

        elif method:
            # Sparse keyword configuration
            self.config["scoring"] = {"method": method, "terms": True, "normalize": True}

    def defaultallowed(self):
        """
        Tests if this embeddings instance can use a default model if not otherwise provided.

        Returns:
            True if a default model is allowed, False otherwise
        """

        params = [("keyword", False), ("sparse", False), ("defaults", True)]
        return all(self.config.get(key, default) == default for key, default in params)

    def loadvectors(self):
        """
        Loads a vector model set in config.

        Returns:
            vector model
        """

        # Create model cache if subindexes are enabled
        if "indexes" in self.config and self.models is None:
            self.models = {}

        # Support path via dense parameter
        dense = self.config.get("dense")
        if not self.config.get("path") and dense and isinstance(dense, str):
            self.config["path"] = dense

        # Load vector model
        return VectorsFactory.create(self.config, self.scoring, self.models)

    def loadquery(self):
        """
        Loads a query model set in config.

        Returns:
            query model
        """

        if "query" in self.config:
            return Query(**self.config["query"])

        return None

    def checkarchive(self, path):
        """
        Checks if path is an archive file.

        Args:
            path: path to check

        Returns:
            (working directory, current path) if this is an archive, original path otherwise
        """

        # Create archive instance, if necessary
        self.archive = ArchiveFactory.create()

        # Check if path is an archive file
        if self.archive.isarchive(path):
            # Return temporary archive working directory and original path
            return self.archive.path(), path

        return path, None

    def createcloud(self, **cloud):
        """
        Creates a cloud instance from config.

        Args:
            cloud: cloud configuration
        """

        # Merge keyword args and keys under the cloud parameter
        config = cloud
        if "cloud" in config and config["cloud"]:
            config.update(config.pop("cloud"))

        # Create cloud instance from config and return
        return CloudFactory.create(config) if config else None

    def createann(self):
        """
        Creates an ANN from config.

        Returns:
            new ANN, if enabled in config
        """

        # Free existing resources
        if self.ann:
            self.ann.close()

        return ANNFactory.create(self.config) if self.config.get("path") or self.defaultallowed() else None

    def createdatabase(self):
        """
        Creates a database from config. This method will also close any existing database connection.

        Returns:
            new database, if enabled in config
        """

        # Free existing resources
        if self.database:
            self.database.close()

        config = self.config.copy()

        # Create references to callable functions
        self.functions = Functions(self) if "functions" in config else None
        if self.functions:
            config["functions"] = self.functions(config)

        # Create database from config and return
        return DatabaseFactory.create(config)

    def creategraph(self):
        """
        Creates a graph from config.

        Returns:
            new graph, if enabled in config
        """

        # Free existing resources
        if self.graph:
            self.graph.close()

        if "graph" in self.config:
            # Get or create graph configuration
            config = self.config["graph"] if "graph" in self.config else {}

            # Create configuration with custom columns, if necessary
            config = self.columns(config)
            return GraphFactory.create(config)

        return None

    def createids(self, ids=None):
        """
        Creates indexids when content is disabled.

        Args:
            ids: optional ids to add

        Returns:
            new indexids, if content disabled
        """

        # Load index ids when content is disabled
        return IndexIds(self, ids) if not self.config.get("content") else None

    def createindexes(self):
        """
        Creates subindexes from config.

        Returns:
            list of subindexes
        """

        # Free existing resources
        if self.indexes:
            self.indexes.close()

        # Load subindexes
        if "indexes" in self.config:
            indexes = {}
            for index, config in self.config["indexes"].items():
                # Create index with shared model cache
                indexes[index] = Embeddings(config, models=self.models)

            # Wrap as Indexes object
            return Indexes(self, indexes)

        return None

    def createscoring(self):
        """
        Creates a scoring from config.

        Returns:
            new scoring, if enabled in config
        """

        # Free existing resources
        if self.scoring:
            self.scoring.close()

        if "scoring" in self.config:
            # Expand scoring to a dictionary, if necessary
            config = self.config["scoring"]
            config = config if isinstance(config, dict) else {"method": config}

            # Create configuration with custom columns, if necessary
            config = self.columns(config)
            return ScoringFactory.create(config, self.models)

        return None

    def hassparse(self):
        """
        Checks is this embeddings database has an associated sparse index.

        Returns:
            True if this embeddings has an associated scoring index
        """

        # Create scoring only if scoring is a sparse keyword/vector index
        return ScoringFactory.issparse(self.config.get("scoring"))

    def columns(self, config):
        """
        Adds custom text/object column information if it's provided.

        Args:
            config: input configuration

        Returns:
            config with column information added
        """

        # Add text/object columns if custom
        if "columns" in self.config:
            # Work on copy of configuration
            config = config.copy()

            # Copy columns to config
            config["columns"] = self.config["columns"]

        return config



================================================
FILE: src/python/txtai/embeddings/index/__init__.py
================================================
"""
Index imports
"""

from .action import Action
from .autoid import AutoId
from .configuration import Configuration
from .documents import Documents
from .functions import Functions
from .indexes import Indexes
from .indexids import IndexIds
from .reducer import Reducer
from .stream import Stream
from .transform import Transform



================================================
FILE: src/python/txtai/embeddings/index/action.py
================================================
"""
Action module
"""

from enum import Enum


class Action(Enum):
    """
    Index action types
    """

    INDEX = 1
    UPSERT = 2
    REINDEX = 3



================================================
FILE: src/python/txtai/embeddings/index/autoid.py
================================================
"""
AutoId module
"""

import inspect
import uuid


class AutoId:
    """
    Generates unique ids.
    """

    def __init__(self, method=None):
        """
        Creates a unique id generator.

        Args:
            method: generation method - supports int sequence (default) or UUID function
        """

        # Initialize variables
        self.method, self.function, self.value = None, None, None

        # Set id generation method
        if not method or isinstance(method, int):
            # Incrementing sequence (default)
            self.method = self.sequence
            self.value = method if method else 0
        else:
            # UUID generation function
            self.method = self.uuid
            self.function = getattr(uuid, method)

        # Check if signature takes a namespace argument (deterministic)
        args = inspect.getfullargspec(self.function).args if self.function else []
        self.deterministic = "namespace" in args

    def __call__(self, data=None):
        """
        Generates a unique id.

        Args:
            data: optional data to use for deterministic algorithms (i.e. uuid3, uuid5)

        Returns:
            unique id
        """

        return self.method(data)

    # pylint: disable=W0613
    def sequence(self, data):
        """
        Gets and increments sequence.

        Args:
            data: not used

        Returns:
            current sequence value
        """

        # Get and increment sequence
        value = self.value
        self.value += 1

        return value

    def uuid(self, data):
        """
        Generates a UUID and return as a string.

        Args:
            data: used with determistic algorithms (uuid3, uuid5)

        Returns:
            UUID string
        """

        uid = self.function(uuid.NAMESPACE_DNS, str(data)) if self.deterministic else self.function()
        return str(uid)

    def current(self):
        """
        Get the current sequence value. Only applicable for sequence ids, will be None for UUID methods.

        Returns:
            current sequence value
        """

        return self.value



================================================
FILE: src/python/txtai/embeddings/index/configuration.py
================================================
"""
Configuration module
"""

import json
import os

from ...serialize import SerializeFactory


class Configuration:
    """
    Loads and saves index configuration.
    """

    def load(self, path):
        """
        Loads index configuration. This method supports both config.json and config pickle files.

        Args:
            path: path to directory

        Returns:
            dict
        """

        # Configuration
        config = None

        # Determine if config is json or pickle
        jsonconfig = os.path.exists(f"{path}/config.json")

        # Set config file name
        name = "config.json" if jsonconfig else "config"

        # Load configuration
        with open(f"{path}/{name}", "r" if jsonconfig else "rb", encoding="utf-8" if jsonconfig else None) as handle:
            # Load JSON, also backwards-compatible with pickle configuration
            config = json.load(handle) if jsonconfig else SerializeFactory.create("pickle").loadstream(handle)

        # Add format parameter
        config["format"] = "json" if jsonconfig else "pickle"

        return config

    def save(self, config, path):
        """
        Saves index configuration. This method defaults to JSON and falls back to pickle.

        Args:
            config: configuration to save
            path: path to directory

        Returns:
            dict
        """

        # Default to JSON config
        jsonconfig = config.get("format", "json") == "json"

        # Set config file name
        name = "config.json" if jsonconfig else "config"

        # Write configuration
        with open(f"{path}/{name}", "w" if jsonconfig else "wb", encoding="utf-8" if jsonconfig else None) as handle:
            if jsonconfig:
                # Write config as JSON
                json.dump(config, handle, default=str, indent=2)
            else:
                # Backwards compatible method to save pickle configuration
                SerializeFactory.create("pickle").savestream(config, handle)



================================================
FILE: src/python/txtai/embeddings/index/documents.py
================================================
"""
Documents module
"""

import os
import tempfile

from ...serialize import SerializeFactory


class Documents:
    """
    Streams documents to temporary storage. Allows queuing large volumes of content for later indexing.
    """

    def __init__(self):
        """
        Creates a new documents stream.
        """

        self.documents = None
        self.batch = 0
        self.size = 0

        # Pickle serialization - local temporary data
        self.serializer = SerializeFactory.create("pickle", allowpickle=True)

    def __len__(self):
        """
        Returns total number of queued documents.
        """

        return self.size

    def __iter__(self):
        """
        Streams all queued documents.
        """

        # Close streaming file
        self.documents.close()

        # Open stream file
        with open(self.documents.name, "rb") as queue:
            # Read each batch
            for _ in range(self.batch):
                documents = self.serializer.loadstream(queue)

                # Yield each document
                yield from documents

    def add(self, documents):
        """
        Adds a batch of documents for indexing.

        Args:
            documents: list of (id, data, tag) tuples

        Returns:
            documents
        """

        # Create documents file if not already open
        # pylint: disable=R1732
        if not self.documents:
            self.documents = tempfile.NamedTemporaryFile(mode="wb", suffix=".docs", delete=False)

        # Add batch
        self.serializer.savestream(documents, self.documents)
        self.batch += 1
        self.size += len(documents)

        return documents

    def close(self):
        """
        Closes and resets this instance. New sets of documents can be added with additional calls to add.
        """

        # Cleanup stream file
        os.remove(self.documents.name)

        # Reset document parameters
        self.documents = None
        self.batch = 0
        self.size = 0



================================================
FILE: src/python/txtai/embeddings/index/functions.py
================================================
"""
Functions module
"""

from types import FunctionType, MethodType


class Functions:
    """
    Resolves function configuration to function references.
    """

    def __init__(self, embeddings):
        """
        Creates a new function resolver.

        Args:
            embeddings: embeddings instance
        """

        self.embeddings = embeddings

        # Handle to all reference objects
        self.references = None

    def __call__(self, config):
        """
        Resolves a list of functions to function references.

        Args:
            config: configuration

        Returns:
            list of function references
        """

        # Initialize stored references array
        self.references = []

        # Resolve callable functions
        functions = []
        for fn in config["functions"]:
            if isinstance(fn, dict):
                fn = fn.copy()
                fn["function"] = self.function(fn["function"])
            else:
                fn = self.function(fn)
            functions.append(fn)

        return functions

    def reset(self):
        """
        Clears all resolved references.
        """

        if self.references:
            for reference in self.references:
                reference.reset()

    def function(self, function):
        """
        Resolves function configuration. If function is a string, it's split on '.' and each part
        is separately resolved to an object, attribute or function. Each part is resolved upon the
        first invocation of the function. Otherwise, the input is returned.

        Args:
            function: function configuration

        Returns:
            function reference
        """

        if isinstance(function, str):
            parts = function.split(".")

            if hasattr(self.embeddings, parts[0]):
                m = Reference(self.embeddings, parts[0])
                self.references.append(m)
            else:
                module = ".".join(parts[:-1])
                m = __import__(module)

            for comp in parts[1:]:
                m = Reference(m, comp)
                self.references.append(m)

            return m

        return function


class Reference:
    """
    Stores a reference to an object attribute. This attribute is resolved by invoking the __call__ method.
    This allows for functions to be independent of the initialization order of an embeddings instance.
    """

    def __init__(self, obj, attribute):
        """
        Create a new reference.

        Args:
            obj: object handle
            attribute: attribute name
        """

        # Object handle and attribute
        self.obj = obj
        self.attribute = attribute

        # Keep a handle to the original inputs
        self.inputs = (obj, attribute)

        # True if the object and attribute have been resolved
        self.resolved = False

        # True if the attribute is a function
        self.function = None

    def __call__(self, *args):
        """
        Resolves an object attribute reference. If the attribute is a function, the function is executed.
        Otherwise, the object attribute value is returned.

        Args:
            args: list of function arguments to the object attribute, when attribute is a function

        Returns:
            object attribute function result or object attribute value
        """

        # Resolve nested function arguments, if necessary
        if not self.resolved:
            self.obj = self.obj() if isinstance(self.obj, Reference) else self.obj
            self.attribute = self.attribute() if isinstance(self.attribute, Reference) else self.attribute
            self.resolved = True

        # Lookup attribute
        attribute = getattr(self.obj, self.attribute)

        # Determine if attribute is a function
        if self.function is None:
            self.function = isinstance(attribute, (FunctionType, MethodType)) or (hasattr(attribute, "__call__") and args)

        # If attribute is a function, execute and return, otherwise return attribute
        return attribute(*args) if self.function else attribute

    def reset(self):
        """
        Clears resolved references.
        """

        self.obj, self.attribute = self.inputs
        self.resolved = False



================================================
FILE: src/python/txtai/embeddings/index/indexes.py
================================================
"""
Indexes module
"""

import os

from .documents import Documents


class Indexes:
    """
    Manages a collection of subindexes for an embeddings instance.
    """

    def __init__(self, embeddings, indexes):
        """
        Creates a new indexes instance.

        Args:
            embeddings: embeddings instance
            indexes: dict of subindexes to add
        """

        self.embeddings = embeddings
        self.indexes = indexes

        self.documents = None
        self.checkpoint = None

        # Transform columns
        columns = embeddings.config.get("columns", {})
        self.text = columns.get("text", "text")
        self.object = columns.get("object", "object")

        # Check if top-level indexing is enabled for this embeddings instance
        self.indexing = embeddings.model or embeddings.scoring

    def __contains__(self, name):
        """
        Returns True if name is in this instance, False otherwise.

        Returns:
            True if name is in this instance, False otherwise
        """

        return name in self.indexes

    def __getitem__(self, name):
        """
        Looks up an index by name.

        Args:
            name: index name

        Returns:
            index
        """

        return self.indexes[name]

    def __getattr__(self, name):
        """
        Looks up an index by attribute name.

        Args:
            name: index name

        Returns:
            index
        """

        try:
            return self.indexes[name]
        except Exception as e:
            raise AttributeError(e) from e

    def default(self):
        """
        Gets the default/first index.

        Returns:
            default index
        """

        return list(self.indexes.keys())[0]

    def findmodel(self, index=None):
        """
        Finds a vector model. If index is empty, the first vector model is returned.

        Args:
            index: index name to match

        Returns:
            Vectors
        """

        # Find vector model
        matches = [self.indexes[index].findmodel()] if index else [index.findmodel() for index in self.indexes.values() if index.findmodel()]
        return matches[0] if matches else None

    def insert(self, documents, index=None, checkpoint=None):
        """
        Inserts a batch of documents into each subindex.

        Args:
            documents: list of (id, data, tags)
            index: indexid offset
            checkpoint: optional checkpoint directory, enables indexing restart
        """

        if not self.documents:
            self.documents = Documents()
            self.checkpoint = checkpoint

        # Create batch containing documents added to parent index
        batch = []
        for _, document, _ in documents:
            # Add to documents collection if text or object field is set
            parent = document
            if isinstance(parent, dict):
                parent = parent.get(self.text, document.get(self.object))

            # Add if field is available or top-level indexing is disabled
            if parent is not None or not self.indexing:
                batch.append((index, document, None))
                index += 1

        # Add filtered documents batch
        self.documents.add(batch)

    def delete(self, ids):
        """
        Deletes ids from each subindex.

        Args:
            ids: list of ids to delete
        """

        for index in self.indexes.values():
            index.delete(ids)

    def index(self):
        """
        Builds each subindex.
        """

        for name, index in self.indexes.items():
            index.index(self.documents, checkpoint=f"{self.checkpoint}/{name}" if self.checkpoint else None)

        # Reset document stream
        self.documents.close()
        self.documents = None
        self.checkpoint = None

    def upsert(self):
        """
        Runs upsert for each subindex.
        """

        for index in self.indexes.values():
            index.upsert(self.documents)

        # Reset document stream
        self.documents.close()
        self.documents = None

    def load(self, path):
        """
        Loads each subindex from path.

        Args:
            path: directory path to load subindexes
        """

        for name, index in self.indexes.items():
            # Load subindex if it exists, subindexes aren't required to have data
            directory = os.path.join(path, name)
            if index.exists(directory):
                index.load(directory)

    def save(self, path):
        """
        Saves each subindex to path.

        Args:
            path: directory path to save subindexes
        """

        for name, index in self.indexes.items():
            index.save(os.path.join(path, name))

    def close(self):
        """
        Close and free resources used by this instance.
        """

        for index in self.indexes.values():
            index.close()



================================================
FILE: src/python/txtai/embeddings/index/indexids.py
================================================
"""
IndexIds module
"""

from ...serialize import Serializer


class IndexIds:
    """
    Stores index ids when content is disabled.
    """

    def __init__(self, embeddings, ids=None):
        """
        Creates an IndexIds instance.

        Args:
            embeddings: embeddings instance
            ids: ids to store
        """

        self.config = embeddings.config
        self.ids = ids

    def __iter__(self):
        yield from self.ids

    def __getitem__(self, index):
        return self.ids[index]

    def __setitem__(self, index, value):
        self.ids[index] = value

    def __add__(self, ids):
        return self.ids + ids

    def load(self, path):
        """
        Loads IndexIds from path.

        Args:
            path: path to load
        """

        if "ids" in self.config:
            # Legacy ids format
            self.ids = self.config.pop("ids")
        else:
            # Standard ids format
            self.ids = Serializer.load(path)

    def save(self, path):
        """
        Saves IndexIds to path.

        Args:
            path: path to save
        """

        Serializer.save(self.ids, path)



================================================
FILE: src/python/txtai/embeddings/index/reducer.py
================================================
"""
Reducer module
"""

from zipfile import BadZipFile

# Conditionally import dimensionality reduction libraries as they aren't installed by default
try:
    import skops.io as sio

    from sklearn.decomposition import TruncatedSVD

    REDUCER = True
except ImportError:
    REDUCER = False

from ...serialize import SerializeFactory


class Reducer:
    """
    LSA dimensionality reduction model
    """

    def __init__(self, embeddings=None, components=None):
        """
        Creates a dimensionality reduction model.

        Args:
            embeddings: input embeddings matrix
            components: number of model components
        """

        if not REDUCER:
            raise ImportError('Dimensionality reduction is not available - install "vectors" extra to enable')

        self.model = self.build(embeddings, components) if embeddings is not None and components else None

    def __call__(self, embeddings):
        """
        Applies a dimensionality reduction model to embeddings, removed the top n principal components. Operation applied
        directly on array.

        Args:
            embeddings: input embeddings matrix
        """

        pc = self.model.components_
        factor = embeddings.dot(pc.transpose())

        # Apply LSA model
        # Calculation is different if n_components = 1
        if pc.shape[0] == 1:
            embeddings -= factor * pc
        elif len(embeddings.shape) > 1:
            # Apply model on a row-wise basis to limit memory usage
            for x in range(embeddings.shape[0]):
                embeddings[x] -= factor[x].dot(pc)
        else:
            # Single embedding
            embeddings -= factor.dot(pc)

    def build(self, embeddings, components):
        """
        Builds a LSA model. This model is used to remove the principal component within embeddings. This helps to
        smooth out noisy embeddings (common words with less value).

        Args:
            embeddings: input embeddings matrix
            components: number of model components

        Returns:
            LSA model
        """

        model = TruncatedSVD(n_components=components, random_state=0)
        model.fit(embeddings)

        return model

    def load(self, path):
        """
        Loads a Reducer object from path.

        Args:
            path: directory path to load model
        """

        # Dimensionality reduction
        try:
            self.model = sio.load(path)
        except (BadZipFile, KeyError):
            # Backwards compatible support for pickled models
            self.model = SerializeFactory.create("pickle").load(path)

    def save(self, path):
        """
        Saves a Reducer object to path.

        Args:
            path: directory path to save model
        """

        sio.dump(self.model, path)



================================================
FILE: src/python/txtai/embeddings/index/stream.py
================================================
"""
Stream module
"""

from .autoid import AutoId
from .transform import Action


class Stream:
    """
    Yields input document as standard (id, data, tags) tuples.
    """

    def __init__(self, embeddings, action=None):
        """
        Create a new stream.

        Args:
            embeddings: embeddings instance
            action: optional index action
        """

        self.embeddings = embeddings
        self.action = action

        # Alias embeddings attributes
        self.config = embeddings.config

        # Get config parameters
        self.offset = self.config.get("offset", 0) if action == Action.UPSERT else 0
        autoid = self.config.get("autoid", self.offset)

        # Create autoid generator, reset int sequence if this isn't an UPSERT
        autoid = 0 if isinstance(autoid, int) and action != Action.UPSERT else autoid
        self.autoid = AutoId(autoid)

    def __call__(self, documents):
        """
        Yield (id, data, tags) tuples from a stream of documents.

        Args:
            documents: input documents
        """

        # Iterate over documents and yield standard (id, data, tag) tuples
        for document in documents:
            if isinstance(document, dict):
                # Create (id, data, tags) tuple from dictionary
                document = document.get("id"), document, document.get("tags")
            elif isinstance(document, tuple):
                # Create (id, data, tags) tuple
                document = document if len(document) >= 3 else (document[0], document[1], None)
            else:
                # Create (id, data, tags) tuple with empty fields
                document = None, document, None

            # Set autoid if the action is set
            if self.action and document[0] is None:
                document = (self.autoid(document[1]), document[1], document[2])

            # Yield (id, data, tags) tuple
            yield document

        # Save autoid sequence if used
        current = self.autoid.current()
        if self.action and current:
            self.config["autoid"] = current



================================================
FILE: src/python/txtai/embeddings/index/transform.py
================================================
"""
Transform module
"""

import numpy as np

from .action import Action


class Transform:
    """
    Executes a transform. Processes a stream of documents, loads batches into enabled data stores and vectorizes documents.
    """

    def __init__(self, embeddings, action, checkpoint=None):
        """
        Creates a new transform.

        Args:
            embeddings: embeddings instance
            action: index action
            checkpoint: optional checkpoint directory, enables indexing restart
        """

        self.embeddings = embeddings
        self.action = action
        self.checkpoint = checkpoint

        # Alias embeddings attributes
        self.config = embeddings.config
        self.delete = embeddings.delete
        self.model = embeddings.model
        self.database = embeddings.database
        self.graph = embeddings.graph
        self.indexes = embeddings.indexes
        self.scoring = embeddings.scoring if embeddings.issparse() else None

        # Get config parameters
        self.offset = embeddings.config.get("offset", 0) if action == Action.UPSERT else 0
        self.batch = embeddings.config.get("batch", 1024)

        # Scalar quantization
        quantize = embeddings.config.get("quantize")
        self.qbits = quantize if isinstance(quantize, int) and not isinstance(quantize, bool) else None

        # Transform columns
        columns = embeddings.config.get("columns", {})
        self.text = columns.get("text", "text")
        self.object = columns.get("object", "object")

        # Check if top-level indexing is enabled for this embeddings
        self.indexing = embeddings.model or embeddings.scoring

        # List of deleted ids with this action
        self.deletes = set()

    def __call__(self, documents, buffer):
        """
        Processes an iterable collection of documents, handles any iterable including generators.

        This method loads a stream of documents into enabled data stores and vectorizes documents into an embeddings array.

        Args:
            documents: iterable of (id, data, tags)
            buffer: file path used for memmap buffer

        Returns:
            (document ids, dimensions, embeddings)
        """

        # Return parameters
        ids, dimensions, embeddings = None, None, None

        if self.model:
            ids, dimensions, embeddings = self.vectors(documents, buffer)
        else:
            ids = self.ids(documents)

        return (ids, dimensions, embeddings)

    def vectors(self, documents, buffer):
        """
        Runs a vectors transform operation when dense indexing is enabled.

        Args:
            documents: iterable of (id, data, tags)
            buffer: file path used for memmap buffer

        Returns:
            (document ids, dimensions, embeddings)
        """

        # Determine dtype
        dtype = np.uint8 if self.qbits else np.float32

        # Transform documents into vectors
        return self.model.vectors(self.stream(documents), self.batch, self.checkpoint, buffer, dtype)

    def ids(self, documents):
        """
        Runs an ids transform operation when dense indexing is disabled.

        Args:
            documents: iterable of (id, data, tags)

        Returns:
            document ids
        """

        # Consume stream and build extract ids
        ids = []
        for uid, _, _ in self.stream(documents):
            ids.append(uid)

        # Save offset when dense indexing is disabled
        self.config["offset"] = self.offset

        return ids

    def stream(self, documents):
        """
        This method does two things:

        1. Filter and yield data to vectorize
        2. Batch and load original documents into enabled data stores (database, graph, scoring)

        Documents are yielded for vectorization if one of the following is True:
            - dict with a text or object field
            - not a dict

        Otherwise, documents are only batched and inserted into data stores

        Args:
            documents: iterable collection (id, data, tags)
        """

        # Batch and index offset. Index offset increments by count of documents streamed for vectorization
        batch, offset = [], 0

        # Iterate and process documents stream
        for document in documents:
            if isinstance(document[1], dict):
                # Set text field to uid when top-level indexing is disabled and text empty
                if not self.indexing and not document[1].get(self.text):
                    document[1][self.text] = str(document[0])

                if self.text in document[1]:
                    yield (document[0], document[1][self.text], document[2])
                    offset += 1
                elif self.object in document[1]:
                    yield (document[0], document[1][self.object], document[2])
                    offset += 1
            else:
                yield document
                offset += 1

            # Batch document
            batch.append(document)
            if len(batch) == self.batch:
                self.load(batch, offset)
                batch, offset = [], 0

        # Final batch
        if batch:
            self.load(batch, offset)

    def load(self, batch, offset):
        """
        Loads a document batch. This method deletes existing ids from an embeddings index and
        loads into enabled data stores (database, graph, scoring).

        Args:
            batch: list of (id, data, tags)
            offset: index offset for batch
        """

        # Delete from embeddings index first (which deletes from underlying indexes and datastores) if this is an upsert
        if self.action == Action.UPSERT:
            # Get list of ids not yet seen and deleted
            deletes = [uid for uid, _, _ in batch if uid not in self.deletes]
            if deletes:
                # Execute delete
                self.delete(deletes)

                # Save deleted ids as a delete must only occur once per action
                self.deletes.update(deletes)

        # Load batch into database except if this is a reindex
        if self.database and self.action != Action.REINDEX:
            self.database.insert(batch, self.offset)

        # Load batch into scoring
        if self.scoring:
            self.scoring.insert(batch, self.offset, self.checkpoint)

        # Load batch into subindex documents stream
        if self.indexes:
            self.indexes.insert(batch, self.offset, self.checkpoint)

        # Load batch into graph
        if self.graph:
            self.graph.insert(batch, self.offset)

        # Increment offset
        self.offset += offset



================================================
FILE: src/python/txtai/embeddings/search/__init__.py
================================================
"""
Search imports
"""

from .base import Search
from .errors import *
from .explain import Explain
from .ids import Ids
from .query import Query
from .scan import Scan
from .terms import Terms



================================================
FILE: src/python/txtai/embeddings/search/base.py
================================================
"""
Search module
"""

import logging

from .errors import IndexNotFoundError
from .scan import Scan

# Logging configuration
logger = logging.getLogger(__name__)


class Search:
    """
    Executes a batch search action. A search can be both index and/or database driven.
    """

    def __init__(self, embeddings, indexids=False, indexonly=False):
        """
        Creates a new search action.

        Args:
            embeddings: embeddings instance
            indexids: searches return indexids when True, otherwise run standard search
            indexonly: always runs an index search even when a database is available
        """

        self.embeddings = embeddings
        self.indexids = indexids or indexonly
        self.indexonly = indexonly

        # Alias embeddings attributes
        self.ann = embeddings.ann
        self.batchtransform = embeddings.batchtransform
        self.database = embeddings.database
        self.ids = embeddings.ids
        self.indexes = embeddings.indexes
        self.graph = embeddings.graph
        self.query = embeddings.query
        self.scoring = embeddings.scoring if embeddings.issparse() else None

    def __call__(self, queries, limit=None, weights=None, index=None, parameters=None):
        """
        Executes a batch search for queries. This method will run either an index search or an index + database search
        depending on if a database is available.

        Args:
            queries: list of queries
            limit: maximum results
            weights: hybrid score weights
            index: index name
            parameters: list of dicts of named parameters to bind to placeholders

        Returns:
            list of (id, score) per query for index search
            list of dict per query for an index + database search
            list of graph results for a graph index search
        """

        # Default input parameters
        limit = limit if limit else 3
        weights = weights if weights is not None else 0.5

        # Return empty results if there is no database and indexes
        if not self.ann and not self.scoring and not self.indexes and not self.database:
            return [[]] * len(queries)

        # Default index name if only subindexes set
        if not index and not self.ann and not self.scoring and self.indexes:
            index = self.indexes.default()

        # Graph search
        if self.graph and self.graph.isquery(queries):
            return self.graphsearch(queries, limit, weights, index)

        # Database search
        if not self.indexonly and self.database:
            return self.dbsearch(queries, limit, weights, index, parameters)

        # Default vector index query (sparse, dense or hybrid)
        return self.search(queries, limit, weights, index)

    def search(self, queries, limit, weights, index):
        """
        Executes an index search. When only a sparse index is enabled, this is a a keyword search. When only
        a dense index is enabled, this is an ann search. When both are enabled, this is a hybrid search.

        This method will also query subindexes, if available.

        Args:
            queries: list of queries
            limit: maximum results
            weights: hybrid score weights
            index: index name

        Returns:
            list of (id, score) per query
        """

        # Run against specified subindex
        if index:
            return self.subindex(queries, limit, weights, index)

        # Run against base indexes
        hybrid = self.ann and self.scoring
        dense = self.dense(queries, limit * 10 if hybrid else limit) if self.ann else None
        sparse = self.sparse(queries, limit * 10 if hybrid else limit) if self.scoring else None

        # Combine scores together
        if hybrid:
            # Create weights array if single number passed
            if isinstance(weights, (int, float)):
                weights = [weights, 1 - weights]

            # Create weighted scores
            results = []
            for vectors in zip(dense, sparse):
                uids = {}
                for v, scores in enumerate(vectors):
                    for r, (uid, score) in enumerate(scores if weights[v] > 0 else []):
                        # Initialize score
                        if uid not in uids:
                            uids[uid] = 0.0

                        # Create hybrid score
                        #  - Convex Combination when sparse scores are normalized
                        #  - Reciprocal Rank Fusion (RRF) when sparse scores aren't normalized
                        if self.scoring.isnormalized():
                            uids[uid] += score * weights[v]
                        else:
                            uids[uid] += (1.0 / (r + 1)) * weights[v]

                results.append(sorted(uids.items(), key=lambda x: x[1], reverse=True)[:limit])

            return results

        # Raise an error if when no indexes are available
        if not sparse and not dense:
            raise IndexNotFoundError("No indexes available")

        # Return single query results
        return dense if dense else sparse

    def subindex(self, queries, limit, weights, index):
        """
        Executes a subindex search.

        Args:
            queries: list of queries
            limit: maximum results
            weights: hybrid score weights
            index: index name

        Returns:
            list of (id, score) per query
        """

        # Check that index exists
        if not self.indexes or index not in self.indexes:
            raise IndexNotFoundError(f"Index '{index}' not found")

        # Run subindex search
        results = self.indexes[index].batchsearch(queries, limit, weights)
        return self.resolve(results)

    def dense(self, queries, limit):
        """
        Executes an dense vector search with an approximate nearest neighbor index.

        Args:
            queries: list of queries
            limit: maximum results

        Returns:
            list of (id, score) per query
        """

        # Convert queries to embedding vectors
        embeddings = self.batchtransform((None, query, None) for query in queries)

        # Search approximate nearest neighbor index
        results = self.ann.search(embeddings, limit)

        # Require scores to be greater than 0
        results = [[(i, score) for i, score in r if score > 0] for r in results]

        return self.resolve(results)

    def sparse(self, queries, limit):
        """
        Executes a sparse vector search with a sparse keyword or sparse vector index.

        Args:
            queries: list of queries
            limit: maximum results

        Returns:
            list of (id, score) per query
        """

        # Search sparse index
        results = self.scoring.batchsearch(queries, limit)

        # Require scores to be greater than 0
        results = [[(i, score) for i, score in r if score > 0] for r in results]

        return self.resolve(results)

    def resolve(self, results):
        """
        Resolves index ids. This is only executed when content is disabled.

        Args:
            results: results

        Returns:
            results with resolved ids
        """

        # Map indexids to ids if embeddings ids are available
        if not self.indexids and self.ids:
            return [[(self.ids[i], score) for i, score in r] for r in results]

        return results

    def dbsearch(self, queries, limit, weights, index, parameters):
        """
        Executes an index + database search.

        Args:
            queries: list of queries
            limit: maximum results
            weights: default hybrid score weights
            index: default index name
            parameters: list of dicts of named parameters to bind to placeholders

        Returns:
            list of dict per query
        """

        # Parse queries
        queries = self.parse(queries)

        # Override limit with query limit, if applicable
        limit = max(limit, self.limit(queries))

        # Bulk index scan
        scan = Scan(self.search, limit, weights, index)(queries, parameters)

        # Combine index search results with database search results
        results = []
        for x, query in enumerate(queries):
            # Run the database query, get matching bulk searches for current query
            result = self.database.search(
                query, [r for y, r in scan if x == y], limit, parameters[x] if parameters and parameters[x] else None, self.indexids
            )
            results.append(result)

        return results

    def parse(self, queries):
        """
        Parses a list of database queries.

        Args:
            queries: list of queries

        Returns:
            parsed queries
        """

        # Parsed queries
        parsed = []

        for query in queries:
            # Parse query
            parse = self.database.parse(query)

            # Transform query if SQL not parsed and reparse
            if self.query and "select" not in parse:
                # Generate query
                query = self.query(query)
                logger.debug(query)

                # Reparse query
                parse = self.database.parse(query)

            parsed.append(parse)

        return parsed

    def limit(self, queries):
        """
        Parses the largest LIMIT clause from queries.

        Args:
            queries: list of queries

        Returns:
            largest limit number or 0 if not found
        """

        # Override limit with largest limit from database queries
        qlimit = 0
        for query in queries:
            # Parse out qlimit
            l = query.get("limit")
            if l and l.isdigit():
                l = int(l)

            qlimit = l if l and l > qlimit else qlimit

        return qlimit

    def graphsearch(self, queries, limit, weights, index):
        """
        Executes an index + graph search.

        Args:
            queries: list of queries
            limit: maximum results
            weights: default hybrid score weights
            index: default index name

        Returns:
            graph search results
        """

        # Parse queries
        queries = [self.graph.parse(query) for query in queries]

        # Override limit with query limit, if applicable
        limit = max(limit, self.limit(queries))

        # Bulk index scan
        scan = Scan(self.search, limit, weights, index)(queries, None)

        # Combine index search results with database search results
        for x, query in enumerate(queries):
            # Add search results to query
            query["results"] = [r for y, r in scan if x == y]

        return self.graph.batchsearch(queries, limit, self.indexids)



================================================
FILE: src/python/txtai/embeddings/search/errors.py
================================================
"""
Errors module
"""


class IndexNotFoundError(Exception):
    """
    Raised when an embeddings query fails to locate an index
    """



================================================
FILE: src/python/txtai/embeddings/search/explain.py
================================================
"""
Explain module
"""

import numpy as np


class Explain:
    """
    Explains the importance of each token in an input text element for a query. This method creates n permutations of the input text, where n
    is the number of tokens in the input text. This effectively masks each token to determine its importance to the query.
    """

    def __init__(self, embeddings):
        """
        Creates a new explain action.

        Args:
            embeddings: embeddings instance
        """

        self.embeddings = embeddings
        self.content = embeddings.config.get("content")

        # Alias embeddings attributes
        self.database = embeddings.database

    def __call__(self, queries, texts, limit):
        """
        Explains the importance of each input token in text for a list of queries.

        Args:
            query: input queries
            texts: optional list of (text|list of tokens), otherwise runs search queries
            limit: optional limit if texts is None

        Returns:
            list of dict per input text per query where a higher token scores represents higher importance relative to the query
        """

        # Construct texts elements per query
        texts = self.texts(queries, texts, limit)

        # Explain each query-texts combination
        return [self.explain(query, texts[x]) for x, query in enumerate(queries)]

    def texts(self, queries, texts, limit):
        """
        Constructs lists of dict for each input query.

        Args:
            queries: input queries
            texts: optional list of texts
            limit: optional limit if texts is None

        Returns:
            lists of dict for each input query
        """

        # Calculate similarity scores per query if texts present
        if texts:
            results = []
            for scores in self.embeddings.batchsimilarity(queries, texts):
                results.append([{"id": uid, "text": texts[uid], "score": score} for uid, score in scores])

            return results

        # Query for results if texts is None and content is enabled
        return self.embeddings.batchsearch(queries, limit) if self.content else [[]] * len(queries)

    def explain(self, query, texts):
        """
        Explains the importance of each input token in text for a list of queries.

        Args:
            query: input query
            texts: list of text

        Returns:
            list of {"id": value, "text": value, "score": value, "tokens": value} covering each input text element
        """

        # Explain results
        results = []

        # Parse out similar clauses, if necessary
        if self.database:
            # Parse query
            query = self.database.parse(query)

            # Extract query from similar clause
            query = " ".join([" ".join(clause) for clause in query["similar"]]) if "similar" in query else None

        # Return original texts if query, text or score not present
        if not query or not texts or "score" not in texts[0] or "text" not in texts[0]:
            return texts

        # Calculate result per input text element
        for result in texts:
            text = result["text"]
            tokens = text if isinstance(text, list) else text.split()

            # Create permutations of input text, masking each token to determine importance
            permutations = []
            for i in range(len(tokens)):
                data = tokens.copy()
                data.pop(i)
                permutations.append([" ".join(data)])

            # Calculate similarity for each input text permutation and get score delta as importance
            scores = [(i, result["score"] - np.abs(s)) for i, s in self.embeddings.similarity(query, permutations)]

            # Append tokens to result
            result["tokens"] = [(tokens[i], score) for i, score in sorted(scores, key=lambda x: x[0])]

            # Add data sorted in index order
            results.append(result)

        # Sort score descending and return
        return sorted(results, key=lambda x: x["score"], reverse=True)



================================================
FILE: src/python/txtai/embeddings/search/ids.py
================================================
"""
Ids module
"""


class Ids:
    """
    Resolves internal ids for lists of ids.
    """

    def __init__(self, embeddings):
        """
        Create a new ids action.

        Args:
            embeddings: embeddings instance
        """

        self.database = embeddings.database
        self.ids = embeddings.ids

    def __call__(self, ids):
        """
        Resolve internal ids.

        Args:
            ids: ids

        Returns:
            internal ids
        """

        # Resolve ids using database if available, otherwise fallback to embeddings ids
        results = self.database.ids(ids) if self.database else self.scan(ids)

        # Create dict of id: [iids] given there is a one to many relationship
        ids = {}
        for iid, uid in results:
            if uid not in ids:
                ids[uid] = []
            ids[uid].append(iid)

        return ids

    def scan(self, ids):
        """
        Scans embeddings ids array for matches when content is disabled.

        Args:
            ids: search ids

        Returns:
            internal ids
        """

        # Find existing ids
        indices = []
        for uid in ids:
            indices.extend([(index, value) for index, value in enumerate(self.ids) if uid == value])

        return indices



================================================
FILE: src/python/txtai/embeddings/search/query.py
================================================
"""
Query module
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration


class Query:
    """
    Query translation model.
    """

    def __init__(self, path, prefix=None, maxlength=512):
        """
        Creates a query translation model.

        Args:
            path: path to query model
            prefix: text prefix
            maxlength: max sequence length to generate
        """

        self.tokenizer = AutoTokenizer.from_pretrained(path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(path)

        # Default prefix if not provided for T5 models
        if not prefix and isinstance(self.model, T5ForConditionalGeneration):
            prefix = "translate English to SQL: "

        self.prefix = prefix
        self.maxlength = maxlength

    def __call__(self, query):
        """
        Runs query translation model.

        Args:
            query: input query

        Returns:
            transformed query
        """

        # Add prefix, if necessary
        if self.prefix:
            query = f"{self.prefix}{query}"

        # Tokenize and generate text using model
        features = self.tokenizer([query], return_tensors="pt")
        output = self.model.generate(input_ids=features["input_ids"], attention_mask=features["attention_mask"], max_length=self.maxlength)

        # Decode tokens to text
        result = self.tokenizer.decode(output[0], skip_special_tokens=True)

        # Clean and return generated text
        return self.clean(result)

    def clean(self, text):
        """
        Applies a series of rules to clean generated text.

        Args:
            text: input text

        Returns:
            clean text
        """

        return text.replace("$=", "<=")



================================================
FILE: src/python/txtai/embeddings/search/scan.py
================================================
"""
Scan module
"""


class Scan:
    """
    Scans indexes for query matches.
    """

    def __init__(self, search, limit, weights, index):
        """
        Creates a new scan instance.

        Args:
            search: index search function
            limit: maximum results
            weights: default hybrid score weights
            index: default index name
        """

        # Index search function
        self.search = search

        # Default query limit
        self.limit = limit

        # Default number of candidates
        self.candidates = None

        # Default query weights
        self.weights = weights

        # Default index
        self.index = index

    def __call__(self, queries, parameters):
        """
        Executes a scan for a list of queries.

        Args:
            queries: list of queries to run
            parameters: list of dicts of named parameters to bind to placeholders

        Returns:
            list of (id, score) per query
        """

        # Query results group by unique query clause id
        results = {}

        # Default number of candidates
        default = None

        # Group by index and run
        for index, iqueries in self.parse(queries, parameters).items():
            # Query limit to pass to batch search
            candidates = [query.candidates for query in iqueries if query.candidates]
            if not candidates and not default:
                default = self.default(queries)

            candidates = max(candidates) if candidates else default

            # Query weights to pass to batch search
            weights = [query.weights for query in iqueries if query.weights is not None]
            weights = max(weights) if weights else self.weights

            # Index to run query against
            index = index if index else self.index

            # Run index searches
            for x, result in enumerate(self.search([query.text for query in iqueries], candidates, weights, index)):
                # Save query id and results to later join to original query
                results[iqueries[x].uid] = (iqueries[x].qid, result)

        # Sort by query uid and return results
        return [result for _, result in sorted(results.items())]

    def parse(self, queries, parameters):
        """
        Parse index query clauses from a list of parsed queries.

        Args:
            queries: list of parsed queries
            parameters: list of dicts of named parameters to bind to placeholders

        Returns:
            index query clauses grouped by index
        """

        results, uid = {}, 0
        for x, query in enumerate(queries):
            if "similar" in query:
                # Extract similar query clauses
                for params in query["similar"]:
                    # Resolve bind parameters
                    if parameters and parameters[x]:
                        params = self.bind(params, parameters[x])

                    # Parse query clause
                    clause = Clause(uid, x, params)

                    # Create clause list for index
                    if clause.index not in results:
                        results[clause.index] = []

                    # Add query to index list, increment uid
                    results[clause.index].append(clause)
                    uid += 1

        return results

    def bind(self, similar, parameters):
        """
        Resolves bind parameters for a similar function call.

        Args:
            similar: similar function call arguments
            parameters: bind parameters

        Returns:
            similar function call arguments with resolved bind parameters
        """

        resolved = []
        for p in similar:
            # Resolve bind parameters
            if isinstance(p, str) and p.startswith(":") and p[1:] in parameters:
                resolved.append(parameters[p[1:]])
            else:
                resolved.append(p)

        return resolved

    def default(self, queries):
        """
        Derives the default number of candidates. The number of candidates are the number of results to bring back
        from index queries. This is an optional argument to similar() clauses.

        For a single query filter clause, the default is the query limit. With multiple filtering clauses, the default is
        10x the query limit. This ensures that limit results are still returned with additional filtering after an index query.

        Args:
            queries: list of queries

        Returns:
            default candidate list size
        """

        multitoken = any(query.get("where") and len(query["where"].split()) > 1 for query in queries)
        return self.limit * 10 if multitoken else self.limit


class Clause:
    """
    Parses and stores query clause parameters.
    """

    def __init__(self, uid, qid, params):
        """
        Creates a new query clause.

        Args:
            uid: query clause id
            qid: query id clause is a part of
            params: query parameters to parse
        """

        self.uid, self.qid = uid, qid
        self.text, self.index = params[0], None
        self.candidates, self.weights = None, None

        # Parse additional similar clause parameters
        if len(params) > 1:
            self.parse(params[1:])

    def parse(self, params):
        """
        Parses clause parameters into this instance.

        Args:
            params: query clause parameters
        """

        for param in params:
            if (isinstance(param, str) and param.isdigit()) or isinstance(param, int):
                # Number of query candidates
                self.candidates = int(param)

            elif (isinstance(param, str) and param.replace(".", "").isdigit()) or isinstance(param, float):
                # Hybrid score weights
                self.weights = float(param)

            else:
                # Target index
                self.index = param



================================================
FILE: src/python/txtai/embeddings/search/terms.py
================================================
"""
Terms module
"""


class Terms:
    """
    Reduces a query statement down to keyword terms. This method extracts the query text from similar clauses if it's a SQL statement.
    Otherwise, the original query is returned.
    """

    def __init__(self, embeddings):
        """
        Create a new terms action.

        Args:
            embeddings: embeddings instance
        """

        self.database = embeddings.database

    def __call__(self, queries):
        """
        Extracts keyword terms from a list of queries.

        Args:
            queries: list of queries

        Returns:
            list of queries reduced down to keyword term strings
        """

        # Parse queries and extract keyword terms for each query
        if self.database:
            terms = []
            for query in queries:
                # Parse query
                parse = self.database.parse(query)

                # Join terms from similar clauses
                terms.append(" ".join(" ".join(s) for s in parse["similar"]))

            return terms

        # Return original query when database is None
        return queries



================================================
FILE: src/python/txtai/graph/__init__.py
================================================
"""
Graph imports
"""

from .base import Graph
from .factory import GraphFactory
from .networkx import NetworkX
from .query import Query
from .rdbms import RDBMS
from .topics import Topics



================================================
FILE: src/python/txtai/graph/base.py
================================================
"""
Graph module
"""

from collections import Counter

from .topics import Topics


# pylint: disable=R0904
class Graph:
    """
    Base class for Graph instances. This class builds graph networks. Supports topic modeling
    and relationship traversal.
    """

    def __init__(self, config):
        """
        Creates a new Graph.

        Args:
            config: graph configuration
        """

        # Graph configuration
        self.config = config if config is not None else {}

        # Graph backend
        self.backend = None

        # Topic modeling
        self.categories = None
        self.topics = None

        # Transform columns
        columns = config.get("columns", {})
        self.text = columns.get("text", "text")
        self.object = columns.get("object", "object")

        # Attributes to copy - skips text/object/relationship fields - set to True to copy all
        self.copyattributes = config.get("copyattributes", False)

        # Relationships are manually-provided edges
        self.relationships = columns.get("relationships", "relationships")
        self.relations = {}

    def create(self):
        """
        Creates the graph network.
        """

        raise NotImplementedError

    def count(self):
        """
        Returns the total number of nodes in graph.

        Returns:
            total nodes in graph
        """

        raise NotImplementedError

    def scan(self, attribute=None, data=False):
        """
        Iterates over nodes that match a criteria. If no criteria specified, all nodes
        are returned.

        Args:
            attribute: if specified, nodes having this attribute are returned
            data: if True, attribute data is also returned

        Returns:
            node id iterator if data is False or (id, attribute dictionary) iterator if data is True
        """

        raise NotImplementedError

    def node(self, node):
        """
        Get node by id. Returns None if not found.

        Args:
            node: node id

        Returns:
            graph node
        """

        raise NotImplementedError

    def addnode(self, node, **attrs):
        """
        Adds a node to the graph.

        Args:
            node: node id
            attrs: node attributes
        """

        raise NotImplementedError

    def addnodes(self, nodes):
        """
        Adds nodes to the graph.

        Args:
            nodes: list of (node, attributes) to add
        """

        raise NotImplementedError

    def removenode(self, node):
        """
        Removes a node and all it's edges from graph.

        Args:
            node: node id
        """

        raise NotImplementedError

    def hasnode(self, node):
        """
        Returns True if node found, False otherwise.

        Args:
            node: node id

        Returns:
            True if node found, False otherwise
        """

        raise NotImplementedError

    def attribute(self, node, field):
        """
        Gets a node attribute.

        Args:
            node: node id
            field: attribute name

        Returns:
            attribute value
        """

        raise NotImplementedError

    def addattribute(self, node, field, value):
        """
        Adds an attribute to node.

        Args:
            node: node id
            field: attribute name
            value: attribute value
        """

        raise NotImplementedError

    def removeattribute(self, node, field):
        """
        Removes an attribute from node.

        Args:
            node: node id
            field: attribute name

        Returns:
            attribute value or None if not present
        """

        raise NotImplementedError

    def edgecount(self):
        """
        Returns the total number of edges.

        Returns:
            total number of edges in graph
        """

        raise NotImplementedError

    def edges(self, node):
        """
        Gets edges of node by id.

        Args:
            node: node id

        Returns:
            list of edge node ids
        """

        raise NotImplementedError

    def addedge(self, source, target, **attrs):
        """
        Adds an edge to graph.

        Args:
            source: node 1 id
            target: node 2 id
        """

        raise NotImplementedError

    def addedges(self, edges):
        """
        Adds an edge to graph.

        Args:
            edges: list of (source, target, attributes) to add
        """

        raise NotImplementedError

    def hasedge(self, source, target=None):
        """
        Returns True if edge found, False otherwise. If target is None, this method
        returns True if any edge is found.

        Args:
            source: node 1 id
            target: node 2 id

        Returns:
            True if edge found, False otherwise
        """

        raise NotImplementedError

    def centrality(self):
        """
        Runs a centrality algorithm on the graph.

        Returns:
            dict of {node id: centrality score}
        """

        raise NotImplementedError

    def pagerank(self):
        """
        Runs the pagerank algorithm on the graph.

        Returns:
            dict of {node id, page rank score}
        """

        raise NotImplementedError

    def showpath(self, source, target):
        """
        Gets the shortest path between source and target.

        Args:
            source: start node id
            target: end node id

        Returns:
            list of node ids representing the shortest path
        """

        raise NotImplementedError

    def isquery(self, queries):
        """
        Checks if queries are supported graph queries.

        Args:
            queries: queries to check

        Returns:
            True if all the queries are supported graph queries, False otherwise
        """

        raise NotImplementedError

    def parse(self, query):
        """
        Parses a graph query into query components.

        Args:
            query: graph query

        Returns:
            query components as a dictionary
        """

        raise NotImplementedError

    def search(self, query, limit=None, graph=False):
        """
        Searches graph for nodes matching query.

        Args:
            query: graph query
            limit: maximum results
            graph: return graph results if True

        Returns:
            list of dict if graph is set to False
            filtered graph if graph is set to True
        """

        raise NotImplementedError

    def batchsearch(self, queries, limit=None, graph=False):
        """
        Searches graph for nodes matching query.

        Args:
            query: graph query
            limit: maximum results
            graph: return graph results if True

        Returns:
            list of dict if graph is set to False
            filtered graph if graph is set to True
        """

        return [self.search(query, limit, graph) for query in queries]

    def communities(self, config):
        """
        Run community detection on the graph.

        Args:
            config: configuration

        Returns:
            dictionary of {topic name:[ids]}
        """

        raise NotImplementedError

    def load(self, path):
        """
        Loads a graph at path.

        Args:
            path: path to graph
        """

        raise NotImplementedError

    def save(self, path):
        """
        Saves a graph at path.

        Args:
            path: path to save graph
        """

        raise NotImplementedError

    def loaddict(self, data):
        """
        Loads data from input dictionary into this graph.

        Args:
            data: input dictionary
        """

        raise NotImplementedError

    def savedict(self):
        """
        Saves graph data to a dictionary.

        Returns:
            dict
        """

        raise NotImplementedError

    def initialize(self):
        """
        Initialize graph instance.
        """

        if not self.backend:
            self.backend = self.create()

    def close(self):
        """
        Closes this graph.
        """

        self.backend, self.categories, self.topics = None, None, None

    def insert(self, documents, index=0):
        """
        Insert graph nodes for each document.

        Args:
            documents: list of (id, data, tags)
            index: indexid offset, used for node ids
        """

        # Initialize graph backend
        self.initialize()

        nodes = []
        for uid, document, _ in documents:
            # Manually provided relationships and attributes to copy
            relations, attributes = None, {}

            # Extract data from dictionary
            if isinstance(document, dict):
                # Extract relationships
                relations = document.get(self.relationships)

                # Attributes to copy, if any
                search = self.copyattributes if isinstance(self.copyattributes, list) else []
                attributes = {
                    k: v
                    for k, v in document.items()
                    if k not in [self.text, self.object, self.relationships] and (self.copyattributes is True or k in search)
                }

                # Require text or object field
                document = document.get(self.text, document.get(self.object))

            if document is not None:
                if isinstance(document, list):
                    # Join tokens as text
                    document = " ".join(document)

                # Create node
                nodes.append((index, {**{"id": uid, "data": document}, **attributes}))

                # Add relationships
                self.addrelations(index, relations)

                index += 1

        # Add nodes
        self.addnodes(nodes)

    def delete(self, ids):
        """
        Deletes ids from graph.

        Args:
            ids: node ids to delete
        """

        for node in ids:
            # Remove existing node, if it exists
            if self.hasnode(node):
                # Delete from topics
                topic = self.attribute(node, "topic")
                if topic and self.topics:
                    # Delete id from topic
                    self.topics[topic].remove(node)

                    # Also delete topic, if it's empty
                    if not self.topics[topic]:
                        self.topics.pop(topic)

                # Delete node
                self.removenode(node)

    def index(self, search, ids, similarity):
        """
        Build relationships between graph nodes using a score-based search function.

        Args:
            search: batch search function - takes a list of queries and returns lists of (id, scores) to use as edge weights
            ids: ids function - internal id resolver
            similarity: batch similarity function - takes a list of text and labels and returns best matches
        """

        # Add relationship edges
        self.resolverelations(ids)

        # Infer node edges using search function
        self.inferedges(self.scan(), search)

        # Label categories/topics
        if "topics" in self.config:
            self.addtopics(similarity)

    def upsert(self, search, ids, similarity=None):
        """
        Adds relationships for new graph nodes using a score-based search function.

        Args:
            search: batch search function - takes a list of queries and returns lists of (id, scores) to use as edge weights
            ids: ids function - internal id resolver
            similarity: batch similarity function - takes a list of text and labels and returns best matches
        """

        # Detect if topics processing is enabled
        hastopics = "topics" in self.config

        # Add relationship edges
        self.resolverelations(ids)

        # Infer node edges using new/updated nodes, set updated flag for topic processing, if necessary
        self.inferedges(self.scan(attribute="data"), search, {"updated": True} if hastopics else None)

        # Infer topics with topics of connected nodes
        if hastopics:
            # Infer topics if there is at least one topic, otherwise rebuild
            if self.topics:
                self.infertopics()
            else:
                self.addtopics(similarity)

    def filter(self, nodes, graph=None):
        """
        Creates a subgraph of this graph using the list of input nodes. This method creates a new graph
        selecting only matching nodes, edges, topics and categories.

        Args:
            nodes: nodes to select as a list of ids or list of (id, score) tuples
            graph: optional graph used to store filtered results

        Returns:
            graph
        """

        # Set graph if available, otherwise create a new empty graph of the same type
        graph = graph if graph else type(self)(self.config)

        # Initalize subgraph
        graph.initialize()

        nodeids = {node[0] if isinstance(node, tuple) else node for node in nodes}
        for node in nodes:
            # Unpack node and score, if available
            node, score = node if isinstance(node, tuple) else (node, None)

            # Add nodes
            graph.addnode(node, **self.node(node))

            # Add score if present
            if score is not None:
                graph.addattribute(node, "score", score)

            # Add edges
            edges = self.edges(node)
            if edges:
                for target, attributes in self.edges(node).items():
                    if target in nodeids:
                        graph.addedge(node, target, **attributes)

        # Filter categories and topics
        if self.topics:
            topics = {}
            for i, (topic, ids) in enumerate(self.topics.items()):
                ids = [x for x in ids if x in nodeids]
                if ids:
                    topics[topic] = (self.categories[i] if self.categories else None, ids)

            # Sort by number of nodes descending
            topics = sorted(topics.items(), key=lambda x: len(x[1][1]), reverse=True)

            # Copy filtered categories and topics
            graph.categories = [category for _, (category, _) in topics] if self.categories else None
            graph.topics = {topic: ids for topic, (_, ids) in topics}

        return graph

    def addrelations(self, node, relations):
        """
        Add manually-provided relationships.

        Args:
            node: node id
            relations: list of relationships to add
        """

        # Add relationships, if any
        if relations:
            if node not in self.relations:
                self.relations[node] = []

            # Add each relationship
            for relation in relations:
                # Support both dict and string ids
                relation = {"id": relation} if not isinstance(relation, dict) else relation
                self.relations[node].append(relation)

    def resolverelations(self, ids):
        """
        Resolves ids and creates edges for manually-provided relationships.

        Args:
            ids: internal id resolver
        """

        # Relationship edges
        edges = []

        # Resolve ids and create edges for relationships
        for node, relations in self.relations.items():
            # Resolve internal ids
            iids = ids(y["id"] for y in relations)

            # Add each edge
            for relation in relations:
                # Make copy of relation
                relation = relation.copy()

                # Lookup targets for relationship
                targets = iids.get(str(relation.pop("id")))

                # Create edge for each instance of id - internal id pair
                if targets:
                    for target in targets:
                        # Add weight, if not provided
                        relation["weight"] = relation.get("weight", 1.0)

                        # Add edge and all other attributes
                        edges.append((node, target, relation))

        # Add relationships
        if edges:
            self.addedges(edges)

        # Clear temporary relationship storage
        self.relations = {}

    def inferedges(self, nodes, search, attributes=None):
        """
        Infers edges for a list of nodes using a score-based search function.

        Args:
            nodes: list of nodes
            search: search function to use to identify edges
            attribute: dictionary of attributes to add to each node
        """

        # Read graph parameters
        batchsize, limit, minscore = self.config.get("batchsize", 256), self.config.get("limit", 15), self.config.get("minscore", 0.1)
        approximate = self.config.get("approximate", True)

        batch = []
        for node in nodes:
            # Get data attribute
            data = self.removeattribute(node, "data")

            # Set text field when data is a string
            if isinstance(data, str):
                self.addattribute(node, "text", data)

            # Add additional attributes, if specified
            if attributes:
                for field, value in attributes.items():
                    self.addattribute(node, field, value)

            # Skip nodes with existing edges when building an approximate network
            if not approximate or not self.hasedge(node):
                batch.append((node, data))

            # Process batch
            if len(batch) == batchsize:
                self.addbatch(search, batch, limit, minscore)
                batch = []

        if batch:
            self.addbatch(search, batch, limit, minscore)

    def addbatch(self, search, batch, limit, minscore):
        """
        Adds batch of documents to graph. This method runs the search function for each item in batch
        and adds node edges between the input and each search result.

        Args:
            search: search function to use to identify edges
            batch: batch to add
            limit: max edges to add per node
            minscore: min score to add node edge
        """

        edges = []
        for x, result in enumerate(search([data for _, data in batch], limit)):
            # Get input node id
            x, _ = batch[x]

            # Add edges for each input node id and result node id pair that meets specified criteria
            for y, score in result:
                if str(x) != str(y) and score > minscore:
                    edges.append((x, y, {"weight": score}))

        self.addedges(edges)

    def addtopics(self, similarity=None):
        """
        Identifies and adds topics using community detection.

        Args:
            similarity: similarity function for labeling categories
        """

        # Clear previous topics, if any
        self.cleartopics()

        # Use community detection to get topics
        topics = Topics(self.config["topics"])
        config = topics.config
        self.topics = topics(self)

        # Label each topic with a higher level category
        if "categories" in config and similarity:
            self.categories = []
            results = similarity(self.topics.keys(), config["categories"])
            for result in results:
                self.categories.append(config["categories"][result[0][0]])

        # Add topic-related node attributes
        for x, topic in enumerate(self.topics):
            for r, node in enumerate(self.topics[topic]):
                self.addattribute(node, "topic", topic)
                self.addattribute(node, "topicrank", r)

                if self.categories:
                    self.addattribute(node, "category", self.categories[x])

    def cleartopics(self):
        """
        Clears topic fields from all nodes.
        """

        # Clear previous topics, if any
        if self.topics:
            for node in self.scan():
                self.removeattribute(node, "topic")
                self.removeattribute(node, "topicrank")

                if self.categories:
                    self.removeattribute(node, "category")

            self.topics, self.categories = None, None

    def infertopics(self):
        """
        Infers topics for all nodes with an "updated" attribute. This method analyzes the direct node
        neighbors and set the most commonly occuring topic and category for each node.
        """

        # Iterate over nodes missing topic attribute (only occurs for new nodes)
        for node in self.scan(attribute="updated"):
            # Remove updated attribute
            self.removeattribute(node, "updated")

            # Get list of neighboring nodes
            ids = self.edges(node)
            ids = ids.keys() if ids else None

            # Infer topic
            topic = Counter(self.attribute(x, "topic") for x in ids).most_common(1)[0][0] if ids else None
            if topic:
                # Add id to topic list and set topic attribute
                self.topics[topic].append(node)
                self.addattribute(node, "topic", topic)

                # Set topic rank
                self.addattribute(node, "topicrank", len(self.topics[topic]) - 1)

                # Infer category
                category = Counter(self.attribute(x, "category") for x in ids).most_common(1)[0][0]
                self.addattribute(node, "category", category)



================================================
FILE: src/python/txtai/graph/factory.py
================================================
"""
Factory module
"""

from ..util import Resolver

from .networkx import NetworkX
from .rdbms import RDBMS


class GraphFactory:
    """
    Methods to create graphs.
    """

    @staticmethod
    def create(config):
        """
        Create a Graph.

        Args:
            config: graph configuration

        Returns:
            Graph
        """

        # Graph instance
        graph = None
        backend = config.get("backend", "networkx")

        # Create graph instance
        if backend == "networkx":
            graph = NetworkX(config)
        elif backend == "rdbms":
            graph = RDBMS(config)
        else:
            graph = GraphFactory.resolve(backend, config)

        # Store config back
        config["backend"] = backend

        return graph

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: index configuration parameters

        Returns:
            Graph
        """

        try:
            return Resolver()(backend)(config)
        except Exception as e:
            raise ImportError(f"Unable to resolve graph backend: '{backend}'") from e



================================================
FILE: src/python/txtai/graph/networkx.py
================================================
"""
NetworkX module
"""

import os

from tempfile import TemporaryDirectory

# Conditional import
try:
    import networkx as nx

    from networkx.algorithms.community import asyn_lpa_communities, greedy_modularity_communities, louvain_partitions
    from networkx.readwrite import json_graph

    NETWORKX = True
except ImportError:
    NETWORKX = False

from ..archive import ArchiveFactory
from ..serialize import SerializeError, SerializeFactory

from .base import Graph
from .query import Query


# pylint: disable=R0904
class NetworkX(Graph):
    """
    Graph instance backed by NetworkX.
    """

    def __init__(self, config):
        super().__init__(config)

        if not NETWORKX:
            raise ImportError('NetworkX is not available - install "graph" extra to enable')

    def create(self):
        return nx.Graph()

    def count(self):
        return self.backend.number_of_nodes()

    def scan(self, attribute=None, data=False):
        # Full graph
        graph = self.backend

        # Filter graph to nodes having a specified attribute
        if attribute:
            graph = nx.subgraph_view(self.backend, filter_node=lambda x: attribute in self.node(x))

        # Return either list of matching ids or tuple of (id, attribute dictionary)
        return graph.nodes(data=True) if data else graph

    def node(self, node):
        return self.backend.nodes.get(node)

    def addnode(self, node, **attrs):
        self.backend.add_node(node, **attrs)

    def addnodes(self, nodes):
        self.backend.add_nodes_from(nodes)

    def removenode(self, node):
        if self.hasnode(node):
            self.backend.remove_node(node)

    def hasnode(self, node):
        return self.backend.has_node(node)

    def attribute(self, node, field):
        return self.node(node).get(field) if self.hasnode(node) else None

    def addattribute(self, node, field, value):
        if self.hasnode(node):
            self.node(node)[field] = value

    def removeattribute(self, node, field):
        return self.node(node).pop(field, None) if self.hasnode(node) else None

    def edgecount(self):
        return self.backend.number_of_edges()

    def edges(self, node):
        edges = self.backend.adj.get(node)
        if edges:
            return dict(sorted(edges.items(), key=lambda x: x[1].get("weight", 0), reverse=True))

        return None

    def addedge(self, source, target, **attrs):
        self.backend.add_edge(source, target, **attrs)

    def addedges(self, edges):
        self.backend.add_edges_from(edges)

    def hasedge(self, source, target=None):
        if target is None:
            edges = self.backend.adj.get(source)
            return len(edges) > 0 if edges else False

        return self.backend.has_edge(source, target)

    def centrality(self):
        rank = nx.degree_centrality(self.backend)
        return dict(sorted(rank.items(), key=lambda x: x[1], reverse=True))

    def pagerank(self):
        rank = nx.pagerank(self.backend, weight="weight")
        return dict(sorted(rank.items(), key=lambda x: x[1], reverse=True))

    def showpath(self, source, target):
        # pylint: disable=E1121
        return nx.shortest_path(self.backend, source, target, self.distance)

    def isquery(self, queries):
        return Query().isquery(queries)

    def parse(self, query):
        return Query().parse(query)

    def search(self, query, limit=None, graph=False):
        # Run graph query
        results = Query()(self, query, limit)

        # Transform into filtered graph
        if graph:
            nodes = set()
            for column in results.values():
                for value in column:
                    if isinstance(value, list):
                        # Path group
                        nodes.update([node for node in value if node and not isinstance(node, dict)])
                    elif isinstance(value, dict):
                        # Nodes by id attribute
                        nodes.update(uid for uid, attr in self.scan(data=True) if attr["id"] == value["id"])
                    elif value is not None:
                        # Single node id
                        nodes.add(value)

            return self.filter(list(nodes))

        # Transform columnar structure into rows
        keys = list(results.keys())
        rows, count = [], len(results[keys[0]])

        for x in range(count):
            rows.append({str(key): results[key][x] for key in keys})

        return rows

    def communities(self, config):
        # Get community detection algorithm
        algorithm = config.get("algorithm")

        if algorithm == "greedy":
            communities = greedy_modularity_communities(self.backend, weight="weight", resolution=config.get("resolution", 100))
        elif algorithm == "lpa":
            communities = asyn_lpa_communities(self.backend, weight="weight", seed=0)
        else:
            communities = self.louvain(config)

        return communities

    def load(self, path):
        try:
            # Load graph data
            data = SerializeFactory.create().load(path)

            # Add data to graph
            self.backend = self.create()
            self.backend.add_nodes_from(data["nodes"])
            self.backend.add_edges_from(data["edges"])

            # Load categories
            self.categories = data.get("categories")

            # Load topics
            self.topics = data.get("topics")

        except SerializeError:
            # Backwards compatible support for legacy TAR format
            self.loadtar(path)

    def save(self, path):
        # Save graph data
        SerializeFactory.create().save(
            {
                "nodes": [(uid, self.node(uid)) for uid in self.scan()],
                "edges": list(self.backend.edges(data=True)),
                "categories": self.categories,
                "topics": self.topics,
            },
            path,
        )

    def loaddict(self, data):
        self.backend = json_graph.node_link_graph(data, name="indexid")
        self.categories, self.topics = data.get("categories"), data.get("topics")

    def savedict(self):
        data = json_graph.node_link_data(self.backend, name="indexid")
        data["categories"] = self.categories
        data["topics"] = self.topics

        return data

    def louvain(self, config):
        """
        Runs the Louvain community detection algorithm.

        Args:
            config: topic configuration

        Returns:
            list of [ids] per community
        """

        # Partition level to use
        level = config.get("level", "best")

        # Run community detection
        results = list(louvain_partitions(self.backend, weight="weight", resolution=config.get("resolution", 100), seed=0))

        # Get partition level (first or best)
        return results[0] if level == "first" else results[-1]

    # pylint: disable=W0613
    def distance(self, source, target, attrs):
        """
        Computes distance between source and target nodes using weight.

        Args:
            source: source node
            target: target node
            attrs: edge attributes

        Returns:
            distance between source and target
        """

        # Distance is 1 - score. Skip minimal distances as they are near duplicates.
        distance = max(1.0 - attrs["weight"], 0.0)
        return distance if distance >= 0.15 else 1.00

    def loadtar(self, path):
        """
        Loads a graph from the legacy TAR file.

        Args:
            path: path to graph
        """

        # Pickle serialization - backwards compatible
        serializer = SerializeFactory.create("pickle")

        # Extract files to temporary directory and load content
        with TemporaryDirectory() as directory:
            # Unpack files
            archive = ArchiveFactory.create(directory)
            archive.load(path, "tar")

            # Load graph backend
            self.backend = serializer.load(f"{directory}/graph")

            # Load categories, if necessary
            path = f"{directory}/categories"
            if os.path.exists(path):
                self.categories = serializer.load(path)

            # Load topics, if necessary
            path = f"{directory}/topics"
            if os.path.exists(path):
                self.topics = serializer.load(path)



================================================
FILE: src/python/txtai/graph/query.py
================================================
"""
Query module
"""

import logging
import re

try:
    from grandcypher import GrandCypher

    GRANDCYPHER = True
except ImportError:
    GRANDCYPHER = False

# Logging configuration
logger = logging.getLogger(__name__)


class Query:
    """
    Runs openCypher graph queries using the GrandCypher library. This class also supports search functions.
    """

    # Similar token
    SIMILAR = "__SIMILAR__"

    def __init__(self):
        """
        Create a new graph query instance.
        """

        if not GRANDCYPHER:
            raise ImportError('GrandCypher is not available - install "graph" extra to enable')

    def __call__(self, graph, query, limit):
        """
        Runs a graph query.

        Args:
            graph: graph instance
            query: graph query, can be a full query string or a parsed query dictionary
            limit: number of results

        Returns:
            results
        """

        # Results by attribute and ids filter
        attributes, uids = None, None

        # Build the query from a parsed query
        if isinstance(query, dict):
            query, attributes, uids = self.build(query)

        # Filter graph, if applicable
        if uids:
            graph = self.filter(graph, attributes, uids)

        # Debug log graph query
        logger.debug(query)

        # Run openCypher query
        return GrandCypher(graph.backend, limit if limit else 3).run(query)

    def isquery(self, queries):
        """
        Checks a list of queries to see if all queries are openCypher queries.

        Args:
            queries: list of queries to check

        Returns:
            True if all queries are openCypher queries
        """

        # Check for required graph query clauses
        return all(query and query.strip().startswith("MATCH ") and "RETURN " in query for query in queries)

    def parse(self, query):
        """
        Parses a graph query. This method supports parsing search functions and replacing them with placeholders.

        Args:
            query: graph query

        Returns:
            parsed query as a dictionary
        """

        # Parameters
        where, limit, nodes, similar = None, None, [], []

        # Parse where clause
        match = re.search(r"where(.+?)return", query, flags=re.DOTALL | re.IGNORECASE)
        if match:
            where = match.group(1).strip()

        # Parse limit clause
        match = re.search(r"limit\s+(\d+)", query, flags=re.DOTALL | re.IGNORECASE)
        if match:
            limit = match.group(1)

        # Parse similar clauses
        for x, match in enumerate(re.finditer(r"similar\((.+?)\)", query, flags=re.DOTALL | re.IGNORECASE)):
            # Replace similar clause with placeholder
            query = query.replace(match.group(0), f"{Query.SIMILAR}{x}")

            # Parse similar clause parameters
            params = [param.strip().replace("'", "").replace('"', "") for param in match.group(1).split(",")]
            nodes.append(params[0])
            similar.append(params[1:])

        # Return parsed query
        return {
            "query": query,
            "where": where,
            "limit": limit,
            "nodes": nodes,
            "similar": similar,
        }

    def build(self, parse):
        """
        Constructs a full query from a parsed query. This method supports substituting placeholders with search results.

        Args:
            parse: parsed query

        Returns:
            graph query
        """

        # Get query. Initialize attributes and uids.
        query, attributes, uids = parse["query"], {}, {}

        # Replace similar clause with id query
        if "results" in parse:
            for x, result in enumerate(parse["results"]):
                # Get query node
                node = parse["nodes"][x]

                # Add similar match attribute
                attribute = f"match_{x}"
                clause = f"{node}.{attribute} > 0"

                # Replace placeholder with earch results
                query = query.replace(f"{Query.SIMILAR}{x}", f"{clause}")

                # Add uids and scores
                for uid, score in result:
                    if uid not in uids:
                        uids[uid] = score

                # Add results by attribute matched
                attributes[attribute] = result

        # Return query, results by attribute matched and ids filter
        return query, attributes, uids.items()

    def filter(self, graph, attributes, uids):
        """
        Filters the input graph by uids. This method also adds similar match attributes.

        Args:
            graph: graph instance
            attributes: results by attribute matched
            uids: single list with all matching ids

        Returns:
            filtered graph
        """

        # Filter the graph
        graph = graph.filter(uids)

        # Add similar match attributes
        for attribute, result in attributes.items():
            for uid, score in result:
                graph.addattribute(uid, attribute, score)

        return graph



================================================
FILE: src/python/txtai/graph/rdbms.py
================================================
"""
RDBMS module
"""

import os

# Conditional import
try:
    from grand import Graph
    from grand.backends import SQLBackend, InMemoryCachedBackend

    from sqlalchemy import create_engine, text, StaticPool
    from sqlalchemy.schema import CreateSchema

    ORM = True
except ImportError:
    ORM = False

from .networkx import NetworkX


class RDBMS(NetworkX):
    """
    Graph instance backed by a relational database.
    """

    def __init__(self, config):
        # Check before super() in case those required libraries are also not available
        if not ORM:
            raise ImportError('RDBMS is not available - install "graph" extra to enable')

        super().__init__(config)

        # Graph and database instances
        self.graph = None
        self.database = None

    def __del__(self):
        if hasattr(self, "database") and self.database:
            self.database.close()

    def create(self):
        # Create graph instance
        self.graph, self.database = self.connect()

        # Clear previous graph, if available
        for table in [self.config.get("nodes", "nodes"), self.config.get("edges", "edges")]:
            self.database.execute(text(f"DELETE FROM {table}"))

        # Return NetworkX compatible backend
        return self.graph.nx

    def scan(self, attribute=None, data=False):
        if attribute:
            for node in self.backend:
                attributes = self.node(node)
                if attribute in attributes:
                    yield (node, attributes) if data else node
        else:
            yield from super().scan(attribute, data)

    def load(self, path):
        # Create graph instance
        self.graph, self.database = self.connect()

        # Store NetworkX compatible backend
        self.backend = self.graph.nx

    def save(self, path):
        self.database.commit()

    def close(self):
        # Parent logic
        super().close()

        # Close database connection
        self.database.close()

    def filter(self, nodes, graph=None):
        return super().filter(nodes, graph if graph else NetworkX(self.config))

    def connect(self):
        """
        Connects to a graph backed by a relational database.

        Args:
            Graph database instance
        """

        # Keyword arguments for SQLAlchemy
        kwargs = {"poolclass": StaticPool, "echo": False}
        url = self.config.get("url", os.environ.get("GRAPH_URL"))

        # Set default schema, if necessary
        schema = self.config.get("schema")
        if schema:
            # Check that schema exists
            engine = create_engine(url)
            with engine.begin() as connection:
                connection.execute(CreateSchema(schema, if_not_exists=True) if "postgresql" in url else text("SELECT 1"))

            # Set default schema
            kwargs["connect_args"] = {"options": f'-c search_path="{schema}"'} if "postgresql" in url else {}

        backend = SQLBackend(
            db_url=url,
            node_table_name=self.config.get("nodes", "nodes"),
            edge_table_name=self.config.get("edges", "edges"),
            sqlalchemy_kwargs=kwargs,
        )

        # pylint: disable=W0212
        return Graph(backend=InMemoryCachedBackend(backend, maxsize=None)), backend._connection



================================================
FILE: src/python/txtai/graph/topics.py
================================================
"""
Topics module
"""

from ..pipeline import Tokenizer
from ..scoring import ScoringFactory


class Topics:
    """
    Topic modeling using community detection.
    """

    def __init__(self, config):
        """
        Creates a new Topics instance.

        Args:
            config: topic configuration
        """

        self.config = config if config else {}
        self.tokenizer = Tokenizer(stopwords=True)

        # Additional stopwords to ignore when building topic names
        self.stopwords = set()
        if "stopwords" in self.config:
            self.stopwords.update(self.config["stopwords"])

    def __call__(self, graph):
        """
        Runs topic modeling for input graph.

        Args:
            graph: Graph instance

        Returns:
            dictionary of {topic name: [ids]}
        """

        # Detect communities
        communities = graph.communities(self.config)

        # Sort by community size, largest to smallest
        communities = sorted(communities, key=len, reverse=True)

        # Calculate centrality of graph
        centrality = graph.centrality()

        # Score communities and generate topn terms
        topics = [self.score(graph, x, community, centrality) for x, community in enumerate(communities)]

        # Merge duplicate topics and return
        return self.merge(topics)

    def score(self, graph, index, community, centrality):
        """
        Scores a community of nodes and generates the topn terms in the community.

        Args:
            graph: Graph instance
            index: community index
            community: community of nodes
            centrality: node centrality scores

        Returns:
            (topn topic terms, topic ids sorted by score descending)
        """

        # Tokenize input and build scoring index
        scoring = ScoringFactory.create({"method": self.config.get("labels", "bm25"), "terms": True})
        scoring.index(((node, self.tokenize(graph, node), None) for node in community))

        # Check if scoring index has data
        if scoring.idf:
            # Sort by most commonly occurring terms (i.e. lowest score)
            idf = sorted(scoring.idf, key=scoring.idf.get)

            # Term count for generating topic labels
            topn = self.config.get("terms", 4)

            # Get topn terms
            terms = self.topn(idf, topn)

            # Sort community by score descending
            community = [uid for uid, _ in scoring.search(terms, len(community))]
        else:
            # No text found for topic, generate topic name
            terms = ["topic", str(index)]

            # Sort community by centrality scores
            community = sorted(community, key=lambda x: centrality[x], reverse=True)

        return (terms, community)

    def tokenize(self, graph, node):
        """
        Tokenizes node text.

        Args:
            graph: Graph instance
            node: node id

        Returns:
            list of node tokens
        """

        text = graph.attribute(node, "text")
        return self.tokenizer(text) if text else []

    def topn(self, terms, n):
        """
        Gets topn terms.

        Args:
            terms: list of terms
            n: topn

        Returns:
            topn terms
        """

        topn = []

        for term in terms:
            # Add terms that pass tokenization rules
            if self.tokenizer(term) and term not in self.stopwords:
                topn.append(term)

            # Break once topn terms collected
            if len(topn) == n:
                break

        return topn

    def merge(self, topics):
        """
        Merges duplicate topics

        Args:
            topics: list of (topn terms, topic ids)

        Returns:
            dictionary of {topic name:[ids]}
        """

        merge, termslist = {}, {}

        for terms, uids in topics:
            # Use topic terms as key
            key = frozenset(terms)

            # Add key to merged topics, if necessary
            if key not in merge:
                merge[key], termslist[key] = [], terms

            # Merge communities
            merge[key].extend(uids)

        # Sort communities largest to smallest since the order could have changed with merges
        results = {}
        for k, v in sorted(merge.items(), key=lambda x: len(x[1]), reverse=True):
            # Create composite string key using topic terms and store ids
            results["_".join(termslist[k])] = v

        return results



================================================
FILE: src/python/txtai/models/__init__.py
================================================
"""
Models imports
"""

from .models import Models
from .onnx import OnnxModel
from .pooling import *
from .registry import Registry
from .tokendetection import TokenDetection



================================================
FILE: src/python/txtai/models/models.py
================================================
"""
Models module
"""

import os

import torch

from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForQuestionAnswering,
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from transformers.models.auto.modeling_auto import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES

from .onnx import OnnxModel


class Models:
    """
    Utility methods for working with machine learning models
    """

    @staticmethod
    def checklength(config, tokenizer):
        """
        Checks the length for a Hugging Face Transformers tokenizer using a Hugging Face Transformers config. Copies the
        max_position_embeddings parameter if the tokenizer has no max_length set. This helps with backwards compatibility
        with older tokenizers.

        Args:
            config: transformers config
            tokenizer: transformers tokenizer
        """

        # Unpack nested config, handles passing model directly
        if hasattr(config, "config"):
            config = config.config

        if (
            hasattr(config, "max_position_embeddings")
            and tokenizer
            and hasattr(tokenizer, "model_max_length")
            and tokenizer.model_max_length == int(1e30)
        ):
            tokenizer.model_max_length = config.max_position_embeddings

    @staticmethod
    def maxlength(config, tokenizer):
        """
        Gets the best max length to use for generate calls. This method will return config.max_length if it's set. Otherwise, it will return
        tokenizer.model_max_length.

        Args:
            config: transformers config
            tokenizer: transformers tokenizer
        """

        # Unpack nested config, handles passing model directly
        if hasattr(config, "config"):
            config = config.config

        # Get non-defaulted fields
        keys = config.to_diff_dict()

        # Use config.max_length if not set to default value, else use tokenizer.model_max_length if available
        return config.max_length if "max_length" in keys or not hasattr(tokenizer, "model_max_length") else tokenizer.model_max_length

    @staticmethod
    def deviceid(gpu):
        """
        Translates input gpu argument into a device id.

        Args:
            gpu: True/False if GPU should be enabled, also supports a device id/string/instance

        Returns:
            device id
        """

        # Return if this is already a torch device
        # pylint: disable=E1101
        if isinstance(gpu, torch.device):
            return gpu

        # Always return -1 if gpu is None or an accelerator device is unavailable
        if gpu is None or not Models.hasaccelerator():
            return -1

        # Default to device 0 if gpu is True and not otherwise specified
        if isinstance(gpu, bool):
            return 0 if gpu else -1

        # Return gpu as device id if gpu flag is an int
        return int(gpu)

    @staticmethod
    def device(deviceid):
        """
        Gets a tensor device.

        Args:
            deviceid: device id

        Returns:
            tensor device
        """

        # Torch device
        # pylint: disable=E1101
        return deviceid if isinstance(deviceid, torch.device) else torch.device(Models.reference(deviceid))

    @staticmethod
    def reference(deviceid):
        """
        Gets a tensor device reference.

        Args:
            deviceid: device id

        Returns:
            device reference
        """

        return (
            deviceid
            if isinstance(deviceid, str)
            else (
                "cpu"
                if deviceid < 0
                else f"cuda:{deviceid}" if torch.cuda.is_available() else "mps" if Models.hasmpsdevice() else Models.finddevice()
            )
        )

    @staticmethod
    def acceleratorcount():
        """
        Gets the number of accelerator devices available.

        Returns:
            number of accelerators available
        """

        return max(torch.cuda.device_count(), int(Models.hasaccelerator()))

    @staticmethod
    def hasaccelerator():
        """
        Checks if there is an accelerator device available.

        Returns:
            True if an accelerator device is available, False otherwise
        """

        return torch.cuda.is_available() or Models.hasmpsdevice() or bool(Models.finddevice())

    @staticmethod
    def hasmpsdevice():
        """
        Checks if there is a MPS device available.

        Returns:
            True if a MPS device is available, False otherwise
        """

        return os.environ.get("PYTORCH_MPS_DISABLE") != "1" and torch.backends.mps.is_available()

    @staticmethod
    def finddevice():
        """
        Attempts to find an alternative accelerator device.

        Returns:
            name of first alternative accelerator available or None if not found
        """

        return next((device for device in ["xpu"] if hasattr(torch, device) and getattr(torch, device).is_available()), None)

    @staticmethod
    def load(path, config=None, task="default", modelargs=None):
        """
        Loads a machine learning model. Handles multiple model frameworks (ONNX, Transformers).

        Args:
            path: path to model
            config: path to model configuration
            task: task name used to lookup model type

        Returns:
            machine learning model
        """

        # Detect ONNX models
        if isinstance(path, bytes) or (isinstance(path, str) and os.path.isfile(path)):
            return OnnxModel(path, config)

        # Return path, if path isn't a string
        if not isinstance(path, str):
            return path

        # Transformer models
        models = {
            "default": AutoModel.from_pretrained,
            "question-answering": AutoModelForQuestionAnswering.from_pretrained,
            "summarization": AutoModelForSeq2SeqLM.from_pretrained,
            "text-classification": AutoModelForSequenceClassification.from_pretrained,
            "zero-shot-classification": AutoModelForSequenceClassification.from_pretrained,
        }

        # Pass modelargs as keyword arguments
        modelargs = modelargs if modelargs else {}

        # Load model for supported tasks. Return path for unsupported tasks.
        return models[task](path, **modelargs) if task in models else path

    @staticmethod
    def tokenizer(path, **kwargs):
        """
        Loads a tokenizer from path.

        Args:
            path: path to tokenizer
            kwargs: optional additional keyword arguments

        Returns:
            tokenizer
        """

        return AutoTokenizer.from_pretrained(path, **kwargs) if isinstance(path, str) else path

    @staticmethod
    def task(path, **kwargs):
        """
        Attempts to detect the model task from path.

        Args:
            path: path to model
            kwargs: optional additional keyword arguments

        Returns:
            inferred model task
        """

        # Get model configuration
        config = None
        if isinstance(path, (list, tuple)) and hasattr(path[0], "config"):
            config = path[0].config
        elif isinstance(path, str):
            config = AutoConfig.from_pretrained(path, **kwargs)

        # Attempt to resolve task using configuration
        task = None
        if config:
            architecture = config.architectures[0] if config.architectures else None
            if architecture:
                if architecture in MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values():
                    task = "vision"
                elif any(x for x in ["LMHead", "CausalLM"] if x in architecture):
                    task = "language-generation"
                elif "QuestionAnswering" in architecture:
                    task = "question-answering"
                elif "ConditionalGeneration" in architecture:
                    task = "sequence-sequence"

        return task



================================================
FILE: src/python/txtai/models/onnx.py
================================================
"""
ONNX module
"""

# Conditional import
try:
    import onnxruntime as ort

    ONNX_RUNTIME = True
except ImportError:
    ONNX_RUNTIME = False

import numpy as np
import torch

from transformers import AutoConfig
from transformers.configuration_utils import PretrainedConfig
from transformers.modeling_outputs import SequenceClassifierOutput
from transformers.modeling_utils import PreTrainedModel

from .registry import Registry


# pylint: disable=W0223
class OnnxModel(PreTrainedModel):
    """
    Provides a Transformers/PyTorch compatible interface for ONNX models. Handles casting inputs
    and outputs with minimal to no copying of data.
    """

    def __init__(self, model, config=None):
        """
        Creates a new OnnxModel.

        Args:
            model: path to model or InferenceSession
            config: path to model configuration
        """

        if not ONNX_RUNTIME:
            raise ImportError('onnxruntime is not available - install "model" extra to enable')

        super().__init__(AutoConfig.from_pretrained(config) if config else OnnxConfig())

        # Create ONNX session
        self.model = ort.InferenceSession(model, ort.SessionOptions(), self.providers())

        # Add references for this class to supported AutoModel classes
        Registry.register(self)

    @property
    def device(self):
        """
        Returns model device id.

        Returns:
            model device id
        """

        return -1

    def providers(self):
        """
        Returns a list of available and usable providers.

        Returns:
            list of available and usable providers
        """

        # Create list of providers, prefer CUDA provider if available
        # CUDA provider only available if GPU is available and onnxruntime-gpu installed
        if torch.cuda.is_available() and "CUDAExecutionProvider" in ort.get_available_providers():
            return ["CUDAExecutionProvider", "CPUExecutionProvider"]

        # Default when CUDA provider isn't available
        return ["CPUExecutionProvider"]

    def forward(self, **inputs):
        """
        Runs inputs through an ONNX model and returns outputs. This method handles casting inputs
        and outputs between torch tensors and numpy arrays as shared memory (no copy).

        Args:
            inputs: model inputs

        Returns:
            model outputs
        """

        inputs = self.parse(inputs)

        # Run inputs through ONNX model
        results = self.model.run(None, inputs)

        # pylint: disable=E1101
        # Detect if logits is an output and return classifier output in that case
        if any(x.name for x in self.model.get_outputs() if x.name == "logits"):
            return SequenceClassifierOutput(logits=torch.from_numpy(np.array(results[0])))

        return torch.from_numpy(np.array(results))

    def parse(self, inputs):
        """
        Parse model inputs and handle converting to ONNX compatible inputs.

        Args:
            inputs: model inputs

        Returns:
            ONNX compatible model inputs
        """

        features = {}

        # Select features from inputs
        for key in ["input_ids", "attention_mask", "token_type_ids"]:
            if key in inputs:
                value = inputs[key]

                # Cast torch tensors to numpy
                if hasattr(value, "cpu"):
                    value = value.cpu().numpy()

                # Cast to numpy array if not already one
                features[key] = np.asarray(value)

        return features


class OnnxConfig(PretrainedConfig):
    """
    Configuration for ONNX models.
    """



================================================
FILE: src/python/txtai/models/registry.py
================================================
"""
Registry module
"""

from transformers import AutoModel, AutoModelForQuestionAnswering, AutoModelForSequenceClassification
from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING


class Registry:
    """
    Methods to register models and fully support pipelines.
    """

    @staticmethod
    def register(model, config=None):
        """
        Registers a model with auto model and tokenizer configuration to fully support pipelines.

        Args:
            model: model to register
            config: config class name
        """

        # Default config class to model class if not provided
        config = config if config else model.__class__

        # Default model config_class if empty
        if hasattr(model.__class__, "config_class") and not model.__class__.config_class:
            model.__class__.config_class = config

        # Add references for this class to supported AutoModel classes
        for mapping in [AutoModel, AutoModelForQuestionAnswering, AutoModelForSequenceClassification]:
            mapping.register(config, model.__class__)

        # Add references for this class to support pipeline AutoTokenizers
        if hasattr(model, "config") and type(model.config) not in TOKENIZER_MAPPING:
            TOKENIZER_MAPPING.register(type(model.config), type(model.config).__name__)



================================================
FILE: src/python/txtai/models/tokendetection.py
================================================
"""
Token Detection module
"""

import inspect
import os

import torch

from transformers import PreTrainedModel


class TokenDetection(PreTrainedModel):
    """
    Runs the replaced token detection training objective. This method was first proposed by the ELECTRA model.
    The method consists of a masked language model generator feeding data to a discriminator that determines
    which of the tokens are incorrect. More on this training objective can be found in the ELECTRA paper.
    """

    def __init__(self, generator, discriminator, tokenizer, weight=50.0):
        """
        Creates a new TokenDetection class.

        Args:
            generator: Generator model, must be a masked language model
            discriminator: Discriminator model, must be a model that can detect replaced tokens. Any model can
                           can be customized for this task. See ElectraForPretraining for more.
        """

        # Initialize model with discriminator config
        super().__init__(discriminator.config)

        self.generator = generator
        self.discriminator = discriminator

        # Tokenizer to save with generator and discriminator
        self.tokenizer = tokenizer

        # Discriminator weight
        self.weight = weight

        # Share embeddings if both models are the same type
        # Embeddings must be same size
        if self.generator.config.model_type == self.discriminator.config.model_type:
            self.discriminator.set_input_embeddings(self.generator.get_input_embeddings())

        # Set attention mask present flags
        self.gattention = "attention_mask" in inspect.signature(self.generator.forward).parameters
        self.dattention = "attention_mask" in inspect.signature(self.discriminator.forward).parameters

    # pylint: disable=E1101
    def forward(self, input_ids=None, labels=None, attention_mask=None, token_type_ids=None):
        """
        Runs a forward pass through the model. This method runs the masked language model then randomly samples
        the generated tokens and builds a binary classification problem for the discriminator (detecting if each token is correct).

        Args:
            input_ids: token ids
            labels: token labels
            attention_mask: attention mask
            token_type_ids: segment token indices

        Returns:
            (loss, generator outputs, discriminator outputs, discriminator labels)
        """

        # Copy input ids
        dinputs = input_ids.clone()

        # Run inputs through masked language model
        inputs = {"attention_mask": attention_mask} if self.gattention else {}
        goutputs = self.generator(input_ids, labels=labels, token_type_ids=token_type_ids, **inputs)

        # Get predictions
        preds = torch.softmax(goutputs[1], dim=-1)
        preds = preds.view(-1, self.config.vocab_size)

        tokens = torch.multinomial(preds, 1).view(-1)
        tokens = tokens.view(dinputs.shape[0], -1)

        # Labels have a -100 value to ignore loss from unchanged tokens
        mask = labels.ne(-100)

        # Replace the masked out tokens of the input with the generator predictions
        dinputs[mask] = tokens[mask]

        # Turn mask into new target labels - 1 (True) for corrupted, 0 otherwise.
        # If the prediction was correct, mark it as uncorrupted.
        correct = tokens == labels
        dlabels = mask.long()
        dlabels[correct] = 0

        # Run token classification, predict whether each token was corrupted
        inputs = {"attention_mask": attention_mask} if self.dattention else {}
        doutputs = self.discriminator(dinputs, labels=dlabels, token_type_ids=token_type_ids, **inputs)

        # Compute combined loss
        loss = goutputs[0] + self.weight * doutputs[0]
        return loss, goutputs[1], doutputs[1], dlabels

    def save_pretrained(self, output, state_dict=None, **kwargs):
        """
        Saves current model to output directory.

        Args:
            output: output directory
            state_dict: model state
            kwargs: additional keyword arguments
        """

        # Save combined model to support training from checkpoints
        super().save_pretrained(output, state_dict, **kwargs)

        # Save generator tokenizer and model
        gpath = os.path.join(output, "generator")
        self.tokenizer.save_pretrained(gpath)
        self.generator.save_pretrained(gpath)

        # Save discriminator tokenizer and model
        dpath = os.path.join(output, "discriminator")
        self.tokenizer.save_pretrained(dpath)
        self.discriminator.save_pretrained(dpath)



================================================
FILE: src/python/txtai/models/pooling/__init__.py
================================================
"""
Pooling imports
"""

from .base import Pooling
from .cls import ClsPooling
from .factory import PoolingFactory
from .late import LatePooling
from .mean import MeanPooling



================================================
FILE: src/python/txtai/models/pooling/base.py
================================================
"""
Pooling module
"""

import numpy as np
import torch

from torch import nn

from ..models import Models


class Pooling(nn.Module):
    """
    Builds pooled vectors usings outputs from a transformers model.
    """

    def __init__(self, path, device, tokenizer=None, maxlength=None, modelargs=None):
        """
        Creates a new Pooling model.

        Args:
            path: path to model, accepts Hugging Face model hub id or local path
            device: tensor device id
            tokenizer: optional path to tokenizer
            maxlength: max sequence length
            modelargs: additional model arguments
        """

        super().__init__()

        self.model = Models.load(path, modelargs=modelargs)
        self.tokenizer = Models.tokenizer(tokenizer if tokenizer else path)
        self.device = Models.device(device)

        # Detect unbounded tokenizer typically found in older models
        Models.checklength(self.model, self.tokenizer)

        # Set max length
        self.maxlength = maxlength if maxlength else self.tokenizer.model_max_length if self.tokenizer.model_max_length != int(1e30) else None

        # Move to device
        self.to(self.device)

    def encode(self, documents, batch=32, category=None):
        """
        Builds an array of pooled embeddings for documents.

        Args:
            documents: list of documents used to build embeddings
            batch: model batch size
            category: embeddings category (query or data)

        Returns:
            pooled embeddings
        """

        # Split documents into batches and process
        results = []

        # Apply pre encoding transformation logic
        documents = self.preencode(documents, category)

        # Sort document indices from largest to smallest to enable efficient batching
        # This performance tweak matches logic in sentence-transformers
        lengths = np.argsort([-len(x) if x else 0 for x in documents])
        documents = [documents[x] for x in lengths]

        for chunk in self.chunk(documents, batch):
            # Tokenize input
            inputs = self.tokenizer(chunk, padding=True, truncation="longest_first", return_tensors="pt", max_length=self.maxlength)

            # Move inputs to device
            inputs = inputs.to(self.device)

            # Run inputs through model
            with torch.no_grad():
                outputs = self.forward(**inputs)

            # Add batch result
            results.extend(outputs.cpu().to(torch.float32).numpy())

        # Apply post encoding transformation logic
        results = self.postencode(results, category)

        # Restore original order and return array
        return np.asarray([results[x] for x in np.argsort(lengths)])

    def chunk(self, texts, size):
        """
        Splits texts into separate batch sizes specified by size.

        Args:
            texts: text elements
            size: batch size

        Returns:
            list of evenly sized batches with the last batch having the remaining elements
        """

        return [texts[x : x + size] for x in range(0, len(texts), size)]

    def forward(self, **inputs):
        """
        Runs inputs through transformers model and returns outputs.

        Args:
            inputs: model inputs

        Returns:
            model outputs
        """

        return self.model(**inputs)[0]

    # pylint: disable=W0613
    def preencode(self, documents, category):
        """
        Applies pre encoding transformation logic.

        Args:
            documents: list of documents used to build embeddings
            category: embeddings category (query or data)
        """

        return documents

    # pylint: disable=W0613
    def postencode(self, results, category):
        """
        Applies post encoding transformation logic.

        Args:
            results: list of results
            category: embeddings category (query or data)

        Returns:
            results with transformation logic applied
        """

        return results



================================================
FILE: src/python/txtai/models/pooling/cls.py
================================================
"""
CLS module
"""

from .base import Pooling


class ClsPooling(Pooling):
    """
    Builds CLS pooled vectors using outputs from a transformers model.
    """

    def forward(self, **inputs):
        """
        Runs CLS pooling on token embeddings.

        Args:
            inputs: model inputs

        Returns:
            CLS pooled embeddings using output token embeddings (i.e. last hidden state)
        """

        # Run through transformers model
        tokens = super().forward(**inputs)

        # CLS token pooling
        return tokens[:, 0]



================================================
FILE: src/python/txtai/models/pooling/factory.py
================================================
"""
Factory module
"""

import json
import os

from huggingface_hub.errors import HFValidationError
from transformers.utils import cached_file

from .base import Pooling
from .cls import ClsPooling
from .late import LatePooling
from .mean import MeanPooling


class PoolingFactory:
    """
    Method to create pooling models.
    """

    @staticmethod
    def create(config):
        """
        Create a Pooling model.

        Args:
            config: pooling configuration

        Returns:
            Pooling
        """

        # Unpack parameters
        method, path, device, tokenizer, maxlength, modelargs = [
            config.get(x) for x in ["method", "path", "device", "tokenizer", "maxlength", "modelargs"]
        ]

        # Derive maxlength, if applicable
        maxlength = PoolingFactory.maxlength(path) if isinstance(maxlength, bool) and maxlength else maxlength

        # Default pooling returns hidden state
        if isinstance(path, bytes) or (isinstance(path, str) and os.path.isfile(path)) or method == "pooling":
            return Pooling(path, device, tokenizer, maxlength, modelargs)

        # Derive pooling method if it's not specified and path is a string
        if (not method or method not in ("clspooling", "meanpooling", "latepooling")) and isinstance(path, str):
            method = PoolingFactory.method(path)

        # Check for cls pooling
        if method == "clspooling":
            return ClsPooling(path, device, tokenizer, maxlength, modelargs)

        # Check for late pooling
        if method == "latepooling":
            return LatePooling(path, device, tokenizer, maxlength, modelargs)

        # Default to mean pooling
        return MeanPooling(path, device, tokenizer, maxlength, modelargs)

    @staticmethod
    def method(path):
        """
        Determines the pooling method using the sentence transformers pooling config.

        Args:
            path: model path

        Returns:
            pooling method
        """

        # Default method
        method = "meanpooling"

        # Load 1_Pooling/config.json file
        config = PoolingFactory.load(path, "1_Pooling/config.json")

        # Set to CLS pooling if it's enabled and mean pooling is disabled
        if config and config["pooling_mode_cls_token"] and not config["pooling_mode_mean_tokens"]:
            method = "clspooling"

        # Check for late interaction pooling
        if not config:
            # Load 1_Dense/config.json
            config = PoolingFactory.load(path, "1_Dense/config.json")
            if config:
                method = "latepooling"

            # Load config.json and check architecture
            else:
                config = PoolingFactory.load(path, "config.json")
                if config and "HF_ColBERT" in config.get("architectures", []):
                    method = "latepooling"

        return method

    @staticmethod
    def maxlength(path):
        """
        Reads the max_seq_length parameter from sentence transformers config.

        Args:
            path: model path

        Returns:
            max sequence length
        """

        # Default length is unset
        maxlength = None

        # Read max_seq_length from sentence_bert_config.json
        config = PoolingFactory.load(path, "sentence_bert_config.json")
        maxlength = config.get("max_seq_length") if config else maxlength

        return maxlength

    @staticmethod
    def load(path, name):
        """
        Loads a JSON config file from the Hugging Face Hub.

        Args:
            path: model path
            name: file to load

        Returns:
            config
        """

        # Download file and parse JSON
        config = None
        try:
            path = cached_file(path_or_repo_id=path, filename=name)
            if path:
                with open(path, encoding="utf-8") as f:
                    config = json.load(f)

        # Ignore this error - invalid repo or directory
        except (HFValidationError, OSError):
            pass

        return config



================================================
FILE: src/python/txtai/models/pooling/late.py
================================================
"""
Late module
"""

import json

import numpy as np
import torch

from huggingface_hub.errors import HFValidationError
from safetensors import safe_open
from torch import nn
from transformers.utils import cached_file

from .base import Pooling
from .muvera import Muvera


class LatePooling(Pooling):
    """
    Builds late pooled vectors using outputs from a transformers model.
    """

    def __init__(self, path, device, tokenizer=None, maxlength=None, modelargs=None):
        # Check if fixed dimensional encoder is enabled
        modelargs = modelargs.copy() if modelargs else {}
        muvera = modelargs.pop("muvera", {})
        self.encoder = Muvera(**muvera) if muvera is not None else None

        # Call parent initialization
        super().__init__(path, device, tokenizer, maxlength, modelargs)

        # Get linear weights path
        config = self.load(path, "1_Dense/config.json")
        if config:
            # PyLate weights format
            name = "1_Dense/model.safetensors"
        else:
            # Stanford weights format
            name = "model.safetensors"

        # Read model settings
        self.qprefix, self.qlength, self.dprefix, self.dlength = self.settings(path, config)

        # Load linear layer
        path = cached_file(path_or_repo_id=path, filename=name)
        with safe_open(filename=path, framework="pt") as f:
            weights = f.get_tensor("linear.weight")

            # Load weights into linear layer
            self.linear = nn.Linear(weights.shape[1], weights.shape[0], bias=False, device=self.device, dtype=weights.dtype)
            with torch.no_grad():
                self.linear.weight.copy_(weights)

    def forward(self, **inputs):
        """
        Runs late pooling on token embeddings.

        Args:
            inputs: model inputs

        Returns:
            Late pooled embeddings using output token embeddings (i.e. last hidden state)
        """

        # Run through transformers model
        tokens = super().forward(**inputs)

        # Run through final linear layer and return
        return self.linear(tokens)

    def preencode(self, documents, category):
        """
        Apply prefixes and lengths to data.

        Args:
            documents: list of documents used to build embeddings
            category: embeddings category (query or data)
        """

        results = []

        # Apply prefix
        for text in documents:
            prefix = self.qprefix if category == "query" else self.dprefix
            if prefix:
                text = f"{prefix}{text}"

            results.append(text)

        # Set maxlength
        maxlength = self.qlength if category == "query" else self.dlength
        if maxlength:
            self.maxlength = maxlength

        return results

    def postencode(self, results, category):
        """
        Normalizes and pads results.

        Args:
            results: input results

        Returns:
            normalized results with padding
        """

        length = 0
        for vectors in results:
            # Get max length
            if vectors.shape[0] > length:
                length = vectors.shape[0]

            # Normalize vectors
            vectors /= np.linalg.norm(vectors, axis=1)[:, np.newaxis]

        # Pad values
        data = []
        for vectors in results:
            data.append(np.pad(vectors, [(0, length - vectors.shape[0]), (0, 0)]))

        # Build NumPy array
        data = np.asarray(data)

        # Apply fixed dimesional encoder, if necessary
        return self.encoder(data, category) if self.encoder else data

    def settings(self, path, config):
        """
        Reads model settings.

        Args:
            path: model path
            config: PyLate model format if provided, otherwise read from Stanford format
        """

        if config:
            # PyLate format
            config = self.load(path, "config_sentence_transformers.json")
            params = ["query_prefix", "query_length", "document_prefix", "document_length"]
        else:
            # Stanford format
            config = self.load(path, "artifact.metadata")
            params = ["query_token_id", "query_maxlen", "doc_token_id", "doc_maxlen"]

        return [config.get(p) for p in params]

    def load(self, path, name):
        """
        Loads a JSON config file from the Hugging Face Hub.

        Args:
            path: model path
            name: file to load

        Returns:
            config
        """

        # Download file and parse JSON
        config = None
        try:
            path = cached_file(path_or_repo_id=path, filename=name)
            if path:
                with open(path, encoding="utf-8") as f:
                    config = json.load(f)

        # Ignore this error - invalid repo or directory
        except (HFValidationError, OSError):
            pass

        return config



================================================
FILE: src/python/txtai/models/pooling/mean.py
================================================
"""
Mean module
"""

import torch

from .base import Pooling


class MeanPooling(Pooling):
    """
    Builds mean pooled vectors usings outputs from a transformers model.
    """

    def forward(self, **inputs):
        """
        Runs mean pooling on token embeddings taking the input mask into account.

        Args:
            inputs: model inputs

        Returns:
            mean pooled embeddings using output token embeddings (i.e. last hidden state)
        """

        # Run through transformers model
        tokens = super().forward(**inputs)
        mask = inputs["attention_mask"]

        # Mean pooling
        # pylint: disable=E1101
        mask = mask.unsqueeze(-1).expand(tokens.size()).float()
        return torch.sum(tokens * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)



================================================
FILE: src/python/txtai/models/pooling/muvera.py
================================================
"""
Muvera module
"""

import numpy as np


class Muvera:
    """
    Implements the MUVERA (Multi-Vector Retrieval via Fixed Dimensional Encodings) algorithm. This reduces late interaction multi-vector
    outputs to a single fixed vector.

    The size of the output vectors are set using the following parameters

        output dimensions = repetitions * 2^hashes * projected

    For example, the default parameters create vectors with the following output dimensions.

        output dimensions = 20 * 2^5 * 16 = 10240

    This code is based on the following:
      - Paper: https://arxiv.org/abs/2405.19504
      - GitHub: https://github.com/google/graph-mining/tree/main/sketching/point_cloud
      - Python port of the original C++ code: https://github.com/sigridjineth/muvera-py
    """

    def __init__(self, repetitions=20, hashes=5, projection=16, seed=42):
        """
        Creates a Muvera instance.

        Args:
            repetitions: number of iterations
            hashes: number of simhash partitions as 2^hashes
            projection: dimensionality reduction, uses an identity projection when set to None
            seed: random seed
        """

        # Number of repetitions
        self.repetitions = repetitions

        # Number of simhash projections
        self.hashes = hashes

        # Optional number of projected dimensions
        self.projection = projection

        # Seed
        self.seed = seed

    def __call__(self, data, category):
        """
        Transforms a list of multi-vector collections into single fixed vector outputs.

        Args:
            data: array of multi-vector vectors
            category: embeddings category (query or data)
        """

        # Get stats
        dimension, length = data[0].shape[1], len(data)

        # Determine projection dimension
        identity = not self.projection
        projection = dimension if identity else self.projection

        # Number of simhash partitions
        partitions = 2**self.hashes

        # Document tracking
        lengths = np.array([len(doc) for doc in data], dtype=np.int32)
        total = np.sum(lengths)
        documents = np.repeat(np.arange(length), lengths)

        # Stack all vectors
        points = np.vstack(data).astype(np.float32)

        # Output vectors
        size = self.repetitions * partitions * projection
        vectors = np.zeros((length, size), dtype=np.float32)

        # Process each repetition
        for number in range(self.repetitions):
            seed = self.seed + number

            # Calculate the simhash
            sketches = points @ self.random(dimension, self.hashes, seed)

            # Dimensionality reduction, if necessary
            projected = points if identity else (points @ self.reducer(dimension, projection, seed))

            # Get partition indices
            bits = (sketches > 0).astype(np.uint32)
            indices = np.zeros(total, dtype=np.uint32)

            # Calculate vector indices
            for x in range(self.hashes):
                indices = (indices << 1) + (bits[:, x] ^ (indices & 1))

            # Initialize storage
            fdesum = np.zeros((length * partitions * projection,), dtype=np.float32)
            counts = np.zeros((length, partitions), dtype=np.int32)

            # Count vectors per partition per document
            np.add.at(counts, (documents, indices), 1)

            # Aggregate vectors using flattened indexing for efficiency
            part = documents * partitions + indices
            base = part * projection

            for d in range(projection):
                flat = base + d
                np.add.at(fdesum, flat, projected[:, d])

            # Reshape for easier manipulation
            # pylint: disable=E1121
            fdesum = fdesum.reshape(length, partitions, projection)

            # Convert sums to averages for data category
            if category == "data":
                # Safe division (avoid divide by zero)
                counts = counts[:, :, np.newaxis]
                np.divide(fdesum, counts, out=fdesum, where=counts > 0)

            # Save results
            start = number * partitions * projection
            vectors[:, start : start + partitions * projection] = fdesum.reshape(length, -1)

        return vectors

    def random(self, dimension, projection, seed):
        """
        Generates a random matrix for simhash projections.

        Args:
            dimensions: number of dimensions for input vectors
            projections: number of projection dimensions
            seed: random seed

        Returns:
            random matrix for simhash projections
        """

        rng = np.random.default_rng(seed)
        return rng.normal(loc=0.0, scale=1.0, size=(dimension, projection)).astype(np.float32)

    def reducer(self, dimension, projection, seed):
        """
        Generates a random matrix for dimensionality reduction using the AMS sketch algorithm.

        Args:
            dimension: number of input dimensions
            projected: number of dimensions to project inputs to

        Returns:
            Dimensionality reduced matrix
        """

        rng = np.random.default_rng(seed)
        out = np.zeros((dimension, projection), dtype=np.float32)
        indices = rng.integers(0, projection, size=dimension)
        signs = rng.choice([-1.0, 1.0], size=dimension)
        out[np.arange(dimension), indices] = signs

        return out



================================================
FILE: src/python/txtai/pipeline/__init__.py
================================================
"""
Pipeline imports
"""

from .audio import *
from .base import Pipeline
from .data import *
from .factory import PipelineFactory
from .hfmodel import HFModel
from .hfpipeline import HFPipeline
from .image import *
from .llm import *
from .llm import RAG as Extractor
from .nop import Nop
from .text import *
from .tensors import Tensors
from .train import *



================================================
FILE: src/python/txtai/pipeline/base.py
================================================
"""
Pipeline module
"""


class Pipeline:
    """
    Base class for all Pipelines. The only interface requirement is to define a __call___ method.
    """

    def batch(self, data, size):
        """
        Splits data into separate batch sizes specified by size.

        Args:
            data: data elements
            size: batch size

        Returns:
            list of evenly sized batches with the last batch having the remaining elements
        """

        return [data[x : x + size] for x in range(0, len(data), size)]



================================================
FILE: src/python/txtai/pipeline/factory.py
================================================
"""
Pipeline factory module
"""

import inspect
import sys
import types

from ..util import Resolver

from .base import Pipeline


class PipelineFactory:
    """
    Pipeline factory. Creates new Pipeline instances.
    """

    @staticmethod
    def get(pipeline):
        """
        Gets a new instance of pipeline class.

        Args:
            pclass: Pipeline instance class

        Returns:
            Pipeline class
        """

        # Local pipeline if no package
        if "." not in pipeline:
            return PipelineFactory.list()[pipeline]

        # Attempt to load custom pipeline
        return Resolver()(pipeline)

    @staticmethod
    def create(config, pipeline):
        """
        Creates a new Pipeline instance.

        Args:
            config: Pipeline configuration
            pipeline: Pipeline instance class

        Returns:
            Pipeline
        """

        # Resolve pipeline
        pipeline = PipelineFactory.get(pipeline)

        # Return functions directly, otherwise create pipeline instance
        return pipeline if isinstance(pipeline, types.FunctionType) else pipeline(**config)

    @staticmethod
    def list():
        """
        Lists callable pipelines.

        Returns:
            {short name: pipeline class}
        """

        pipelines = {}

        # Get handle to pipeline module
        pipeline = sys.modules[".".join(__name__.split(".")[:-1])]

        # Get list of callable pipelines
        for x in inspect.getmembers(pipeline, inspect.isclass):
            if issubclass(x[1], Pipeline) and [y for y, _ in inspect.getmembers(x[1], inspect.isfunction) if y == "__call__"]:
                # short name: pipeline class
                pipelines[x[0].lower()] = x[1]

        return pipelines



================================================
FILE: src/python/txtai/pipeline/hfmodel.py
================================================
"""
Hugging Face Transformers model wrapper module
"""

from ..models import Models
from .tensors import Tensors


class HFModel(Tensors):
    """
    Pipeline backed by a Hugging Face Transformers model.
    """

    def __init__(self, path=None, quantize=False, gpu=False, batch=64):
        """
        Creates a new HFModel.

        Args:
            path: optional path to model, accepts Hugging Face model hub id or local path,
                  uses default model for task if not provided
            quantize: if model should be quantized, defaults to False
            gpu: True/False if GPU should be enabled, also supports a GPU device id
            batch: batch size used to incrementally process content
        """

        # Default model path
        self.path = path

        # Quantization flag
        self.quantization = quantize

        # Get tensor device reference
        self.deviceid = Models.deviceid(gpu)
        self.device = Models.device(self.deviceid)

        # Process batch size
        self.batchsize = batch

    def prepare(self, model):
        """
        Prepares a model for processing. Applies dynamic quantization if necessary.

        Args:
            model: input model

        Returns:
            model
        """

        if self.deviceid == -1 and self.quantization:
            model = self.quantize(model)

        return model

    def tokenize(self, tokenizer, texts):
        """
        Tokenizes text using tokenizer. This method handles overflowing tokens and automatically splits
        them into separate elements. Indices of each element is returned to allow reconstructing the
        transformed elements after running through the model.

        Args:
            tokenizer: Tokenizer
            texts: list of text

        Returns:
            (tokenization result, indices)
        """

        # Pre-process and split on newlines
        batch, positions = [], []
        for x, text in enumerate(texts):
            elements = [t + " " for t in text.split("\n") if t]
            batch.extend(elements)
            positions.extend([x] * len(elements))

        # Run tokenizer
        tokens = tokenizer(batch, padding=True)

        inputids, attention, indices = [], [], []
        for x, ids in enumerate(tokens["input_ids"]):
            if len(ids) > tokenizer.model_max_length:
                # Remove padding characters, if any
                ids = [i for i in ids if i != tokenizer.pad_token_id]

                # Split into model_max_length chunks
                for chunk in self.batch(ids, tokenizer.model_max_length - 1):
                    # Append EOS token if necessary
                    if chunk[-1] != tokenizer.eos_token_id:
                        chunk.append(tokenizer.eos_token_id)

                    # Set attention mask
                    mask = [1] * len(chunk)

                    # Append padding if necessary
                    if len(chunk) < tokenizer.model_max_length:
                        pad = tokenizer.model_max_length - len(chunk)
                        chunk.extend([tokenizer.pad_token_id] * pad)
                        mask.extend([0] * pad)

                    inputids.append(chunk)
                    attention.append(mask)
                    indices.append(positions[x])
            else:
                inputids.append(ids)
                attention.append(tokens["attention_mask"][x])
                indices.append(positions[x])

        tokens = {"input_ids": inputids, "attention_mask": attention}

        # pylint: disable=E1102
        return ({name: self.tensor(tensor).to(self.device) for name, tensor in tokens.items()}, indices)



================================================
FILE: src/python/txtai/pipeline/hfpipeline.py
================================================
"""
Hugging Face Transformers pipeline wrapper module
"""

import inspect

from transformers import pipeline

from ..models import Models
from ..util import Resolver

from .tensors import Tensors


class HFPipeline(Tensors):
    """
    Light wrapper around Hugging Face Transformers pipeline component for selected tasks. Adds support for model
    quantization and minor interface changes.
    """

    def __init__(self, task, path=None, quantize=False, gpu=False, model=None, **kwargs):
        """
        Loads a new pipeline model.

        Args:
            task: pipeline task or category
            path: optional path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple.
                  uses default model for task if not provided
            quantize: if model should be quantized, defaults to False
            gpu: True/False if GPU should be enabled, also supports a GPU device id
            model: optional existing pipeline model to wrap
            kwargs: additional keyword arguments to pass to pipeline model
        """

        if model:
            # Check if input model is a Pipeline or a HF pipeline
            self.pipeline = model.pipeline if isinstance(model, HFPipeline) else model
        else:
            # Get device
            deviceid = Models.deviceid(gpu) if "device_map" not in kwargs else None
            device = Models.device(deviceid) if deviceid is not None else None

            # Split into model args, pipeline args
            modelargs, kwargs = self.parseargs(**kwargs)

            # Transformer pipeline task
            if isinstance(path, (list, tuple)):
                # Derive configuration, if possible
                config = path[1] if path[1] and isinstance(path[1], str) else None

                # Load model
                model = Models.load(path[0], config, task)

                self.pipeline = pipeline(task, model=model, tokenizer=path[1], device=device, model_kwargs=modelargs, **kwargs)
            else:
                self.pipeline = pipeline(task, model=path, device=device, model_kwargs=modelargs, **kwargs)

            # Model quantization. Compresses model to int8 precision, improves runtime performance. Only supported on CPU.
            if deviceid == -1 and quantize:
                # pylint: disable=E1101
                self.pipeline.model = self.quantize(self.pipeline.model)

        # Detect unbounded tokenizer typically found in older models
        Models.checklength(self.pipeline.model, self.pipeline.tokenizer)

    def parseargs(self, **kwargs):
        """
        Inspects the pipeline method and splits kwargs into model args and pipeline args.

        Args:
            kwargs: all keyword arguments

        Returns:
            (model args, pipeline args)
        """

        # Get pipeline method arguments
        args = inspect.getfullargspec(pipeline).args

        # Resolve torch dtype, if necessary
        dtype = kwargs.get("torch_dtype")
        if dtype and isinstance(dtype, str) and dtype != "auto":
            kwargs["torch_dtype"] = Resolver()(dtype)

        # Split into modelargs and kwargs
        return ({arg: value for arg, value in kwargs.items() if arg not in args}, {arg: value for arg, value in kwargs.items() if arg in args})

    def maxlength(self):
        """
        Gets the max length to use for generate calls.

        Returns:
            max length
        """

        return Models.maxlength(self.pipeline.model, self.pipeline.tokenizer)



================================================
FILE: src/python/txtai/pipeline/nop.py
================================================
"""
No-Op module
"""

from .base import Pipeline


class Nop(Pipeline):
    """
    Simple no-op pipeline that returns inputs
    """

    def __call__(self, inputs):
        return inputs



================================================
FILE: src/python/txtai/pipeline/tensors.py
================================================
"""
Tensor processing framework module
"""

import torch

from .base import Pipeline


class Tensors(Pipeline):
    """
    Pipeline backed by a tensor processing framework. Currently supports PyTorch.
    """

    def quantize(self, model):
        """
        Quantizes input model and returns. This only is supported for CPU devices.

        Args:
            model: torch model

        Returns:
            quantized torch model
        """

        # pylint: disable=E1101
        return torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

    def tensor(self, data):
        """
        Creates a tensor array.

        Args:
            data: input data

        Returns:
            tensor
        """

        # pylint: disable=E1102
        return torch.tensor(data)

    def context(self):
        """
        Defines a context used to wrap processing with the tensor processing framework.

        Returns:
            processing context
        """

        # pylint: disable=E1101
        return torch.no_grad()



================================================
FILE: src/python/txtai/pipeline/audio/__init__.py
================================================
"""
Audio imports
"""

from .audiomixer import AudioMixer
from .audiostream import AudioStream
from .microphone import Microphone
from .signal import Signal
from .texttoaudio import TextToAudio
from .texttospeech import TextToSpeech
from .transcription import Transcription



================================================
FILE: src/python/txtai/pipeline/audio/audiomixer.py
================================================
"""
AudioMixer module
"""

from ..base import Pipeline
from .signal import Signal, SCIPY


class AudioMixer(Pipeline):
    """
    Mixes multiple audio streams into a single stream.
    """

    def __init__(self, rate=None):
        """
        Creates an AudioMixer pipeline.

        Args:
            rate: optional target sample rate, otherwise uses input target rate with each audio segment
        """

        if not SCIPY:
            raise ImportError('AudioMixer pipeline is not available - install "pipeline" extra to enable.')

        # Target sample rate
        self.rate = rate

    def __call__(self, segment, scale1=1, scale2=1):
        """
        Mixes multiple audio streams into a single stream.

        Args:
            segment: ((audio1, sample rate), (audio2, sample rate))|list
            scale1: optional scaling factor for segment1
            scale2: optional scaling factor for segment2

        Returns:
            list of (audio, sample rate)
        """

        # Convert single element to list
        segments = [segment] if isinstance(segment, tuple) else segment

        results = []
        for segment1, segment2 in segments:
            audio1, rate1 = segment1
            audio2, rate2 = segment2

            # Resample audio, as necessary
            target = self.rate if self.rate else rate1
            audio1 = Signal.resample(audio1, rate1, target)
            audio2 = Signal.resample(audio2, rate2, target)

            # Mix audio into single segment
            results.append((Signal.mix(audio1, audio2, scale1, scale2), target))

        # Return single element if single element passed in
        return results[0] if isinstance(segment, tuple) else results



================================================
FILE: src/python/txtai/pipeline/audio/audiostream.py
================================================
"""
AudioStream module
"""

from queue import Queue
from threading import Thread

# Conditional import
try:
    import sounddevice as sd

    from .signal import Signal, SCIPY

    AUDIOSTREAM = SCIPY
except (ImportError, OSError):
    AUDIOSTREAM = False

from ..base import Pipeline


class AudioStream(Pipeline):
    """
    Threaded pipeline that streams audio segments to an output audio device. This pipeline is designed
    to run on local machines given that it requires access to write to an output device.
    """

    # End of stream message
    COMPLETE = (1, None)

    def __init__(self, rate=None):
        """
        Creates an AudioStream pipeline.

        Args:
            rate: optional target sample rate, otherwise uses input target rate with each audio segment
        """

        if not AUDIOSTREAM:
            raise ImportError(
                (
                    'AudioStream pipeline is not available - install "pipeline" extra to enable. '
                    "Also check that the portaudio system library is available."
                )
            )

        # Target sample rate
        self.rate = rate

        self.queue = Queue()
        self.thread = Thread(target=self.play)
        self.thread.start()

    def __call__(self, segment):
        """
        Queues audio segments for the audio player.

        Args:
            segment: (audio, sample rate)|list

        Returns:
            segment
        """

        # Convert single element to list
        segments = [segment] if isinstance(segment, tuple) else segment

        for x in segments:
            self.queue.put(x)

        # Return single element if single element passed in
        return segments[0] if isinstance(segment, tuple) else segments

    def wait(self):
        """
        Waits for all input audio segments to be played.
        """

        self.thread.join()

    def play(self):
        """
        Reads audio segments from queue. This method runs in a separate non-blocking thread.
        """

        audio, rate = self.queue.get()
        while not isinstance(audio, int) or (audio, rate) != AudioStream.COMPLETE:
            # Resample to target sample rate, if necessary
            audio, rate = (Signal.resample(audio, rate, self.rate), self.rate) if self.rate else (audio, rate)

            # Play audio segment
            sd.play(audio, rate, blocking=True)

            # Get next segment
            audio, rate = self.queue.get()



================================================
FILE: src/python/txtai/pipeline/audio/microphone.py
================================================
"""
Microphone module
"""

import logging

import numpy as np

# Conditional import
try:
    import sounddevice as sd
    import webrtcvad

    from scipy.signal import butter, sosfilt

    from .signal import Signal, SCIPY

    MICROPHONE = SCIPY
except (ImportError, OSError):
    MICROPHONE = False

from ..base import Pipeline

# Logging configuration
logger = logging.getLogger(__name__)


class Microphone(Pipeline):
    """
    Reads input speech from a microphone device. This pipeline is designed to run on local machines given
    that it requires access to read from an input device.
    """

    def __init__(self, rate=16000, vadmode=3, vadframe=20, vadthreshold=0.6, voicestart=300, voiceend=3400, active=5, pause=8):
        """
        Creates a new Microphone pipeline.

        Args:
            rate: sample rate to record audio in, defaults to 16000 (16 kHz)
            vadmode: aggressiveness of the voice activity detector (1 - 3), defaults to 3, which is the most aggressive filter
            vadframe: voice activity detector frame size in ms, defaults to 20
            vadthreshold: percentage of frames (0.0 - 1.0) that must be voice to be considered speech, defaults to 0.6
            voicestart: starting frequency to use for voice filtering, defaults to 300
            voiceend: ending frequency to use for voice filtering, defaults to 3400
            active: minimum number of active speech chunks to require before considering this speech, defaults to 5
            pause: number of non-speech chunks to keep before considering speech complete, defaults to 8
        """

        if not MICROPHONE:
            raise ImportError(
                (
                    'Microphone pipeline is not available - install "pipeline" extra to enable. '
                    "Also check that the portaudio system library is available."
                )
            )

        # Sample rate
        self.rate = rate

        # Voice activity detector
        self.vad = webrtcvad.Vad(vadmode)
        self.vadframe = vadframe
        self.vadthreshold = vadthreshold

        # Voice spectrum
        self.voicestart = voicestart
        self.voiceend = voiceend

        # Audio chunks counts
        self.active = active
        self.pause = pause

    def __call__(self, device=None):
        """
        Reads audio from an input device.

        Args:
            device: optional input device id, otherwise uses system default

        Returns:
            list of (audio, sample rate)
        """

        # Listen for audio
        audio = self.listen(device[0] if isinstance(device, list) else device)

        # Return single element if single element passed in
        return (audio, self.rate) if device is None or not isinstance(device, list) else [(audio, self.rate)]

    def listen(self, device):
        """
        Listens for speech. Detected speech is converted to 32-bit floats for compatibility with
        automatic speech recognition (ASR) pipelines.

        This method blocks until speech is detected.

        Args:
            device: input device

        Returns:
            audio
        """

        # Record in 100ms chunks
        chunksize = self.rate // 10

        # Open input stream
        stream = sd.RawInputStream(device=device, samplerate=self.rate, channels=1, blocksize=chunksize, dtype=np.int16)

        # Start the input stream
        stream.start()

        record, speech, nospeech, chunks = True, 0, 0, []
        while record:
            # Read chunk
            chunk, _ = stream.read(chunksize)

            # Detect speech using WebRTC VAD for audio chunk
            detect = self.detect(chunk)
            speech = speech + 1 if detect else speech
            nospeech = 0 if detect else nospeech + 1

            # Save chunk, if this is an active stream
            if speech:
                chunks.append(chunk)

                # Pause limit has been reached, check if this audio should be accepted
                if nospeech >= self.pause:
                    logger.debug("Audio detected and being analyzed")
                    if speech >= self.active and self.isspeech(chunks[:-nospeech]):
                        # Disable recording
                        record = False
                    else:
                        # Reset parameters and keep recording
                        logger.debug("Speech not detected")
                        speech, nospeech, chunks = 0, 0, []

        # Stop the input stream
        stream.stop()

        # Convert to float32 and return
        audio = np.frombuffer(b"".join(chunks), np.int16)
        return Signal.float32(audio)

    def isspeech(self, chunks):
        """
        Runs an ensemble of Voice Activity Detection (VAD) methods. Returns true if speech is
        detected in the input audio chunks.

        Args:
            chunks: input audio chunks as byte buffers

        Returns:
            True if speech is detected, False otherwise
        """

        # Convert to NumPy array for processing
        audio = np.frombuffer(b"".join(chunks), dtype=np.int16)

        # Ensemble of:
        #  - WebRTC VAD with a human voice range butterworth bandpass filter applied to the signal
        #  - FFT applied to detect the energy ratio for human voice range vs total range
        return self.detectband(audio) and self.detectenergy(audio)

    def detect(self, buffer):
        """
        Detect speech using the WebRTC Voice Activity Detector (VAD).

        Args:
            buffer: input audio buffer frame as bytes

        Returns:
            True if the number of audio frames with audio pass vadthreshold, False otherwise
        """

        n = int(self.rate * (self.vadframe / 1000.0) * 2)
        offset = 0

        detects = []
        while offset + n <= len(buffer):
            detects.append(1 if self.vad.is_speech(buffer[offset : offset + n], self.rate) else 0)
            offset += n

        # Calculate detection ratio and return
        ratio = sum(detects) / len(detects) if detects else 0
        if ratio > 0:
            logger.debug("DETECT %.4f", ratio)

        return ratio >= self.vadthreshold

    def detectband(self, audio):
        """
        Detects speech using audio data filtered through a butterworth band filter
        with the human voice range.

        Args:
            audio: input audio data as an NumPy array

        Returns:
            True if speech is detected, False otherwise
        """

        # Upsample to float32
        audio = Signal.float32(audio)

        # Human voice frequency range
        low = self.voicestart / (0.5 * self.rate)
        high = self.voiceend / (0.5 * self.rate)

        # Low and high pass filter using human voice range
        sos = butter(5, Wn=[low, high], btype="band", output="sos")
        audio = sosfilt(sos, audio)

        # Scale back to int16
        audio = Signal.int16(audio)

        # Pass filtered signal to WebRTC VAD
        return self.detect(audio.tobytes())

    def detectenergy(self, audio):
        """
        Detects speech by comparing the signal energy of the human voice range
        to the overall signal energy.

        Args:
            audio: input audio data as an NumPy array

        Returns:
            True if speech is detected, False otherwise
        """

        # Calculate signal energy
        energyfreq = Signal.energy(audio, self.rate)

        # Sum speech energy
        speechenergy = 0
        for f, e in energyfreq.items():
            if self.voicestart <= f <= self.voiceend:
                speechenergy += e

        # Calculate ratio of speech energy to total energy and return
        ratio = speechenergy / sum(energyfreq.values())
        logger.debug("SPEECH %.4f", ratio)
        return ratio >= self.vadthreshold



================================================
FILE: src/python/txtai/pipeline/audio/signal.py
================================================
"""
Signal module
"""

import numpy as np

# Conditional import
try:
    from scipy import signal
    from scipy.fft import rfft, rfftfreq

    SCIPY = True
except ImportError:
    SCIPY = False


class Signal:
    """
    Utility methods for audio signal processing.
    """

    @staticmethod
    def mono(audio):
        """
        Convert stereo to mono audio.

        Args:
            audio: audio data

        Returns:
            audio data with a single channel
        """

        return audio.mean(axis=1) if len(audio.shape) > 1 else audio

    @staticmethod
    def resample(audio, rate, target):
        """
        Resample audio if the sample rate doesn't match the target sample rate.

        Args:
            audio: audio data
            rate: current sample rate
            target: target sample rate

        Returns:
            audio resampled if necessary or original audio
        """

        if rate != target:
            # Transpose audio
            audio = audio.T

            # Resample audio and tranpose back
            samples = round(len(audio) * float(target) / rate)
            audio = signal.resample(audio, samples).T

        return audio

    @staticmethod
    def float32(audio):
        """
        Converts an input NumPy array with 16-bit ints to 32-bit floats.

        Args:
            audio: input audio array as 16-bit ints

        Returns:
            audio array as 32-bit floats
        """

        i = np.iinfo(audio.dtype)
        abs_max = 2 ** (i.bits - 1)
        offset = i.min + abs_max
        return (audio.astype(np.float32) - offset) / abs_max

    @staticmethod
    def int16(audio):
        """
        Converts an input NumPy array with 32-bit floats to 16-bit ints.

        Args:
            audio: input audio array as 32-bit floats

        Returns:
            audio array as 16-bit ints
        """

        i = np.iinfo(np.int16)
        absmax = 2 ** (i.bits - 1)
        offset = i.min + absmax
        return (audio * absmax + offset).clip(i.min, i.max).astype(np.int16)

    @staticmethod
    def mix(audio1, audio2, scale1=1, scale2=1):
        """
        Mixes audio1 and audio 2 into a single output audio segment.

        Args:
            audio1: audio segment 1
            audio2: audio segment 2
            scale1: scale factor for audio segment 1
            scale2: scale factor for audio segment 2
        """

        # Reshape audio, as necessary
        audio1 = audio1.reshape(1, -1) if len(audio1.shape) <= 1 else audio1
        audio2 = audio2.reshape(1, -1) if len(audio2.shape) <= 1 else audio2

        # Scale audio
        audio1 = audio1 * scale1
        audio2 = audio2 * scale2

        # Make audio files the same length
        large, small = (audio1, audio2) if audio1.shape[1] > audio2.shape[1] else (audio2, audio1)
        small = np.tile(small, (large.shape[1] // small.shape[1]) + 1).take(axis=1, indices=range(0, large.shape[1]))

        # Mix audio together
        return small + large

    @staticmethod
    def energy(audio, rate):
        """
        Calculates the signal energy for the input audio. Energy is defined as:

          Energy = 2 * Signal Amplitude

        Args:
            audio: audio data
            rate: sample rate

        Returns:
            {frequency: energy at that frequency}
        """

        # Calculate signal frequency
        frequency = rfftfreq(len(audio), 1.0 / rate)
        frequency = frequency[1:]

        # Calculate signal energy using amplitude
        energy = np.abs(rfft(audio))
        energy = energy[1:]
        energy = energy**2

        # Get energy for each frequency
        energyfreq = {}
        for x, freq in enumerate(frequency):
            if abs(freq) not in energyfreq:
                energyfreq[abs(freq)] = energy[x] * 2

        return energyfreq

    @staticmethod
    def trim(audio, rate, threshold=1, leading=True, trailing=True):
        """
        Removes leading and trailing silence from audio data.

        Args:
            audio: audio data
            rate: sample rate
            threshold: energy below this level will be considered silence, defaults to 1.0
            leading: trim leading silence, defaults to True
            trailing: trim trailing silence, defauls to True

        Returns:
            audio with silence removed
        """

        # Process in 20ms chunks
        n, offset = int(rate * (20 / 1000.0) * 2), 0

        chunks = []
        while offset + n <= len(audio):
            # Calculate energy for chunk and detection result
            chunk = audio[offset : offset + n]
            energyfreq = Signal.energy(chunk, rate)
            chunks.append((chunk, sum(energyfreq.values()) >= threshold))

            offset += n

        # Find first and last active chunks
        start = next((i for i, (_, active) in enumerate(chunks) if active), 0) if leading else 0
        end = (len(chunks) - next((i for i, (_, active) in enumerate(chunks[::-1]) if active), 0)) if trailing else len(chunks)

        # Concatenate active audio
        return np.concatenate([chunk for chunk, _ in chunks[start:end]])



================================================
FILE: src/python/txtai/pipeline/audio/texttoaudio.py
================================================
"""
TextToAudio module
"""

from ..hfpipeline import HFPipeline
from .signal import Signal, SCIPY


class TextToAudio(HFPipeline):
    """
    Generates audio from text.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, rate=None, **kwargs):
        if not SCIPY:
            raise ImportError('TextToAudio pipeline is not available - install "pipeline" extra to enable.')

        # Call parent constructor
        super().__init__("text-to-audio", path, quantize, gpu, model, **kwargs)

        # Target sample rate, defaults to model sample rate
        self.rate = rate

    def __call__(self, text, maxlength=512):
        """
        Generates audio from text.

        This method supports text as a string or a list. If the input is a string,
        the return type is a single audio output. If text is a list, the return type is a list.

        Args:
            text: text|list
            maxlength: maximum audio length to generate

        Returns:
            list of (audio, sample rate)
        """

        # Format inputs
        texts = [text] if isinstance(text, str) else text

        # Run pipeline
        results = [self.convert(x) for x in self.pipeline(texts, forward_params={"max_new_tokens": maxlength})]

        # Extract results
        return results[0] if isinstance(text, str) else results

    def convert(self, result):
        """
        Converts audio result to target sample rate for this pipeline, if set.

        Args:
            result: dict with audio samples and sample rate

        Returns:
            (audio, sample rate)
        """

        audio, rate = result["audio"].squeeze(), result["sampling_rate"]
        return (Signal.resample(audio, rate, self.rate), self.rate) if self.rate else (audio, rate)



================================================
FILE: src/python/txtai/pipeline/audio/texttospeech.py
================================================
"""
TextToSpeech module
"""

# Conditional import
try:
    import onnxruntime as ort
    import soundfile as sf

    from ttstokenizer import IPATokenizer, TTSTokenizer

    from .signal import Signal, SCIPY

    TTS = SCIPY
except ImportError:
    TTS = False

import json
import logging

from io import BytesIO

import torch
import yaml

import numpy as np

from huggingface_hub.errors import HFValidationError
from transformers import SpeechT5Processor
from transformers.utils import cached_file

from ..base import Pipeline

# Logging configuration
logger = logging.getLogger(__name__)


class TextToSpeech(Pipeline):
    """
    Generates speech from text.
    """

    def __init__(self, path=None, maxtokens=512, rate=22050):
        """
        Creates a new TextToSpeech pipeline.

        Args:
            path: optional model path
            maxtokens: maximum number of tokens model can process, defaults to 512
            rate: target sample rate, defaults to 22050
        """

        if not TTS:
            raise ImportError('TextToSpeech pipeline is not available - install "pipeline" extra to enable')

        # Default path
        path = path if path else "neuml/ljspeech-jets-onnx"

        # Target sample rate
        self.rate = rate

        # Load target tts pipeline
        self.pipeline = None
        if self.hasfile(path, "model.onnx") and self.hasfile(path, "config.yaml"):
            self.pipeline = ESPnet(path, maxtokens, self.providers())
        elif self.hasfile(path, "model.onnx") and self.hasfile(path, "voices.json"):
            self.pipeline = Kokoro(path, maxtokens, self.providers())
        else:
            self.pipeline = SpeechT5(path, maxtokens, self.providers())

    def __call__(self, text, stream=False, speaker=1, encoding=None, **kwargs):
        """
        Generates speech from text. Text longer than maxtokens will be batched and returned
        as a single waveform per text input.

        This method supports text as a string or a list. If the input is a string,
        the return type is audio. If text is a list, the return type is a list.

        Args:
            text: text|list
            stream: stream response if True, defaults to False
            speaker: speaker id, defaults to 1
            encoding: optional audio encoding format
            kwargs: additional keyword args

        Returns:
            list of (audio, sample rate) or list of audio depending on encoding parameter
        """

        # Convert results to a list if necessary
        texts = [text] if isinstance(text, str) else text

        # Streaming response
        if stream:
            return self.stream(texts, speaker, encoding)

        # Transform text to speech
        results = [self.execute(x, speaker, encoding, **kwargs) for x in texts]

        # Return results
        return results[0] if isinstance(text, str) else results

    def providers(self):
        """
        Returns a list of available and usable providers.

        Returns:
            list of available and usable providers
        """

        # Create list of providers, prefer CUDA provider if available
        # CUDA provider only available if GPU is available and onnxruntime-gpu installed
        if torch.cuda.is_available() and "CUDAExecutionProvider" in ort.get_available_providers():
            return [("CUDAExecutionProvider", {"cudnn_conv_algo_search": "DEFAULT"}), "CPUExecutionProvider"]

        # Default when CUDA provider isn't available
        return ["CPUExecutionProvider"]

    def hasfile(self, path, name):
        """
        Tests if a file exists in a local or remote repo.

        Args:
            path: model path
            name: file name

        Returns:
            True if name exists in path, False otherwise
        """

        exists = False
        try:
            # Check if file exists
            exists = cached_file(path_or_repo_id=path, filename=name) is not None
        except (HFValidationError, OSError):
            return False

        return exists

    def stream(self, texts, speaker, encoding):
        """
        Iterates over texts, splits into segments and yields snippets of audio.
        This method is designed to integrate with streaming LLM generation.

        Args:
            texts: list of input texts
            speaker: speaker id
            encoding: audio encoding format

        Returns:
            snippets of audio as NumPy arrays or audio bytes depending on encoding parameter
        """

        buffer = []
        for x in texts:
            buffer.append(x)

            if x == "\n" or (x.strip().endswith(".") and len([y for y in buffer if y]) > 2):
                data, buffer = "".join(buffer), []
                yield self.execute(data, speaker, encoding)

        if buffer:
            data = "".join(buffer)
            yield self.execute(data, speaker, encoding)

    def execute(self, text, speaker, encoding, **kwargs):
        """
        Executes model run for an input array of tokens. This method will build batches
        of tokens when len(tokens) > maxtokens.

        Args:
            text: text to tokenize and pass to model
            speaker: speaker id
            encoding: audio encoding format
            kwargs: additional keyword args

        Returns:
            (audio, sample rate) or audio bytes depending on encoding parameter
        """

        # Run pipeline model
        audio, rate = self.pipeline(text, speaker, **kwargs)

        # Resample, if necessary and return
        audio, rate = (Signal.resample(audio, rate, self.rate), self.rate) if self.rate else (audio, rate)

        # Encoding audio data
        if encoding:
            data = BytesIO()
            sf.write(data, audio, rate, format=encoding)
            return data.getvalue()

        # Default to (audio, rate) tuple
        return (audio, rate)


class SpeechPipeline(Pipeline):
    """
    Base class for speech pipelines
    """

    # pylint: disable=W0221
    def chunk(self, data, size, punctids):
        """
        Batching method that takes punctuation into account. This method splits data up to size
        chunks. But it also searches the batch and splits on the last punctuation token id.

        Args:
            data: data
            size: batch size
            punctids: list of punctuation token ids

        Returns:
            yields batches of data
        """

        # Iterate over each token
        punct, index = 0, 0
        for i, x in enumerate(data):
            # Check if token is a punctuation token
            if x in punctids:
                punct = i

            # Batch size reached, leave a spot for the punctuation token
            if i - index >= (size - 1):
                end = (punct if punct > index else i) + 1
                yield data[index:end]
                index = end

        # Last batch
        if index < len(data):
            yield data[index : len(data)]


class ESPnet(SpeechPipeline):
    """
    Text to Speech pipeline with an ESPnet ONNX model.
    """

    def __init__(self, path, maxtokens, providers):
        """
        Creates a new ESPnet pipeline.

        Args:
            path: model path
            maxtokens: maximum number of tokens model can process
            providers: list of supported ONNX providers
        """

        # Get path to model and config
        config = cached_file(path_or_repo_id=path, filename="config.yaml")
        model = cached_file(path_or_repo_id=path, filename="model.onnx")

        # Read yaml config
        with open(config, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # Create tokenizer
        tokens = config.get("token", {}).get("list")
        self.tokenizer = TTSTokenizer(tokens)

        # Create ONNX Session
        self.model = ort.InferenceSession(model, ort.SessionOptions(), providers)

        # Max number of input tokens model can handle
        self.maxtokens = maxtokens

        # Get model input name, typically "text"
        self.input = self.model.get_inputs()[0].name

        # Get parameter names
        self.params = set(x.name for x in self.model.get_inputs())

    def __call__(self, text, speaker):
        """
        Executes a model run. This method will build batches of tokens when len(tokens) > maxtokens.

        Args:
            text: text to tokenize and pass to model
            speaker: speaker id

        Returns:
            (audio, sample rate)
        """

        # Debug logging for input text
        logger.debug("%s", text)

        # Sample rate
        rate = 22050

        # Tokenize input
        tokens = self.tokenizer(text)

        # Split into batches and process
        results = []
        for i, x in enumerate(self.chunk(tokens, self.maxtokens, self.tokenizer.punctuation())):
            # Format input parameters
            params = {self.input: x}
            params = {**params, **{"sids": np.array([speaker])}} if "sids" in self.params else params

            # Run text through TTS model and save waveform
            output = self.model.run(None, params)
            results.append(Signal.trim(output[0], rate, trailing=False) if i > 0 else output[0])

        # Concatenate results and return
        return (np.concatenate(results), rate)


class Kokoro(SpeechPipeline):
    """
    Text to Speech pipeline with an Kokoro ONNX model.
    """

    def __init__(self, path, maxtokens, providers):
        """
        Creates a new Kokoro pipeline.

        Args:
            path: model path
            maxtokens: maximum number of tokens model can process
            providers: list of supported ONNX providers
        """

        # Get path to model and config
        voices = cached_file(path_or_repo_id=path, filename="voices.json")
        model = cached_file(path_or_repo_id=path, filename="model.onnx")

        # Read voices config
        with open(voices, "r", encoding="utf-8") as f:
            self.voices = json.load(f)

        # Create tokenizer
        self.tokenizer = IPATokenizer()

        # Create ONNX Session
        self.model = ort.InferenceSession(model, ort.SessionOptions(), providers)

        # Max number of input tokens model can handle
        self.maxtokens = min(maxtokens, 510)

        # Get model input name
        self.input = self.model.get_inputs()[0].name

        # Get parameter names
        self.params = set(x.name for x in self.model.get_inputs())

    def __call__(self, text, speaker=None, speed=1.0, transcribe=True):
        """
        Executes a model run. This method will build batches of tokens when len(tokens) > maxtokens.

        Args:
            text: text to tokenize and pass to model
            speaker: speaker id, defaults to first speaker
            speed: defaults to 1.0
            transcribe: if text should be transcriped to IPA text, defaults to True

        Returns:
            (audio, sample rate)
        """

        # Debug logging for input text
        logger.debug("%s", text)

        # Sample rate
        rate = 24000

        # Looks up speaker, falls back to default
        speaker = speaker if speaker in self.voices else next(iter(self.voices))
        speaker = np.array(self.voices[speaker], dtype=np.float32)

        # Tokenize input
        self.tokenizer.transcribe = transcribe
        tokens = self.tokenizer(text)

        # Split into batches and process
        results = []
        for i, x in enumerate(self.chunk(tokens, self.maxtokens, self.tokenizer.punctuation())):
            # Format input parameters
            params = {self.input: [[0, *x, 0]], "style": speaker[len(x)], "speed": np.ones(1, dtype=np.float32) * speed}

            # Run text through TTS model and save waveform
            output = self.model.run(None, params)
            results.append(Signal.trim(output[0], rate, trailing=False) if i > 0 else output[0])

        # Concatenate results and return
        return (np.concatenate(results), rate)


class SpeechT5(SpeechPipeline):
    """
    Text to Speech pipeline with a SpeechT5 ONNX model.
    """

    def __init__(self, path, maxtokens, providers):
        """
        Creates a new SpeechT5 pipeline.

        Args:
            path: model path
            maxtokens: maximum number of tokens model can process
            providers: list of supported ONNX providers
        """

        self.encoder = ort.InferenceSession(cached_file(path_or_repo_id=path, filename="encoder_model.onnx"), providers=providers)
        self.decoder = ort.InferenceSession(cached_file(path_or_repo_id=path, filename="decoder_model_merged.onnx"), providers=providers)
        self.vocoder = ort.InferenceSession(cached_file(path_or_repo_id=path, filename="decoder_postnet_and_vocoder.onnx"), providers=providers)

        self.processor = SpeechT5Processor.from_pretrained(path)
        self.defaultspeaker = np.load(cached_file(path_or_repo_id=path, filename="speaker.npy"), allow_pickle=False)

        # Max number of input tokens model can handle
        self.maxtokens = maxtokens

        # pylint: disable=E1101
        # Punctuation token ids
        self.punctids = [v for k, v in self.processor.tokenizer.get_vocab().items() if k in ".,!?;"]

    def __call__(self, text, speaker):
        """
        Executes a model run. This method will build batches of tokens when len(tokens) > maxtokens.

        Args:
            text: text to tokenize and pass to model
            speaker: speaker embeddings

        Returns:
            (audio, sample rate)
        """

        # Debug logging for input text
        logger.debug("%s", text)

        # Sample rate
        rate = 16000

        # Tokenize text
        inputs = self.processor(text=text, return_tensors="np", normalize=True)

        # Split into batches and process
        results = []
        for i, x in enumerate(self.chunk(inputs["input_ids"][0], self.maxtokens, self.punctids)):
            # Run text through TTS model and save waveform
            chunk = self.process(np.array([x], dtype=np.int64), speaker)
            results.append(Signal.trim(chunk, rate, trailing=False) if i > 0 else chunk)

        # Concatenate results and return
        return (np.concatenate(results), rate)

    def process(self, inputs, speaker):
        """
        Runs model inference.

        Args:
            inputs: input token ids
            speaker: speaker embeddings

        Returns:
            waveform as NumPy array
        """

        # Run through encoder model
        outputs = self.encoder.run(None, {"input_ids": inputs})
        outputs = {key.name: outputs[x] for x, key in enumerate(self.encoder.get_outputs())}

        # Encoder outputs and parameters
        hiddenstate, attentionmask = outputs["encoder_outputs"], outputs["encoder_attention_mask"]
        minlenratio, maxlenratio = 0.0, 20.0
        reduction, threshold, melbins = 2, 0.5, 80

        maxlen = int(hiddenstate.shape[1] * maxlenratio / reduction)
        minlen = int(hiddenstate.shape[1] * minlenratio / reduction)

        # Main processing loop
        spectrogram, index, crossattention, branch, outputs = [], 0, None, False, {}
        while True:
            index += 1

            inputs = {
                "use_cache_branch": np.array([branch]),
                "encoder_attention_mask": attentionmask,
                "speaker_embeddings": speaker if speaker is not None and isinstance(speaker, np.ndarray) else self.defaultspeaker,
            }

            if index == 1:
                inputs = self.placeholders(inputs)
                inputs["output_sequence"] = np.zeros((1, 1, melbins)).astype(np.float32)
                inputs["encoder_hidden_states"] = hiddenstate
                branch = True
            else:
                inputs = self.inputs(inputs, outputs, crossattention)
                inputs["output_sequence"] = outputs["output_sequence_out"]
                inputs["encoder_hidden_states"] = np.zeros((1, 0, 768)).astype(np.float32)

            # Run inputs through decoder
            outputs = self.decoder.run(None, inputs)
            outputs = {key.name: outputs[x] for x, key in enumerate(self.decoder.get_outputs())}

            # Get cross attention with 1st pass
            if index == 1:
                crossattention = {key: val for key, val in outputs.items() if ("encoder" in key and "present" in key)}

            # Decoder outputs
            prob = outputs["prob"]
            spectrum = outputs["spectrum"]
            spectrogram.append(spectrum)

            # Done when stop token or maximum length is reached.
            if index >= minlen and (int(sum(prob >= threshold)) > 0 or index >= maxlen):
                spectrogram = np.concatenate(spectrogram)
                return self.vocoder.run(None, {"spectrogram": spectrogram})[0]

    def placeholders(self, inputs):
        """
        Creates decoder model inputs for initial inference pass.

        Args:
            inputs: current decoder inputs

        Returns:
            updated decoder inputs
        """

        length = inputs["encoder_attention_mask"].shape[1]

        for x in range(6):
            inputs[f"past_key_values.{x}.encoder.key"] = np.zeros((1, 12, length, 64)).astype(np.float32)
            inputs[f"past_key_values.{x}.encoder.value"] = np.zeros((1, 12, length, 64)).astype(np.float32)
            inputs[f"past_key_values.{x}.decoder.key"] = np.zeros((1, 12, 1, 64)).astype(np.float32)
            inputs[f"past_key_values.{x}.decoder.value"] = np.zeros((1, 12, 1, 64)).astype(np.float32)

        return inputs

    def inputs(self, inputs, previous, crossattention):
        """
        Creates decoder model inputs for follow-on inference passes.

        Args:
            inputs: current decoder inputs
            previous: previous decoder outputs
            crossattention: crossattention parameters

        Returns:
            updated decoder inputs
        """

        for x in range(6):
            inputs[f"past_key_values.{x}.encoder.key"] = crossattention[f"present.{x}.encoder.key"]
            inputs[f"past_key_values.{x}.encoder.value"] = crossattention[f"present.{x}.encoder.value"]
            inputs[f"past_key_values.{x}.decoder.key"] = previous[f"present.{x}.decoder.key"]
            inputs[f"past_key_values.{x}.decoder.value"] = previous[f"present.{x}.decoder.value"]

        return inputs



================================================
FILE: src/python/txtai/pipeline/audio/transcription.py
================================================
"""
Transcription module
"""

import numpy as np

# Conditional import
try:
    import soundfile as sf

    from .signal import Signal, SCIPY

    TRANSCRIPTION = SCIPY
except (ImportError, OSError):
    TRANSCRIPTION = False

from ..hfpipeline import HFPipeline


class Transcription(HFPipeline):
    """
    Transcribes audio files or data to text.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        if not TRANSCRIPTION:
            raise ImportError(
                'Transcription pipeline is not available - install "pipeline" extra to enable. Also check that libsndfile is available.'
            )

        # Call parent constructor
        super().__init__("automatic-speech-recognition", path, quantize, gpu, model, **kwargs)

    def __call__(self, audio, rate=None, chunk=10, join=True, **kwargs):
        """
        Transcribes audio files or data to text.

        This method supports a single audio element or a list of audio. If the input is audio, the return
        type is a string. If text is a list, a list of strings is returned

        Args:
            audio: audio|list
            rate: sample rate, only required with raw audio data
            chunk: process audio in chunk second sized segments
            join: if True (default), combine each chunk back together into a single text output.
                  When False, chunks are returned as a list of dicts, each having raw associated audio and
                  sample rate in addition to text
            kwargs: generate keyword arguments

        Returns:
            list of transcribed text
        """

        # Convert single element to list
        values = [audio] if self.isaudio(audio) else audio

        # Read input audio
        speech = self.read(values, rate)

        # Apply transformation rules and store results
        results = self.batchprocess(speech, chunk, **kwargs) if chunk and not join else self.process(speech, chunk, **kwargs)

        # Return single element if single element passed in
        return results[0] if self.isaudio(audio) else results

    def isaudio(self, audio):
        """
        Checks if input is a single audio element.

        Args:
            audio: audio|list

        Returns:
            True if input is an audio element, False otherwise
        """

        return isinstance(audio, (str, tuple, np.ndarray)) or hasattr(audio, "read")

    def read(self, audio, rate):
        """
        Read audio to raw waveforms and sample rates.

        Args:
            audio: audio|list
            rate: optional sample rate

        Returns:
            list of (audio data, sample rate)
        """

        speech = []
        for x in audio:
            if isinstance(x, str) or hasattr(x, "read"):
                # Read file or file-like object
                raw, samplerate = sf.read(x)
            elif isinstance(x, tuple):
                # Input is NumPy array and sample rate
                raw, samplerate = x
            else:
                # Input is NumPy array
                raw, samplerate = x, rate

            speech.append((raw, samplerate))

        return speech

    def process(self, speech, chunk, **kwargs):
        """
        Standard processing loop. Runs a single pipeline call for all speech inputs along
        with the chunk size. Returns text for each input.

        Args:
            speech: list of (audio data, sample rate)
            chunk: split audio into chunk seconds sized segments for processing
            kwargs: generate keyword arguments

        Returns:
            list of transcribed text
        """

        results = []
        for result in self.pipeline([self.convert(*x) for x in speech], chunk_length_s=chunk, ignore_warning=True, generate_kwargs=kwargs):
            # Store result
            results.append(self.clean(result["text"]))

        return results

    def batchprocess(self, speech, chunk, **kwargs):
        """
        Batch processing loop. Runs a pipeline call per speech input. Each speech input is split
        into chunk duration segments. Each segment is individually transcribed and returned along with
        the raw wav snippets.

        Args:
            speech: list of (audio data, sample rate)
            chunk: split audio into chunk seconds sized segments for processing
            kwargs: generate keyword arguments

        Returns:
            list of lists of dicts - each dict has text, raw wav data for text and sample rate
        """

        results = []

        # Process each element individually to get time-sliced chunks
        for raw, rate in speech:
            # Get segments for current speech entry
            segments = self.segments(raw, rate, chunk)

            # Process segments, store raw data before processing given pipeline modifies it
            sresults = []
            for x, result in enumerate(self.pipeline([self.convert(*x) for x in segments], generate_kwargs=kwargs)):
                sresults.append({"text": self.clean(result["text"]), "raw": segments[x][0], "rate": segments[x][1]})

            results.append(sresults)

        return results

    def segments(self, raw, rate, chunk):
        """
        Builds chunk duration batches.

        Args:
            raw: raw audio data
            rate: sample rate
            chunk: chunk duration size
        """

        segments = []

        # Split into batches, use sample rate * chunk seconds
        for segment in self.batch(raw, rate * chunk):
            segments.append((segment, rate))

        return segments

    def convert(self, raw, rate):
        """
        Converts input audio to mono with a sample rate equal to the pipeline model's
        sample rate.

        Args:
            raw: raw audio data
            rate: target sample rate

        Returns:
            audio data ready for pipeline model
        """

        # Convert stereo to mono, if necessary
        raw = Signal.mono(raw)

        # Resample to target sample rate
        target = self.pipeline.feature_extractor.sampling_rate
        return {"raw": Signal.resample(raw, rate, target), "sampling_rate": target}

    def clean(self, text):
        """
        Applies text normalization rules.

        Args:
            text: input text

        Returns:
            clean text
        """

        # Trim whitespace
        text = text.strip()

        # Convert all upper case strings to capitalized case
        return text.capitalize() if text.isupper() else text



================================================
FILE: src/python/txtai/pipeline/data/__init__.py
================================================
"""
Segment imports
"""

from .filetohtml import FileToHTML
from .htmltomd import HTMLToMarkdown
from .segmentation import Segmentation
from .tabular import Tabular
from .textractor import Textractor
from .tokenizer import Tokenizer



================================================
FILE: src/python/txtai/pipeline/data/filetohtml.py
================================================
"""
FileToHTML module
"""

import os
import re

from subprocess import Popen

# Conditional import
try:
    from tika import detector, parser

    TIKA = True
except ImportError:
    TIKA = False

# Conditional import
try:
    from docling.document_converter import DocumentConverter

    DOCLING = True
except ImportError:
    DOCLING = False

from ..base import Pipeline


class FileToHTML(Pipeline):
    """
    File to HTML pipeline.
    """

    def __init__(self, backend="available"):
        """
        Creates a new File to HTML pipeline.

        Args:
            backend: backend to use to extract content, supports "tika", "docling" or "available" (default) which finds the first available
        """

        # Lowercase backend parameter
        backend = backend.lower() if backend else None

        # Check for available backend
        if backend == "available":
            backend = "tika" if Tika.available() else "docling" if Docling.available() else None

        # Create backend instance
        self.backend = Tika() if backend == "tika" else Docling() if backend == "docling" else None

    def __call__(self, path):
        """
        Converts file at path to HTML. Returns None if no backend is available.

        Args:
            path: input file path

        Returns:
            html if a backend is available, otherwise returns None
        """

        return self.backend(path) if self.backend else None


class Tika:
    """
    File to HTML conversion via Apache Tika.
    """

    @staticmethod
    def available():
        """
        Checks if a Java executable is available and Tika is installed.

        Returns:
            True if Java is available and Tika is installed, False otherwise
        """

        # Get path to Java executable
        path = os.environ.get("TIKA_JAVA", "java")

        # pylint: disable=R1732,W0702,W1514
        # Check if Java binary is available on path
        try:
            _ = Popen(path, stdout=open(os.devnull, "w"), stderr=open(os.devnull, "w"))
        except:
            return False

        # Return True if Java is available AND Tika is installed
        return TIKA

    def __init__(self):
        """
        Creates a new Tika instance.
        """

        if not Tika.available():
            raise ImportError('Tika engine is not available - install "pipeline" extra to enable. Also check that Java is available.')

    def __call__(self, path):
        """
        Parses content to HTML.

        Args:
            path: file path

        Returns:
            html
        """

        # Skip parsing if input is plain text or HTML
        mimetype = detector.from_file(path)
        if mimetype in ("text/plain", "text/html", "text/xhtml"):
            return None

        # Parse content to HTML
        parsed = parser.from_file(path, xmlContent=True)
        return parsed["content"]


class Docling:
    """
    File to HTML conversion via Docling.
    """

    @staticmethod
    def available():
        """
        Checks if Docling is available.

        Returns:
            True if Docling is available, False otherwise
        """

        return DOCLING

    def __init__(self):
        """
        Creates a new Docling instance.
        """

        if not Docling.available():
            raise ImportError('Docling engine is not available - install "pipeline" extra to enable')

        self.converter = DocumentConverter()

    def __call__(self, path):
        """
        Parses content to HTML.

        Args:
            path: file path

        Returns:
            html
        """

        # Skip parsing if input is HTML
        if self.ishtml(path):
            return None

        # Parse content to HTML
        html = self.converter.convert(path).document.export_to_html(html_head="<head/>")

        # Normalize HTML and return
        return self.normalize(html)

    def ishtml(self, path):
        """
        Detects if this file looks like HTML.

        Args:
            path: file path

        Returns:
            True if this is HTML
        """

        with open(path, "rb") as f:
            # Read first 1024 bytes, ignore encoding errors and strip leading/trailing whitespace
            content = f.read(1024)
            content = content.decode("ascii", errors="ignore").lower().strip()

            # Check for HTML
            return re.search(r"<!doctype\s+html|<html|<head|<body", content)

    def normalize(self, html):
        """
        Applies normalization rules to make HTML consistent with other text extraction backends.

        Args:
            html: input html

        Returns:
            normalized html
        """

        # Wrap content with a body tag, if necessary
        html = html.replace("<head/>", "<head/><body>").replace("</html>", "</body></html>") if "<body>" not in html else html

        # Remove bullets from list items
        html = re.sub(r"<li>\xb7 ", r"<li>", html)

        # Add spacing between paragraphs
        return html.replace("</p>", "</p><p/>")



================================================
FILE: src/python/txtai/pipeline/data/htmltomd.py
================================================
"""
HTMLToMarkdown module
"""

import re

# Conditional import
try:
    from bs4 import BeautifulSoup, NavigableString

    SOUP = True
except ImportError:
    SOUP = False

from ..base import Pipeline


class HTMLToMarkdown(Pipeline):
    """
    HTML to Markdown pipeline.

    Markdown formatting is applied for headings, blockquotes, lists, code, tables and text. Visual formatting is also
    included (bold, italic etc).

    This pipeline searches for the best node that has relevant text, often found with an article, main or body tag.
    """

    def __init__(self, paragraphs=False, sections=False):
        """
        Create a new Extract instance.

        Args:
            paragraphs: True if paragraph parsing enabled, False otherwise
            sections: True if section parsing enabled, False otherwise
        """

        if not SOUP:
            raise ImportError('HTMLToMarkdown pipeline is not available - install "pipeline" extra to enable')

        self.paragraphs = paragraphs
        self.sections = sections

    def __call__(self, html):
        """
        Transforms input HTML into Markdown formatted text.

        Args:
            html: input html

        Returns:
            markdown formatted text
        """

        # HTML Parser
        soup = BeautifulSoup(html, features="html.parser")

        # Ignore script and style tags
        for script in soup.find_all(["script", "style"]):
            script.decompose()

        # Check for article sections
        article = next((x for x in ["article", "main"] if soup.find(x)), None)

        # Extract text from each section element
        nodes = []
        for node in soup.find_all(article if article else "body"):
            # Skip article sections without at least 1 paragraph
            if not article or node.find("p"):
                nodes.append(self.process(node, article))

        # Return extracted text, fallback to default text extraction if no nodes found
        return "\n".join(self.metadata(soup) + nodes) if nodes else self.default(soup)

    def process(self, node, article):
        """
        Extracts text from a node. This method applies transforms for headings, blockquotes, lists, code, tables and text.
        Page breaks are detected and reflected in the output text as a page break character.

        Args:
            node: input node
            article: True if the main section node is an article

        Returns:
            node text
        """

        if self.isheader(node):
            return self.header(node, article)

        if node.name in ("blockquote", "q"):
            return self.block(node)

        if node.name in ("ul", "ol"):
            return self.items(node, article)

        if node.name in ("code", "pre"):
            return self.code(node)

        if node.name == "table":
            return self.table(node, article)

        # Nodes to skip
        if node.name in ("aside",) + (() if article else ("header", "footer")):
            return ""

        # Get page break symbol, if available
        page = node.name and node.get("class") and "page" in node.get("class")

        # Get node children
        children = self.children(node)

        # Join elements into text
        if self.iscontainer(node, children):
            texts = [self.process(node, article) for node in children]
            text = "\n".join(text for text in texts if text or not article)
        else:
            text = self.text(node, article)

        # Add page breaks, if section parsing enabled. Otherwise add node text.
        return f"{text}\f" if page and self.sections else text

    def metadata(self, node):
        """
        Builds a metadata section. The metadata section consists of the title and
        description fields.

        Args:
            node: input document node

        Returns:
            metadata as a list
        """

        title = node.find("title")
        metadata = [f"**{title.text.strip()}**"] if title and title.text else []

        description = node.find("meta", attrs={"name": "description"})
        if description and description["content"]:
            metadata.append(f"\n*{description['content'].strip()}*")

        # Add separator
        if metadata:
            metadata.append("\f" if self.sections else "\n\n")

        return metadata

    def default(self, soup):
        """
        Default text handler when valid HTML isn't detected.

        Args:
            soup: BeautifulSoup object

        Returns:
            text
        """

        lines = []
        for line in soup.get_text().split("\n"):
            # Detect markdown headings and add page breaks
            lines.append(f"\f{line}" if self.sections and re.search(r"^#+ ", line) else line)

        return "\n".join(lines)

    def text(self, node, article):
        """
        Text handler. This method flattens a node and it's children to text.

        Args:
            node: input node
            article: True if the main section node is an article

        Returns:
            node text
        """

        # Get node children if available, otherwise use node as item
        items = self.children(node)
        items = items if items else [node]

        # Apply emphasis and link formatting
        texts = []
        for x in items:
            target, text = x if x.name else node, x.text

            if text.strip():
                if target.name in ("b", "strong"):
                    text = f"**{text.strip()}** "
                elif target.name in ("i", "em"):
                    text = f"*{text.strip()}* "
                elif target.name == "a":
                    text = f"[{text.strip()}]({target.get('href')}) "

            texts.append(text)

        # Join text elements
        text = "".join(texts)

        # Article text processing
        text = self.articletext(node, text) if article else text

        # Return text, strip leading/trailing whitespace if this is a string only node
        text = text if node.name and text else text.strip()

        return text

    def header(self, node, article):
        """
        Header handler. This method transforms a HTML heading into a Markdown formatted heading.

        Args:
            node: input node
            article: True if the main section node is an article

        Returns:
            heading as markdown
        """

        # Get heading level and text
        level = "#" * int(node.name[1])
        text = self.text(node, article)

        # Add section break or newline, if necessary
        level = f"\f{level}" if self.sections else f"\n{level}"

        # Return formatted header. Remove leading whitespace as it was added before level in step above.
        return f"{level} {text.lstrip()}" if text.strip() else ""

    def block(self, node):
        """
        Blockquote handler. This method transforms a HTML blockquote or q block into a Markdown formatted
        blockquote

        Args:
            node: input node

        Returns:
            block as markdown
        """

        text = "\n".join(f"> {x}" for x in node.text.strip().split("\n"))
        return f"{text}\n\n" if self.paragraphs else f"{text}\n"

    def items(self, node, article):
        """
        List handler. This method transforms a HTML ordered/unordered list into a Markdown formatted list.

        Args:
            node: input node
            article: True if the main section node is an article

        Returns:
            list as markdown
        """

        elements = []
        for x, element in enumerate(node.find_all("li")):
            # Unordered lists use dashes. Ordered lists use numbers.
            prefix = "-" if node.name == "ul" else f"{x + 1}."

            # List item text
            text = self.process(element, article)

            # Add list element
            if text:
                elements.append(f"{prefix} {text}")

        # Join elements together as string
        return "\n".join(elements)

    def code(self, node):
        """
        Code block handler. This method transforms a HTML pre or code block into a Markdown formatted
        code block.

        Args:
            node: input node

        Returns:
            code as markdown
        """

        text = f"```\n{node.text.strip()}\n```"
        return f"{text}\n\n" if self.paragraphs else f"{text}\n"

    def table(self, node, article):
        """
        Table handler. This method transforms a HTML table into a Markdown formatted table.

        Args:
            node: input node
            article: True if the main section node is an article

        Returns:
            table as markdown
        """

        elements, header = [], False

        # Process all rows
        rows = node.find_all("tr")
        for row in rows:
            # Get list of columns for row
            columns = row.find_all(lambda tag: tag.name in ("th", "td"))

            # Add columns with separator
            elements.append(f"|{'|'.join(self.process(column, article) for column in columns)}|")

            # If there are multiple rows, add header format row
            if not header and len(rows) > 1:
                elements.append(f"{'|---' * len(columns)}|")
                header = True

        # Join elements together as string
        return "\n".join(elements)

    def iscontainer(self, node, children):
        """
        Analyzes a node and it's children to determine if this is a container element. A container
        element is defined as being a div, body, article or not having any string elements as children.

        Args:
            node: input node
            nodes: input node's children

        Returns:
            True if this is a container element, False otherwise
        """

        return children and (node.name in ("div", "body", "article") or not any(isinstance(x, NavigableString) for x in children))

    def children(self, node):
        """
        Gets the node children, if available.

        Args:
            node: input node

        Returns:
            node children or None if not available
        """

        if node.name and node.contents:
            # Iterate over children and remove whitespace-only string nodes
            return [node for node in node.contents if node.name or node.text.strip()]

        return None

    def articletext(self, node, text):
        """
        Transforms node text using article parsing rules. Article parsing is designed to extract text content from web articles.
        It ignores navigation headers and other superfluous elements.

        Args:
            node: input node
            text: current text

        Returns:
            article text
        """

        # List of valid text nodes
        valid = ("p", "th", "td", "li", "a", "b", "strong", "i", "em")

        # Check if this node is valid or it's part of a table cell
        valid = node.name in valid or (node.parent and node.parent.name in ("th", "td"))

        # Check if text is valid article text
        text = text if (valid or self.isheader(node)) and not self.islink(node) else ""
        if text:
            # Replace non-breaking space plus newline with double newline
            text = text.replace("\xa0\n", "\n\n")

            # Format paragraph whitespace
            if node.name == "p":
                text = f"{text.strip()}\n\n" if self.paragraphs else f"{text.strip()}\n"

        return text

    def isheader(self, node):
        """
        Checks if node is a header node.

        Args:
            node: input node

        Returns:
            True if node is a header node, False otherwise
        """

        return node.name in ("h1", "h2", "h3", "h4", "h5", "h6")

    def islink(self, node):
        """
        Checks if node is a link node. This method does not consider links without tables as link nodes.

        Args:
            node: input node

        Returns:
            True if node is a link node, False otherwise
        """

        # Check if this is a link node or link container
        link, parent = False, node
        while parent:
            if parent.name == "a":
                link = True
                break

            parent = parent.parent

        # Return if this node or any parents are a link. Ignore links in table cells.
        return link and node.parent.name not in ("th", "td")



================================================
FILE: src/python/txtai/pipeline/data/segmentation.py
================================================
"""
Segmentation module
"""

import re

# Conditional import
try:
    from nltk import sent_tokenize

    NLTK = True
except ImportError:
    NLTK = False

# Conditional import
try:
    import chonkie

    CHONKIE = True
except ImportError:
    CHONKIE = False

from ..base import Pipeline


class Segmentation(Pipeline):
    """
    Segments text into logical units.
    """

    def __init__(
        self,
        sentences=False,
        lines=False,
        paragraphs=False,
        minlength=None,
        join=False,
        sections=False,
        cleantext=True,
        chunker=None,
        tuples=False,
        **kwargs,
    ):
        """
        Creates a new Segmentation pipeline.

        Args:
            sentences: tokenize text into sentences if True, defaults to False
            lines: tokenizes text into lines if True, defaults to False
            paragraphs: tokenizes text into paragraphs if True, defaults to False
            minlength: require at least minlength characters per text element, defaults to None
            join: joins tokenized sections back together if True, defaults to False
            sections: tokenizes text into sections if True, defaults to False. Splits using section or page breaks, depending on what's available
            cleantext: apply text cleaning rules, defaults to True
            chunker: creates a third-party chunker to tokenize text if set, defaults to None
            tuples: return (input, output) tuples, defaults to False
            kwargs: additional keyword arguments
        """

        if not NLTK and sentences:
            raise ImportError('NLTK is not available - install "pipeline" extra to enable')

        if not CHONKIE and chunker:
            raise ImportError('Chonkie is not available - install "pipeline" extra to enable')

        self.sentences = sentences
        self.lines = lines
        self.paragraphs = paragraphs
        self.sections = sections
        self.minlength = minlength
        self.join = join
        self.cleantext = cleantext

        # Create a third-party chunker, if applicable
        self.chunker = self.createchunker(chunker, **kwargs) if chunker else None

        # Return (input, output) tuples as output
        self.tuples = tuples

    def __call__(self, text):
        """
        Segments text into semantic units.

        This method supports text as a string or a list. If the input is a string, the return
        type is text|list. If text is a list, a list of returned, this could be a
        list of text or a list of lists depending on the tokenization strategy.

        Args:
            text: text|list

        Returns:
            segmented text
        """

        # Get inputs
        texts = [text] if not isinstance(text, list) else text

        # Extract text for each input file
        results = []
        for value in texts:
            # Get text
            result = self.text(value)

            # Parse and add extracted results
            result = self.parse(result)

            # Wrap as tuple
            if self.tuples:
                result = [(value, x) for x in result] if isinstance(result, list) else (value, result)

            results.append(result)

        return results[0] if isinstance(text, str) else results

    def text(self, text):
        """
        Hook to allow extracting text out of input text object.

        Args:
            text: object to extract text from
        """

        return text

    def parse(self, text):
        """
        Splits and cleans text based on the current parameters.

        Args:
            text: input text

        Returns:
            parsed and clean content
        """

        content = None

        if self.chunker:
            # pylint: disable=E1102
            content = [self.clean(x.text) for x in self.chunker(text)]
        elif self.sentences:
            content = [self.clean(x) for x in sent_tokenize(text)]
        elif self.lines:
            content = [self.clean(x) for x in re.split(r"\n{1,}", text)]
        elif self.paragraphs:
            content = [self.clean(x) for x in re.split(r"\n{2,}", text)]
        elif self.sections:
            split = r"\f" if "\f" in text else r"\n{3,}"
            content = [self.clean(x) for x in re.split(split, text)]
        else:
            content = self.clean(text)

        # Text tokenization enabled
        if isinstance(content, list):
            # Remove empty strings
            content = [x for x in content if x]
            return " ".join(content) if self.join else content

        # Default method that returns clean text
        return content

    def clean(self, text):
        """
        Applies a series of rules to clean text.

        Args:
            text: input text

        Returns:
            clean text
        """

        # Text cleaning disabled, return original text
        if not self.cleantext:
            return text

        # Collapse and remove excess whitespace
        text = re.sub(r" +", " ", text)
        text = text.strip()

        # If minlength enabled, require at least minlength chars
        return text if not self.minlength or len(text) >= self.minlength else None

    def createchunker(self, chunker, **kwargs):
        """
        Creates a new third-party chunker

        Args:
            chunker: name of chunker to create
            kwargs: additional keyword arguments

        Returns:
            new chunker
        """

        # Resolve and create a third-party chunker
        chunker = f"{chunker[0].upper() + chunker[1:]}Chunker"
        return getattr(chonkie, chunker)(**kwargs)



================================================
FILE: src/python/txtai/pipeline/data/tabular.py
================================================
"""
Tabular module
"""

import os

# Conditional import
try:
    import pandas as pd

    PANDAS = True
except ImportError:
    PANDAS = False

from ..base import Pipeline


class Tabular(Pipeline):
    """
    Splits tabular data into rows and columns.
    """

    def __init__(self, idcolumn=None, textcolumns=None, content=False):
        """
        Creates a new Tabular pipeline.

        Args:
            idcolumn: column name to use for row id
            textcolumns: list of columns to combine as a text field
            content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields
                     is included in the generated rows.
        """

        if not PANDAS:
            raise ImportError('Tabular pipeline is not available - install "pipeline" extra to enable')

        self.idcolumn = idcolumn
        self.textcolumns = textcolumns
        self.content = content

    def __call__(self, data):
        """
        Splits data into rows and columns.

        Args:
            data: input data

        Returns:
            list of (id, text, tag)
        """

        items = [data] if not isinstance(data, list) else data

        # Combine all rows into single return element
        results = []
        dicts = []

        for item in items:
            # File path
            if isinstance(item, str):
                _, extension = os.path.splitext(item)
                extension = extension.replace(".", "").lower()

                if extension == "csv":
                    df = pd.read_csv(item)

                results.append(self.process(df))

            # Dict
            if isinstance(item, dict):
                dicts.append(item)

            # List of dicts
            elif isinstance(item, list):
                df = pd.DataFrame(item)
                results.append(self.process(df))

        if dicts:
            df = pd.DataFrame(dicts)
            results.extend(self.process(df))

        return results[0] if not isinstance(data, list) else results

    def process(self, df):
        """
        Extracts a list of (id, text, tag) tuples from a dataframe.

        Args:
            df: DataFrame to extract content from

        Returns:
            list of (id, text, tag)
        """

        rows = []

        # Columns to use for text
        columns = self.textcolumns
        if not columns:
            columns = list(df.columns)
            if self.idcolumn:
                columns.remove(self.idcolumn)

        # Transform into (id, text, tag) tuples
        for index, row in df.iterrows():
            uid = row[self.idcolumn] if self.idcolumn else index
            uid = uid if uid is not None else index
            text = self.concat(row, columns)

            rows.append((uid, text, None))

            # Also add row for content
            if isinstance(self.content, list):
                row = {column: self.column(value) for column, value in row.to_dict().items() if column in self.content}
                rows.append((uid, row, None))
            elif self.content:
                row = {column: self.column(value) for column, value in row.to_dict().items()}
                rows.append((uid, row, None))

        return rows

    def concat(self, row, columns):
        """
        Builds a text field from row using columns.

        Args:
            row: input row
            columns: list of columns to join together

        Returns:
            text
        """

        parts = []
        for column in columns:
            column = self.column(row[column])
            if column:
                parts.append(str(column))

        return ". ".join(parts) if parts else None

    def column(self, value):
        """
        Applies column standardization logic:
            - Replace NaN values with None

        Args:
            value: input value

        Returns:
            formatted value
        """

        # Check for null - treat lists as not null
        return None if not isinstance(value, list) and pd.isnull(value) else value



================================================
FILE: src/python/txtai/pipeline/data/textractor.py
================================================
"""
Textractor module
"""

import contextlib
import os
import tempfile

from urllib.parse import urlparse
from urllib.request import urlopen, Request

from .filetohtml import FileToHTML
from .htmltomd import HTMLToMarkdown
from .segmentation import Segmentation


class Textractor(Segmentation):
    """
    Extracts text from files.
    """

    # pylint: disable=R0913
    def __init__(
        self,
        sentences=False,
        lines=False,
        paragraphs=False,
        minlength=None,
        join=False,
        sections=False,
        cleantext=True,
        chunker=None,
        headers=None,
        backend="available",
        **kwargs
    ):
        super().__init__(sentences, lines, paragraphs, minlength, join, sections, cleantext, chunker, **kwargs)

        # Get backend parameter - handle legacy tika flag
        backend = "tika" if "tika" in kwargs and kwargs["tika"] else None if "tika" in kwargs else backend

        # File to HTML pipeline
        self.html = FileToHTML(backend) if backend else None

        # HTML to Markdown pipeline
        self.markdown = HTMLToMarkdown(self.paragraphs, self.sections)

        # HTTP headers
        self.headers = headers if headers else {}

    def text(self, text):
        # Check if text is a valid file path or url
        path, exists = self.valid(text)

        if not path:
            # Not a valid file path, treat input as data
            html = text

        elif self.html:
            # Use FileToHTML pipeline, if available
            # Retrieve remote file, if necessary
            path = path if exists else self.download(path)

            # Parse content to HTML
            html = self.html(path)

            # FiletoHTML pipeline returns None when input is already HTML
            html = html if html else self.retrieve(path)

            # Delete temporary file
            if not exists:
                os.remove(path)

        else:
            # Read data from url/path
            html = self.retrieve(path)

        # HTML to Markdown
        return self.markdown(html)

    def valid(self, path):
        """
        Checks if path is a valid local file or web url. Returns path if valid along with a flag
        denoting if the path exists locally.

        Args:
            path: path to check

        Returns:
            (path, exists)
        """

        # Convert file urls to local paths
        path = path.replace("file://", "")

        # Check if this is a local file path or local file url
        exists = os.path.exists(path)

        # Consider local files and HTTP urls valid
        return (path if exists or urlparse(path).scheme in ("http", "https") else None, exists)

    def download(self, url):
        """
        Downloads content of url to a temporary file.

        Args:
            url: input url

        Returns:
            temporary file path
        """

        with tempfile.NamedTemporaryFile(mode="wb", delete=False) as output:
            path = output.name

            # Retrieve and write data to temporary file
            output.write(self.retrieve(url))

        return path

    def retrieve(self, url):
        """
        Retrieves content from url.

        Args:
            url: input url

        Returns:
            data
        """

        # Local file
        if os.path.exists(url):
            with open(url, "rb") as f:
                return f.read()

        # Remote file
        with contextlib.closing(urlopen(Request(url, headers=self.headers))) as connection:
            return connection.read()



================================================
FILE: src/python/txtai/pipeline/data/tokenizer.py
================================================
"""
Tokenizer module
"""

import re
import string

import regex

from ..base import Pipeline


class Tokenizer(Pipeline):
    """
    Tokenizes text into tokens using one of the following methods.

      1. Backwards compatible tokenization that only accepts alphanumeric tokens from the Latin alphabet.

      2. Split using word boundary rules from the Unicode Text Segmentation algorithm (see Unicode Standard Annex #29).
         This is similar to the standard tokenizer in Apache Lucene and works well for most languages.
    """

    # fmt: off
    # English Stop Word List (Standard stop words used by Apache Lucene)
    STOP_WORDS = {"a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it",
                  "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these",
                  "they", "this", "to", "was", "will", "with"}
    # fmt: on

    @staticmethod
    def tokenize(text, lowercase=True, emoji=True, alphanum=True, stopwords=True):
        """
        Tokenizes text into a list of tokens. The default backwards compatible parameters filter out English stop words and only
        accept alphanumeric tokens.

        Args:
            text: input text
            lowercase: lower cases all tokens if True, defaults to True
            emoji: tokenize emoji in text if True, defaults to True
            alphanum: requires 2+ character alphanumeric tokens if True, defaults to True
            stopwords: removes provided stop words if a list, removes default English stop words if True, defaults to True

        Returns:
            list of tokens
        """

        # Create a tokenizer with backwards compatible settings
        return Tokenizer(lowercase, emoji, alphanum, stopwords)(text)

    def __init__(self, lowercase=True, emoji=True, alphanum=False, stopwords=False):
        """
        Creates a new tokenizer. The default parameters segment text per Unicode Standard Annex #29.

        Args:
            lowercase: lower cases all tokens if True, defaults to True
            emoji: tokenize emoji in text if True, defaults to True
            alphanum: requires 2+ character alphanumeric tokens if True, defaults to False
            stopwords: removes provided stop words if a list, removes default English stop words if True, defaults to False
        """

        # Lowercase
        self.lowercase = lowercase

        # Text segmentation
        self.alphanum, self.segment = None, None
        if alphanum:
            # Alphanumeric regex that accepts tokens that meet following rules:
            #  - Strings to be at least 2 characters long AND
            #  - At least 1 non-trailing alpha character in string
            # Note: The standard Python re module is much faster than regex for this expression
            self.alphanum = re.compile(r"^\d*[a-z][\-.0-9:_a-z]{1,}$")
        else:
            # Text segmentation per Unicode Standard Annex #29
            pattern = r"\w\p{Extended_Pictographic}\p{WB:RegionalIndicator}" if emoji else r"\w"
            self.segment = regex.compile(rf"[{pattern}](?:\B\S)*", flags=regex.WORD)

        # Stop words
        self.stopwords = stopwords if isinstance(stopwords, list) else Tokenizer.STOP_WORDS if stopwords else False

    def __call__(self, text):
        """
        Tokenizes text into a list of tokens.

        Args:
            text: input text

        Returns:
            list of tokens
        """

        # Check for None and skip processing
        if text is None:
            return None

        # Lowercase
        text = text.lower() if self.lowercase else text

        if self.alphanum:
            # Text segmentation using standard split
            tokens = [token.strip(string.punctuation) for token in text.split()]

            # Filter on alphanumeric strings.
            tokens = [token for token in tokens if re.match(self.alphanum, token)]
        else:
            # Text segmentation per Unicode Standard Annex #29
            tokens = regex.findall(self.segment, text)

        # Stop words
        if self.stopwords:
            tokens = [token for token in tokens if token not in self.stopwords]

        return tokens



================================================
FILE: src/python/txtai/pipeline/image/__init__.py
================================================
"""
Image imports
"""

from .caption import Caption
from .imagehash import ImageHash
from .objects import Objects



================================================
FILE: src/python/txtai/pipeline/image/caption.py
================================================
"""
Caption module
"""

# Conditional import
try:
    from PIL import Image

    PIL = True
except ImportError:
    PIL = False

from ..hfpipeline import HFPipeline


class Caption(HFPipeline):
    """
    Constructs captions for images.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        if not PIL:
            raise ImportError('Captions pipeline is not available - install "pipeline" extra to enable')

        # Call parent constructor
        super().__init__("image-to-text", path, quantize, gpu, model, **kwargs)

    def __call__(self, images):
        """
        Builds captions for images.

        This method supports a single image or a list of images. If the input is an image, the return
        type is a string. If text is a list, a list of strings is returned

        Args:
            images: image|list

        Returns:
            list of captions
        """

        # Convert single element to list
        values = [images] if not isinstance(images, list) else images

        # Open images if file strings
        values = [Image.open(image) if isinstance(image, str) else image for image in values]

        # Get and clean captions
        captions = []
        for result in self.pipeline(values):
            text = " ".join([r["generated_text"] for r in result]).strip()
            captions.append(text)

        # Return single element if single element passed in
        return captions[0] if not isinstance(images, list) else captions



================================================
FILE: src/python/txtai/pipeline/image/imagehash.py
================================================
"""
ImageHash module
"""

import numpy as np

# Conditional import
try:
    from PIL import Image
    import imagehash

    PIL = True
except ImportError:
    PIL = False

from ..base import Pipeline


class ImageHash(Pipeline):
    """
    Generates perceptual image hashes. These hashes can be used to detect near-duplicate images. This method is not
    backed by machine learning models and not intended to find conceptually similar images.
    """

    def __init__(self, algorithm="average", size=8, strings=True):
        """
        Creates an ImageHash pipeline.

        Args:
            algorithm: image hashing algorithm (average, perceptual, difference, wavelet, color)
            size: hash size
            strings: outputs hex strings if True (default), otherwise the pipeline returns numpy arrays
        """

        if not PIL:
            raise ImportError('ImageHash pipeline is not available - install "pipeline" extra to enable')

        self.algorithm = algorithm
        self.size = size
        self.strings = strings

    def __call__(self, images):
        """
        Generates perceptual image hashes.

        Args:
            images: image|list

        Returns:
            list of hashes
        """

        # Convert single element to list
        values = [images] if not isinstance(images, list) else images

        # Open images if file strings
        values = [Image.open(image) if isinstance(image, str) else image for image in values]

        # Convert images to hashes
        hashes = [self.ihash(image) for image in values]

        # Return single element if single element passed in
        return hashes[0] if not isinstance(images, list) else hashes

    def ihash(self, image):
        """
        Gets an image hash for image.

        Args:
            image: PIL image

        Returns:
            hash as hex string
        """

        # Apply hash algorithm
        if self.algorithm == "perceptual":
            data = imagehash.phash(image, self.size)
        elif self.algorithm == "difference":
            data = imagehash.dhash(image, self.size)
        elif self.algorithm == "wavelet":
            data = imagehash.whash(image, self.size)
        elif self.algorithm == "color":
            data = imagehash.colorhash(image, self.size)
        else:
            # Default to average hash
            data = imagehash.average_hash(image, self.size)

        # Convert to output data type
        return str(data) if self.strings else data.hash.astype(np.float32).reshape(-1)



================================================
FILE: src/python/txtai/pipeline/image/objects.py
================================================
"""
Objects module
"""

# Conditional import
try:
    from PIL import Image

    PIL = True
except ImportError:
    PIL = False

from ..hfpipeline import HFPipeline


class Objects(HFPipeline):
    """
    Applies object detection models to images. Supports both object detection models and image classification models.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, classification=False, threshold=0.9, **kwargs):
        if not PIL:
            raise ImportError('Objects pipeline is not available - install "pipeline" extra to enable')

        super().__init__("image-classification" if classification else "object-detection", path, quantize, gpu, model, **kwargs)

        self.classification = classification
        self.threshold = threshold

    def __call__(self, images, flatten=False, workers=0):
        """
        Applies object detection/image classification models to images. Returns a list of (label, score).

        This method supports a single image or a list of images. If the input is an image, the return
        type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is
        returned with a row per image.

        Args:
            images: image|list
            flatten: flatten output to a list of objects
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            list of (label, score)
        """

        # Convert single element to list
        values = [images] if not isinstance(images, list) else images

        # Open images if file strings
        values = [Image.open(image) if isinstance(image, str) else image for image in values]

        # Run pipeline
        results = (
            self.pipeline(values, num_workers=workers)
            if self.classification
            else self.pipeline(values, threshold=self.threshold, num_workers=workers)
        )

        # Build list of (id, score)
        outputs = []
        for result in results:
            # Convert to (label, score) tuples
            result = [(x["label"], x["score"]) for x in result if x["score"] > self.threshold]

            # Sort by score descending
            result = sorted(result, key=lambda x: x[1], reverse=True)

            # Deduplicate labels
            unique = set()
            elements = []
            for label, score in result:
                if label not in unique:
                    elements.append(label if flatten else (label, score))
                    unique.add(label)

            outputs.append(elements)

        # Return single element if single element passed in
        return outputs[0] if not isinstance(images, list) else outputs



================================================
FILE: src/python/txtai/pipeline/llm/__init__.py
================================================
"""
LLM imports
"""

from .factory import GenerationFactory
from .generation import Generation
from .huggingface import *
from .litellm import LiteLLM
from .llama import LlamaCpp
from .llm import LLM
from .rag import RAG



================================================
FILE: src/python/txtai/pipeline/llm/factory.py
================================================
"""
Factory module
"""

from ...util import Resolver

from .huggingface import HFGeneration
from .litellm import LiteLLM
from .llama import LlamaCpp


class GenerationFactory:
    """
    Methods to create generative models.
    """

    @staticmethod
    def create(path, method, **kwargs):
        """
        Creates a new Generation instance.

        Args:
            path: model path
            method: llm framework
            kwargs: model keyword arguments
        """

        # Derive method
        method = GenerationFactory.method(path, method)

        # LiteLLM generation
        if method == "litellm":
            return LiteLLM(path, **kwargs)

        # llama.cpp generation
        if method == "llama.cpp":
            return LlamaCpp(path, **kwargs)

        # Hugging Face Transformers generation
        if method == "transformers":
            return HFGeneration(path, **kwargs)

        # Resolve custom method
        return GenerationFactory.resolve(path, method, **kwargs)

    @staticmethod
    def method(path, method):
        """
        Get or derives the LLM framework.

        Args:
            path: model path
            method: llm framework

        Return:
            llm framework
        """

        if not method:
            if LiteLLM.ismodel(path):
                method = "litellm"
            elif LlamaCpp.ismodel(path):
                method = "llama.cpp"
            else:
                method = "transformers"

        return method

    @staticmethod
    def resolve(path, method, **kwargs):
        """
        Attempt to resolve a custom LLM framework.

        Args:
            path: model path
            method: llm framework
            kwargs: model keyword arguments

        Returns:
            Generation instance
        """

        try:
            return Resolver()(method)(path, **kwargs)
        except Exception as e:
            raise ImportError(f"Unable to resolve generation framework: '{method}'") from e



================================================
FILE: src/python/txtai/pipeline/llm/generation.py
================================================
"""
Generation module
"""

import re

from ...util import TemplateFormatter


class Generation:
    """
    Base class for generative models. This class has common logic for building prompts and cleaning model results.
    """

    def __init__(self, path=None, template=None, **kwargs):
        """
        Creates a new Generation instance.

        Args:
            path: model path
            template: prompt template
            kwargs: additional keyword arguments
        """

        self.path = path
        self.template = template
        self.kwargs = kwargs

    def __call__(self, text, maxlength, stream, stop, defaultrole, stripthink, **kwargs):
        """
        Generates text. Supports the following input formats:

          - String or list of strings (instruction-tuned models must follow chat templates)
          - List of dictionaries with `role` and `content` key-values or lists of lists

        Args:
            text: text|list
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings
            defaultrole: default role to apply to text inputs (prompt for raw prompts (default) or user for user chat messages)
            stripthink: strip thinking text, defaults to False
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # Format inputs
        texts = [text] if isinstance(text, str) or isinstance(text[0], dict) else text

        # Apply template, if necessary
        if self.template:
            formatter = TemplateFormatter()
            texts = [formatter.format(self.template, text=x) if isinstance(x, str) else x for x in texts]

        # Apply default role, if necessary
        if defaultrole == "user":
            texts = [[{"role": "user", "content": x}] if isinstance(x, str) else x for x in texts]

        # Run pipeline
        results = self.execute(texts, maxlength, stream, stop, **kwargs)

        # Streaming generation
        if stream:
            return self.cleanstream(results) if stripthink else results

        # Clean generated text
        results = [self.clean(texts[x], result, stripthink) for x, result in enumerate(results)]

        # Extract results based on inputs
        return results[0] if isinstance(text, str) or isinstance(text[0], dict) else results

    def isvision(self):
        """
        Returns True if this LLM supports vision operations.

        Returns:
            True if this is a vision model
        """

        return False

    def execute(self, texts, maxlength, stream, stop, **kwargs):
        """
        Runs a list of prompts through a generative model.

        Args:
            texts: list of prompts to run
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # Streaming generation
        if stream:
            return self.stream(texts, maxlength, stream, stop, **kwargs)

        # Full response as content elements
        return list(self.stream(texts, maxlength, stream, stop, **kwargs))

    def clean(self, prompt, result, stripthink):
        """
        Applies a series of rules to clean generated text.

        Args:
            prompt: original input prompt
            result: result text
            stripthink: removes thinking text if true

        Returns:
            clean text
        """

        # Replace input prompt
        text = result.replace(prompt, "") if isinstance(prompt, str) else result

        # Replace thinking text, if necessary
        if stripthink:
            text = self.cleanthink(text)

        # Apply text cleaning rules
        return text.replace("$=", "<=").strip()

    def cleanstream(self, results):
        """
        Cleans thinking tokens from streaming results and streams the remaining results.

        Args:
            results: results stream
        """

        # Consume "thinking" tokens
        text, buffer = None, ""
        for chunk in results:
            buffer += chunk
            text = self.cleanthink(buffer)
            if text != buffer:
                break

        # Yield remaining tokens
        yield from text
        yield from results

    def cleanthink(self, text):
        """
        Clean thinking tokens from text.

        Args:
            text: input text

        Returns:
            text with thinking tokens removed
        """

        text = re.sub(r"(?s)<think>.+?</think>", "", text)
        text = text.split("<|channel|>final<|message|>", 1)
        text = text[1] if len(text) > 1 else text[0]
        return text

    def response(self, result):
        """
        Parses response content from the result. This supports both standard and streaming
        generation.

        For standard generation, the full response is returned. For streaming generation,
        this method will stream chunks of content.

        Args:
            result: LLM response

        Returns:
            response
        """

        streamed = False
        for chunk in result:
            # Expects one of the following parameter paths
            #  - text
            #  - message.content
            #  - delta.content
            data = chunk["choices"][0]
            text = data.get("text", data.get("message", data.get("delta")))
            text = text if isinstance(text, str) else text.get("content")

            # Yield result if there is text AND it's not leading stream whitespace
            if text is not None and (streamed or text.strip()):
                yield (text.lstrip() if not streamed else text)
                streamed = True

    def stream(self, texts, maxlength, stream, stop, **kwargs):
        """
        Streams LLM responses.

        Args:
            texts: list of prompts to run
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            responses
        """

        raise NotImplementedError



================================================
FILE: src/python/txtai/pipeline/llm/huggingface.py
================================================
"""
Hugging Face module
"""

from threading import Thread

from transformers import AutoModelForImageTextToText, TextIteratorStreamer

from ...models import Models

from ..hfpipeline import HFPipeline

from .generation import Generation


class HFGeneration(Generation):
    """
    Hugging Face Transformers generative model.
    """

    def __init__(self, path, template=None, **kwargs):
        # Call parent constructor
        super().__init__(path, template, **kwargs)

        # Create HuggingFace LLM pipeline
        self.llm = HFLLM(path, **kwargs)

    def isvision(self):
        return isinstance(self.llm.pipeline.model, AutoModelForImageTextToText)

    def stream(self, texts, maxlength, stream, stop, **kwargs):
        yield from self.llm(texts, maxlength=maxlength, stream=stream, stop=stop, **kwargs)


class HFLLM(HFPipeline):
    """
    Hugging Face Transformers large language model (LLM) pipeline. This pipeline autodetects if the model path
    is a text generation or sequence to sequence model.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, task=None, **kwargs):
        super().__init__(self.task(path, task, **kwargs), path, quantize, gpu, model, **kwargs)

        # Load tokenizer, if necessary
        self.pipeline.tokenizer = self.pipeline.tokenizer if self.pipeline.tokenizer else Models.tokenizer(path, **kwargs)

    def __call__(self, text, prefix=None, maxlength=512, workers=0, stream=False, stop=None, **kwargs):
        """
        Generates text. Supports the following input formats:

          - String or list of strings (instruction-tuned models must follow chat templates)
          - List of dictionaries with `role` and `content` key-values or lists of lists

        Args:
            text: text|list
            prefix: optional prefix to prepend to text elements
            maxlength: maximum sequence length
            workers: number of concurrent workers to use for processing data, defaults to None
            stream: stream response if True, defaults to False
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # List of texts
        texts = text if isinstance(text, list) else [text]

        # Add prefix, if necessary
        if prefix:
            texts = [f"{prefix}{x}" for x in texts]

        # Combine all keyword arguments
        args, kwargs = self.parameters(texts, maxlength, workers, stop, **kwargs)

        # Stream response
        if stream:
            return StreamingResponse(self.pipeline, texts, stop, **kwargs)()

        # Run pipeline and extract generated text
        results = [self.extract(result) for result in self.pipeline(*args, **kwargs)]

        return results[0] if isinstance(text, str) else results

    def parameters(self, texts, maxlength, workers, stop, **kwargs):
        """
        Builds a list of arguments and a combined parameter dictionary to use as keyword arguments.

        Args:
            texts: input texts
            maxlength: maximum sequence length
            workers: number of concurrent workers to use for processing data, defaults to None
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            args, kwargs
        """

        # Set defaults and get underlying model
        defaults, model = {"max_length": maxlength, "max_new_tokens": None, "num_workers": workers}, self.pipeline.model

        # Set parameters for vision models and return
        if self.pipeline.task == "image-text-to-text":
            # Maxlength has to be large enough to accomodate images
            defaults["max_length"] = max(maxlength, 2048)

            # Set default token id
            tokenid = model.generation_config.pad_token_id
            model.generation_config.pad_token_id = tokenid if tokenid else model.generation_config.eos_token_id

            # Vision models take all arguments as keyword arguments
            return [], {**{"text": texts, "truncation": True}, **defaults, **kwargs}

        # Add pad token if it's missing from model config
        if not model.config.pad_token_id:
            tokenid = model.config.eos_token_id
            tokenid = tokenid[0] if isinstance(tokenid, list) else tokenid

            # Set pad_token_id parameter
            defaults["pad_token_id"] = tokenid

            # Update tokenizer for batching
            if "batch_size" in kwargs and self.pipeline.tokenizer.pad_token_id is None:
                self.pipeline.tokenizer.pad_token_id = tokenid
                self.pipeline.tokenizer.padding_side = "left"

        # Set tokenizer when stop strings is set
        if stop:
            defaults["tokenizer"] = self.pipeline.tokenizer

        return [texts], {**defaults, **kwargs}

    def extract(self, result):
        """
        Extracts generated text from a pipeline result.

        Args:
            result: pipeline result

        Returns:
            generated text
        """

        # Extract output from list, if necessary
        result = result[0] if isinstance(result, list) else result
        text = result["generated_text"]
        return text[-1]["content"] if isinstance(text, list) else text

    def task(self, path, task, **kwargs):
        """
        Get the pipeline task name.

        Args:
            path: model path input
            task: task name
            kwargs: optional additional keyword arguments

        Returns:
            pipeline task name
        """

        # Mapping from txtai to Hugging Face pipeline tasks
        mapping = {"language-generation": "text-generation", "sequence-sequence": "text2text-generation", "vision": "image-text-to-text"}

        # Attempt to resolve task
        if path and not task:
            task = Models.task(path, **kwargs)

        # Map to Hugging Face task. Default to text2text-generation pipeline when task not resolved.
        return mapping.get(task, "text2text-generation")


class Generator(HFLLM):
    """
    Generate text with a causal language model.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        super().__init__(path, quantize, gpu, model, "language-generation", **kwargs)


class Sequences(HFLLM):
    """
    Generate text with a sequence-sequence model.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        super().__init__(path, quantize, gpu, model, "sequence-sequence", **kwargs)


class StreamingResponse:
    """
    Generate text as a streaming response.
    """

    def __init__(self, pipeline, texts, stop, **kwargs):
        # Create streamer
        self.stream = TextIteratorStreamer(pipeline.tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=5)
        kwargs["streamer"] = self.stream
        kwargs["stop_strings"] = stop

        # Create thread
        self.thread = Thread(target=pipeline, args=[texts], kwargs=kwargs)

        # Store number of inputs
        self.length = len(texts)

    def __call__(self):
        # Start the process
        self.thread.start()

        return self

    def __iter__(self):
        for _ in range(self.length):
            yield from self.stream



================================================
FILE: src/python/txtai/pipeline/llm/litellm.py
================================================
"""
LiteLLM module
"""

from transformers.utils import cached_file

# Conditional import
try:
    import litellm as api

    LITELLM = True
except ImportError:
    LITELLM = False

from .generation import Generation


class LiteLLM(Generation):
    """
    LiteLLM generative model.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a LiteLLM model.

        Args:
            path: input path

        Returns:
            True if this is a LiteLLM model, False otherwise
        """

        # pylint: disable=W0702
        if isinstance(path, str) and LITELLM:
            debug = api.suppress_debug_info
            try:
                # Suppress debug messages for this test
                api.suppress_debug_info = True
                return api.get_llm_provider(path) and not LiteLLM.ishub(path)
            except:
                return False
            finally:
                # Restore debug info value to original value
                api.suppress_debug_info = debug

        return False

    @staticmethod
    def ishub(path):
        """
        Checks if path is available on the HF Hub.

        Args:
            input path

        Returns:
            True if this is a model on the HF Hub
        """

        # pylint: disable=W0702
        try:
            return cached_file(path_or_repo_id=path, filename="config.json") is not None if "/" in path else False
        except:
            return False

    def __init__(self, path, template=None, **kwargs):
        super().__init__(path, template, **kwargs)

        if not LITELLM:
            raise ImportError('LiteLLM is not available - install "pipeline" extra to enable')

        # Ignore common pipeline parameters
        self.kwargs = {k: v for k, v in self.kwargs.items() if k not in ["quantize", "gpu", "model", "task"]}

    def stream(self, texts, maxlength, stream, stop, **kwargs):
        for text in texts:
            # LLM API call
            result = api.completion(
                model=self.path,
                messages=[{"content": text, "role": "prompt"}] if isinstance(text, str) else text,
                max_tokens=maxlength,
                stream=stream,
                stop=stop,
                **{**self.kwargs, **kwargs}
            )

            # Stream response
            yield from self.response(result if stream else [result])



================================================
FILE: src/python/txtai/pipeline/llm/llama.py
================================================
"""
Llama module
"""

import os

from huggingface_hub import hf_hub_download

# Conditional import
try:
    import llama_cpp as llama

    LLAMA_CPP = True
except ImportError:
    LLAMA_CPP = False

from .generation import Generation


class LlamaCpp(Generation):
    """
    llama.cpp generative model.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a llama.cpp model.

        Args:
            path: input path

        Returns:
            True if this is a llama.cpp model, False otherwise
        """

        return isinstance(path, str) and path.lower().endswith(".gguf")

    def __init__(self, path, template=None, **kwargs):
        super().__init__(path, template, **kwargs)

        if not LLAMA_CPP:
            raise ImportError('llama.cpp is not available - install "pipeline" extra to enable')

        # Check if this is a local path, otherwise download from the HF Hub
        path = path if os.path.exists(path) else self.download(path)

        # Create llama.cpp instance
        self.llm = self.create(path, **kwargs)

    def stream(self, texts, maxlength, stream, stop, **kwargs):
        for text in texts:
            yield from (
                self.messages(text, maxlength, stream, stop, **kwargs)
                if isinstance(text, list)
                else self.prompt(text, maxlength, stream, stop, **kwargs)
            )

    def download(self, path):
        """
        Downloads path from the Hugging Face Hub.

        Args:
            path: full model path

        Returns:
            local cached model path
        """

        # Split into parts
        parts = path.split("/")

        # Calculate repo id split
        repo = 2 if len(parts) > 2 else 1

        # Download and cache file
        return hf_hub_download(repo_id="/".join(parts[:repo]), filename="/".join(parts[repo:]))

    def create(self, path, **kwargs):
        """
        Creates a new llama.cpp model instance.

        Args:
            path: path to model
            kwargs: additional keyword args

        Returns:
            llama.cpp instance
        """

        # Default n_ctx=0 if not already set. This sets n_ctx = n_ctx_train.
        kwargs["n_ctx"] = kwargs.get("n_ctx", 0)

        # Default GPU layers if not already set
        kwargs["n_gpu_layers"] = kwargs.get("n_gpu_layers", -1 if kwargs.get("gpu", os.environ.get("LLAMA_NO_METAL") != "1") else 0)

        # Default verbose flag
        kwargs["verbose"] = kwargs.get("verbose", False)

        # Create llama.cpp instance
        try:
            return llama.Llama(model_path=path, **kwargs)
        except ValueError as e:
            # Fallback to default n_ctx when not enough memory for n_ctx = n_ctx_train
            if not kwargs["n_ctx"]:
                kwargs.pop("n_ctx")
                return llama.Llama(model_path=path, **kwargs)

            # Raise exception if n_ctx manually specified
            raise e

    def messages(self, messages, maxlength, stream, stop, **kwargs):
        """
        Processes a list of messages.

        Args:
            messages: list of dictionaries with `role` and `content` key-values
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # LLM call with messages
        result = self.llm.create_chat_completion(messages=messages, max_tokens=maxlength, stream=stream, stop=stop, **kwargs)

        # Stream response
        yield from self.response(result if stream else [result])

    def prompt(self, text, maxlength, stream, stop, **kwargs):
        """
        Processes a prompt.

        Args:
            prompt: prompt text
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # LLM call with prompt
        result = self.llm(text, max_tokens=maxlength, stream=stream, stop=stop, **kwargs)

        # Stream response
        yield from self.response(result if stream else [result])



================================================
FILE: src/python/txtai/pipeline/llm/llm.py
================================================
"""
LLM module
"""

import logging

from .factory import GenerationFactory

from ..base import Pipeline

# Logging configuration
logger = logging.getLogger(__name__)


class LLM(Pipeline):
    """
    Pipeline for running large language models (LLMs). This class supports the following LLM backends:

      - Local LLMs with Hugging Face Transformers
      - Local LLMs with llama.cpp
      - Remote API LLMs with LiteLLM
      - Custom generation implementations
    """

    def __init__(self, path=None, method=None, **kwargs):
        """
        Creates a new LLM.

        Args:
            path: model path
            method: llm model framework, infers from path if not provided
            kwargs: model keyword arguments
        """

        # Default LLM if not provided
        path = path if path else "google/flan-t5-base"

        # Generation instance
        self.generator = GenerationFactory.create(path, method, **kwargs)

    def __call__(self, text, maxlength=512, stream=False, stop=None, defaultrole="prompt", stripthink=False, **kwargs):
        """
        Generates text. Supports the following input formats:

          - String or list of strings (instruction-tuned models must follow chat templates)
          - List of dictionaries with `role` and `content` key-values or lists of lists

        Args:
            text: text|list
            maxlength: maximum sequence length
            stream: stream response if True, defaults to False
            stop: list of stop strings, defaults to None
            defaultrole: default role to apply to text inputs (prompt for raw prompts (default) or user for user chat messages)
            stripthink: strip thinking tags, defaults to False
            kwargs: additional generation keyword arguments

        Returns:
            generated text
        """

        # Debug logging
        logger.debug(text)

        # Run LLM generation
        return self.generator(text, maxlength, stream, stop, defaultrole, stripthink, **kwargs)

    def isvision(self):
        """
        Returns True if this LLM supports vision operations.

        Returns:
            True if this is a vision model
        """

        return self.generator.isvision()



================================================
FILE: src/python/txtai/pipeline/llm/rag.py
================================================
"""
RAG module
"""

from ...models import Models

from ..base import Pipeline
from ..data import Tokenizer
from ..text import Questions
from ..text import Similarity

from .factory import GenerationFactory
from .llm import LLM


class RAG(Pipeline):
    """
    Extracts knowledge from content by joining a prompt, context data store and generative model together. The data store can be
    an embeddings database or a similarity instance with associated input text. The generative model can be a prompt-driven large
    language model (LLM), an extractive question-answering model or a custom pipeline. This is known as retrieval augmented generation (RAG).
    """

    # pylint: disable=R0913
    def __init__(
        self,
        similarity,
        path,
        quantize=False,
        gpu=True,
        model=None,
        tokenizer=None,
        minscore=None,
        mintokens=None,
        context=None,
        task=None,
        output="default",
        template=None,
        separator=" ",
        system=None,
        **kwargs,
    ):
        """
        Builds a new RAG pipeline.

        Args:
            similarity: similarity instance (embeddings or similarity pipeline)
            path: path to model, supports a LLM, Questions or custom pipeline
            quantize: True if model should be quantized before inference, False otherwise.
            gpu: if gpu inference should be used (only works if GPUs are available)
            model: optional existing pipeline model to wrap
            tokenizer: Tokenizer class
            minscore: minimum score to include context match, defaults to None
            mintokens: minimum number of tokens to include context match, defaults to None
            context: topn context matches to include, defaults to 3
            task: model task (language-generation, sequence-sequence or question-answering), defaults to auto-detect
            output: output format, 'default' returns (name, answer), 'flatten' returns answers and 'reference' returns (name, answer, reference)
            template: prompt template, it must have a parameter for {question} and {context}, defaults to "{question} {context}"
            separator: context separator
            system: system prompt, defaults to None
            kwargs: additional keyword arguments to pass to pipeline model
        """

        # Similarity instance
        self.similarity = similarity

        # Model can be a LLM, Questions or custom pipeline
        self.model = self.load(path, quantize, gpu, model, task, **kwargs)

        # Tokenizer class use default method if not set
        self.tokenizer = tokenizer if tokenizer else Tokenizer() if hasattr(self.similarity, "scoring") and self.similarity.isweighted() else None

        # Minimum score to include context match
        self.minscore = minscore if minscore is not None else 0.0

        # Minimum number of tokens to include context match
        self.mintokens = mintokens if mintokens is not None else 0.0

        # Top n context matches to include for context
        self.context = context if context else 3

        # Output format
        self.output = output

        # Prompt template
        self.template = template if template else "{question} {context}"

        # Context separator
        self.separator = separator

        # System prompt template
        self.system = system

    def __call__(self, queue, texts=None, **kwargs):
        """
        Finds answers to input questions. This method runs queries to find the top n best matches and uses that as the context.
        A model is then run against the context for each input question, with the answer returned.

        Args:
            queue: input question queue (name, query, question, snippet), can be list of tuples/dicts/strings or a single input element
            texts: optional list of text for context, otherwise runs embeddings search
            kwargs: additional keyword arguments to pass to pipeline model

        Returns:
            list of answers matching input format (tuple or dict) containing fields as specified by output format
        """

        # Save original queue format
        inputs = queue

        # Convert queue to list, if necessary
        queue = queue if isinstance(queue, list) else [queue]

        # Convert dictionary inputs to tuples
        if queue and isinstance(queue[0], dict):
            # Convert dict to tuple
            queue = [tuple(row.get(x) for x in ["name", "query", "question", "snippet"]) for row in queue]

        if queue and isinstance(queue[0], str):
            # Convert string questions to tuple
            queue = [(None, row, row, None) for row in queue]

        # Rank texts by similarity for each query
        results = self.query([query for _, query, _, _ in queue], texts)

        # Build question-context pairs
        names, queries, questions, contexts, topns, snippets = [], [], [], [], [], []
        for x, (name, query, question, snippet) in enumerate(queue):
            # Get top n best matching segments
            topn = sorted(results[x], key=lambda y: y[2], reverse=True)[: self.context]

            # Generate context using ordering from texts, if available, otherwise order by score
            context = self.separator.join(text for _, text, _ in (sorted(topn, key=lambda y: y[0]) if texts else topn))

            names.append(name)
            queries.append(query)
            questions.append(question)
            contexts.append(context)
            topns.append(topn)
            snippets.append(snippet)

        # Run pipeline and return answers
        answers = self.answers(questions, contexts, **kwargs)

        # Apply output formatting to answers and return
        return self.apply(inputs, names, queries, answers, topns, snippets) if isinstance(answers, list) else answers

    def load(self, path, quantize, gpu, model, task, **kwargs):
        """
        Loads a LLM, Questions or custom pipeline.

        Args:
            path: path to model, supports a LLM, Questions or custom pipeline
            quantize: True if model should be quantized before inference, False otherwise.
            gpu: if gpu inference should be used (only works if GPUs are available)
            model: optional existing pipeline model to wrap
            task: model task (language-generation, sequence-sequence or question-answering), defaults to auto-detect
            kwargs: additional keyword arguments to pass to pipeline model

        Returns:
            LLM, Questions or custom pipeline
        """

        # Only try to load if path is a string
        if not isinstance(path, str):
            return path

        # Attempt to resolve task if not provided
        task = GenerationFactory.method(path, task)
        task = Models.task(path, **kwargs) if task == "transformers" else task

        # Load Questions pipeline
        if task == "question-answering":
            return Questions(path, quantize, gpu, model, **kwargs)

        # Load LLM pipeline
        return LLM(path=path, quantize=quantize, gpu=gpu, model=model, task=task, **kwargs)

    def query(self, queries, texts):
        """
        Rank texts by similarity for each query. If texts is empty, an embeddings search will be executed.
        Returns results sorted by best match.

        Args:
            queries: list of queries
            texts: optional list of text

        Returns:
            list of (id, data, score) per query
        """

        if not queries:
            return []

        # Score text against queries
        scores, segments, tokenlist = self.score(queries, texts)

        # Build question-context pairs
        results = []
        for i, query in enumerate(queries):
            # Get list of required and prohibited tokens
            must = [token.strip("+") for token in query.split() if token.startswith("+") and len(token) > 1]
            mnot = [token.strip("-") for token in query.split() if token.startswith("-") and len(token) > 1]

            # Segment text is static when texts is passed in but different per query when an embeddings search is run
            segment = segments if texts else segments[i]
            tokens = tokenlist if texts else tokenlist[i]

            # List of matches
            matches = []
            for y, (x, score) in enumerate(scores[i]):
                # Segments and tokens are statically ordered when texts is passed in, need to resolve values with score id
                # Scores, segments and tokens all share the same list ordering when an embeddings search is run
                x = x if texts else y

                # Get segment text
                text = segment[x][1]

                # Add result if:
                #   - all required tokens are present or there are not required tokens AND
                #   - all prohibited tokens are not present or there are not prohibited tokens
                #   - score is above minimum score required
                #   - number of tokens is above minimum number of tokens required
                if (not must or all(token.lower() in text.lower() for token in must)) and (
                    not mnot or all(token.lower() not in text.lower() for token in mnot)
                ):
                    if score >= self.minscore and len(tokens[x]) >= self.mintokens:
                        matches.append(segment[x] + (score,))

            # Add query matches sorted by highest score
            results.append(matches)

        return results

    def score(self, queries, texts):
        """
        Runs queries against texts (or an embeddings search if texts is empty) and builds list of
        similarity scores for each query-text combination.

        Args:
            queries: list of queries
            texts: optional list of text

        Returns:
            scores, segments, tokenlist
        """

        # Tokenize text
        segments, tokenlist = [], []
        if texts:
            for text in texts:
                # Run tokenizer method, if available, otherwise returns original text
                tokens = self.tokenize(text)
                if tokens:
                    segments.append(text)
                    tokenlist.append(tokens)

            # Add index id to segments to preserve ordering after filters
            segments = list(enumerate(segments))

        # Get list of (id, score) - sorted by highest score per query
        if isinstance(self.similarity, Similarity):
            # Score using similarity pipeline
            scores = self.similarity(queries, [t for _, t in segments])
        elif texts:
            # Score using embeddings.batchsimilarity
            scores = self.similarity.batchsimilarity([self.tokenize(x) for x in queries], tokenlist)
        else:
            # Score using embeddings.batchsearch
            scores, segments, tokenlist = self.batchsearch(queries)

        return scores, segments, tokenlist

    def batchsearch(self, queries):
        """
        Runs a batch embeddings search for a set of queries.

        Args:
            queries: list of queries to run

        Returns:
            scores, segments, tokenlist
        """

        scores, segments, tokenlist = [], [], []
        for results in self.similarity.batchsearch([self.tokenize(x) for x in queries], self.context):
            # Assume embeddings content is enabled and results are dictionaries
            scores.append([(result["id"], result["score"]) for result in results])
            segments.append([(result["id"], result["text"]) for result in results])
            tokenlist.append([self.tokenize(result["text"]) for result in results])

        return scores, segments, tokenlist

    def tokenize(self, text):
        """
        Tokenizes text. Returns original text if tokenizer is not available.

        Args:
            text: input text

        Returns:
            tokens if tokenizer available otherwise original text
        """

        return self.tokenizer(text) if self.tokenizer else text

    def answers(self, questions, contexts, **kwargs):
        """
        Executes pipeline and formats extracted answers.

        Args:
            questions: questions
            contexts: question context
            kwargs: additional keyword arguments to pass to model

        Returns:
            answers
        """

        # Run model inference with questions pipeline
        if isinstance(self.model, Questions):
            return self.model(questions, contexts)

        # Run generator pipeline
        return self.model(self.prompts(questions, contexts), **kwargs)

    def prompts(self, questions, contexts):
        """
        Builds a list of prompts using the passed in questions and contexts.

        Args:
            questions: questions
            contexts: question context

        Returns:
            prompts
        """

        # Format prompts for generator pipeline
        prompts = []
        for x, context in enumerate(contexts):
            # Create input prompt
            prompt = self.template.format(question=questions[x], context=context)

            # Add system prompt, if necessary
            if self.system:
                prompt = [
                    {"role": "system", "content": self.system.format(question=questions[x], context=context)},
                    {"role": "user", "content": prompt},
                ]

            prompts.append(prompt)

        return prompts

    def apply(self, inputs, names, queries, answers, topns, snippets):
        """
        Applies the following formatting rules to answers.
            - each answer row matches input format (tuple or dict)
            - if output format is 'flatten' then this method flattens to a list of answers
            - if output format is 'reference' then a list of (name, answer, reference) is returned
            - otherwise, if output format is 'default' or anything else list of (name, answer) is returned

        Args:
            inputs: original inputs
            names: question identifiers/names
            queries: list of input queries
            answers: list of generated answers
            topns: top n records used for context
            snippets: flags to enable answer snippets per answer

        Returns:
            list of answers matching input format (tuple or dict) containing fields as specified by output format
        """

        # Resolve answers as snippets
        answers = self.snippets(names, answers, topns, snippets)

        # Flatten to list of answers and return
        if self.output == "flatten":
            answers = [answer for _, answer in answers]
        else:
            # Resolve id reference for each answer
            if self.output == "reference":
                answers = self.reference(queries, answers, topns)

            # Ensure output format matches input format
            first = inputs[0] if inputs and isinstance(inputs, list) else inputs
            if isinstance(first, (dict, str)):
                # Add name if input queue had name field
                fields = ["name", "answer", "reference"] if isinstance(first, dict) and "name" in first else [None, "answer", "reference"]
                answers = [{fields[x]: column for x, column in enumerate(row) if fields[x]} for row in answers]

        # Unpack single answer, if necessary
        return answers[0] if answers and isinstance(inputs, (tuple, dict, str)) else answers

    def snippets(self, names, answers, topns, snippets):
        """
        Extracts text surrounding the answer within context.

        Args:
            names: question identifiers/names
            answers: list of generated answers
            topns: top n records used for context
            snippets: flags to enable answer snippets per answer

        Returns:
            answers resolved as snippets per question, if necessary
        """

        # Extract and format answer
        results = []

        for x, answer in enumerate(answers):
            # Resolve snippet if necessary
            if answer and snippets[x]:
                # Searches for first text element to contain answer
                for _, text, _ in topns[x]:
                    if answer in text:
                        answer = text
                        break

            results.append((names[x], answer))

        return results

    def reference(self, queries, answers, topns):
        """
        Reference each answer with the best matching context element id.

        Args:
            queries: list of input queries
            answers: list of answers
            topn: top n context elements as (id, data, tag)

        Returns:
            list of (name, answer, reference)
        """

        # Convert queries to terms
        terms = self.terms(queries)

        outputs = []
        for x, (name, answer) in enumerate(answers):
            # Get matching topn
            topn, reference = topns[x], None

            if topn:
                # Build query from keyword terms and the answer text
                query = f"{terms[x]} {answers[x][1]}"

                # Compare answer to topns to find best match
                scores, _, _ = self.score([query], [text for _, text, _ in topn])

                # Get top score index
                index = scores[0][0][0]

                # Use matching topn id as reference
                reference = topn[index][0]

            # Append (name, answer, reference) tuple
            outputs.append((name, answer, reference))

        return outputs

    def terms(self, queries):
        """
        Extracts keyword terms from a list of queries using underlying similarity model.

        Args:
            queries: list of queries

        Returns:
            list of queries reduced down to keyword term strings
        """

        # Extract keyword terms from queries if underlying similarity model supports it
        return self.similarity.batchterms(queries) if hasattr(self.similarity, "batchterms") else queries



================================================
FILE: src/python/txtai/pipeline/text/__init__.py
================================================
"""
Text imports
"""

from .crossencoder import CrossEncoder
from .entity import Entity
from .labels import Labels
from .lateencoder import LateEncoder
from .questions import Questions
from .reranker import Reranker
from .similarity import Similarity
from .summary import Summary
from .translation import Translation



================================================
FILE: src/python/txtai/pipeline/text/crossencoder.py
================================================
"""
CrossEncoder module
"""

import numpy as np

from ..hfpipeline import HFPipeline


class CrossEncoder(HFPipeline):
    """
    Computes similarity between query and list of text using a cross-encoder model
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        super().__init__("text-classification", path, quantize, gpu, model, **kwargs)

    def __call__(self, query, texts, multilabel=True, workers=0):
        """
        Computes the similarity between query and list of text. Returns a list of
        (id, score) sorted by highest score, where id is the index in texts.

        This method supports query as a string or a list. If the input is a string,
        the return type is a 1D list of (id, score). If text is a list, a 2D list
        of (id, score) is returned with a row per string.

        Args:
            query: query text|list
            texts: list of text
            multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            list of (id, score)
        """

        scores = []
        for q in [query] if isinstance(query, str) else query:
            # Pass (query, text) pairs to model
            result = self.pipeline([{"text": q, "text_pair": t} for t in texts], top_k=None, function_to_apply="none", num_workers=workers)

            # Apply score transform function
            scores.append(self.function([r[0]["score"] for r in result], multilabel))

        # Build list of (id, score) per query sorted by highest score
        scores = [sorted(enumerate(row), key=lambda x: x[1], reverse=True) for row in scores]

        return scores[0] if isinstance(query, str) else scores

    def function(self, scores, multilabel):
        """
        Applys an output transformation function based on value of multilabel.

        Args:
            scores: input scores
            multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None

        Returns:
            transformed scores
        """

        # Output functions
        # pylint: disable=C3001
        identity = lambda x: x
        sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))
        softmax = lambda x: np.exp(x) / np.sum(np.exp(x))
        function = identity if multilabel is None else sigmoid if multilabel else softmax

        # Apply output function
        return function(np.array(scores))



================================================
FILE: src/python/txtai/pipeline/text/entity.py
================================================
"""
Entity module
"""

# Conditional import
try:
    from gliner import GLiNER

    GLINER = True
except ImportError:
    GLINER = False

from huggingface_hub.errors import HFValidationError
from transformers.utils import cached_file

from ...models import Models
from ..hfpipeline import HFPipeline


class Entity(HFPipeline):
    """
    Applies a token classifier to text and extracts entity/label combinations.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        # Create a new entity pipeline
        self.gliner = self.isgliner(path)
        if self.gliner:
            if not GLINER:
                raise ImportError('GLiNER is not available - install "pipeline" extra to enable')

            # GLiNER entity pipeline
            self.pipeline = GLiNER.from_pretrained(path)
            self.pipeline = self.pipeline.to(Models.device(Models.deviceid(gpu)))
        else:
            # Standard entity pipeline
            super().__init__("token-classification", path, quantize, gpu, model, **kwargs)

    def __call__(self, text, labels=None, aggregate="simple", flatten=None, join=False, workers=0):
        """
        Applies a token classifier to text and extracts entity/label combinations.

        Args:
            text: text|list
            labels: list of entity type labels to accept, defaults to None which accepts all
            aggregate: method to combine multi token entities - options are "simple" (default), "first", "average" or "max"
            flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.
            join: joins flattened output into a string if True, ignored if flatten not set
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            list of (entity, entity type, score) or list of entities depending on flatten parameter
        """

        # Run token classification pipeline
        results = self.execute(text, labels, aggregate, workers)

        # Convert results to a list if necessary
        if isinstance(text, str):
            results = [results]

        # Score threshold when flatten is set
        threshold = 0.0 if isinstance(flatten, bool) else flatten

        # Extract entities if flatten set, otherwise extract (entity, entity type, score) tuples
        outputs = []
        for result in results:
            if flatten:
                output = [r["word"] for r in result if self.accept(r["entity_group"], labels) and r["score"] >= threshold]
                outputs.append(" ".join(output) if join else output)
            else:
                outputs.append([(r["word"], r["entity_group"], float(r["score"])) for r in result if self.accept(r["entity_group"], labels)])

        return outputs[0] if isinstance(text, str) else outputs

    def isgliner(self, path):
        """
        Tests if path is a GLiNER model.

        Args:
            path: model path

        Returns:
            True if this is a GLiNER model, False otherwise
        """

        try:
            # Test if this model has a gliner_config.json file
            return cached_file(path_or_repo_id=path, filename="gliner_config.json") is not None

        # Ignore this error - invalid repo or directory
        except (HFValidationError, OSError):
            pass

        return False

    def execute(self, text, labels, aggregate, workers):
        """
        Runs the entity extraction pipeline.

        Args:
            text: text|list
            labels: list of entity type labels to accept, defaults to None which accepts all
            aggregate: method to combine multi token entities - options are "simple" (default), "first", "average" or "max"
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            list of entities and labels
        """

        if self.gliner:
            # Extract entities with GLiNER. Use default CoNLL-2003 labels when not otherwise provided.
            results = self.pipeline.batch_predict_entities(
                text if isinstance(text, list) else [text], labels if labels else ["person", "organization", "location"]
            )

            # Map results to same format as Transformers token classifier
            entities = []
            for result in results:
                entities.append([{"word": x["text"], "entity_group": x["label"], "score": x["score"]} for x in result])

            # Return extracted entities
            return entities if isinstance(text, list) else entities[0]

        # Standard Transformers token classification pipeline
        return self.pipeline(text, aggregation_strategy=aggregate, num_workers=workers)

    def accept(self, etype, labels):
        """
        Determines if entity type is in valid entity type.

        Args:
            etype: entity type
            labels: list of entities to accept

        Returns:
            if etype is accepted
        """

        return not labels or etype in labels



================================================
FILE: src/python/txtai/pipeline/text/labels.py
================================================
"""
Labels module
"""

from ..hfpipeline import HFPipeline


class Labels(HFPipeline):
    """
    Applies a text classifier to text. Supports zero shot and standard text classification models
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, **kwargs):
        super().__init__("zero-shot-classification" if dynamic else "text-classification", path, quantize, gpu, model, **kwargs)

        # Set if labels are dynamic (zero shot) or fixed (standard text classification)
        self.dynamic = dynamic

    def __call__(self, text, labels=None, multilabel=False, flatten=None, workers=0, **kwargs):
        """
        Applies a text classifier to text. Returns a list of (id, score) sorted by highest score,
        where id is the index in labels. For zero shot classification, a list of labels is required.
        For text classification models, a list of labels is optional, otherwise all trained labels are returned.

        This method supports text as a string or a list. If the input is a string, the return
        type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is
        returned with a row per string.

        Args:
            text: text|list
            labels: list of labels
            multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None
            flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.
            workers: number of concurrent workers to use for processing data, defaults to None
            kwargs: additional keyword args

        Returns:
            list of (id, score) or list of labels depending on flatten parameter
        """

        if self.dynamic:
            # Run zero shot classification pipeline
            results = self.pipeline(text, labels, multi_label=multilabel, truncation=True, num_workers=workers)
        else:
            # Set classification function based on inputs
            function = "none" if multilabel is None else "sigmoid" if multilabel or len(self.labels()) == 1 else "softmax"

            # Run text classification pipeline
            results = self.pipeline(text, top_k=None, function_to_apply=function, num_workers=workers, **kwargs)

        # Convert results to a list if necessary
        if isinstance(text, str):
            results = [results]

        # Build list of outputs and return
        outputs = self.outputs(results, labels, flatten)
        return outputs[0] if isinstance(text, str) else outputs

    def labels(self):
        """
        Returns a list of all text classification model labels sorted in index order.

        Returns:
            list of labels
        """

        return list(self.pipeline.model.config.id2label.values())

    def outputs(self, results, labels, flatten):
        """
        Processes pipeline results and builds outputs.

        Args:
            results: pipeline results
            labels: list of labels
            flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.

        Returns:
            list of outputs
        """

        outputs = []
        threshold = 0.0 if isinstance(flatten, bool) else flatten

        for result in results:
            if self.dynamic:
                if flatten:
                    result = [label for x, label in enumerate(result["labels"]) if result["scores"][x] >= threshold]
                    outputs.append(result[:1] if isinstance(flatten, bool) else result)
                else:
                    outputs.append([(labels.index(label), result["scores"][x]) for x, label in enumerate(result["labels"])])
            else:
                if flatten:
                    result = [x["label"] for x in result if x["score"] >= threshold and (not labels or x["label"] in labels)]
                    outputs.append(result[:1] if isinstance(flatten, bool) else result)
                else:
                    # Filter results using labels, if provided
                    outputs.append(self.limit(result, labels))

        return outputs

    def limit(self, result, labels):
        """
        Filter result using labels. If labels is None, original result is returned.

        Args:
            result: results array sorted by score descending
            labels: list of labels or None

        Returns:
            filtered results
        """

        # Get config
        config = self.pipeline.model.config

        # Resolve label ids for labels
        result = [(config.label2id.get(x["label"], 0), x["score"]) for x in result]

        if labels:
            matches = []
            for label in labels:
                # Lookup label keys from model config
                if label.isdigit():
                    label = int(label)
                    keys = list(config.id2label.keys())
                else:
                    label = label.lower()
                    keys = [x.lower() for x in config.label2id.keys()]

                # Find and add label match
                if label in keys:
                    matches.append(keys.index(label))

            return [(label, score) for label, score in result if label in matches]

        return result



================================================
FILE: src/python/txtai/pipeline/text/lateencoder.py
================================================
"""
Late encoder module
"""

import numpy as np
import torch

from ...models import Models, PoolingFactory
from ..base import Pipeline


class LateEncoder(Pipeline):
    """
    Computes similarity between query and list of text using a late interaction model.
    """

    def __init__(self, path=None, **kwargs):
        # Get device
        self.device = Models.device(Models.deviceid(kwargs.get("gpu", True)))

        # Load model
        self.model = PoolingFactory.create(
            {
                "method": kwargs.get("method"),
                "path": path if path else "colbert-ir/colbertv2.0",
                "device": self.device,
                "tokenizer": kwargs.get("tokenizer"),
                "maxlength": kwargs.get("maxlength"),
                "modelargs": {**kwargs.get("vectors", {}), **{"muvera": None}},
            }
        )

    def __call__(self, query, texts, limit=None):
        """
        Computes the similarity between query and list of text. Returns a list of
        (id, score) sorted by highest score, where id is the index in texts.

        This method supports query as a string or a list. If the input is a string,
        the return type is a 1D list of (id, score). If text is a list, a 2D list
        of (id, score) is returned with a row per string.

        Args:
            query: query text|list
            texts: list of text
            limit: maximum comparisons to return, defaults to all

        Returns:
            list of (id, score)
        """

        queries = [query] if isinstance(query, str) else query

        # Encode text to vectors
        queries = self.encode(queries, "query")
        data = self.encode(texts, "data") if isinstance(texts[0], str) else texts

        # Compute maximum similarity score
        scores = []
        for q in queries:
            scores.extend(self.score(q.unsqueeze(0), data, limit))

        return scores[0] if isinstance(query, str) else scores

    def encode(self, data, category):
        """
        Encodes a batch of data using the underlying model.

        Args:
            data: input data
            category: encoding category

        Returns:
            encoded data
        """

        return torch.from_numpy(self.model.encode(data, category=category)).to(self.device)

    def score(self, queries, data, limit):
        """
        Computes the maximum similarity score between query vectors and data vectors.

        Args:
            queries: query vectors
            data: data vectors
            limit: query limit

        Returns:
            list of (id, score)
        """

        # Compute bulk dot product using einstein notation
        scores = torch.einsum("ash,bth->abst", queries, data).max(axis=-1).values.mean(axis=-1)
        scores = scores.cpu().numpy()

        # Get top n matching indices and scores
        indices = np.argpartition(-scores, limit if limit and limit < scores.shape[0] else scores.shape[0] - 1)[:, :limit]
        scores = np.take_along_axis(scores, indices, axis=1)

        results = []
        for x, index in enumerate(indices):
            results.append(list(zip(index.tolist(), scores[x].tolist())))

        return results



================================================
FILE: src/python/txtai/pipeline/text/questions.py
================================================
"""
Questions module
"""

from ..hfpipeline import HFPipeline


class Questions(HFPipeline):
    """
    Runs extractive QA for a series of questions and contexts.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        super().__init__("question-answering", path, quantize, gpu, model, **kwargs)

    def __call__(self, questions, contexts, workers=0):
        """
        Runs a extractive question-answering model against each question-context pair, finding the best answers.

        Args:
            questions: list of questions
            contexts: list of contexts to pull answers from
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            list of answers
        """

        answers = []

        for x, question in enumerate(questions):
            if question and contexts[x]:
                # Run the QA pipeline
                result = self.pipeline(question=question, context=contexts[x], num_workers=workers)

                # Get answer and score
                answer, score = result["answer"], result["score"]

                # Require score to be at least 0.05
                if score < 0.05:
                    answer = None

                # Add answer
                answers.append(answer)
            else:
                answers.append(None)

        return answers



================================================
FILE: src/python/txtai/pipeline/text/reranker.py
================================================
"""
Reranker module
"""

from ..base import Pipeline


class Reranker(Pipeline):
    """
    Runs embeddings queries and re-ranks them using a similarity pipeline. Note that content must be enabled with the
    embeddings instance for this to work properly.
    """

    def __init__(self, embeddings, similarity):
        """
        Creates a Reranker pipeline.

        Args:
            embeddings: embeddings instance (content must be enabled)
            similarity: similarity instance
        """

        self.embeddings, self.similarity = embeddings, similarity

    # pylint: disable=W0222
    def __call__(self, query, limit=3, factor=10, **kwargs):
        """
        Runs an embeddings search and re-ranks the results using a Similarity pipeline.

        Args:
            query: query text|list
            limit: maximum results
            factor: factor to multiply limit by for the initial embeddings search
            kwargs: additional arguments to pass to embeddings search

        Returns:
            list of query results rescored using a Similarity pipeline
        """

        queries = [query] if not isinstance(query, list) else query

        # Run searches
        results = self.embeddings.batchsearch(queries, limit * factor, **kwargs)

        # Re-rank using similarity pipeline
        ranked = []
        for x, result in enumerate(results):
            texts = [row["text"] for row in result]

            # Score results and merge
            for uid, score in self.similarity(queries[x], texts):
                result[uid]["score"] = score

            # Sort and take top n sorted results
            ranked.append(sorted(result, key=lambda row: row["score"], reverse=True)[:limit])

        return ranked[0] if isinstance(query, str) else ranked



================================================
FILE: src/python/txtai/pipeline/text/similarity.py
================================================
"""
Similarity module
"""

import numpy as np

from .crossencoder import CrossEncoder
from .labels import Labels
from .lateencoder import LateEncoder


class Similarity(Labels):
    """
    Computes similarity between query and list of text using a transformers model.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, crossencode=False, lateencode=False, **kwargs):
        self.crossencoder, self.lateencoder = None, None

        if lateencode:
            # Load a late interaction encoder if lateencode set to True
            self.lateencoder = LateEncoder(path=path, gpu=gpu, **kwargs)
        else:
            # Use zero-shot classification if dynamic is True and crossencode is False, otherwise use standard text classification
            super().__init__(path, quantize, gpu, model, False if crossencode else dynamic, **kwargs)

            # Load as a cross-encoder if crossencode set to True
            self.crossencoder = CrossEncoder(model=self.pipeline) if crossencode else None

    # pylint: disable=W0222
    def __call__(self, query, texts, multilabel=True, **kwargs):
        """
        Computes the similarity between query and list of text. Returns a list of
        (id, score) sorted by highest score, where id is the index in texts.

        This method supports query as a string or a list. If the input is a string,
        the return type is a 1D list of (id, score). If text is a list, a 2D list
        of (id, score) is returned with a row per string.

        Args:
            query: query text|list
            texts: list of text
            multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None
            kwargs: additional keyword args

        Returns:
            list of (id, score)
        """

        if self.crossencoder:
            # pylint: disable=E1102
            return self.crossencoder(query, texts, multilabel)

        if self.lateencoder:
            return self.lateencoder(query, texts)

        # Call Labels pipeline for texts using input query as the candidate label
        scores = super().__call__(texts, [query] if isinstance(query, str) else query, multilabel, **kwargs)

        # Sort on query index id
        scores = [[score for _, score in sorted(row)] for row in scores]

        # Transpose axes to get a list of text scores for each query
        scores = np.array(scores).T.tolist()

        # Build list of (id, score) per query sorted by highest score
        scores = [sorted(enumerate(row), key=lambda x: x[1], reverse=True) for row in scores]

        return scores[0] if isinstance(query, str) else scores

    def encode(self, data, category):
        """
        Encodes a batch of data using the underlying model.

        Args:
            data: input data
            category: encoding category

        Returns:
            encoded data
        """

        return self.lateencoder.encode(data, category) if self.lateencoder else data



================================================
FILE: src/python/txtai/pipeline/text/summary.py
================================================
"""
Summary module
"""

import re

from ..hfpipeline import HFPipeline


class Summary(HFPipeline):
    """
    Summarizes text.
    """

    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):
        super().__init__("summarization", path, quantize, gpu, model, **kwargs)

    def __call__(self, text, minlength=None, maxlength=None, workers=0):
        """
        Runs a summarization model against a block of text.

        This method supports text as a string or a list. If the input is a string, the return
        type is text. If text is a list, a list of text is returned with a row per block of text.

        Args:
            text: text|list
            minlength: minimum length for summary
            maxlength: maximum length for summary
            workers: number of concurrent workers to use for processing data, defaults to None

        Returns:
            summary text
        """

        # Validate text length greater than max length
        check = maxlength if maxlength else self.maxlength()

        # Skip text shorter than max length
        texts = text if isinstance(text, list) else [text]
        params = [(x, text if len(text) >= check else None) for x, text in enumerate(texts)]

        # Build keyword arguments
        kwargs = self.args(minlength, maxlength)

        inputs = [text for _, text in params if text]
        if inputs:
            # Run summarization pipeline
            results = self.pipeline(inputs, num_workers=workers, **kwargs)

            # Pull out summary text
            results = iter([self.clean(x["summary_text"]) for x in results])
            results = [next(results) if text else texts[x] for x, text in params]
        else:
            # Return original
            results = texts

        return results[0] if isinstance(text, str) else results

    def clean(self, text):
        """
        Applies a series of rules to clean extracted text.

        Args:
            text: input text

        Returns:
            clean text
        """

        text = re.sub(r"\s*\.\s*", ". ", text)
        text = text.strip()

        return text

    def args(self, minlength, maxlength):
        """
        Builds keyword arguments.

        Args:
            minlength: minimum length for summary
            maxlength: maximum length for summary

        Returns:
            keyword arguments
        """

        kwargs = {"truncation": True}
        if minlength:
            kwargs["min_length"] = minlength
        if maxlength:
            kwargs["max_length"] = maxlength
            kwargs["max_new_tokens"] = None

            # Default minlength if not provided or it's bigger than maxlength
            if "min_length" not in kwargs or kwargs["min_length"] > kwargs["max_length"]:
                kwargs["min_length"] = kwargs["max_length"]

        return kwargs



================================================
FILE: src/python/txtai/pipeline/text/translation.py
================================================
"""
Translation module
"""

# Conditional import
try:
    from staticvectors import StaticVectors

    STATICVECTORS = True
except ImportError:
    STATICVECTORS = False

from huggingface_hub.hf_api import HfApi
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from ...models import Models
from ..hfmodel import HFModel


class Translation(HFModel):
    """
    Translates text from source language into target language.
    """

    # Default language detection model
    DEFAULT_LANG_DETECT = "neuml/language-id-quantized"

    def __init__(self, path=None, quantize=False, gpu=True, batch=64, langdetect=None, findmodels=True):
        """
        Constructs a new language translation pipeline.

        Args:
            path: optional path to model, accepts Hugging Face model hub id or local path,
                  uses default model for task if not provided
            quantize: if model should be quantized, defaults to False
            gpu: True/False if GPU should be enabled, also supports a GPU device id
            batch: batch size used to incrementally process content
            langdetect: set a custom language detection function, method must take a list of strings and return
                        language codes for each, uses default language detector if not provided
            findmodels: True/False if the Hugging Face Hub will be searched for source-target translation models
        """

        # Call parent constructor
        super().__init__(path if path else "facebook/m2m100_418M", quantize, gpu, batch)

        # Language detection
        self.detector = None
        self.langdetect = langdetect
        self.findmodels = findmodels

        # Language models
        self.models = {}
        self.ids = None

    def __call__(self, texts, target="en", source=None, showmodels=False):
        """
        Translates text from source language into target language.

        This method supports texts as a string or a list. If the input is a string,
        the return type is string. If text is a list, the return type is a list.

        Args:
            texts: text|list
            target: target language code, defaults to "en"
            source: source language code, detects language if not provided

        Returns:
            list of translated text
        """

        values = [texts] if not isinstance(texts, list) else texts

        # Detect source languages
        languages = self.detect(values) if not source else [source] * len(values)
        unique = set(languages)

        # Build a dict from language to list of (index, text)
        langdict = {}
        for x, lang in enumerate(languages):
            if lang not in langdict:
                langdict[lang] = []
            langdict[lang].append((x, values[x]))

        results = {}
        for language in unique:
            # Get all indices and text values for a language
            inputs = langdict[language]

            # Translate text in batches
            outputs = []
            for chunk in self.batch([text for _, text in inputs], self.batchsize):
                outputs.extend(self.translate(chunk, language, target, showmodels))

            # Store output value
            for y, (x, _) in enumerate(inputs):
                if showmodels:
                    model, op = outputs[y]
                    results[x] = (op.strip(), language, model)
                else:
                    results[x] = outputs[y].strip()

        # Return results in same order as input
        results = [results[x] for x in sorted(results)]
        return results[0] if isinstance(texts, str) else results

    def modelids(self):
        """
        Runs a query to get a list of available language models from the Hugging Face API.

        Returns:
            list of source-target language model ids
        """

        ids = [x.id for x in HfApi().list_models(author="Helsinki-NLP")] if self.findmodels else []
        return set(ids)

    def detect(self, texts):
        """
        Detects the language for each element in texts.

        Args:
            texts: list of text

        Returns:
            list of languages
        """

        # Default detector
        if not self.langdetect or isinstance(self.langdetect, str):
            return self.defaultdetect(texts)

        # Call external language detector
        return self.langdetect(texts)

    def defaultdetect(self, texts):
        """
        Default language detection model.

        Args:
            texts: list of text

        Returns:
            list of languages
        """

        if not self.detector:
            if not STATICVECTORS:
                raise ImportError('Language detection is not available - install "pipeline" extra to enable')

            # Get model path
            path = self.langdetect if self.langdetect else Translation.DEFAULT_LANG_DETECT

            # Load language detection model
            self.detector = StaticVectors(path)

        # Transform texts to format expected by language detection model
        texts = [x.lower().replace("\n", " ").replace("\r\n", " ") for x in texts]

        # Detect languages
        return [x[0][0] for x in self.detector.predict(texts)]

    def translate(self, texts, source, target, showmodels=False):
        """
        Translates text from source to target language.

        Args:
            texts: list of text
            source: source language code
            target: target language code

        Returns:
            list of translated text
        """

        # Return original if already in target language
        if source == target:
            return texts

        # Load model and tokenizer
        path, model, tokenizer = self.lookup(source, target)

        model.to(self.device)
        indices = None
        maxlength = Models.maxlength(model, tokenizer)

        with self.context():
            if hasattr(tokenizer, "lang_code_to_id"):
                source = self.langid(tokenizer.lang_code_to_id, source)
                target = self.langid(tokenizer.lang_code_to_id, target)

                tokenizer.src_lang = source
                tokens, indices = self.tokenize(tokenizer, texts)

                translated = model.generate(**tokens, forced_bos_token_id=tokenizer.lang_code_to_id[target], max_length=maxlength)
            else:
                tokens, indices = self.tokenize(tokenizer, texts)
                translated = model.generate(**tokens, max_length=maxlength)

        # Decode translations
        translated = tokenizer.batch_decode(translated, skip_special_tokens=True)

        # Combine translations - handle splits on large text from tokenizer
        results, last = [], -1
        for x, i in enumerate(indices):
            v = (path, translated[x]) if showmodels else translated[x]
            if i == last:
                results[-1] += v
            else:
                results.append(v)

            last = i

        return results

    def lookup(self, source, target):
        """
        Retrieves a translation model for source->target language. This method caches each model loaded.

        Args:
            source: source language code
            target: target language code

        Returns:
            (model, tokenizer)
        """

        # Determine best translation model to use, load if necessary and return
        path = self.modelpath(source, target)
        if path not in self.models:
            self.models[path] = self.load(path)

        return (path,) + self.models[path]

    def modelpath(self, source, target):
        """
        Derives a translation model path given source and target languages.

        Args:
            source: source language code
            target: target language code

        Returns:
            model path
        """

        # Lazy load model ids
        if self.ids is None:
            self.ids = self.modelids()

        # First try direct model
        template = "Helsinki-NLP/opus-mt-%s-%s"
        path = template % (source, target)
        if path in self.ids:
            return path

        # Use multi-language - english model
        if self.findmodels and target == "en":
            return template % ("mul", target)

        # Default model if no suitable model found
        return self.path

    def load(self, path):
        """
        Loads a model specified by path.

        Args:
            path: model path

        Returns:
            (model, tokenizer)
        """

        model = AutoModelForSeq2SeqLM.from_pretrained(path)
        tokenizer = AutoTokenizer.from_pretrained(path)

        # Apply model initialization routines
        model = self.prepare(model)

        return (model, tokenizer)

    def langid(self, languages, target):
        """
        Searches a list of languages for a prefix match on target.

        Args:
            languages: list of languages
            target: target language code

        Returns:
            best match or None if no match found
        """

        for lang in languages:
            if lang.startswith(target):
                return lang

        return None



================================================
FILE: src/python/txtai/pipeline/train/__init__.py
================================================
"""
Train imports
"""

from .hfonnx import HFOnnx
from .hftrainer import HFTrainer
from .mlonnx import MLOnnx



================================================
FILE: src/python/txtai/pipeline/train/hfonnx.py
================================================
"""
Hugging Face Transformers ONNX export module
"""

from collections import OrderedDict
from io import BytesIO
from itertools import chain
from tempfile import NamedTemporaryFile

# Conditional import
try:
    from onnxruntime.quantization import quantize_dynamic

    ONNX_RUNTIME = True
except ImportError:
    ONNX_RUNTIME = False

from torch import nn
from torch.onnx import export

from transformers import AutoModel, AutoModelForQuestionAnswering, AutoModelForSequenceClassification, AutoTokenizer

from ...models import PoolingFactory
from ..tensors import Tensors


class HFOnnx(Tensors):
    """
    Exports a Hugging Face Transformer model to ONNX.
    """

    def __call__(self, path, task="default", output=None, quantize=False, opset=14):
        """
        Exports a Hugging Face Transformer model to ONNX.

        Args:
            path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple
            task: optional model task or category, determines the model type and outputs, defaults to export hidden state
            output: optional output model path, defaults to return byte array if None
            quantize: if model should be quantized (requires onnx to be installed), defaults to False
            opset: onnx opset, defaults to 14

        Returns:
            path to model output or model as bytes depending on output parameter
        """

        inputs, outputs, model = self.parameters(task)

        if isinstance(path, (list, tuple)):
            model, tokenizer = path
            model = model.cpu()
        else:
            model = model(path)
            tokenizer = AutoTokenizer.from_pretrained(path)

        # Generate dummy inputs
        dummy = dict(tokenizer(["test inputs"], return_tensors="pt"))

        # Default to BytesIO if no output file provided
        output = output if output else BytesIO()

        # Export model to ONNX
        export(
            model,
            (dummy,),
            output,
            opset_version=opset,
            do_constant_folding=True,
            input_names=list(inputs.keys()),
            output_names=list(outputs.keys()),
            dynamic_axes=dict(chain(inputs.items(), outputs.items())),
            dynamo=False,
        )

        # Quantize model
        if quantize:
            if not ONNX_RUNTIME:
                raise ImportError('onnxruntime is not available - install "pipeline" extra to enable')

            output = self.quantization(output)

        if isinstance(output, BytesIO):
            # Reset stream and return bytes
            output.seek(0)
            output = output.read()

        return output

    def quantization(self, output):
        """
        Quantizes an ONNX model.

        Args:
            output: path to ONNX model or BytesIO with model data

        Returns:
            quantized model as file path or bytes
        """

        temp = None
        if isinstance(output, BytesIO):
            with NamedTemporaryFile(suffix=".quant", delete=False) as tmpfile:
                temp = tmpfile.name

            with open(temp, "wb") as f:
                f.write(output.getbuffer())

            output = temp

        # Quantize model
        quantize_dynamic(output, output, extra_options={"MatMulConstBOnly": False})

        # Read file back to bytes if temp file was created
        if temp:
            with open(temp, "rb") as f:
                output = f.read()

        return output

    def parameters(self, task):
        """
        Defines inputs and outputs for an ONNX model.

        Args:
            task: task name used to lookup model configuration

        Returns:
            (inputs, outputs, model function)
        """

        inputs = OrderedDict(
            [
                ("input_ids", {0: "batch", 1: "sequence"}),
                ("attention_mask", {0: "batch", 1: "sequence"}),
                ("token_type_ids", {0: "batch", 1: "sequence"}),
            ]
        )

        config = {
            "default": (OrderedDict({"last_hidden_state": {0: "batch", 1: "sequence"}}), AutoModel.from_pretrained),
            "pooling": (OrderedDict({"embeddings": {0: "batch", 1: "sequence"}}), lambda x: PoolingOnnx(x, -1)),
            "question-answering": (
                OrderedDict(
                    {
                        "start_logits": {0: "batch", 1: "sequence"},
                        "end_logits": {0: "batch", 1: "sequence"},
                    }
                ),
                AutoModelForQuestionAnswering.from_pretrained,
            ),
            "text-classification": (OrderedDict({"logits": {0: "batch"}}), AutoModelForSequenceClassification.from_pretrained),
        }

        # Aliases
        config["zero-shot-classification"] = config["text-classification"]

        return (inputs,) + config[task]


class PoolingOnnx(nn.Module):
    """
    Extends Pooling methods to name inputs to model, which is required to export to ONNX.
    """

    def __init__(self, path, device):
        """
        Creates a new PoolingOnnx instance.

        Args:
            path: path to model, accepts Hugging Face model hub id or local path
            device: tensor device id
        """

        super().__init__()

        # Create pooling method based on configuration
        self.model = PoolingFactory.create({"path": path, "device": device})

    # pylint: disable=W0221
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):
        """
        Runs inputs through pooling model and returns outputs.

        Args:
            inputs: model inputs

        Returns:
            model outputs
        """

        # Build list of arguments dynamically since some models take token_type_ids
        # and others don't
        inputs = {"input_ids": input_ids, "attention_mask": attention_mask}
        if token_type_ids is not None:
            inputs["token_type_ids"] = token_type_ids

        return self.model.forward(**inputs)



================================================
FILE: src/python/txtai/pipeline/train/hftrainer.py
================================================
"""
Hugging Face Transformers trainer wrapper module
"""

import os
import sys

import torch

from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoModelForQuestionAnswering,
    AutoModelForPreTraining,
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, Trainer, set_seed
from transformers import TrainingArguments as HFTrainingArguments

# Conditional import
try:
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training

    # pylint: disable=C0412
    from transformers import BitsAndBytesConfig

    PEFT = True
except ImportError:
    PEFT = False

from ...data import Labels, Questions, Sequences, Texts
from ...models import Models, TokenDetection
from ..tensors import Tensors


class HFTrainer(Tensors):
    """
    Trains a new Hugging Face Transformer model using the Trainer framework.
    """

    # pylint: disable=R0913
    def __call__(
        self,
        base,
        train,
        validation=None,
        columns=None,
        maxlength=None,
        stride=128,
        task="text-classification",
        prefix=None,
        metrics=None,
        tokenizers=None,
        checkpoint=None,
        quantize=None,
        lora=None,
        **args
    ):
        """
        Builds a new model using arguments.

        Args:
            base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple
            train: training data
            validation: validation data
            columns: tuple of columns to use for text/label, defaults to (text, None, label)
            maxlength: maximum sequence length, defaults to tokenizer.model_max_length
            stride: chunk size for splitting data for QA tasks
            task: optional model task or category, determines the model type, defaults to "text-classification"
            prefix: optional source prefix
            metrics: optional function that computes and returns a dict of evaluation metrics
            tokenizers: optional number of concurrent tokenizers, defaults to None
            checkpoint: optional resume from checkpoint flag or path to checkpoint directory, defaults to None
            quantize: quantization configuration to pass to base model
            lora: lora configuration to pass to PEFT model
            args: training arguments

        Returns:
            (model, tokenizer)
        """

        # Quantization / LoRA support
        if (quantize or lora) and not PEFT:
            raise ImportError('PEFT is not available - install "pipeline" extra to enable')

        # Parse TrainingArguments
        args = self.parse(args)

        # Set seed for model reproducibility
        set_seed(args.seed)

        # Load model configuration, tokenizer and max sequence length
        config, tokenizer, maxlength = self.load(base, maxlength)

        # Default tokenizer pad token if it's not set
        tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token

        # Prepare parameters
        process, collator, labels = self.prepare(task, train, tokenizer, columns, maxlength, stride, prefix, args)

        # Tokenize training and validation data
        train, validation = process(train, validation, os.cpu_count() if tokenizers and isinstance(tokenizers, bool) else tokenizers)

        # Create model to train
        model = self.model(task, base, config, labels, tokenizer, quantize)

        # Default config pad token if it's not set
        model.config.pad_token_id = model.config.pad_token_id if model.config.pad_token_id is not None else model.config.eos_token_id

        # Load as PEFT model, if necessary
        model = self.peft(task, lora, model)

        # Add model to collator
        if collator:
            collator.model = model

        # Build trainer
        trainer = Trainer(
            model=model,
            processing_class=tokenizer,
            data_collator=collator,
            args=args,
            train_dataset=train,
            eval_dataset=validation if validation else None,
            compute_metrics=metrics,
        )

        # Run training
        trainer.train(resume_from_checkpoint=checkpoint)

        # Run evaluation
        if validation:
            trainer.evaluate()

        # Save model outputs
        if args.should_save:
            trainer.save_model()
            trainer.save_state()

        # Put model in eval mode to disable weight updates and return (model, tokenizer)
        return (model.eval(), tokenizer)

    def parse(self, updates):
        """
        Parses and merges custom arguments with defaults.

        Args:
            updates: custom arguments

        Returns:
            TrainingArguments
        """

        # Default training arguments
        args = {"output_dir": "", "save_strategy": "no", "report_to": "none", "log_level": "warning", "use_cpu": not Models.hasaccelerator()}

        # Apply custom arguments
        args.update(updates)

        return TrainingArguments(**args)

    def load(self, base, maxlength):
        """
        Loads the base config and tokenizer.

        Args:
            base: base model - supports a file path or (model, tokenizer) tuple
            maxlength: maximum sequence length

        Returns:
            (config, tokenizer, maxlength)
        """

        if isinstance(base, (list, tuple)):
            # Unpack existing config and tokenizer
            model, tokenizer = base
            config = model.config
        else:
            # Load config
            config = AutoConfig.from_pretrained(base)

            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(base)

        # Detect unbounded tokenizer
        Models.checklength(config, tokenizer)

        # Derive max sequence length
        maxlength = min(maxlength if maxlength else sys.maxsize, tokenizer.model_max_length)

        return (config, tokenizer, maxlength)

    def prepare(self, task, train, tokenizer, columns, maxlength, stride, prefix, args):
        """
        Prepares data for model training.

        Args:
            task: optional model task or category, determines the model type, defaults to "text-classification"
            train: training data
            tokenizer: model tokenizer
            columns: tuple of columns to use for text/label, defaults to (text, None, label)
            maxlength: maximum sequence length, defaults to tokenizer.model_max_length
            stride: chunk size for splitting data for QA tasks
            prefix: optional source prefix
            args: training arguments
        """

        process, collator, labels = None, None, None

        if task == "language-generation":
            process = Texts(tokenizer, columns, maxlength)
            collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=8 if args.fp16 else None)
        elif task in ("language-modeling", "token-detection"):
            process = Texts(tokenizer, columns, maxlength)
            collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)
        elif task == "question-answering":
            process = Questions(tokenizer, columns, maxlength, stride)
        elif task == "sequence-sequence":
            process = Sequences(tokenizer, columns, maxlength, prefix)
            collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)
        else:
            process = Labels(tokenizer, columns, maxlength)
            labels = process.labels(train)

        return process, collator, labels

    def model(self, task, base, config, labels, tokenizer, quantize):
        """
        Loads the base model to train.

        Args:
            task: optional model task or category, determines the model type, defaults to "text-classification"
            base: base model - supports a file path or (model, tokenizer) tuple
            config: model configuration
            labels: number of labels
            tokenizer: model tokenizer
            quantize: quantization config

        Returns:
            model
        """

        if labels is not None:
            # Add number of labels to config
            config.update({"num_labels": labels})

        # Format quantization configuration
        quantization = self.quantization(quantize)

        # Clear quantization configuration if GPU is not available
        quantization = quantization if torch.cuda.is_available() else None

        # pylint: disable=E1120
        # Unpack existing model or create new model from config
        if isinstance(base, (list, tuple)) and not isinstance(base[0], str):
            return base[0]
        if task == "language-generation":
            return AutoModelForCausalLM.from_pretrained(base, config=config, quantization_config=quantization)
        if task == "language-modeling":
            return AutoModelForMaskedLM.from_pretrained(base, config=config, quantization_config=quantization)
        if task == "question-answering":
            return AutoModelForQuestionAnswering.from_pretrained(base, config=config, quantization_config=quantization)
        if task == "sequence-sequence":
            return AutoModelForSeq2SeqLM.from_pretrained(base, config=config, quantization_config=quantization)
        if task == "token-detection":
            return TokenDetection(
                AutoModelForMaskedLM.from_pretrained(base, config=config, quantization_config=quantization),
                AutoModelForPreTraining.from_pretrained(base, config=config, quantization_config=quantization),
                tokenizer,
            )

        # Default task
        return AutoModelForSequenceClassification.from_pretrained(base, config=config, quantization_config=quantization)

    def quantization(self, quantize):
        """
        Formats and returns quantization configuration.

        Args:
            quantize: input quantization configuration

        Returns:
            formatted quantization configuration
        """

        if quantize:
            # Default quantization settings when set to True
            if isinstance(quantize, bool):
                quantize = {
                    "load_in_4bit": True,
                    "bnb_4bit_use_double_quant": True,
                    "bnb_4bit_quant_type": "nf4",
                    "bnb_4bit_compute_dtype": "bfloat16",
                }

            # Load dictionary configuration
            if isinstance(quantize, dict):
                quantize = BitsAndBytesConfig(**quantize)

        return quantize if quantize else None

    def peft(self, task, lora, model):
        """
        Wraps the input model as a PEFT model if lora configuration is set.

        Args:
            task: optional model task or category, determines the model type, defaults to "text-classification"
            lora: lora configuration
            model: transformers model

        Returns:
            wrapped model if lora configuration set, otherwise input model is returned
        """

        if lora:
            # Format LoRA configuration
            config = self.lora(task, lora)

            # Wrap as PeftModel
            model = prepare_model_for_kbit_training(model)
            model = get_peft_model(model, config)
            model.print_trainable_parameters()

        return model

    def lora(self, task, lora):
        """
        Formats and returns LoRA configuration.

        Args:
            task: optional model task or category, determines the model type, defaults to "text-classification"
            lora: lora configuration

        Returns:
            formatted lora configuration
        """

        if lora:
            # Default lora settings when set to True
            if isinstance(lora, bool):
                lora = {"r": 16, "lora_alpha": 8, "target_modules": "all-linear", "lora_dropout": 0.05, "bias": "none"}

            # Load dictionary configuration
            if isinstance(lora, dict):
                # Set task type if missing
                if "task_type" not in lora:
                    lora["task_type"] = self.loratask(task)

                lora = LoraConfig(**lora)

        return lora

    def loratask(self, task):
        """
        Looks up the corresponding LoRA task for input task.

        Args:
            task: optional model task or category, determines the model type, defaults to "text-classification"

        Returns:
            lora task
        """

        # Task mapping
        tasks = {
            "language-generation": TaskType.CAUSAL_LM,
            "language-modeling": TaskType.FEATURE_EXTRACTION,
            "question-answering": TaskType.QUESTION_ANS,
            "sequence-sequence": TaskType.SEQ_2_SEQ_LM,
            "text-classification": TaskType.SEQ_CLS,
            "token-detection": TaskType.FEATURE_EXTRACTION,
        }

        # Default task
        task = task if task in tasks else "text-classification"

        # Lookup and return task
        return tasks[task]


class TrainingArguments(HFTrainingArguments):
    """
    Extends standard TrainingArguments to make the output directory optional for transient models.
    """

    @property
    def should_save(self):
        """
        Override should_save to disable model saving when output directory is None.

        Returns:
            If model should be saved
        """

        return super().should_save if self.output_dir else False



================================================
FILE: src/python/txtai/pipeline/train/mlonnx.py
================================================
"""
Machine learning model to ONNX export module
"""

from ..base import Pipeline

try:
    from onnxmltools import convert_sklearn

    from skl2onnx.common.data_types import StringTensorType
    from skl2onnx.helpers.onnx_helper import save_onnx_model, select_model_inputs_outputs

    ONNX_MLTOOLS = True
except ImportError:
    ONNX_MLTOOLS = False


class MLOnnx(Pipeline):
    """
    Exports a machine learning model to ONNX using ONNXMLTools.
    """

    def __init__(self):
        """
        Creates a new MLOnnx pipeline.
        """

        if not ONNX_MLTOOLS:
            raise ImportError('MLOnnx pipeline is not available - install "pipeline" extra to enable')

    def __call__(self, model, task="default", output=None, opset=12):
        """
        Exports a machine learning model to ONNX using ONNXMLTools.

        Args:
            model: model to export
            task: optional model task or category
            output: optional output model path, defaults to return byte array if None
            opset: onnx opset, defaults to 12

        Returns:
            path to model output or model as bytes depending on output parameter
        """

        # Convert scikit-learn model to ONNX
        model = convert_sklearn(model, task, initial_types=[("input_ids", StringTensorType([None, None]))], target_opset=opset)

        # Prune model graph down to only output probabilities
        model = select_model_inputs_outputs(model, outputs="probabilities")

        # pylint: disable=E1101
        # Rename output to logits for consistency with other models
        model.graph.output[0].name = "logits"

        # Find probabilities output node and rename to logits
        for node in model.graph.node:
            for x, _ in enumerate(node.output):
                if node.output[x] == "probabilities":
                    node.output[x] = "logits"

        # Save model to specified output path or return bytes
        model = save_onnx_model(model, output)
        return output if output else model



================================================
FILE: src/python/txtai/scoring/__init__.py
================================================
"""
Scoring imports
"""

from .base import Scoring
from .bm25 import BM25
from .factory import ScoringFactory
from .pgtext import PGText
from .sif import SIF
from .sparse import Sparse
from .terms import Terms
from .tfidf import TFIDF



================================================
FILE: src/python/txtai/scoring/base.py
================================================
"""
Scoring module
"""


class Scoring:
    """
    Base scoring.
    """

    def __init__(self, config=None):
        """
        Creates a new Scoring instance.

        Args:
            config: input configuration
        """

        # Scoring configuration
        self.config = config if config is not None else {}

        # Transform columns
        columns = self.config.get("columns", {})
        self.text = columns.get("text", "text")
        self.object = columns.get("object", "object")

        # Vector model, if available
        self.model = None

    def insert(self, documents, index=None, checkpoint=None):
        """
        Inserts documents into the scoring index.

        Args:
            documents: list of (id, dict|text|tokens, tags)
            index: indexid offset
            checkpoint: optional checkpoint directory, enables indexing restart
        """

        raise NotImplementedError

    def delete(self, ids):
        """
        Deletes documents from scoring index.

        Args:
            ids: list of ids to delete
        """

        raise NotImplementedError

    def index(self, documents=None):
        """
        Indexes a collection of documents using a scoring method.

        Args:
            documents: list of (id, dict|text|tokens, tags)
        """

        # Insert documents
        if documents:
            self.insert(documents)

    def upsert(self, documents=None):
        """
        Convience method for API clarity. Calls index method.

        Args:
            documents: list of (id, dict|text|tokens, tags)
        """

        self.index(documents)

    def weights(self, tokens):
        """
        Builds a weights vector for each token in input tokens.

        Args:
            tokens: input tokens

        Returns:
            list of weights for each token
        """

        raise NotImplementedError

    def search(self, query, limit=3):
        """
        Search index for documents matching query.

        Args:
            query: input query
            limit: maximum results

        Returns:
            list of (id, score) or (data, score) if content is enabled
        """

        raise NotImplementedError

    def batchsearch(self, queries, limit=3, threads=True):
        """
        Search index for documents matching queries.

        Args:
            queries: queries to run
            limit: maximum results
            threads: run as threaded search if True and supported
        """

        raise NotImplementedError

    def count(self):
        """
        Returns the total number of documents indexed.

        Returns:
            total number of documents indexed
        """

        raise NotImplementedError

    def load(self, path):
        """
        Loads a saved Scoring object from path.

        Args:
            path: directory path to load scoring index
        """

        raise NotImplementedError

    def save(self, path):
        """
        Saves a Scoring object to path.

        Args:
            path: directory path to save scoring index
        """

        raise NotImplementedError

    def close(self):
        """
        Closes this Scoring object.
        """

        raise NotImplementedError

    def findmodel(self):
        """
        Returns the associated vector model used by this scoring instance, if any.

        Returns:
            associated vector model
        """

        return self.model

    def issparse(self):
        """
        Check if this scoring instance has an associated sparse keyword or sparse vector index.

        Returns:
            True if this index has an associated sparse index
        """

        raise NotImplementedError

    def isweighted(self):
        """
        Check if this scoring instance is for term weighting (i.e.) it has no associated sparse index.

        Returns:
            True if this index is for term weighting
        """

        return not self.issparse()

    def isnormalized(self):
        """
        Check if this scoring instance returns normalized scores.

        Returns:
            True if normalize is enabled, False otherwise
        """

        raise NotImplementedError



================================================
FILE: src/python/txtai/scoring/bm25.py
================================================
"""
BM25 module
"""

import numpy as np

from .tfidf import TFIDF


class BM25(TFIDF):
    """
    Best matching (BM25) scoring.
    """

    def __init__(self, config=None):
        super().__init__(config)

        # BM25 configurable parameters
        self.k1 = self.config.get("k1", 1.2)
        self.b = self.config.get("b", 0.75)

    def computeidf(self, freq):
        # Calculate BM25 IDF score
        return np.log(1 + (self.total - freq + 0.5) / (freq + 0.5))

    def score(self, freq, idf, length):
        # Calculate BM25 score
        k = self.k1 * ((1 - self.b) + self.b * length / self.avgdl)
        return idf * (freq * (self.k1 + 1)) / (freq + k)



================================================
FILE: src/python/txtai/scoring/factory.py
================================================
"""
Factory module
"""

from ..util import Resolver

from .bm25 import BM25
from .pgtext import PGText
from .sif import SIF
from .sparse import Sparse
from .tfidf import TFIDF


class ScoringFactory:
    """
    Methods to create Scoring indexes.
    """

    @staticmethod
    def create(config, models=None):
        """
        Factory method to construct a Scoring instance.

        Args:
            config: scoring configuration parameters
            models: models cache

        Returns:
            Scoring
        """

        # Scoring instance
        scoring = None

        # Support string and dict configuration
        if isinstance(config, str):
            config = {"method": config}

        # Get scoring method
        method = config.get("method", "bm25")

        if method == "bm25":
            scoring = BM25(config)
        elif method == "pgtext":
            scoring = PGText(config)
        elif method == "sif":
            scoring = SIF(config)
        elif method == "sparse":
            scoring = Sparse(config, models)
        elif method == "tfidf":
            scoring = TFIDF(config)
        else:
            # Resolve custom method
            scoring = ScoringFactory.resolve(method, config)

        # Store config back
        config["method"] = method

        return scoring

    @staticmethod
    def issparse(config):
        """
        Checks if this scoring configuration builds a sparse index.

        Args:
            config: scoring configuration

        Returns:
            True if this config is for a sparse index
        """

        # Types that are always a sparse index
        indexes = ["pgtext", "sparse"]

        # True if this config is for a sparse index
        return config and isinstance(config, dict) and (config.get("method") in indexes or config.get("terms"))

    @staticmethod
    def resolve(backend, config):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: index configuration parameters

        Returns:
            Scoring
        """

        try:
            return Resolver()(backend)(config)
        except Exception as e:
            raise ImportError(f"Unable to resolve scoring backend: '{backend}'") from e



================================================
FILE: src/python/txtai/scoring/pgtext.py
================================================
"""
PGText module
"""

import os

# Conditional import
try:
    from sqlalchemy import create_engine, desc, delete, func, text
    from sqlalchemy import Column, Computed, Index, Integer, MetaData, StaticPool, Table, Text
    from sqlalchemy.dialects.postgresql import TSVECTOR
    from sqlalchemy.orm import Session
    from sqlalchemy.schema import CreateSchema

    PGTEXT = True
except ImportError:
    PGTEXT = False

from .base import Scoring


class PGText(Scoring):
    """
    Postgres full text search (FTS) based scoring.
    """

    def __init__(self, config=None):
        super().__init__(config)

        if not PGTEXT:
            raise ImportError('PGText is not available - install "scoring" extra to enable')

        # Database connection
        self.engine, self.database, self.connection, self.table = None, None, None, None

        # Language
        self.language = self.config.get("language", "english")

    def insert(self, documents, index=None, checkpoint=None):
        # Initialize tables
        self.initialize(recreate=True)

        # Collection of rows to insert
        rows = []

        # Collect rows
        for uid, document, _ in documents:
            # Extract text, if necessary
            if isinstance(document, dict):
                document = document.get(self.text, document.get(self.object))

            if document is not None:
                # If index is passed, use indexid, otherwise use id
                uid = index if index is not None else uid

                # Add row if the data type is accepted
                if isinstance(document, (str, list)):
                    rows.append((uid, " ".join(document) if isinstance(document, list) else document))

                # Increment index
                index = index + 1 if index is not None else None

        # Insert rows
        self.database.execute(self.table.insert(), [{"indexid": x, "text": text} for x, text in rows])

    def delete(self, ids):
        self.database.execute(delete(self.table).where(self.table.c["indexid"].in_(ids)))

    def weights(self, tokens):
        # Not supported
        return None

    def search(self, query, limit=3):
        # Run query
        query = (
            self.database.query(self.table.c["indexid"], text("ts_rank(vector, plainto_tsquery(:language, :query)) rank"))
            .order_by(desc(text("rank")))
            .limit(limit)
            .params({"language": self.language, "query": query})
        )

        return [(uid, score) for uid, score in query if score > 1e-5]

    def batchsearch(self, queries, limit=3, threads=True):
        return [self.search(query, limit) for query in queries]

    def count(self):
        # pylint: disable=E1102
        return self.database.query(func.count(self.table.c["indexid"])).scalar()

    def load(self, path):
        # Reset database to original checkpoint
        if self.database:
            self.database.rollback()
            self.connection.rollback()

        # Initialize tables
        self.initialize()

    def save(self, path):
        # Commit session and connection
        if self.database:
            self.database.commit()
            self.connection.commit()

    def close(self):
        if self.database:
            self.database.close()
            self.engine.dispose()

    def issparse(self):
        return True

    def isnormalized(self):
        return True

    def initialize(self, recreate=False):
        """
        Initializes a new database session.

        Args:
            recreate: Recreates the database tables if True
        """

        if not self.database:
            # Create engine, connection and session
            self.engine = create_engine(self.config.get("url", os.environ.get("SCORING_URL")), poolclass=StaticPool, echo=False)
            self.connection = self.engine.connect()
            self.database = Session(self.connection)

            # Set default schema, if necessary
            schema = self.config.get("schema")
            if schema:
                with self.engine.begin():
                    self.sqldialect(CreateSchema(schema, if_not_exists=True))

                self.sqldialect(text("SET search_path TO :schema"), {"schema": schema})

            # Table name
            table = self.config.get("table", "scoring")

            # Create vectors table
            self.table = Table(
                table,
                MetaData(),
                Column("indexid", Integer, primary_key=True, autoincrement=False),
                Column("text", Text),
                (
                    Column("vector", TSVECTOR, Computed(f"to_tsvector('{self.language}', text)", persisted=True))
                    if self.engine.dialect.name == "postgresql"
                    else Column("vector", Integer)
                ),
            )

            # Create text index
            index = Index(
                f"{table}-index",
                self.table.c["vector"],
                postgresql_using="gin",
            )

            # Drop and recreate table
            if recreate:
                self.table.drop(self.connection, checkfirst=True)
                index.drop(self.connection, checkfirst=True)

            # Create table and index
            self.table.create(self.connection, checkfirst=True)
            index.create(self.connection, checkfirst=True)

    def sqldialect(self, sql, parameters=None):
        """
        Executes a SQL statement based on the current SQL dialect.

        Args:
            sql: SQL to execute
            parameters: optional bind parameters
        """

        args = (sql, parameters) if self.engine.dialect.name == "postgresql" else (text("SELECT 1"),)
        self.database.execute(*args)



================================================
FILE: src/python/txtai/scoring/sif.py
================================================
"""
SIF module
"""

import numpy as np

from .tfidf import TFIDF


class SIF(TFIDF):
    """
    Smooth Inverse Frequency (SIF) scoring.
    """

    def __init__(self, config=None):
        super().__init__(config)

        # SIF configurable parameters
        self.a = self.config.get("a", 1e-3)

    def computefreq(self, tokens):
        # Default method computes frequency for a single entry
        # SIF uses word frequencies across entire index
        return {token: self.wordfreq[token] for token in tokens}

    def score(self, freq, idf, length):
        # Set freq to word frequencies across entire index when freq and idf shape don't match
        if isinstance(freq, np.ndarray) and freq.shape != np.array(idf).shape:
            freq.fill(freq.sum())

        # Calculate SIF score
        return self.a / (self.a + freq / self.tokens)



================================================
FILE: src/python/txtai/scoring/sparse.py
================================================
"""
Sparse module
"""

from queue import Queue
from threading import Thread

from ..ann import SparseANNFactory
from ..vectors import SparseVectorsFactory

from .base import Scoring


class Sparse(Scoring):
    """
    Sparse vector scoring.
    """

    # End of stream message
    COMPLETE = 1

    def __init__(self, config=None, models=None):
        super().__init__(config)

        # Vector configuration
        mapping = {"vectormethod": "method", "vectornormalize": "normalize"}
        config = {k: v for k, v in config.items() if k not in mapping.values()}
        for k, v in mapping.items():
            if k in config:
                config[v] = config[k]

        # Load the SparseVectors model
        self.model = SparseVectorsFactory.create(config, models)

        # Normalize search outputs if vectors are not normalized already
        # A float can also be provided to set the normalization factor (defaults to 30.0)
        self.isnormalize = self.config.get("normalize", True)

        # Sparse ANN
        self.ann = None

        # Encoding processing parameters
        self.batch = self.config.get("batch", 1024)
        self.thread, self.queue, self.data = None, None, None

    def insert(self, documents, index=None, checkpoint=None):
        # Start processing thread, if necessary
        self.start(checkpoint)

        data = []
        for uid, document, tags in documents:
            # Extract text, if necessary
            if isinstance(document, dict):
                document = document.get(self.text, document.get(self.object))

            if document is not None:
                # Add data
                data.append((uid, " ".join(document) if isinstance(document, list) else document, tags))

        # Add batch of data
        self.queue.put(data)

    def delete(self, ids):
        self.ann.delete(ids)

    def index(self, documents=None):
        # Insert documents, if provided
        if documents:
            self.insert(documents)

        # Create ANN, if there is pending data
        embeddings = self.stop()
        if embeddings is not None:
            self.ann = SparseANNFactory.create(self.config)
            self.ann.index(embeddings)

    def upsert(self, documents=None):
        # Insert documents, if provided
        if documents:
            self.insert(documents)

        # Check for existing index and pending data
        if self.ann:
            embeddings = self.stop()
            if embeddings is not None:
                self.ann.append(embeddings)
        else:
            self.index()

    def weights(self, tokens):
        # Not supported
        return None

    def search(self, query, limit=3):
        return self.batchsearch([query], limit)[0]

    def batchsearch(self, queries, limit=3, threads=True):
        # Convert queries to embedding vectors
        embeddings = self.model.batchtransform((None, query, None) for query in queries)

        # Run ANN search
        scores = self.ann.search(embeddings, limit)

        # Normalize scores if normalization IS enabled AND vector normalization IS NOT enabled
        return self.normalize(embeddings, scores) if self.isnormalize and not self.model.isnormalize else scores

    def count(self):
        return self.ann.count()

    def load(self, path):
        self.ann = SparseANNFactory.create(self.config)
        self.ann.load(path)

    def save(self, path):
        # Save Sparse ANN
        if self.ann:
            self.ann.save(path)

    def close(self):
        # Close Sparse ANN
        if self.ann:
            self.ann.close()

        # Clear parameters
        self.model, self.ann, self.thread, self.queue = None, None, None, None

    def issparse(self):
        return True

    def isnormalized(self):
        return self.isnormalize or self.model.isnormalize

    def start(self, checkpoint):
        """
        Starts an encoding processing thread.

        Args:
            checkpoint: checkpoint directory
        """

        if not self.thread:
            self.queue = Queue(5)
            self.thread = Thread(target=self.encode, args=(checkpoint,))
            self.thread.start()

    def stop(self):
        """
        Stops an encoding processing thread. Return processed results.

        Returns:
            results
        """

        results = None
        if self.thread:
            # Send EOS message
            self.queue.put(Sparse.COMPLETE)

            self.thread.join()
            self.thread, self.queue = None, None

            # Get return value
            results = self.data
            self.data = None

        return results

    def encode(self, checkpoint):
        """
        Encodes streaming data.

        Args:
            checkpoint: checkpoint directory
        """

        # Streaming encoding of data
        _, dimensions, self.data = self.model.vectors(self.stream(), self.batch, checkpoint)

        # Save number of dimensions
        self.config["dimensions"] = dimensions

    def stream(self):
        """
        Streams data from an input queue until end of stream message received.
        """

        batch = self.queue.get()
        while batch != Sparse.COMPLETE:
            yield from batch
            batch = self.queue.get()

    def normalize(self, queries, scores):
        """
        Normalize query result using the max query score.

        Args:
            queries: query vectors
            scores: query results

        Returns:
            normalized query results
        """

        # Get normalize scale factor
        scale = 30.0 if isinstance(self.isnormalize, bool) else self.isnormalize

        # Normalize scores using max scores
        maxscores = self.model.dot(queries, queries)

        # Normalize results and return
        results = []
        for x, result in enumerate(scores):
            maxscore = max(maxscores[x][x] / scale, scale)
            maxscore = max(maxscore, result[0][1]) if result else maxscore

            results.append([(uid, score / maxscore) for uid, score in result])

        return results



================================================
FILE: src/python/txtai/scoring/terms.py
================================================
"""
Terms module
"""

import functools
import os
import sqlite3
import sys

from array import array
from collections import Counter
from threading import RLock

import numpy as np


class Terms:
    """
    Builds, searches and stores memory efficient term frequency sparse arrays for a scoring instance.
    """

    # Term frequency sparse arrays
    CREATE_TERMS = """
        CREATE TABLE IF NOT EXISTS terms (
            term TEXT PRIMARY KEY,
            ids BLOB,
            freqs BLOB
        )
    """

    INSERT_TERM = "INSERT OR REPLACE INTO terms VALUES (?, ?, ?)"
    SELECT_TERMS = "SELECT ids, freqs FROM terms WHERE term = ?"

    # Documents table
    CREATE_DOCUMENTS = """
        CREATE TABLE IF NOT EXISTS documents (
            indexid INTEGER PRIMARY KEY,
            id TEXT,
            deleted INTEGER,
            length INTEGER
        )
    """

    DELETE_DOCUMENTS = "DELETE FROM documents"
    INSERT_DOCUMENT = "INSERT OR REPLACE INTO documents VALUES (?, ?, ?, ?)"
    SELECT_DOCUMENTS = "SELECT indexid, id, deleted, length FROM documents ORDER BY indexid"

    def __init__(self, config, score, idf):
        """
        Creates a new terms index.

        Args:
            config: configuration
            score: score function
            idf: idf weights
        """

        # Terms index configuration
        self.config = config if isinstance(config, dict) else {}
        self.cachelimit = self.config.get("cachelimit", 250000000)
        self.cutoff = self.config.get("cutoff", 0.1)

        # Scoring function
        self.score, self.idf = score, idf

        # Document attributes
        self.ids, self.deletes, self.lengths = [], [], array("q")

        # Terms cache
        self.terms, self.cachesize = {}, 0

        # Terms database
        self.connection, self.cursor, self.path = None, None, None

        # Database thread lock
        self.lock = RLock()

    def insert(self, uid, terms):
        """
        Insert term into index.

        Args:
            uid: document id
            terms: document terms
        """

        # Initialize database, if necessary
        self.initialize()

        # Get next internal index id
        indexid = len(self.ids)

        # Calculate term frequency and document length
        freqs, length = Counter(terms), len(terms)

        # Add document terms
        for term, count in freqs.items():
            # Add term entry
            self.add(indexid, term, count)

            # Each term and freq is a 8-bit signed long long
            self.cachesize += 16

        # Flush cached terms to the database
        if self.cachesize >= self.cachelimit:
            self.index()

        # Save id and length
        self.ids.append(uid)
        self.lengths.append(length)

    def delete(self, ids):
        """
        Mark ids as deleted. This prevents deleted results from showing up in search results.
        The data is not removed from the underlying term frequency sparse arrays.

        Args:
            ids: ids to delete
        """

        # Set index ids as deleted
        self.deletes.extend([self.ids.index(i) for i in ids])

    def index(self):
        """
        Saves any remaining cached terms to the database.
        """

        for term, (nuids, nfreqs) in self.terms.items():
            # Retrieve existing uids/freqs
            uids, freqs = self.lookup(term)

            if uids:
                uids.extend(nuids)
                freqs.extend(nfreqs)
            else:
                uids, freqs = nuids, nfreqs

            # Always save as little endian
            if sys.byteorder == "big":
                uids.byteswap()
                freqs.byteswap()

            # Insert or replace term
            self.cursor.execute(Terms.INSERT_TERM, [term, uids.tobytes(), freqs.tobytes()])

        # Clear cached weights
        self.weights.cache_clear()

        # Reset term cache size
        self.terms, self.cachesize = {}, 0

    def search(self, terms, limit):
        """
        Searches term index a term-at-a-time. Each term frequency sparse array is retrieved
        and used to calculate term match scores.

        This method calculates term scores in two steps as shown below.

          1. Query and score less common term scores first
          2. Merge in common term scores for all documents matching the first query

        This is similar to the common terms query in Apache Lucene.

        Args:
            terms: query terms
            limit: maximum results

        Returns:
            list of (id, score)
        """

        # Initialize scores array
        scores = np.zeros(len(self.ids), dtype=np.float32)

        # Score less common terms
        terms, skipped, hasscores = Counter(terms), {}, False
        for term, freq in terms.items():
            # Compute or lookup term weights
            uids, weights = self.weights(term)
            if uids is not None:
                # Term considered common if it appears in more than 10% of index
                if len(uids) <= self.cutoff * len(self.ids):
                    # Add scores
                    scores[uids] += freq * weights

                    # Set flag that scores have been calculated for at least one term
                    hasscores = True
                else:
                    skipped[term] = freq

        # Merge in common term scores and return top n matches
        return self.topn(scores, limit, hasscores, skipped)

    def count(self):
        """
        Number of elements in the scoring index.

        Returns:
            count
        """

        return len(self.ids) - len(self.deletes)

    def load(self, path):
        """
        Loads terms database from path. This method loads document attributes into memory.

        Args:
            path: path to read terms database
        """

        # Load an existing terms database
        self.connection = self.connect(path)
        self.cursor = self.connection.cursor()
        self.path = path

        # Load document attributes
        self.ids, self.deletes, self.lengths = [], [], array("q")

        self.cursor.execute(Terms.SELECT_DOCUMENTS)
        for indexid, uid, deleted, length in self.cursor:
            # Index id - id
            self.ids.append(uid)

            # Deleted flag
            if deleted:
                self.deletes.append(indexid)

            # Index id - length
            self.lengths.append(length)

        # Cast ids to int if every id is an integer
        if all(uid.isdigit() for uid in self.ids):
            self.ids = [int(uid) for uid in self.ids]

        # Clear cache
        self.weights.cache_clear()

    def save(self, path):
        """
        Saves terms database to path. This method creates or replaces document attributes into the database.

        Args:
            path: path to write terms database
        """

        # Clear documents table
        self.cursor.execute(Terms.DELETE_DOCUMENTS)

        # Save document attributes
        for i, uid in enumerate(self.ids):
            self.cursor.execute(Terms.INSERT_DOCUMENT, [i, uid, 1 if i in self.deletes else 0, self.lengths[i]])

        # Temporary database
        if not self.path:
            # Save temporary database
            self.connection.commit()

            # Copy data from current to new
            connection = self.copy(path)

            # Close temporary database
            self.connection.close()

            # Point connection to new connection
            self.connection = connection
            self.cursor = self.connection.cursor()
            self.path = path

        # Paths are equal, commit changes
        elif self.path == path:
            self.connection.commit()

        # New path is different from current path, copy data and continue using current connection
        else:
            self.copy(path).close()

    def close(self):
        """
        Close and free resources used by this instance.
        """

        # Close connection
        if self.connection:
            self.connection.close()

    def initialize(self):
        """
        Creates connection and initial database schema if no connection exists.
        """

        if not self.connection:
            # Create term database
            self.connection = self.connect()
            self.cursor = self.connection.cursor()

            # Create initial schema
            self.cursor.execute(Terms.CREATE_TERMS)
            self.cursor.execute(Terms.CREATE_DOCUMENTS)

    def connect(self, path=""):
        """
        Creates a new term database connection.

        Args:
            path: path to term database file

        Returns:
            connection
        """

        connection = sqlite3.connect(path, check_same_thread=False)

        # Enable WAL mode, if necessary
        if self.config.get("wal"):
            connection.execute("PRAGMA journal_mode=WAL")

        return connection

    def copy(self, path):
        """
        Copies content from current terms database into target.

        Args:
            path: target database path

        Returns:
            new database connection
        """

        # Delete existing file, if necessary
        if os.path.exists(path):
            os.remove(path)

        # Create new connection
        connection = self.connect(path)

        if self.connection.in_transaction:
            # The backup call will hang if there are uncommitted changes, need to copy over
            # with iterdump (which is much slower)
            for sql in self.connection.iterdump():
                connection.execute(sql)
        else:
            # Database is up to date, can do a more efficient copy with SQLite C API
            self.connection.backup(connection)

        return connection

    def add(self, indexid, term, freq):
        """
        Adds a term frequency entry.

        Args:
            indexid: internal index id
            term: term
            freq: term frequency
        """

        # Get or create uids and freqs arrays
        if term not in self.terms:
            self.terms[term] = (array("q"), array("q"))

        # Append uids and freqs
        ids, freqs = self.terms[term]
        ids.append(indexid)
        freqs.append(freq)

    def lookup(self, term):
        """
        Retrieves a term frequency sparse array.

        Args:
            term: term to lookup

        Returns:
            term frequency sparse array
        """

        uids, freqs = None, None

        result = self.cursor.execute(Terms.SELECT_TERMS, [term]).fetchone()
        if result:
            uids, freqs = (array("q"), array("q"))
            uids.frombytes(result[0])
            freqs.frombytes(result[1])

            # Storage format is always little endian
            if sys.byteorder == "big":
                uids.byteswap()
                freqs.byteswap()

        return uids, freqs

    @functools.lru_cache(maxsize=500)
    def weights(self, term):
        """
        Computes a term weights sparse array for term. This method is wrapped with a least recently used cache,
        which will return common term weights from the cache.

        Args:
            term: term

        Returns:
            term weights sparse array
        """

        lengths = np.frombuffer(self.lengths, dtype=np.int64)

        with self.lock:
            uids, freqs = self.lookup(term)
            weights = None

        if uids:
            uids = np.frombuffer(uids, dtype=np.int64)
            weights = self.score(np.frombuffer(freqs, dtype=np.int64), self.idf[term], lengths[uids]).astype(np.float32)

        return uids, weights

    def topn(self, scores, limit, hasscores, skipped):
        """
        Get topn scores from an partial scores array.

        Args:
            scores: partial scores array with scores for less common terms
            limit: maximum results
            hasscores: True if partial scores array has any nonzero scores, False otherwise
            skipped: terms skipped in initial query

        Returns:
            topn scores
        """

        # Calculate topn candidates to consider
        # Require at least one positive score, set topn to smaller of limit * 5 or number of scores
        topn = min(len(scores), limit * 5)

        # Get topn candidates, allows for score shifting when adding in common term scores
        matches = self.candidates(scores, topn)

        # Merge in scores for more common terms
        self.merge(scores, matches, hasscores, skipped)

        # Get topn candidates since it was initially skipped above
        if not hasscores:
            matches = self.candidates(scores, topn)

        # Reorder matches using updated scores
        matches = matches[np.argsort(-scores[matches])]

        # Combine ids with scores. Require score > 0.
        return [(self.ids[x], float(scores[x])) for x in matches[:limit] if scores[x] > 0]

    def merge(self, scores, matches, hasscores, terms):
        """
        Merges common term scores into scores array.

        Args:
            scores: partial scores array
            matches: current matches, if any
            hasscores: True if scores has current matches, False otherwise
            terms: common terms
        """

        for term, freq in terms.items():
            # Compute or lookup term weights
            uids, weights = self.weights(term)

            # Filter to topn matches when partial scores array has nonzero scores
            if hasscores:
                # Find indices in match ids for uids
                indices = np.searchsorted(uids, matches)

                # Filter matches that don't exist in uids
                indices = [x for i, x in enumerate(indices) if x < len(uids) and uids[x] == matches[i]]

                # Filter to matching uids and weights
                uids, weights = uids[indices], weights[indices]

            # Update scores
            scores[uids] += freq * weights

    def candidates(self, scores, topn):
        """
        Gets the topn scored candidates. This method ignores deleted documents.

        Args:
            scores: scores array
            topn: topn elements

        Returns:
            topn scored candidates
        """

        # Clear deletes
        scores[self.deletes] = 0

        # Get topn candidates
        return np.argpartition(scores, -topn)[-topn:]



================================================
FILE: src/python/txtai/scoring/tfidf.py
================================================
"""
TFIDF module
"""

import math
import os

from collections import Counter
from multiprocessing.pool import ThreadPool

import numpy as np

from ..pipeline import Tokenizer
from ..serialize import Serializer

from .base import Scoring
from .terms import Terms


class TFIDF(Scoring):
    """
    Term frequency-inverse document frequency (TF-IDF) scoring.
    """

    def __init__(self, config=None):
        super().__init__(config)

        # Document stats
        self.total = 0
        self.tokens = 0
        self.avgdl = 0

        # Word frequency
        self.docfreq = Counter()
        self.wordfreq = Counter()
        self.avgfreq = 0

        # IDF index
        self.idf = {}
        self.avgidf = 0

        # Tag boosting
        self.tags = Counter()

        # Tokenizer, lazily loaded as needed
        self.tokenizer = None

        # Term index
        self.terms = Terms(self.config["terms"], self.score, self.idf) if self.config.get("terms") else None

        # Document data
        self.documents = {} if self.config.get("content") else None

        # Normalize scores
        self.normalize = self.config.get("normalize")
        self.avgscore = None

    def insert(self, documents, index=None, checkpoint=None):
        # Insert documents, calculate word frequency, total tokens and total documents
        for uid, document, tags in documents:
            # Extract text, if necessary
            if isinstance(document, dict):
                document = document.get(self.text, document.get(self.object))

            if document is not None:
                # If index is passed, use indexid, otherwise use id
                uid = index if index is not None else uid

                # Add entry to index if the data type is accepted
                if isinstance(document, (str, list)):
                    # Store content
                    if self.documents is not None:
                        self.documents[uid] = document

                    # Convert to tokens, if necessary
                    tokens = self.tokenize(document) if isinstance(document, str) else document

                    # Add tokens for id to term index
                    if self.terms is not None:
                        self.terms.insert(uid, tokens)

                    # Add tokens and tags to stats
                    self.addstats(tokens, tags)

                # Increment index
                index = index + 1 if index is not None else None

    def delete(self, ids):
        # Delete from terms index
        if self.terms:
            self.terms.delete(ids)

        # Delete content
        if self.documents:
            for uid in ids:
                self.documents.pop(uid)

    def index(self, documents=None):
        # Call base method
        super().index(documents)

        # Build index if tokens parsed
        if self.wordfreq:
            # Calculate total token frequency
            self.tokens = sum(self.wordfreq.values())

            # Calculate average frequency per token
            self.avgfreq = self.tokens / len(self.wordfreq.values())

            # Calculate average document length in tokens
            self.avgdl = self.tokens / self.total

            # Compute IDF scores
            idfs = self.computeidf(np.array(list(self.docfreq.values())))
            for x, word in enumerate(self.docfreq):
                self.idf[word] = float(idfs[x])

            # Average IDF score per token
            self.avgidf = float(np.mean(idfs))

            # Calculate average score across index
            self.avgscore = self.score(self.avgfreq, self.avgidf, self.avgdl)

            # Filter for tags that appear in at least 1% of the documents
            self.tags = Counter({tag: number for tag, number in self.tags.items() if number >= self.total * 0.005})

        # Index terms, if available
        if self.terms:
            self.terms.index()

    def weights(self, tokens):
        # Document length
        length = len(tokens)

        # Calculate token counts
        freq = self.computefreq(tokens)
        freq = np.array([freq[token] for token in tokens])

        # Get idf scores
        idf = np.array([self.idf[token] if token in self.idf else self.avgidf for token in tokens])

        # Calculate score for each token, use as weight
        weights = self.score(freq, idf, length).tolist()

        # Boost weights of tag tokens to match the largest weight in the list
        if self.tags:
            tags = {token: self.tags[token] for token in tokens if token in self.tags}
            if tags:
                maxWeight = max(weights)
                maxTag = max(tags.values())

                weights = [max(maxWeight * (tags[tokens[x]] / maxTag), weight) if tokens[x] in tags else weight for x, weight in enumerate(weights)]

        return weights

    def search(self, query, limit=3):
        # Check if term index available
        if self.terms:
            # Parse query into terms
            query = self.tokenize(query) if isinstance(query, str) else query

            # Get topn term query matches
            scores = self.terms.search(query, limit)

            # Normalize scores, if enabled
            if self.normalize and scores:
                # Calculate max score = best score for this query + average index score
                # Limit max to 6 * average index score
                maxscore = min(scores[0][1] + self.avgscore, 6 * self.avgscore)

                # Normalize scores between 0 - 1 using maxscore
                scores = [(x, min(score / maxscore, 1.0)) for x, score in scores]

            # Add content, if available
            return self.results(scores)

        return None

    def batchsearch(self, queries, limit=3, threads=True):
        # Calculate number of threads using a thread per 25k records in index
        threads = math.ceil(self.count() / 25000) if isinstance(threads, bool) and threads else int(threads)
        threads = min(max(threads, 1), os.cpu_count())

        # This method is able to run as multiple threads due to a number of regex and numpy method calls that drop the GIL.
        results = []
        with ThreadPool(threads) as pool:
            for result in pool.starmap(self.search, [(x, limit) for x in queries]):
                results.append(result)

        return results

    def count(self):
        return self.terms.count() if self.terms else self.total

    def load(self, path):
        # Load scoring
        state = Serializer.load(path)

        # Convert to Counter instances
        for key in ["docfreq", "wordfreq", "tags"]:
            state[key] = Counter(state[key])

        # Convert documents to dict
        state["documents"] = dict(state["documents"]) if state["documents"] else state["documents"]

        # Set parameters on this object
        self.__dict__.update(state)

        # Load terms
        if self.config.get("terms"):
            self.terms = Terms(self.config["terms"], self.score, self.idf)
            self.terms.load(path + ".terms")

    def save(self, path):
        # Don't serialize following fields
        skipfields = ("config", "terms", "tokenizer")

        # Get object state
        state = {key: value for key, value in self.__dict__.items() if key not in skipfields}

        # Update documents to tuples
        state["documents"] = list(state["documents"].items()) if state["documents"] else state["documents"]

        # Save scoring
        Serializer.save(state, path)

        # Save terms
        if self.terms:
            self.terms.save(path + ".terms")

    def close(self):
        if self.terms:
            self.terms.close()

    def issparse(self):
        return self.terms is not None

    def isnormalized(self):
        return self.normalize

    def computefreq(self, tokens):
        """
        Computes token frequency. Used for token weighting.

        Args:
            tokens: input tokens

        Returns:
            {token: count}
        """

        return Counter(tokens)

    def computeidf(self, freq):
        """
        Computes an idf score for word frequency.

        Args:
            freq: word frequency

        Returns:
            idf score
        """

        return np.log((self.total + 1) / (freq + 1)) + 1

    # pylint: disable=W0613
    def score(self, freq, idf, length):
        """
        Calculates a score for each token.

        Args:
            freq: token frequency
            idf: token idf score
            length: total number of tokens in source document

        Returns:
            token score
        """

        return idf * np.sqrt(freq) * (1 / np.sqrt(length))

    def addstats(self, tokens, tags):
        """
        Add tokens and tags to stats.

        Args:
            tokens: list of tokens
            tags: list of tags
        """

        # Total number of times token appears, count all tokens
        self.wordfreq.update(tokens)

        # Total number of documents a token is in, count unique tokens
        self.docfreq.update(set(tokens))

        # Get list of unique tags
        if tags:
            self.tags.update(tags.split())

        # Total document count
        self.total += 1

    def tokenize(self, text):
        """
        Tokenizes text using default tokenizer.

        Args:
            text: input text

        Returns:
            tokens
        """

        # Load tokenizer
        if not self.tokenizer:
            self.tokenizer = self.loadtokenizer()

        return self.tokenizer(text)

    def loadtokenizer(self):
        """
        Load default tokenizer.

        Returns:
            tokenize method
        """

        # Custom tokenizer settings
        if self.config.get("tokenizer"):
            return Tokenizer(**self.config.get("tokenizer"))

        # Terms index use a standard tokenizer
        if self.config.get("terms"):
            return Tokenizer()

        # Standard scoring index without a terms index uses backwards compatible static tokenize method
        return Tokenizer.tokenize

    def results(self, scores):
        """
        Resolves a list of (id, score) with document content, if available. Otherwise, the original input is returned.

        Args:
            scores: list of (id, score)

        Returns:
            resolved results
        """

        # Convert to Python values
        scores = [(x, float(score)) for x, score in scores]

        if self.documents:
            return [{"id": x, "text": self.documents[x], "score": score} for x, score in scores]

        return scores



================================================
FILE: src/python/txtai/serialize/__init__.py
================================================
"""
Serialize imports
"""

from .base import Serialize
from .errors import SerializeError
from .factory import SerializeFactory
from .messagepack import MessagePack
from .pickle import Pickle
from .serializer import Serializer



================================================
FILE: src/python/txtai/serialize/base.py
================================================
"""
Serialize module
"""


class Serialize:
    """
    Base class for Serialize instances. This class serializes data to files, streams and bytes.
    """

    def load(self, path):
        """
        Loads data from path.

        Args:
            path: input path

        Returns:
            deserialized data
        """

        with open(path, "rb") as handle:
            return self.loadstream(handle)

    def save(self, data, path):
        """
        Saves data to path.

        Args:
            data: data to save
            path: output path
        """

        with open(path, "wb") as handle:
            self.savestream(data, handle)

    def loadstream(self, stream):
        """
        Loads data from stream.

        Args:
            stream: input stream

        Returns:
            deserialized data
        """

        raise NotImplementedError

    def savestream(self, data, stream):
        """
        Saves data to stream.

        Args:
            data: data to save
            stream: output stream
        """

        raise NotImplementedError

    def loadbytes(self, data):
        """
        Loads data from bytes.

        Args:
            data: input bytes

        Returns:
            deserialized data
        """

        raise NotImplementedError

    def savebytes(self, data):
        """
        Saves data as bytes.

        Args:
            data: data to save

        Returns:
            serialized data
        """

        raise NotImplementedError



================================================
FILE: src/python/txtai/serialize/errors.py
================================================
"""
Errors module
"""


class SerializeError(Exception):
    """
    Raised when data serialization fails
    """



================================================
FILE: src/python/txtai/serialize/factory.py
================================================
"""
Factory module
"""

from .messagepack import MessagePack
from .pickle import Pickle


class SerializeFactory:
    """
    Methods to create data serializers.
    """

    @staticmethod
    def create(method=None, **kwargs):
        """
        Creates a new Serialize instance.

        Args:
            method: serialization method
            kwargs: additional keyword arguments to pass to serialize instance
        """

        # Pickle serialization
        if method == "pickle":
            return Pickle(**kwargs)

        # Default serialization
        return MessagePack(**kwargs)



================================================
FILE: src/python/txtai/serialize/messagepack.py
================================================
"""
MessagePack module
"""

import msgpack
from msgpack import Unpacker
from msgpack.exceptions import ExtraData

from .base import Serialize
from .errors import SerializeError


class MessagePack(Serialize):
    """
    MessagePack serialization.
    """

    def __init__(self, streaming=False, **kwargs):
        # Parent constructor
        super().__init__()

        # Streaming unpacker
        self.streaming = streaming

        # Additional streaming unpacker keyword arguments
        self.kwargs = kwargs

    def loadstream(self, stream):
        try:
            # Support both streaming and non-streaming unpacking of data
            return Unpacker(stream, **self.kwargs) if self.streaming else msgpack.unpack(stream)
        except ExtraData as e:
            raise SerializeError(e) from e

    def savestream(self, data, stream):
        msgpack.pack(data, stream)

    def loadbytes(self, data):
        return msgpack.unpackb(data)

    def savebytes(self, data):
        return msgpack.packb(data)



================================================
FILE: src/python/txtai/serialize/pickle.py
================================================
"""
Pickle module
"""

import os
import logging
import pickle
import warnings

from .base import Serialize

# Logging configuration
logger = logging.getLogger(__name__)


class Pickle(Serialize):
    """
    Pickle serialization.
    """

    def __init__(self, allowpickle=False):
        """
        Creates a new instance for Pickle serialization.

        This class ensures the allowpickle parameter or the `ALLOW_PICKLE` environment variable is True. All methods will
        raise errors if this isn't the case.

        Pickle serialization is OK for local data but it isn't recommended when sharing data externally.

        Args:
            allowpickle: default pickle allow mode, only True with methods that generate local temporary data
        """

        # Parent constructor
        super().__init__()

        # Default allow pickle mode
        self.allowpickle = allowpickle

        # Current pickle protocol
        self.version = 4

    def load(self, path):
        # Load pickled data from path, if allowed
        return super().load(path) if self.allow(path) else None

    def save(self, data, path):
        # Save pickled data to path, if allowed
        if self.allow():
            super().save(data, path)

    def loadstream(self, stream):
        # Load pickled data from stream, if allowed
        return pickle.load(stream) if self.allow() else None

    def savestream(self, data, stream):
        # Save pickled data to stream, if allowed
        if self.allow():
            pickle.dump(data, stream, protocol=self.version)

    def loadbytes(self, data):
        # Load pickled data from bytes, if allowed
        return pickle.loads(data) if self.allow() else None

    def savebytes(self, data):
        # Save pickled data to stream, if allowed
        return pickle.dumps(data, protocol=self.version) if self.allow() else None

    def allow(self, path=None):
        """
        Checks if loading and saving pickled data is allowed. Raises an error if it's not allowed.

        Args:
            path: optional path to add to generated error messages
        """

        enablepickle = self.allowpickle or os.environ.get("ALLOW_PICKLE", "False") in ("True", "1")
        if not enablepickle:
            raise ValueError(
                (
                    "Loading of pickled index data is disabled. "
                    f"`{path if path else 'stream'}` was not loaded. "
                    "Set the env variable `ALLOW_PICKLE=True` to enable loading pickled index data. "
                    "This should only be done for trusted and/or local data."
                )
            )

        if not self.allowpickle:
            warnings.warn(
                (
                    "Loading of pickled data enabled through `ALLOW_PICKLE=True` env variable. "
                    "This setting should only be used with trusted and/or local data. "
                    "Saving this index will replace pickled index data formats with the latest index formats and remove this warning."
                ),
                RuntimeWarning,
            )

        return enablepickle



================================================
FILE: src/python/txtai/serialize/serializer.py
================================================
"""
Serializer module
"""

from .errors import SerializeError
from .factory import SerializeFactory


class Serializer:
    """
    Methods to serialize and deserialize data.
    """

    @staticmethod
    def load(path):
        """
        Loads data from path. This method first tries to load the default serialization format.
        If that fails, it will fallback to pickle format for backwards-compatability purposes.

        Note that loading pickle files requires the env variable `ALLOW_PICKLE=True`.

        Args:
            path: data to load

        Returns:
            data
        """

        try:
            return SerializeFactory.create().load(path)
        except SerializeError:
            # Backwards compatible check for pickled data
            return SerializeFactory.create("pickle").load(path)

    @staticmethod
    def save(data, path):
        """
        Saves data to path.

        Args:
            data: data to save
            path: output path
        """

        # Save using default serialization method
        SerializeFactory.create().save(data, path)



================================================
FILE: src/python/txtai/util/__init__.py
================================================
"""
Utility imports
"""

from .resolver import Resolver
from .sparsearray import SparseArray
from .template import TemplateFormatter



================================================
FILE: src/python/txtai/util/resolver.py
================================================
"""
Resolver module
"""


class Resolver:
    """
    Resolves a Python class path
    """

    def __call__(self, path):
        """
        Class instance to resolve.

        Args:
            path: path to class

        Returns:
            class instance
        """

        # Split into path components
        parts = path.split(".")

        # Resolve each path component
        module = ".".join(parts[:-1])
        m = __import__(module)
        for comp in parts[1:]:
            m = getattr(m, comp)

        # Return class instance
        return m



================================================
FILE: src/python/txtai/util/sparsearray.py
================================================
"""
SparseArray module
"""

import numpy as np

# Conditional import
try:
    from scipy.sparse import csr_matrix

    SCIPY = True
except ImportError:
    SCIPY = False


class SparseArray:
    """
    Methods to load and save sparse arrays to file.
    """

    def __init__(self):
        """
        Creates a SparseArray instance.
        """

        if not SCIPY:
            raise ImportError("SciPy is not available - install scipy to enable")

    def load(self, f):
        """
        Loads a sparse array from file.

        Args:
            f: input file handle

        Returns:
            sparse array
        """

        # Load raw data
        data, indices, indptr, shape = (
            np.load(f, allow_pickle=False),
            np.load(f, allow_pickle=False),
            np.load(f, allow_pickle=False),
            np.load(f, allow_pickle=False),
        )

        # Load data into sparse array
        return csr_matrix((data, indices, indptr), shape=shape)

    def save(self, f, array):
        """
        Saves a sparse array to file.

        Args:
            f: output file handle
            array: sparse array
        """

        # Save sparse array to file
        for x in [array.data, array.indices, array.indptr, array.shape]:
            np.save(f, x, allow_pickle=False)



================================================
FILE: src/python/txtai/util/template.py
================================================
"""
Template module
"""

from string import Formatter


class TemplateFormatter(Formatter):
    """
    Custom Formatter that requires each argument to be consumed.
    """

    def check_unused_args(self, used_args, args, kwargs):
        difference = set(kwargs).difference(used_args)
        if difference:
            raise KeyError(difference)



================================================
FILE: src/python/txtai/vectors/__init__.py
================================================
"""
Vectors imports
"""

from .base import Vectors
from .dense import *
from .recovery import Recovery
from .sparse import *



================================================
FILE: src/python/txtai/vectors/base.py
================================================
"""
Vectors module
"""

import json
import os
import tempfile
import uuid

import numpy as np

from ..pipeline import Tokenizer

from .recovery import Recovery


class Vectors:
    """
    Base class for vector models. Vector models transform input content into numeric vectors.
    """

    def __init__(self, config, scoring, models):
        """
        Creates a new vectors instance.

        Args:
            config: vector configuration
            scoring: optional scoring instance for term weighting
            models: models cache
        """

        # Store parameters
        self.config = config
        self.scoring = scoring
        self.models = models

        if config:
            # Detect if this is an initialized configuration
            self.initialized = "dimensions" in config

            # Enables optional string tokenization
            self.tokenize = config.get("tokenize")

            # Load model
            self.model = self.load(config.get("path"))

            # Encode batch size - controls underlying model batch size when encoding vectors
            self.encodebatch = config.get("encodebatch", 32)

            # Embeddings instructions
            self.instructions = config.get("instructions")

            # Truncate embeddings to this dimensionality
            self.dimensionality = config.get("dimensionality")

            # Scalar quantization - supports 1-bit through 8-bit quantization
            quantize = config.get("quantize")
            self.qbits = max(min(quantize, 8), 1) if isinstance(quantize, int) and not isinstance(quantize, bool) else None

    def loadmodel(self, path):
        """
        Loads vector model at path.

        Args:
            path: path to vector model

        Returns:
            vector model
        """

        raise NotImplementedError

    def encode(self, data, category=None):
        """
        Encodes a batch of data using vector model.

        Args:
            data: batch of data
            category: optional category for instruction-based embeddings

        Return:
            transformed data
        """

        raise NotImplementedError

    def load(self, path):
        """
        Loads a model using the current configuration. This method will return previously cached models
        if available.

        Returns:
            model
        """

        # Check if model is cached
        if self.models and path in self.models:
            return self.models[path]

        # Create new model
        model = self.loadmodel(path)

        # Store model in cache
        if self.models is not None and path:
            self.models[path] = model

        return model

    def index(self, documents, batchsize=500, checkpoint=None):
        """
        Converts a list of documents to a temporary file with embeddings arrays. Returns a tuple of document ids,
        number of dimensions and temporary file with embeddings.

        Args:
            documents: list of (id, data, tags)
            batchsize: index batch size
            checkpoint: optional checkpoint directory, enables indexing restart

        Returns:
            (ids, dimensions, batches, stream)
        """

        ids, dimensions, batches, stream = [], None, 0, None

        # Generate recovery config if checkpoint is set
        vectorsid = self.vectorsid() if checkpoint else None
        recovery = Recovery(checkpoint, vectorsid, self.loadembeddings) if checkpoint else None

        # Convert all documents to embedding arrays, stream embeddings to disk to control memory usage
        with self.spool(checkpoint, vectorsid) as output:
            stream = output.name
            batch = []
            for document in documents:
                batch.append(document)

                if len(batch) == batchsize:
                    # Convert batch to embeddings
                    uids, dimensions = self.batch(batch, output, recovery)
                    ids.extend(uids)
                    batches += 1

                    batch = []

            # Final batch
            if batch:
                uids, dimensions = self.batch(batch, output, recovery)
                ids.extend(uids)
                batches += 1

        return (ids, dimensions, batches, stream)

    def vectors(self, documents, batchsize=500, checkpoint=None, buffer=None, dtype=None):
        """
        Bulk encodes documents into vectors using index(). Return the data as a mmap-ed array.

        Args:
            documents: list of (id, data, tags)
            batchsize: index batch size
            checkpoint: optional checkpoint directory, enables indexing restart
            buffer: file path used for memmap buffer
            dtype: dtype for buffer

        Returns:
            (ids, dimensions, embeddings)
        """

        # Consume stream and transform documents to vectors
        ids, dimensions, batches, stream = self.index(documents, batchsize, checkpoint)

        # Check that embeddings are available and load as a memmap
        embeddings = None
        if ids:
            # Write batches
            embeddings = np.memmap(buffer, dtype=dtype, shape=(len(ids), dimensions), mode="w+")
            with open(stream, "rb") as queue:
                x = 0
                for _ in range(batches):
                    batch = self.loadembeddings(queue)
                    embeddings[x : x + batch.shape[0]] = batch
                    x += batch.shape[0]

        # Remove temporary file (if checkpointing is disabled)
        if not checkpoint:
            os.remove(stream)

        return (ids, dimensions, embeddings)

    def close(self):
        """
        Closes this vectors instance.
        """

        self.model = None

    def transform(self, document):
        """
        Transforms document into an embeddings vector.

        Args:
            document: (id, data, tags)

        Returns:
            embeddings vector
        """

        # Prepare input document for vectors model and build embeddings
        return self.batchtransform([document])[0]

    def batchtransform(self, documents, category=None):
        """
        Transforms batch of documents into embeddings vectors.

        Args:
            documents: list of documents used to build embeddings
            category: category for instruction-based embeddings

        Returns:
            embeddings vectors
        """

        # Prepare input documents for vectors model
        documents = [self.prepare(data, category) for _, data, _ in documents]

        # Skip encoding data if it's already an array
        if documents and isinstance(documents[0], np.ndarray):
            return np.array(documents, dtype=np.float32)

        return self.vectorize(documents, category)

    def dot(self, queries, data):
        """
        Calculates the dot product similarity between queries and documents. This method
        assumes each of the inputs are normalized.

        Args:
            queries: queries
            data: search data

        Returns:
            dot product scores
        """

        return np.dot(queries, data.T).tolist()

    def vectorsid(self):
        """
        Generates vectors uid for this vectors instance.

        Returns:
            vectors uid
        """

        # Select config options that determine uniqueness
        select = ["path", "method", "tokenizer", "maxlength", "tokenize", "instructions", "dimensionality", "quantize"]
        config = {k: v for k, v in self.config.items() if k in select}
        config.update(self.config.get("vectors", {}))

        # Generate a deterministic UUID
        return str(uuid.uuid5(uuid.NAMESPACE_DNS, json.dumps(config, sort_keys=True)))

    def spool(self, checkpoint, vectorsid):
        """
        Opens a spool file for queuing generated vectors.

        Args:
            checkpoint: optional checkpoint directory, enables indexing restart
            vectorsid: vectors uid for current configuration

        Returns:
            vectors spool file
        """

        # Spool to vectors checkpoint file
        if checkpoint:
            os.makedirs(checkpoint, exist_ok=True)
            return open(f"{checkpoint}/{vectorsid}", "wb")

        # Spool to temporary file
        return tempfile.NamedTemporaryFile(mode="wb", suffix=".npy", delete=False)

    def batch(self, documents, output, recovery):
        """
        Builds a batch of embeddings.

        Args:
            documents: list of documents used to build embeddings
            output: output temp file to store embeddings
            recovery: optional recovery instance

        Returns:
            (ids, dimensions) list of ids and number of dimensions in embeddings
        """

        # Extract ids and prepare input documents for vectors model
        ids = [uid for uid, _, _ in documents]
        documents = [self.prepare(data, "data") for _, data, _ in documents]
        dimensions = None

        # Attempt to read embeddings from a recovery file
        embeddings = recovery() if recovery else None
        embeddings = self.vectorize(documents, "data") if embeddings is None else embeddings
        if embeddings is not None:
            dimensions = embeddings.shape[1]
            self.saveembeddings(output, embeddings)

        return (ids, dimensions)

    def prepare(self, data, category=None):
        """
        Prepares input data for vector model.

        Args:
            data: input data
            category: category for instruction-based embeddings

        Returns:
            data formatted for vector model
        """

        # Prepares tokens for the model
        data = self.tokens(data)

        # Default instruction category
        category = category if category else "query"

        # Prepend instructions, if applicable
        if self.instructions and category in self.instructions and isinstance(data, str):
            # Prepend category instruction
            data = f"{self.instructions[category]}{data}"

        return data

    def tokens(self, data):
        """
        Prepare data as tokens model can accept.

        Args:
            data: input data

        Returns:
            tokens formatted for model
        """

        # Optional string tokenization
        if self.tokenize and isinstance(data, str):
            data = Tokenizer.tokenize(data)

        # Convert token list to string
        if isinstance(data, list):
            data = " ".join(data)

        return data

    def vectorize(self, data, category=None):
        """
        Runs data vectorization, which consists of the following steps.

          1. Encode data into vectors using underlying model
          2. Truncate vectors, if necessary
          3. Normalize vectors
          4. Quantize vectors, if necessary

        Args:
            data: input data
            category: category for instruction-based embeddings

        Returns:
            embeddings vectors
        """

        # Default instruction category
        category = category if category else "query"

        # Transform data into vectors
        embeddings = self.encode(data, category)

        if embeddings is not None:
            # Truncate embeddings, if necessary
            if self.dimensionality and self.dimensionality < embeddings.shape[1]:
                embeddings = self.truncate(embeddings)

            # Normalize data
            embeddings = self.normalize(embeddings)

            # Apply quantization, if necessary
            if self.qbits:
                embeddings = self.quantize(embeddings)

        return embeddings

    def loadembeddings(self, f):
        """
        Loads embeddings from file.

        Args:
            f: file to load from

        Returns:
            embeddings
        """

        return np.load(f, allow_pickle=False)

    def saveembeddings(self, f, embeddings):
        """
        Saves embeddings to output.

        Args:
            f: output file
            embeddings: embeddings to save
        """

        np.save(f, embeddings, allow_pickle=False)

    def truncate(self, embeddings):
        """
        Truncates embeddings to the configured dimensionality.

        This is only useful for models trained to store more important information in
        earlier dimensions such as Matryoshka Representation Learning (MRL).

        Args:
            embeddings: input embeddings

        Returns:
            truncated embeddings
        """

        return embeddings[:, : self.dimensionality]

    def normalize(self, embeddings):
        """
        Normalizes embeddings using L2 normalization. Operation applied directly on array.

        Args:
            embeddings: input embeddings

        Returns:
            embeddings
        """

        # Calculation is different for matrices vs vectors
        if len(embeddings.shape) > 1:
            embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]
        else:
            embeddings /= np.linalg.norm(embeddings)

        return embeddings

    def quantize(self, embeddings):
        """
        Quantizes embeddings using scalar quantization.

        Args:
            embeddings: input embeddings

        Returns:
            quantized embeddings
        """

        # Scale factor is midpoint in range
        factor = 2 ** (self.qbits - 1)

        # Quantize to uint8
        scalars = embeddings * factor
        scalars = scalars.clip(-factor, factor - 1) + factor
        scalars = scalars.astype(np.uint8)

        # Transform uint8 to bits
        bits = np.unpackbits(scalars.reshape(-1, 1), axis=1)

        # Remove unused bits (i.e. for 3-bit quantization, the leading 5 bits are removed)
        bits = bits[:, -self.qbits :]

        # Reshape using original data dimensions and pack bits into uint8 array
        return np.packbits(bits.reshape(embeddings.shape[0], embeddings.shape[1] * self.qbits), axis=1)



================================================
FILE: src/python/txtai/vectors/recovery.py
================================================
"""
Recovery module
"""

import os
import shutil


class Recovery:
    """
    Vector embeddings recovery. This class handles streaming embeddings from a vector checkpoint file.
    """

    def __init__(self, checkpoint, vectorsid, load):
        """
        Creates a Recovery instance.

        Args:
            checkpoint: checkpoint directory
            vectorsid: vectors uid for current configuration
            load: load embeddings method
        """

        self.spool, self.path, self.load = None, None, load

        # Get unique file id
        path = f"{checkpoint}/{vectorsid}"
        if os.path.exists(path):
            # Generate recovery path
            self.path = f"{checkpoint}/recovery"

            # Copy current checkpoint to recovery
            shutil.copyfile(path, self.path)

            # Open file an return
            # pylint: disable=R1732
            self.spool = open(self.path, "rb")

    def __call__(self):
        """
        Reads and returns the next batch of embeddings.

        Returns
            batch of embeddings
        """

        try:
            return self.load(self.spool) if self.spool else None
        except EOFError:
            # End of spool file, cleanup
            self.spool.close()
            os.remove(self.path)

            # Clear parameters
            self.spool, self.path = None, None

            return None



================================================
FILE: src/python/txtai/vectors/dense/__init__.py
================================================
"""
Dense vectors imports
"""

from .external import External
from .factory import VectorsFactory
from .huggingface import HFVectors
from .litellm import LiteLLM
from .llama import LlamaCpp
from .m2v import Model2Vec
from .sbert import STVectors
from .words import WordVectors



================================================
FILE: src/python/txtai/vectors/dense/external.py
================================================
"""
External module
"""

import types

import numpy as np

from ...util import Resolver

from ..base import Vectors


class External(Vectors):
    """
    Builds vectors using an external method. This can be a local function or an external API call.
    """

    def __init__(self, config, scoring, models):
        super().__init__(config, scoring, models)

        # Lookup and resolve transform function
        self.transform = self.resolve(config.get("transform"))

    def loadmodel(self, path):
        return None

    def encode(self, data, category=None):
        # Call external transform function, if available and data not already an array
        # Batching is handed by the external transform function
        if self.transform and data and not isinstance(data[0], np.ndarray):
            data = self.transform(data)

        # Cast to float32
        return data.astype(np.float32) if isinstance(data, np.ndarray) else np.array(data, dtype=np.float32)

    def resolve(self, transform):
        """
        Resolves a transform function.

        Args:
            transform: transform function

        Returns:
            resolved transform function
        """

        if transform:
            # Resolve transform instance, if necessary
            transform = Resolver()(transform) if transform and isinstance(transform, str) else transform

            # Get function or callable instance
            transform = transform if isinstance(transform, types.FunctionType) else transform()

        return transform



================================================
FILE: src/python/txtai/vectors/dense/factory.py
================================================
"""
Factory module
"""

from ...util import Resolver

from .external import External
from .huggingface import HFVectors
from .litellm import LiteLLM
from .llama import LlamaCpp
from .m2v import Model2Vec
from .sbert import STVectors
from .words import WordVectors


class VectorsFactory:
    """
    Methods to create dense vector models.
    """

    @staticmethod
    def create(config, scoring=None, models=None):
        """
        Create a Vectors model instance.

        Args:
            config: vector configuration
            scoring: scoring instance
            models: models cache

        Returns:
            Vectors
        """

        # Determine vector method
        method = VectorsFactory.method(config)

        # External vectors
        if method == "external":
            return External(config, scoring, models)

        # LiteLLM vectors
        if method == "litellm":
            return LiteLLM(config, scoring, models)

        # llama.cpp vectors
        if method == "llama.cpp":
            return LlamaCpp(config, scoring, models)

        # Model2vec vectors
        if method == "model2vec":
            return Model2Vec(config, scoring, models)

        # Sentence Transformers vectors
        if method == "sentence-transformers":
            return STVectors(config, scoring, models) if config and config.get("path") else None

        # Word vectors
        if method == "words":
            return WordVectors(config, scoring, models)

        # Transformers vectors
        if HFVectors.ismethod(method):
            return HFVectors(config, scoring, models) if config and config.get("path") else None

        # Resolve custom method
        return VectorsFactory.resolve(method, config, scoring, models) if method else None

    @staticmethod
    def method(config):
        """
        Get or derive the vector method.

        Args:
            config: vector configuration

        Returns:
            vector method
        """

        # Determine vector method
        method = config.get("method")
        path = config.get("path")

        # Infer method from path, if blank
        if not method:
            if path:
                if LiteLLM.ismodel(path):
                    method = "litellm"
                elif LlamaCpp.ismodel(path):
                    method = "llama.cpp"
                elif Model2Vec.ismodel(path):
                    method = "model2vec"
                elif WordVectors.ismodel(path):
                    method = "words"
                else:
                    method = "transformers"
            elif config.get("transform"):
                method = "external"

        return method

    @staticmethod
    def resolve(backend, config, scoring, models):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: vector configuration
            scoring: scoring instance
            models: models cache

        Returns:
            Vectors
        """

        try:
            return Resolver()(backend)(config, scoring, models)
        except Exception as e:
            raise ImportError(f"Unable to resolve vectors backend: '{backend}'") from e



================================================
FILE: src/python/txtai/vectors/dense/huggingface.py
================================================
"""
Hugging Face module
"""

from ...models import Models, PoolingFactory

from ..base import Vectors


class HFVectors(Vectors):
    """
    Builds vectors using the Hugging Face transformers library.
    """

    @staticmethod
    def ismethod(method):
        """
        Checks if this method uses local transformers-based models.

        Args:
            method: input method

        Returns:
            True if this is a local transformers-based model, False otherwise
        """

        return method in ("transformers", "pooling", "clspooling", "meanpooling")

    def loadmodel(self, path):
        # Build embeddings with transformers pooling
        return PoolingFactory.create(
            {
                "method": self.config.get("method"),
                "path": path,
                "device": Models.deviceid(self.config.get("gpu", True)),
                "tokenizer": self.config.get("tokenizer"),
                "maxlength": self.config.get("maxlength"),
                "modelargs": self.config.get("vectors", {}),
            }
        )

    def encode(self, data, category=None):
        # Encode data using vectors model
        return self.model.encode(data, batch=self.encodebatch, category=category)



================================================
FILE: src/python/txtai/vectors/dense/litellm.py
================================================
"""
LiteLLM module
"""

import numpy as np

from transformers.utils import cached_file

# Conditional import
try:
    import litellm as api

    LITELLM = True
except ImportError:
    LITELLM = False

from ..base import Vectors


class LiteLLM(Vectors):
    """
    Builds vectors using an external embeddings API via LiteLLM.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a LiteLLM model.

        Args:
            path: input path

        Returns:
            True if this is a LiteLLM model, False otherwise
        """

        # pylint: disable=W0702
        if isinstance(path, str) and LITELLM:
            debug = api.suppress_debug_info
            try:
                # Suppress debug messages for this test
                api.suppress_debug_info = True
                return api.get_llm_provider(path) and not LiteLLM.ishub(path)
            except:
                return False
            finally:
                # Restore debug info value to original value
                api.suppress_debug_info = debug

        return False

    @staticmethod
    def ishub(path):
        """
        Checks if path is available on the HF Hub.

        Args:
            input path

        Returns:
            True if this is a model on the HF Hub
        """

        # pylint: disable=W0702
        try:
            return cached_file(path_or_repo_id=path, filename="config.json") is not None if "/" in path else False
        except:
            return False

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not LITELLM:
            raise ImportError('LiteLLM is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

    def loadmodel(self, path):
        return None

    def encode(self, data, category=None):
        # Call external embeddings API using LiteLLM
        # Batching is handled server-side
        response = api.embedding(model=self.config.get("path"), input=data, **self.config.get("vectors", {}))

        # Read response into a NumPy array
        return np.array([x["embedding"] for x in response.data], dtype=np.float32)



================================================
FILE: src/python/txtai/vectors/dense/llama.py
================================================
"""
Llama module
"""

import os

import numpy as np

from huggingface_hub import hf_hub_download

# Conditional import
try:
    import llama_cpp as llama

    LLAMA_CPP = True
except ImportError:
    LLAMA_CPP = False

from ..base import Vectors


class LlamaCpp(Vectors):
    """
    Builds vectors using llama.cpp.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a llama.cpp model.

        Args:
            path: input path

        Returns:
            True if this is a llama.cpp model, False otherwise
        """

        return isinstance(path, str) and path.lower().endswith(".gguf")

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not LLAMA_CPP:
            raise ImportError('llama.cpp is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

    def loadmodel(self, path):
        # Check if this is a local path, otherwise download from the HF Hub
        path = path if os.path.exists(path) else self.download(path)

        # Additional model arguments
        modelargs = self.config.get("vectors", {})

        # Default n_ctx to maxlength if available. Otherwise default n_ctx=0, which sets n_ctx=n_ctx_train.
        modelargs["n_ctx"] = modelargs.get("n_ctx", self.config.get("maxlength", 0))

        # Default n_batch to encode batch
        modelargs["n_batch"] = modelargs.get("n_batch", self.config.get("encodebatch", 64))

        # Default GPU layers if not already set
        modelargs["n_gpu_layers"] = modelargs.get("n_gpu_layers", -1 if self.config.get("gpu", os.environ.get("LLAMA_NO_METAL") != "1") else 0)

        # Default verbose flag
        modelargs["verbose"] = modelargs.get("verbose", False)

        # Create llama.cpp instance
        return llama.Llama(model_path=path, embedding=True, **modelargs)

    def encode(self, data, category=None):
        # Generate embeddings and return as a NumPy array
        # llama-cpp-python has it's own batching built-in using n_batch parameter
        return np.array(self.model.embed(data), dtype=np.float32)

    def download(self, path):
        """
        Downloads path from the Hugging Face Hub.

        Args:
            path: full model path

        Returns:
            local cached model path
        """

        # Split into parts
        parts = path.split("/")

        # Calculate repo id split
        repo = 2 if len(parts) > 2 else 1

        # Download and cache file
        return hf_hub_download(repo_id="/".join(parts[:repo]), filename="/".join(parts[repo:]))



================================================
FILE: src/python/txtai/vectors/dense/m2v.py
================================================
"""
Model2Vec module
"""

import json

from huggingface_hub.errors import HFValidationError
from transformers.utils import cached_file

# Conditional import
try:
    from model2vec import StaticModel

    MODEL2VEC = True
except ImportError:
    MODEL2VEC = False

from ..base import Vectors


class Model2Vec(Vectors):
    """
    Builds vectors using Model2Vec.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a Model2Vec model.

        Args:
            path: input path

        Returns:
            True if this is a Model2Vec model, False otherwise
        """

        try:
            # Download file and parse JSON
            path = cached_file(path_or_repo_id=path, filename="config.json")
            if path:
                with open(path, encoding="utf-8") as f:
                    config = json.load(f)
                    return config.get("model_type") == "model2vec"

        # Ignore this error - invalid repo or directory
        except (HFValidationError, OSError):
            pass

        return False

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not MODEL2VEC:
            raise ImportError('Model2Vec is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

    def loadmodel(self, path):
        return StaticModel.from_pretrained(path)

    def encode(self, data, category=None):
        # Additional model arguments
        modelargs = self.config.get("vectors", {})

        # Encode data
        return self.model.encode(data, batch_size=self.encodebatch, **modelargs)



================================================
FILE: src/python/txtai/vectors/dense/sbert.py
================================================
"""
Sentence Transformers module
"""

# Conditional import
try:
    from sentence_transformers import SentenceTransformer

    SENTENCE_TRANSFORMERS = True
except ImportError:
    SENTENCE_TRANSFORMERS = False

from ...models import Models

from ..base import Vectors


class STVectors(Vectors):
    """
    Builds vectors using sentence-transformers (aka SBERT).
    """

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not SENTENCE_TRANSFORMERS:
            raise ImportError('sentence-transformers is not available - install "vectors" extra to enable')

        # Pool parameter created here since loadmodel is called from parent constructor
        self.pool = None

        super().__init__(config, scoring, models)

    def loadmodel(self, path):
        # Get target device
        gpu, pool = self.config.get("gpu", True), False

        # Default mode uses a single GPU. Setting to all spawns a process per GPU.
        if isinstance(gpu, str) and gpu == "all":
            # Get number of accelerator devices available
            devices = Models.acceleratorcount()

            # Enable multiprocessing pooling only when multiple devices are available
            gpu, pool = devices <= 1, devices > 1

        # Tensor device id
        deviceid = Models.deviceid(gpu)

        # Additional model arguments
        modelargs = self.config.get("vectors", {})

        # Load sentence-transformers encoder
        model = self.loadencoder(path, device=Models.device(deviceid), **modelargs)

        # Start process pool for multiple GPUs
        if pool:
            self.pool = model.start_multi_process_pool()

        # Return model
        return model

    def encode(self, data, category=None):
        # Get encode method based on input category
        encode = self.model.encode_query if category == "query" else self.model.encode_document if category == "data" else self.model.encode

        # Additional encoding arguments
        encodeargs = self.config.get("encodeargs", {})

        # Encode with sentence transformers encoder
        return encode(data, pool=self.pool, batch_size=self.encodebatch, **encodeargs)

    def close(self):
        # Close pool before model is closed in parent method
        if self.pool:
            self.model.stop_multi_process_pool(self.pool)
            self.pool = None

        super().close()

    def loadencoder(self, path, device, **kwargs):
        """
        Loads the embeddings encoder model from path.

        Args:
            path: model path
            device: tensor device
            kwargs: additional keyword args

        Returns:
            embeddings encoder
        """

        return SentenceTransformer(path, device=device, **kwargs)



================================================
FILE: src/python/txtai/vectors/dense/words.py
================================================
"""
Word Vectors module
"""

import json
import logging
import os
import tempfile

from multiprocessing import Pool

import numpy as np

from huggingface_hub.errors import HFValidationError
from transformers.utils import cached_file

# Conditional import
try:
    from staticvectors import Database, StaticVectors

    STATICVECTORS = True
except ImportError:
    STATICVECTORS = False

from ...pipeline import Tokenizer

from ..base import Vectors

# Logging configuration
logger = logging.getLogger(__name__)

# Multiprocessing helper methods
# pylint: disable=W0603
PARAMETERS, VECTORS = None, None


def create(config, scoring):
    """
    Multiprocessing helper method. Creates a global embeddings object to be accessed in a new subprocess.

    Args:
        config: vector configuration
        scoring: scoring instance
    """

    global PARAMETERS
    global VECTORS

    # Store model parameters for lazy loading
    PARAMETERS, VECTORS = (config, scoring, None), None


def transform(document):
    """
    Multiprocessing helper method. Transforms document into an embeddings vector.

    Args:
        document: (id, data, tags)

    Returns:
        (id, embedding)
    """

    # Lazy load vectors model
    global VECTORS
    if not VECTORS:
        VECTORS = WordVectors(*PARAMETERS)

    return (document[0], VECTORS.transform(document))


class WordVectors(Vectors):
    """
    Builds vectors using weighted word embeddings.
    """

    @staticmethod
    def ismodel(path):
        """
        Checks if path is a WordVectors model.

        Args:
            path: input path

        Returns:
            True if this is a WordVectors model, False otherwise
        """

        # Check if this is a SQLite database
        if WordVectors.isdatabase(path):
            return True

        try:
            # Download file and parse JSON
            path = cached_file(path_or_repo_id=path, filename="config.json")
            if path:
                with open(path, encoding="utf-8") as f:
                    config = json.load(f)
                    return config.get("model_type") == "staticvectors"

        # Ignore this error - invalid repo or directory
        except (HFValidationError, OSError):
            pass

        return False

    @staticmethod
    def isdatabase(path):
        """
        Checks if this is a SQLite database file which is the file format used for word vectors databases.

        Args:
            path: path to check

        Returns:
            True if this is a SQLite database
        """

        return isinstance(path, str) and STATICVECTORS and Database.isdatabase(path)

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not STATICVECTORS:
            raise ImportError('staticvectors is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

    def loadmodel(self, path):
        return StaticVectors(path)

    def encode(self, data, category=None):
        # Iterate over each data element, tokenize (if necessary) and build an aggregated embeddings vector
        embeddings = []
        for tokens in data:
            # Convert to tokens, if necessary. If tokenized list is empty, use input string.
            if isinstance(tokens, str):
                tokenlist = Tokenizer.tokenize(tokens)
                tokens = tokenlist if tokenlist else [tokens]

            # Generate weights for each vector using a scoring method
            weights = self.scoring.weights(tokens) if self.scoring else None

            # pylint: disable=E1133
            if weights and [x for x in weights if x > 0]:
                # Build weighted average embeddings vector. Create weights array as float32 to match embeddings precision.
                embedding = np.average(self.lookup(tokens), weights=np.array(weights, dtype=np.float32), axis=0)
            else:
                # If no weights, use mean
                embedding = np.mean(self.lookup(tokens), axis=0)

            embeddings.append(embedding)

        return np.array(embeddings, dtype=np.float32)

    def index(self, documents, batchsize=500, checkpoint=None):
        # Derive number of parallel processes
        parallel = self.config.get("parallel", True)
        parallel = os.cpu_count() if parallel and isinstance(parallel, bool) else int(parallel)

        # Use default single process indexing logic
        if not parallel:
            return super().index(documents, batchsize)

        # Customize indexing logic with multiprocessing pool to efficiently build vectors
        ids, dimensions, batches, stream = [], None, 0, None

        # Shared objects with Pool
        args = (self.config, self.scoring)

        # Convert all documents to embedding arrays, stream embeddings to disk to control memory usage
        with Pool(parallel, initializer=create, initargs=args) as pool:
            with tempfile.NamedTemporaryFile(mode="wb", suffix=".npy", delete=False) as output:
                stream = output.name
                embeddings = []
                for uid, embedding in pool.imap(transform, documents, self.encodebatch):
                    if not dimensions:
                        # Set number of dimensions for embeddings
                        dimensions = embedding.shape[0]

                    ids.append(uid)
                    embeddings.append(embedding)

                    if len(embeddings) == batchsize:
                        np.save(output, np.array(embeddings, dtype=np.float32), allow_pickle=False)
                        batches += 1

                        embeddings = []

                # Final embeddings batch
                if embeddings:
                    np.save(output, np.array(embeddings, dtype=np.float32), allow_pickle=False)
                    batches += 1

        return (ids, dimensions, batches, stream)

    def lookup(self, tokens):
        """
        Queries word vectors for given list of input tokens.

        Args:
            tokens: list of tokens to query

        Returns:
            word vectors array
        """

        return self.model.embeddings(tokens)

    def tokens(self, data):
        # Skip tokenization rules
        return data



================================================
FILE: src/python/txtai/vectors/sparse/__init__.py
================================================
"""
Sparse vectors imports
"""

from .base import SparseVectors
from .factory import SparseVectorsFactory
from .sbert import SparseSTVectors



================================================
FILE: src/python/txtai/vectors/sparse/base.py
================================================
"""
SparseVectors module
"""

# Conditional import
try:
    from scipy.sparse import csr_matrix, vstack
    from sklearn.preprocessing import normalize
    from sklearn.utils.extmath import safe_sparse_dot

    SPARSE = True
except ImportError:
    SPARSE = False

from ...util import SparseArray
from ..base import Vectors


# pylint: disable=W0223
class SparseVectors(Vectors):
    """
    Base class for sparse vector models. Vector models transform input content into sparse arrays.
    """

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not SPARSE:
            raise ImportError('SparseVectors is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

        # Get normalization setting
        self.isnormalize = self.config.get("normalize", self.defaultnormalize()) if self.config else None

    def encode(self, data, category=None):
        # Encode data to embeddings
        embeddings = super().encode(data, category)

        # Get sparse torch vector attributes
        embeddings = embeddings.cpu().coalesce()
        indices = embeddings.indices().numpy()
        values = embeddings.values().numpy()

        # Return as SciPy CSR Matrix
        return csr_matrix((values, indices), shape=embeddings.size())

    def vectors(self, documents, batchsize=500, checkpoint=None, buffer=None, dtype=None):
        # Run indexing
        ids, dimensions, batches, stream = self.index(documents, batchsize, checkpoint)

        # Rebuild sparse array
        embeddings = None
        with open(stream, "rb") as queue:
            for _ in range(batches):
                # Read in array batch
                data = self.loadembeddings(queue)
                embeddings = vstack((embeddings, data)) if embeddings is not None else data

        # Return sparse array
        return (ids, dimensions, embeddings)

    def dot(self, queries, data):
        return safe_sparse_dot(queries, data.T, dense_output=True).tolist()

    def loadembeddings(self, f):
        return SparseArray().load(f)

    def saveembeddings(self, f, embeddings):
        SparseArray().save(f, embeddings)

    def truncate(self, embeddings):
        raise ValueError("Truncate is not supported for sparse vectors")

    def normalize(self, embeddings):
        # Optionally normalize embeddings using method that supports sparse vectors
        return normalize(embeddings, copy=False) if self.isnormalize else embeddings

    def quantize(self, embeddings):
        raise ValueError("Quantize is not supported for sparse vectors")

    def defaultnormalize(self):
        """
        Returns the default normalization setting.

        Returns:
            default normalization setting
        """

        # Sparse vector embeddings typically perform better as unnormalized
        return False



================================================
FILE: src/python/txtai/vectors/sparse/factory.py
================================================
"""
Factory module
"""

from ...util import Resolver

from .sbert import SparseSTVectors


class SparseVectorsFactory:
    """
    Methods to create sparse vector models.
    """

    @staticmethod
    def create(config, models=None):
        """
        Create a Vectors model instance.

        Args:
            config: vector configuration
            models: models cache

        Returns:
            Vectors
        """

        # Get vector method
        method = config.get("method", "sentence-transformers")

        # Sentence Transformers vectors
        if method == "sentence-transformers":
            return SparseSTVectors(config, None, models) if config and config.get("path") else None

        # Resolve custom method
        return SparseVectorsFactory.resolve(method, config, models) if method else None

    @staticmethod
    def resolve(backend, config, models):
        """
        Attempt to resolve a custom backend.

        Args:
            backend: backend class
            config: vector configuration
            models: models cache

        Returns:
            Vectors
        """

        try:
            return Resolver()(backend)(config, None, models)
        except Exception as e:
            raise ImportError(f"Unable to resolve sparse vectors backend: '{backend}'") from e



================================================
FILE: src/python/txtai/vectors/sparse/sbert.py
================================================
"""
Sparse Sentence Transformers module
"""

# Conditional import
try:
    from sentence_transformers import SparseEncoder

    SENTENCE_TRANSFORMERS = True
except ImportError:
    SENTENCE_TRANSFORMERS = False

from ..dense.sbert import STVectors
from .base import SparseVectors


class SparseSTVectors(SparseVectors, STVectors):
    """
    Builds sparse vectors using sentence-transformers (aka SBERT).
    """

    def __init__(self, config, scoring, models):
        # Check before parent constructor since it calls loadmodel
        if not SENTENCE_TRANSFORMERS:
            raise ImportError('sentence-transformers is not available - install "vectors" extra to enable')

        super().__init__(config, scoring, models)

    def loadencoder(self, path, device, **kwargs):
        return SparseEncoder(path, device=device, **kwargs)

    def defaultnormalize(self):
        # Enable normalization by default if similarity function is cosine
        return self.model and self.model.similarity_fn_name == "cosine"



================================================
FILE: src/python/txtai/workflow/__init__.py
================================================
"""
Workflow imports
"""

from .base import Workflow
from .execute import Execute
from .factory import WorkflowFactory
from .task import *



================================================
FILE: src/python/txtai/workflow/base.py
================================================
"""
Workflow module
"""

import logging
import time
import traceback

from datetime import datetime

# Conditional import
try:
    from croniter import croniter

    CRONITER = True
except ImportError:
    CRONITER = False

from .execute import Execute

# Logging configuration
logger = logging.getLogger(__name__)


class Workflow:
    """
    Base class for all workflows.
    """

    def __init__(self, tasks, batch=100, workers=None, name=None, stream=None):
        """
        Creates a new workflow. Workflows are lists of tasks to execute.

        Args:
            tasks: list of workflow tasks
            batch: how many items to process at a time, defaults to 100
            workers: number of concurrent workers
            name: workflow name
            stream: workflow stream processor
        """

        self.tasks = tasks
        self.batch = batch
        self.workers = workers
        self.name = name
        self.stream = stream

        # Set default number of executor workers to max number of actions in a task
        self.workers = max(len(task.action) for task in self.tasks) if not self.workers else self.workers

    def __call__(self, elements):
        """
        Executes a workflow for input elements. This method returns a generator that yields transformed
        data elements.

        Args:
            elements: iterable data elements

        Returns:
            generator that yields transformed data elements
        """

        # Create execute instance for this run
        with Execute(self.workers) as executor:
            # Run task initializers
            self.initialize()

            # Process elements with stream processor, if available
            elements = self.stream(elements) if self.stream else elements

            # Process elements in batches
            for batch in self.chunk(elements):
                yield from self.process(batch, executor)

            # Run task finalizers
            self.finalize()

    def schedule(self, cron, elements, iterations=None):
        """
        Schedules a workflow using a cron expression and elements.

        Args:
            cron: cron expression
            elements: iterable data elements passed to workflow each call
            iterations: number of times to run workflow, defaults to run indefinitely
        """

        # Check that croniter is installed
        if not CRONITER:
            raise ImportError('Workflow scheduling is not available - install "workflow" extra to enable')

        logger.info("'%s' scheduler started with schedule %s", self.name, cron)

        maxiterations = iterations
        while iterations is None or iterations > 0:
            # Schedule using localtime
            schedule = croniter(cron, datetime.now().astimezone()).get_next(datetime)
            logger.info("'%s' next run scheduled for %s", self.name, schedule.isoformat())
            time.sleep(schedule.timestamp() - time.time())

            # Run workflow
            # pylint: disable=W0703
            try:
                for _ in self(elements):
                    pass
            except Exception:
                logger.error(traceback.format_exc())

            # Decrement iterations remaining, if necessary
            if iterations is not None:
                iterations -= 1

        logger.info("'%s' max iterations (%d) reached", self.name, maxiterations)

    def initialize(self):
        """
        Runs task initializer methods (if any) before processing data.
        """

        # Run task initializers
        for task in self.tasks:
            if task.initialize:
                task.initialize()

    def chunk(self, elements):
        """
        Splits elements into batches. This method efficiently processes both fixed size inputs and
        dynamically generated inputs.

        Args:
            elements: iterable data elements

        Returns:
            evenly sized batches with the last batch having the remaining elements
        """

        # Build batches by slicing elements, more efficient for fixed sized inputs
        if hasattr(elements, "__len__") and hasattr(elements, "__getitem__"):
            for x in range(0, len(elements), self.batch):
                yield elements[x : x + self.batch]

        # Build batches by iterating over elements when inputs are dynamically generated (i.e. generators)
        else:
            batch = []
            for x in elements:
                batch.append(x)

                if len(batch) == self.batch:
                    yield batch
                    batch = []

            # Final batch
            if batch:
                yield batch

    def process(self, elements, executor):
        """
        Processes a batch of data elements.

        Args:
            elements: iterable data elements
            executor: execute instance, enables concurrent task actions

        Returns:
            transformed data elements
        """

        # Run elements through each task
        for x, task in enumerate(self.tasks):
            logger.debug("Running Task #%d", x)
            elements = task(elements, executor)

        # Yield results processed by all tasks
        yield from elements

    def finalize(self):
        """
        Runs task finalizer methods (if any) after all data processed.
        """

        # Run task finalizers
        for task in self.tasks:
            if task.finalize:
                task.finalize()



================================================
FILE: src/python/txtai/workflow/execute.py
================================================
"""
Execute module
"""

from multiprocessing.pool import Pool, ThreadPool

import torch.multiprocessing


class Execute:
    """
    Supports sequential, multithreading and multiprocessing based execution of tasks.
    """

    def __init__(self, workers=None):
        """
        Creates a new execute instance. Functions can be executed sequentially, in a thread pool
        or in a process pool. Once created, the thread and/or process pool will stay open until the
        close method is called.

        Args:
            workers: number of workers for thread/process pools
        """

        # Number of workers to use in thread/process pools
        self.workers = workers

        self.thread = None
        self.process = None

    def __del__(self):
        self.close()

    def __enter__(self):
        return self

    def __exit__(self, etype, value, traceback):
        self.close()

    def run(self, method, function, args):
        """
        Runs multiple calls of function for each tuple in args. The method parameter controls if the calls are
        sequential (method = None), multithreaded (method = "thread") or with multiprocessing (method="process").

        Args:
            method: run method - "thread" for multithreading, "process" for multiprocessing, otherwise runs sequentially
            function: function to run
            args: list of tuples with arguments to each call
        """

        # Concurrent processing
        if method and len(args) > 1:
            pool = self.pool(method)
            if pool:
                return pool.starmap(function, args, 1)

        # Sequential processing
        return [function(*arg) for arg in args]

    def pool(self, method):
        """
        Gets a handle to a concurrent processing pool. This method will create the pool if it doesn't already exist.

        Args:
            method: pool type - "thread" or "process"

        Returns:
            concurrent processing pool or None if no pool of that type available
        """

        if method == "thread":
            if not self.thread:
                self.thread = ThreadPool(self.workers)

            return self.thread

        if method == "process":
            if not self.process:
                # Importing torch.multiprocessing will register torch shared memory serialization for cuda
                self.process = Pool(self.workers, context=torch.multiprocessing.get_context("spawn"))

            return self.process

        return None

    def close(self):
        """
        Closes concurrent processing pools.
        """

        if hasattr(self, "thread") and self.thread:
            self.thread.close()
            self.thread.join()
            self.thread = None

        if hasattr(self, "process") and self.process:
            self.process.close()
            self.process.join()
            self.process = None



================================================
FILE: src/python/txtai/workflow/factory.py
================================================
"""
Workflow factory module
"""

from .base import Workflow
from .task import TaskFactory


class WorkflowFactory:
    """
    Workflow factory. Creates new Workflow instances.
    """

    @staticmethod
    def create(config, name):
        """
        Creates a new Workflow instance.

        Args:
            config: Workflow configuration
            name: Workflow name

        Returns:
            Workflow
        """

        # Resolve workflow tasks
        tasks = []
        for tconfig in config["tasks"]:
            task = tconfig.pop("task") if "task" in tconfig else ""
            tasks.append(TaskFactory.create(tconfig, task))

        config["tasks"] = tasks

        if "stream" in config:
            sconfig = config["stream"]
            task = sconfig.pop("task") if "task" in sconfig else "stream"

            config["stream"] = TaskFactory.create(sconfig, task)

        # Create workflow
        return Workflow(**config, name=name)



================================================
FILE: src/python/txtai/workflow/task/__init__.py
================================================
"""
Task imports
"""

from .base import Task
from .console import ConsoleTask
from .export import ExportTask
from .factory import TaskFactory
from .file import FileTask
from .image import ImageTask
from .retrieve import RetrieveTask
from .service import ServiceTask
from .storage import StorageTask
from .stream import StreamTask
from .template import *
from .template import RagTask as ExtractorTask
from .url import UrlTask
from .workflow import WorkflowTask



================================================
FILE: src/python/txtai/workflow/task/base.py
================================================
"""
Task module
"""

import logging
import re
import types

import numpy as np
import torch

# Logging configuration
logger = logging.getLogger(__name__)


class Task:
    """
    Base class for all workflow tasks.
    """

    def __init__(
        self,
        action=None,
        select=None,
        unpack=True,
        column=None,
        merge="hstack",
        initialize=None,
        finalize=None,
        concurrency=None,
        onetomany=True,
        **kwargs,
    ):
        """
        Creates a new task. A task defines two methods, type of data it accepts and the action to execute
        for each data element. Action is a callable function or list of callable functions.

        Args:
            action: action(s) to execute on each data element
            select: filter(s) used to select data to process
            unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples
            column: column index to select if element is a tuple, defaults to all
            merge: merge mode for joining multi-action outputs, defaults to hstack
            initialize: action to execute before processing
            finalize: action to execute after processing
            concurrency: sets concurrency method when execute instance available
                         valid values: "thread" for thread-based concurrency, "process" for process-based concurrency
            onetomany: if one-to-many data transformations should be enabled, defaults to True
            kwargs: additional keyword arguments
        """

        # Standardize into list of actions
        if not action:
            action = []
        elif not isinstance(action, list):
            action = [action]

        self.action = action
        self.select = select
        self.unpack = unpack
        self.column = column
        self.merge = merge
        self.initialize = initialize
        self.finalize = finalize
        self.concurrency = concurrency
        self.onetomany = onetomany

        # Check for custom registration. Adds additional instance members and validates required dependencies available.
        if hasattr(self, "register"):
            self.register(**kwargs)
        elif kwargs:
            # Raise error if additional keyword arguments passed in without register method
            kwargs = ", ".join(f"'{kw}'" for kw in kwargs)
            raise TypeError(f"__init__() got unexpected keyword arguments: {kwargs}")

    def __call__(self, elements, executor=None):
        """
        Executes action for a list of data elements.

        Args:
            elements: iterable data elements
            executor: execute instance, enables concurrent task actions

        Returns:
            transformed data elements
        """

        if isinstance(elements, list):
            return self.filteredrun(elements, executor)

        return self.run(elements, executor)

    def filteredrun(self, elements, executor):
        """
        Executes a filtered run, which will tag all inputs with a process id, filter elements down to elements the
        task can handle and execute on that subset. Items not selected for processing will be returned unmodified.

        Args:
            elements: iterable data elements
            executor: execute instance, enables concurrent task actions

        Returns:
            transformed data elements
        """

        # Build list of elements with unique process ids
        indexed = list(enumerate(elements))

        # Filter data down to data this task handles
        data = [(x, self.upack(element)) for x, element in indexed if self.accept(self.upack(element, True))]

        # Get list of filtered process ids
        ids = [x for x, _ in data]

        # Prepare elements and execute task action(s)
        results = self.execute([self.prepare(element) for _, element in data], executor)

        # Pack results back into elements
        if self.merge:
            elements = self.filteredpack(results, indexed, ids)
        else:
            elements = [self.filteredpack(r, indexed, ids) for r in results]

        return elements

    def filteredpack(self, results, indexed, ids):
        """
        Processes and packs results back into original input elements.

        Args:
            results: task results
            indexed: original elements indexed by process id
            ids: process ids accepted by this task

        Returns:
            packed elements
        """

        # Update with transformed elements. Handle one to many transformations.
        elements = []
        for x, element in indexed:
            if x in ids:
                # Get result for process id
                result = results[ids.index(x)]

                if isinstance(result, OneToMany):
                    # One to many transformations
                    elements.extend([self.pack(element, r) for r in result])
                else:
                    # One to one transformations
                    elements.append(self.pack(element, result))
            else:
                # Pass unprocessed elements through
                elements.append(element)

        return elements

    def run(self, elements, executor):
        """
        Executes a task run for elements. A standard run processes all elements.

        Args:
            elements: iterable data elements
            executor: execute instance, enables concurrent task actions

        Returns:
            transformed data elements
        """

        # Execute task actions
        results = self.execute(elements, executor)

        # Handle one to many transformations
        if isinstance(results, list):
            elements = []
            for result in results:
                if isinstance(result, OneToMany):
                    # One to many transformations
                    elements.extend(result)
                else:
                    # One to one transformations
                    elements.append(result)

            return elements

        return results

    def accept(self, element):
        """
        Determines if this task can handle the input data format.

        Args:
            element: input data element

        Returns:
            True if this task can process this data element, False otherwise
        """

        return (isinstance(element, str) and re.search(self.select, element.lower())) if element is not None and self.select else True

    def upack(self, element, force=False):
        """
        Unpacks data for processing.

        Args:
            element: input data element
            force: if True, data is unpacked even if task has unpack set to False

        Returns:
            data
        """

        # Extract data from (id, data, tag) formatted elements
        if (self.unpack or force) and isinstance(element, tuple) and len(element) > 1:
            return element[1]

        return element

    def pack(self, element, data):
        """
        Packs data after processing.

        Args:
            element: transformed data element
            data: item to pack element into

        Returns:
            packed data
        """

        # Pack data into (id, data, tag) formatted elements
        if self.unpack and isinstance(element, tuple) and len(element) > 1:
            # If new data is a (id, data, tag) tuple use that except for multi-action "hstack" merges which produce tuples
            if isinstance(data, tuple) and (len(self.action) <= 1 or self.merge != "hstack"):
                return data

            # Create a copy of tuple, update data element and return
            element = list(element)
            element[1] = data
            return tuple(element)

        return data

    def prepare(self, element):
        """
        Method that allows downstream tasks to prepare data element for processing.

        Args:
            element: input data element

        Returns:
            data element ready for processing
        """

        return element

    def execute(self, elements, executor):
        """
        Executes action(s) on elements.

        Args:
            elements: list of data elements
            executor: execute instance, enables concurrent task actions

        Returns:
            transformed data elements
        """

        if self.action:
            # Run actions
            outputs = []
            for x, action in enumerate(self.action):
                # Filter elements by column index if necessary - supports a single int or an action index to column index mapping
                index = self.column[x] if isinstance(self.column, dict) else self.column
                inputs = [self.extract(e, index) for e in elements] if index is not None else elements

                # Queue arguments for executor, process immediately if no executor available
                outputs.append((action, inputs) if executor else self.process(action, inputs))

            # Run with executor if available
            if executor:
                outputs = executor.run(self.concurrency, self.process, outputs)

            # Run post process operations
            return self.postprocess(outputs)

        return elements

    def extract(self, element, index):
        """
        Extracts a column from element by index if the element is a tuple.

        Args:
            element: input element
            index: column index

        Returns:
            extracted column
        """

        if isinstance(element, tuple):
            if not self.unpack and len(element) == 3 and isinstance(element[1], tuple):
                return (element[0], element[1][index], element[2])

            return element[index]

        return element

    def process(self, action, inputs):
        """
        Executes action using inputs as arguments.

        Args:
            action: callable object
            inputs: action inputs

        Returns:
            action outputs
        """

        # Log inputs
        logger.debug("Inputs: %s", inputs)

        # Execute action and get outputs
        outputs = action(inputs)

        # Consume generator output, if necessary
        if isinstance(outputs, types.GeneratorType):
            outputs = list(outputs)

        # Log outputs
        logger.debug("Outputs: %s", outputs)

        return outputs

    def postprocess(self, outputs):
        """
        Runs post process routines after a task action.

        Args:
            outputs: task outputs

        Returns:
            postprocessed outputs
        """

        # Unpack single action tasks
        if len(self.action) == 1:
            return self.single(outputs[0])

        # Return unmodified outputs when merge set to None
        if not self.merge:
            return outputs

        if self.merge == "vstack":
            return self.vstack(outputs)
        if self.merge == "concat":
            return self.concat(outputs)

        # Default mode is hstack
        return self.hstack(outputs)

    def single(self, outputs):
        """
        Post processes and returns single action outputs.

        Args:
            outputs: outputs from a single task

        Returns:
            post processed outputs
        """

        if self.onetomany and isinstance(outputs, list):
            # Wrap one to many transformations
            outputs = [OneToMany(output) if isinstance(output, list) else output for output in outputs]

        return outputs

    def vstack(self, outputs):
        """
        Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation.

        Row-wise merge example (2 actions)

          Inputs: [a, b, c]

          Outputs => [[a1, b1, c1], [a2, b2, c2]]

          Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2]

        Args:
            outputs: task outputs

        Returns:
            list of aggregated/zipped outputs as one to many transforms (row-wise)
        """

        # If all outputs are numpy arrays, use native method
        if all(isinstance(output, np.ndarray) for output in outputs):
            return np.concatenate(np.stack(outputs, axis=1))

        # If all outputs are torch tensors, use native method
        # pylint: disable=E1101
        if all(torch.is_tensor(output) for output in outputs):
            return torch.cat(tuple(torch.stack(outputs, axis=1)))

        # Flatten into lists of outputs per input row. Wrap as one to many transformation.
        merge = []
        for x in zip(*outputs):
            combine = []
            for y in x:
                if isinstance(y, list):
                    combine.extend(y)
                else:
                    combine.append(y)

            merge.append(OneToMany(combine))

        return merge

    def hstack(self, outputs):
        """
        Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation.

        Column-wise merge example (2 actions)

          Inputs: [a, b, c]

          Outputs => [[a1, b1, c1], [a2, b2, c2]]

          Column Merge => [(a1, a2), (b1, b2), (c1, c2)]

        Args:
            outputs: task outputs

        Returns:
            list of aggregated/zipped outputs as tuples (column-wise)
        """

        # If all outputs are numpy arrays, use native method
        if all(isinstance(output, np.ndarray) for output in outputs):
            return np.stack(outputs, axis=1)

        # If all outputs are torch tensors, use native method
        # pylint: disable=E1101
        if all(torch.is_tensor(output) for output in outputs):
            return torch.stack(outputs, axis=1)

        return list(zip(*outputs))

    def concat(self, outputs):
        """
        Merges outputs column-wise and concats values together into a string. Returns a list of strings.

        Concat merge example (2 actions)

          Inputs: [a, b, c]

          Outputs => [[a1, b1, c1], [a2, b2, c2]]

          Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => ["a1. a2", "b1. b2", "c1. c2"]

        Args:
            outputs: task outputs

        Returns:
            list of concat outputs
        """

        return [". ".join([str(y) for y in x if y]) for x in self.hstack(outputs)]


class OneToMany:
    """
    Encapsulates list output for a one to many transformation.
    """

    def __init__(self, values):
        """
        Creates a new OneToMany transformation.

        Args:
            values: list of outputs
        """

        self.values = values

    def __iter__(self):
        return self.values.__iter__()



================================================
FILE: src/python/txtai/workflow/task/console.py
================================================
"""
ConsoleTask module
"""

import json

from .base import Task


class ConsoleTask(Task):
    """
    Task that prints task elements to the console.
    """

    def __call__(self, elements, executor=None):
        # Run task
        outputs = super().__call__(elements, executor)

        # Print inputs and outputs to console
        print("Inputs:", json.dumps(elements, indent=2))
        print("Outputs:", json.dumps(outputs, indent=2))

        # Return results
        return outputs



================================================
FILE: src/python/txtai/workflow/task/export.py
================================================
"""
ExportTask module
"""

import datetime
import os

# Conditional import
try:
    import pandas as pd

    PANDAS = True
except ImportError:
    PANDAS = False

from .base import Task


class ExportTask(Task):
    """
    Task that exports task elements using Pandas.
    """

    def register(self, output=None, timestamp=None):
        """
        Add export parameters to task. Checks if required dependencies are installed.

        Args:
            output: output file path
            timestamp: true if output file should be timestamped
        """

        if not PANDAS:
            raise ImportError('ExportTask is not available - install "workflow" extra to enable')

        # pylint: disable=W0201
        self.output = output
        self.timestamp = timestamp

    def __call__(self, elements, executor=None):
        # Run task
        outputs = super().__call__(elements, executor)

        # Get output file extension
        output = self.output
        parts = list(os.path.splitext(output))
        extension = parts[-1].lower()

        # Add timestamp to filename
        if self.timestamp:
            timestamp = datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
            parts[-1] = timestamp + parts[-1]

            # Create full path to output file
            output = ".".join(parts)

        # Write output
        if extension == ".xlsx":
            pd.DataFrame(outputs).to_excel(output, index=False)
        else:
            pd.DataFrame(outputs).to_csv(output, index=False)

        # Return results
        return outputs



================================================
FILE: src/python/txtai/workflow/task/factory.py
================================================
"""
Task factory module
"""

import functools

from ...util import Resolver


class TaskFactory:
    """
    Task factory. Creates new Task instances.
    """

    @staticmethod
    def get(task):
        """
        Gets a new instance of task class.

        Args:
            task: Task instance class

        Returns:
            Task class
        """

        # Local task if no package
        if "." not in task:
            # Get parent package
            task = ".".join(__name__.split(".")[:-1]) + "." + task.capitalize() + "Task"

        # Attempt to load custom task
        return Resolver()(task)

    @staticmethod
    def create(config, task):
        """
        Creates a new Task instance.

        Args:
            config: Task configuration
            task: Task instance class

        Returns:
            Task
        """

        # Create lambda function if additional arguments present
        if "args" in config:
            args = config.pop("args")
            action = config["action"]
            if action:
                if isinstance(action, list):
                    config["action"] = [Partial.create(a, args[i]) for i, a in enumerate(action)]
                else:
                    # Accept keyword or positional arguments
                    config["action"] = lambda x: action(x, **args) if isinstance(args, dict) else action(x, *args)

        # Get Task instance
        return TaskFactory.get(task)(**config)


class Partial(functools.partial):
    """
    Modifies functools.partial to prepend arguments vs append.
    """

    @staticmethod
    def create(action, args):
        """
        Creates a new Partial function.

        Args:
            action: action to execute
            args: arguments

        Returns:
            Partial
        """

        return Partial(action, **args) if isinstance(args, dict) else Partial(action, *args) if args else Partial(action)

    def __call__(self, *args, **kwargs):
        # Update keyword arguments
        kw = self.keywords.copy()
        kw.update(kwargs)

        # Execute function with new arguments prepended to default arguments
        return self.func(*(args + self.args), **kw)



================================================
FILE: src/python/txtai/workflow/task/file.py
================================================
"""
FileTask module
"""

import os
import re

from .base import Task


class FileTask(Task):
    """
    Task that processes file paths
    """

    # File prefix
    FILE = r"file:\/\/"

    def accept(self, element):
        # Replace file prefixes
        element = re.sub(FileTask.FILE, "", element)

        # Only accept file paths that exist
        return super().accept(element) and isinstance(element, str) and os.path.exists(element)

    def prepare(self, element):
        # Replace file prefixes
        return re.sub(FileTask.FILE, "", element)



================================================
FILE: src/python/txtai/workflow/task/image.py
================================================
"""
ImageTask module
"""

import re

# Conditional import
try:
    from PIL import Image

    PIL = True
except ImportError:
    PIL = False

from .file import FileTask


class ImageTask(FileTask):
    """
    Task that processes image file urls
    """

    def register(self):
        """
        Checks if required dependencies are installed.
        """

        if not PIL:
            raise ImportError('ImageTask is not available - install "workflow" extra to enable')

    def accept(self, element):
        # Only accept image files
        return super().accept(element) and re.search(r"\.(gif|bmp|jpg|jpeg|png|webp)$", element.lower())

    def prepare(self, element):
        return Image.open(super().prepare(element))



================================================
FILE: src/python/txtai/workflow/task/retrieve.py
================================================
"""
RetrieveTask module
"""

import os
import tempfile

from urllib.request import urlretrieve
from urllib.parse import urlparse

from .url import UrlTask


class RetrieveTask(UrlTask):
    """
    Task that retrieves urls (local or remote) to a local directory.
    """

    def register(self, directory=None, flatten=True):
        """
        Adds retrieve parameters to task.

        Args:
            directory: local directory used to store retrieved files
            flatten: flatten input directory structure, defaults to True
        """

        # pylint: disable=W0201
        # Create default temporary directory if not specified
        if not directory:
            # Save tempdir to prevent content from being deleted until this task is out of scope
            # pylint: disable=R1732
            self.tempdir = tempfile.TemporaryDirectory()
            directory = self.tempdir.name

        # Create output directory if necessary
        os.makedirs(directory, exist_ok=True)

        self.directory = directory
        self.flatten = flatten

    def prepare(self, element):
        # Extract file path from URL
        path = urlparse(element).path

        if self.flatten:
            # Flatten directory structure (default)
            path = os.path.join(self.directory, os.path.basename(path))
        else:
            # Derive output path
            path = os.path.join(self.directory, os.path.normpath(path.lstrip("/")))
            directory = os.path.dirname(path)

            # Create local directory, if necessary
            os.makedirs(directory, exist_ok=True)

        # Retrieve URL
        urlretrieve(element, path)

        # Return new file path
        return path



================================================
FILE: src/python/txtai/workflow/task/service.py
================================================
"""
ServiceTask module
"""

# Conditional import
try:
    import requests
    import xmltodict

    XML_TO_DICT = True
except ImportError:
    XML_TO_DICT = False

from .base import Task


class ServiceTask(Task):
    """
    Task to runs requests against remote service urls.
    """

    def register(self, url=None, method=None, params=None, batch=True, extract=None):
        """
        Adds service parameters to task. Checks if required dependencies are installed.

        Args:
            url: url to connect to
            method: http method, GET or POST
            params: default query parameters
            batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element
            extract: list of sections to extract from response
        """

        if not XML_TO_DICT:
            raise ImportError('ServiceTask is not available - install "workflow" extra to enable')

        # pylint: disable=W0201
        # Save URL, method and parameter defaults
        self.url = url
        self.method = method
        self.params = params

        # If True, all elements are passed in a single batch request, otherwise a service call is executed per element
        self.batch = batch

        # Save sections to extract. Supports both a single string and a hierarchical list of sections.
        self.extract = extract
        if self.extract:
            self.extract = [self.extract] if isinstance(self.extract, str) else self.extract

    def execute(self, elements, executor=None):
        if self.batch:
            elements = self.request(elements)
        else:
            elements = [self.request(element) for element in elements]

        return super().execute(elements, executor)

    def request(self, data):
        """
        Execute service request.

        Args:
            url: service url
            method: method (get or post)
            params: dict of constant parameters to pass to request
            data: dynamic data for this specific request

        Returns:
            response as JSON
        """

        if not self.params:
            params = data
        else:
            # Create copy of parameters
            params = self.params.copy()

            # Add data to parameters
            for key in params:
                if not params[key]:
                    params[key] = data

        # Run request
        if self.method and self.method.lower() == "get":
            response = requests.get(self.url, params=params)
        else:
            response = requests.post(self.url, json=params)

        # Parse data based on content-type
        mimetype = response.headers["Content-Type"].split(";")[0]
        if mimetype.lower().endswith("xml"):
            data = xmltodict.parse(response.text)
        else:
            data = response.json()

        # Extract content from response, if necessary
        if self.extract:
            for tag in self.extract:
                data = data[tag]

        return data



================================================
FILE: src/python/txtai/workflow/task/storage.py
================================================
"""
StorageTask module
"""

import os
import re

# Conditional import
try:
    from libcloud.storage.providers import get_driver

    LIBCLOUD = True
except ImportError:
    LIBCLOUD = False

from .base import Task


class StorageTask(Task):
    """
    Task that processes object storage buckets. Supports local and cloud providers in Apache libcloud.
    """

    # URL prefix
    PREFIX = r"(\w+):\/\/.*"
    PATH = r"\w+:\/\/(.*)"

    def register(self, key=None, secret=None, host=None, port=None, token=None, region=None):
        """
        Checks if required dependencies are installed. Reads in cloud storage parameters.

        Args:
            key: provider-specific access key
            secret: provider-specific access secret
            host: server host name
            port: server port
            token: temporary session token
            region: storage region
        """

        if not LIBCLOUD:
            raise ImportError('StorageTask is not available - install "workflow" extra to enable')

        # pylint: disable=W0201
        self.key = key
        self.secret = secret
        self.host = host
        self.port = port
        self.token = token
        self.region = region

    def __call__(self, elements, executor=None):
        # Create aggregated directory listing for all elements
        outputs = []
        for element in elements:
            if self.matches(element):
                # Get directory listing and run actions
                outputs.extend(super().__call__(self.list(element), executor))
            else:
                outputs.append(element)

        return outputs

    def matches(self, element):
        """
        Determines if this element is a storage element.

        Args:
            element: input storage element

        Returns:
            True if this is a storage element
        """

        # Only accept file URLs
        return re.match(StorageTask.PREFIX, self.upack(element, True).lower())

    def list(self, element):
        """
        Gets a list of urls for a object container.

        Args:
            element: object container

        Returns:
            list of urls
        """

        provider = re.sub(StorageTask.PREFIX, r"\1", element.lower())
        path = re.sub(StorageTask.PATH, r"\1", element)

        # Load key and secret, if applicable
        key = self.key if self.key is not None else os.environ.get("ACCESS_KEY")
        secret = self.secret if self.secret is not None else os.environ.get("ACCESS_SECRET")

        # Parse key and container
        key, container = (os.path.dirname(path), os.path.basename(path)) if key is None else (key, path)

        # Parse optional prefix from container
        parts = container.split("/", 1)
        container, prefix = (parts[0], parts[1]) if len(parts) > 1 else (container, None)

        # Get driver for provider
        driver = get_driver(provider)

        # Get client connection
        client = driver(key, secret, **{field: getattr(self, field) for field in ["host", "port", "region", "token"] if getattr(self, field)})

        container = client.get_container(container_name=container)
        return [client.get_object_cdn_url(obj) for obj in client.list_container_objects(container=container, prefix=prefix)]



================================================
FILE: src/python/txtai/workflow/task/stream.py
================================================
"""
StreamTask module
"""

from .base import Task


class StreamTask(Task):
    """
    Task that calls a task action and yields results.
    """

    def register(self, batch=False):
        """
        Adds stream parameters to task.

        Args:
            batch: all elements are passed to a single action call if True, otherwise an action call is executed per element, defaults to False
        """

        # pylint: disable=W0201
        # All elements are passed to a single action call if True, otherwise an action call is executed per element, defaults to False
        self.batch = batch

    def __call__(self, elements, executor=None):
        for action in self.action:
            if self.batch:
                # Single batch call
                yield from action(elements)
            else:
                # Call action for each element
                for x in elements:
                    yield from action(x)



================================================
FILE: src/python/txtai/workflow/task/template.py
================================================
"""
Template module
"""

from string import Formatter

from ...util import TemplateFormatter
from .file import Task


class TemplateTask(Task):
    """
    Task that generates text from a template and task inputs. Templates can be used to prepare data for a number of tasks
    including generating large language model (LLM) prompts.
    """

    def register(self, template=None, rules=None, strict=True):
        """
        Read template parameters.

        Args:
            template: prompt template
            rules: parameter rules
            strict: requires all task inputs to be consumed by template, defaults to True
        """

        # pylint: disable=W0201
        # Template text
        self.template = template if template else self.defaulttemplate()

        # Template processing rules
        self.rules = rules if rules else self.defaultrules()

        # Create formatter
        self.formatter = TemplateFormatter() if strict else Formatter()

    def prepare(self, element):
        # Check if element matches any processing rules
        match = self.match(element)
        if match:
            return match

        # Apply template processing, if applicable
        if self.template:
            # Pass dictionary as named prompt template parameters
            if isinstance(element, dict):
                return self.formatter.format(self.template, **element)

            # Pass tuple as prompt template parameters (arg0 - argN)
            if isinstance(element, tuple):
                return self.formatter.format(self.template, **{f"arg{i}": x for i, x in enumerate(element)})

            # Default behavior is to use input as {text} parameter in prompt template
            return self.formatter.format(self.template, text=element)

        # Return original inputs when no prompt provided
        return element

    def defaulttemplate(self):
        """
        Generates a default template for this task. Base method returns None.

        Returns:
            default template
        """

        return None

    def defaultrules(self):
        """
        Generates a default rules for this task. Base method returns an empty dictionary.

        Returns:
            default rules
        """

        return {}

    def match(self, element):
        """
        Check if element matches any processing rules.

        Args:
            element: input element

        Returns:
            matching value if found, None otherwise
        """

        if self.rules and isinstance(element, dict):
            # Check if any rules are matched
            for key, value in self.rules.items():
                if element[key] == value:
                    return element[key]

        return None


class RagTask(TemplateTask):
    """
    Template task that prepares input for a rag pipeline.
    """

    def prepare(self, element):
        # Apply prompt template using all variables except "query" and use output as question
        if isinstance(element, dict):
            # Make a copy without query and run through template
            params = dict(element)
            params.pop("query", None)
            params["text"] = params.pop("question")

            element["question"] = super().prepare(params)
            return element

        # Default mode is to use element text for both query and question
        return {"query": element, "question": super().prepare(element)}



================================================
FILE: src/python/txtai/workflow/task/url.py
================================================
"""
UrlTask module
"""

import re

from .base import Task


class UrlTask(Task):
    """
    Task that processes urls
    """

    # URL prefix
    PREFIX = r"\w+:\/\/"

    def accept(self, element):
        # Only accept elements that start with a url prefix
        return super().accept(element) and re.match(UrlTask.PREFIX, element.lower())



================================================
FILE: src/python/txtai/workflow/task/workflow.py
================================================
"""
WorkflowTask module
"""

from .base import Task


class WorkflowTask(Task):
    """
    Task that executes a separate Workflow
    """

    def process(self, action, inputs):
        return list(super().process(action, inputs))



================================================
FILE: test/python/testagent.py
================================================
"""
Agent module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from datetime import datetime

from smolagents import CodeAgent, PythonInterpreterTool

from txtai.agent import Agent
from txtai.embeddings import Embeddings


class TestAgent(unittest.TestCase):
    """
    Agent tests.
    """

    def testExecute(self):
        """
        Test executing main agent loop
        """

        agent = Agent(llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_steps=1)

        # Patch LLM to generate answer
        agent.process.model.llm = lambda *args, **kwargs: 'Action:\n{"name": "final_answer", "arguments": "Hi"}'

        self.assertEqual(agent("Hello"), "Hi")

    def testMethod(self):
        """
        Test agent process methods
        """

        agent = Agent(method="code", llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_iterations=1)
        self.assertIsInstance(agent.process, CodeAgent)

    def testToolsBasic(self):
        """
        Test adding basic function tools
        """

        class DateTime:
            """
            Date time instance
            """

            def __call__(self, iso):
                """
                Gets the current date and time

                Args:
                    iso: date will be converted to iso format if True

                Returns:
                    current date and time
                """

                return datetime.today().isoformat() if iso else datetime.today()

        today = {"name": "today", "description": "Gets the current date and time", "target": DateTime()}

        def current(iso: str) -> str:
            """
            Gets the current date and time

            Args:
                iso: date will be converted to iso format if True

            Returns:
                current date and time
            """

            return datetime.today().isoformat() if iso else datetime.today()

        agent = Agent(tools=[today, current, "websearch"], llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_steps=1)

        self.assertIsNotNone(agent)
        self.assertIsInstance(agent.tools["today"](True), str)
        self.assertIsInstance(agent.tools["current"](True), str)

    def testToolsEmbeddings(self):
        """
        Test adding Embeddings as a tool
        """

        embeddings = Embeddings()
        embeddings.index(["test"])

        # Generate temp file path and save
        index = os.path.join(tempfile.gettempdir(), "embeddings.agent")
        embeddings.save(index)

        embeddings1 = {
            "name": "embeddings1",
            "description": "Searches a test database",
            "target": embeddings,
        }

        embeddings2 = {"name": "embeddings2", "description": "Searches a test database", "path": index}

        agent = Agent(tools=[embeddings1, embeddings2], llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_steps=1)

        self.assertIsNotNone(agent)
        self.assertIsInstance(agent.tools["embeddings1"]("test"), list)

    # pylint: disable=C0115,C0116
    @patch("mcpadapt.core.MCPAdapt")
    def testToolsMCP(self, mcp):
        """
        Test adding a MCP tool collection
        """

        class MCPAdapt:
            def __init__(self, *args):
                self.args = args

            def tools(self):
                return [PythonInterpreterTool()]

        # Patch MCP adapter for testing
        mcp.side_effect = MCPAdapt

        agent = Agent(tools=["http://localhost:8000/mcp"], llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_steps=1)
        self.assertEqual(len(agent.tools), 2)



================================================
FILE: test/python/testapp.py
================================================
"""
Application module tests
"""

import unittest
import types

from txtai.app import Application
from txtai.pipeline import Pipeline


class TestApp(unittest.TestCase):
    """
    Application tests.
    """

    def testConfig(self):
        """
        Test a file not found config exception
        """

        with self.assertRaises(FileNotFoundError):
            Application.read("No file here")

    def testParameter(self):
        """
        Test resolving application parameter
        """

        app = Application(
            """
            testapp.TestPipeline:
                application:
        """
        )

        # Check that application instance is not None
        self.assertIsNotNone(app.pipelines["testapp.TestPipeline"].application)

    def testStream(self):
        """
        Test workflow streams
        """

        app = Application(
            """
            workflow:
                stream:
                    stream:
                        action: testapp.TestStream
                    tasks:
                        - nop
                batchstream:
                    stream:
                        action: testapp.TestStream
                        batch: True
                    tasks:
                        - nop
        """
        )

        def generator():
            yield 10

        # Test single stream
        self.assertEqual(list(app.workflow("stream", [10])), list(range(10)))

        # Test batch stream
        self.assertEqual(list(app.workflow("batchstream", generator())), list(range(10)))


class TestPipeline(Pipeline):
    """
    Test pipeline with an application parameter.
    """

    def __init__(self, application):
        self.application = application


class TestStream:
    """
    Test workflow stream
    """

    def __call__(self, arg):
        if isinstance(arg, types.GeneratorType):
            for x in arg:
                yield from range(int(x))
        else:
            yield from range(int(arg))



================================================
FILE: test/python/testarchive.py
================================================
"""
Compress module tests
"""

import os
import tarfile
import tempfile
import unittest

from zipfile import ZipFile, ZIP_DEFLATED

from txtai.archive import ArchiveFactory, Compress

# pylint: disable=C0411
from utils import Utils


class TestArchive(unittest.TestCase):
    """
    Archive tests.
    """

    def testDirectory(self):
        """
        Test directory included in compressed files
        """

        for extension in ["tar", "zip"]:
            # Create archive instance
            archive = ArchiveFactory.create()

            # Create subdirectory in archive working path
            path = os.path.join(archive.path(), "dir")
            os.makedirs(path, exist_ok=True)

            # Create file in archive working path
            with open(os.path.join(path, "test"), "w", encoding="utf-8") as f:
                f.write("test")

            # Save archive
            path = os.path.join(tempfile.gettempdir(), f"subdir.{extension}")
            archive.save(path)

            # Extract files from archive
            archive = ArchiveFactory.create()
            archive.load(path)

            # Check if file properly extracted
            path = os.path.join(archive.path(), "dir", "test")
            self.assertTrue(os.path.exists(path))

    def testInvalidTarLink(self):
        """
        Test invalid tar file with symlinks
        """

        symlink = os.path.join(tempfile.gettempdir(), "link")

        # Remove symlink if it already exists
        try:
            os.remove(symlink)
        except OSError:
            pass

        # Create symlink and add to TAR file
        os.symlink(os.path.join(tempfile.gettempdir(), "noexist"), symlink)

        path = os.path.join(tempfile.gettempdir(), "badtarlink")
        with tarfile.open(path, "w") as tar:
            tar.add(symlink, arcname="l")

        archive = ArchiveFactory.create()

        # Validate error is thrown for file
        with self.assertRaises(IOError):
            archive.load(path, "tar")

    def testInvalidTarPath(self):
        """
        Test invalid tar file with a path outside of base directory
        """

        path = os.path.join(tempfile.gettempdir(), "badtarpath")
        with tarfile.open(path, "w") as tar:
            tar.add(Utils.PATH, arcname="..")

        archive = ArchiveFactory.create()

        # Validate error is thrown for file
        with self.assertRaises(IOError):
            archive.load(path, "tar")

    def testInvalidZipPath(self):
        """
        Test invalid zip file with a path outside of base directory
        """

        path = os.path.join(tempfile.gettempdir(), "badzippath")
        with ZipFile(path, "w", ZIP_DEFLATED) as zfile:
            zfile.write(Utils.PATH + "/article.pdf", arcname="../article.pdf")

        archive = ArchiveFactory.create()

        # Validate error is thrown for file
        with self.assertRaises(IOError):
            archive.load(path, "zip")

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        compress = Compress()

        self.assertRaises(NotImplementedError, compress.pack, None, None)
        self.assertRaises(NotImplementedError, compress.unpack, None, None)



================================================
FILE: test/python/testcloud.py
================================================
"""
Cloud module tests
"""

import os
import tempfile
import time
import unittest

from unittest.mock import patch

from txtai.cloud import Cloud
from txtai.embeddings import Embeddings


class TestCloud(unittest.TestCase):
    """
    Cloud tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"format": "json", "path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def testCustom(self):
        """
        Test custom provider
        """

        # pylint: disable=E1120
        self.runHub("txtai.cloud.HuggingFaceHub")

    def testHub(self):
        """
        Test huggingface-hub integration
        """

        # pylint: disable=E1120
        self.runHub("huggingface-hub")

    def testInvalidProvider(self):
        """
        Test invalid provider identifier
        """

        # Test invalid external provider
        with self.assertRaises(ImportError):
            embeddings = Embeddings()
            embeddings.load(provider="ProviderNoExist", container="Invalid")

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        cloud = Cloud({})

        self.assertRaises(NotImplementedError, cloud.exists, None)
        self.assertRaises(NotImplementedError, cloud.load, None)
        self.assertRaises(NotImplementedError, cloud.save, None)

    def testObjectStorage(self):
        """
        Test object storage integration
        """

        # Run tests with uncompressed and compressed index
        for path in ["cloud.object", "cloud.object.tar.gz"]:
            self.runTests(path, {"provider": "local", "container": f"cloud.{time.time()}", "key": tempfile.gettempdir()})

    @patch("huggingface_hub.hf_hub_download")
    @patch("huggingface_hub.get_hf_file_metadata")
    @patch("huggingface_hub.upload_file")
    @patch("huggingface_hub.create_repo")
    def runHub(self, provider, create, upload, metadata, download):
        """
        Run huggingface-hub tests. This method mocks write operations since a token won't be available.
        """

        def filemeta(url, token):
            return (url, token) if "Invalid" not in url else None

        def filedownload(**kwargs):
            if "Invalid" in kwargs["repo_id"]:
                raise FileNotFoundError

            # Return either .gitattributes file or index
            return attributes if kwargs["filename"] == ".gitattributes" else index

        # Patch write methods since token will not be available
        create.return_value = None
        upload.return_value = None
        metadata.side_effect = filemeta
        download.side_effect = filedownload

        # Create dummy index
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), f"cloud.{provider}.tar.gz")
        self.embeddings.save(index)

        # Initialize attributes file
        # pylint: disable=R1732
        with tempfile.NamedTemporaryFile(mode="w", delete=False) as tmp:
            tmp.write("*.bin filter=lfs diff=lfs merge=lfs -text\n")
            attributes = tmp.name

        # Run tests with uncompressed and compressed index
        for path in [f"cloud.{provider}", f"cloud.{provider}.tar.gz"]:
            self.runTests(path, {"provider": provider, "container": "neuml/txtai-intro"})

    def runTests(self, path, cloud):
        """
        Runs a series of cloud sync tests.
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), path)

        # Test exists handles missing cloud storage object
        invalid = cloud.copy()
        invalid["container"] = "InvalidPathToTest"
        self.assertFalse(self.embeddings.exists(index, invalid))

        # Test exception raised when trying to load index and doesn't exist in cloud storage
        # pylint: disable=W0719
        with self.assertRaises(Exception):
            self.embeddings.load(index, invalid)

        # Save index
        self.embeddings.save(index, cloud)

        # Test object exists in cloud storage
        self.assertTrue(self.embeddings.exists(index, cloud))

        # Test object exists locally
        self.assertTrue(self.embeddings.exists(index))

        # Test index can be reloaded
        self.embeddings.load(index, cloud)

        # Search for best match
        result = self.embeddings.search("feel good story", 1)[0]
        self.assertEqual(result["text"], self.data[4])



================================================
FILE: test/python/testconsole.py
================================================
"""
Console module tests
"""

import contextlib
import io
import os
import tempfile
import unittest

from txtai.console import Console
from txtai.embeddings import Embeddings

APPLICATION = """
path: %s

workflow:
  test:
     tasks:
       - task: console
"""


class TestConsole(unittest.TestCase):
    """
    Console tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})

        # Create an index for the list of text
        cls.embeddings.index([(uid, text, None) for uid, text in enumerate(cls.data)])

        # Create app paths
        cls.apppath = os.path.join(tempfile.gettempdir(), "console.yml")
        cls.embedpath = os.path.join(tempfile.gettempdir(), "embeddings.console")

        # Create app.yml
        with open(cls.apppath, "w", encoding="utf-8") as out:
            out.write(APPLICATION % cls.embedpath)

        # Save index as uncompressed and compressed
        cls.embeddings.save(cls.embedpath)
        cls.embeddings.save(f"{cls.embedpath}.tar.gz")

        # Create console
        cls.console = Console(cls.embedpath)

    def testApplication(self):
        """
        Test application
        """

        self.assertNotIn("Traceback", self.command(f".load {self.apppath}"))
        self.assertIn("1", self.command(".limit 1"))
        self.assertIn("Maine man wins", self.command("feel good story"))

    def testConfig(self):
        """
        Test .config command
        """

        self.assertIn("tasks", self.command(".config"))

    def testEmbeddings(self):
        """
        Test embeddings index
        """

        self.assertNotIn("Traceback", self.command(f".load {self.embedpath}.tar.gz"))
        self.assertNotIn("Traceback", self.command(f".load {self.embedpath}"))
        self.assertIn("1", self.command(".limit 1"))
        self.assertIn("Maine man wins", self.command("feel good story"))

    def testEmbeddingsNoDatabase(self):
        """
        Test embeddings with no database/content
        """

        console = Console()

        # Create embeddings model, backed by sentence-transformers & transformers
        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

        # Create an index for the list of text
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Set embeddings on console
        console.app = embeddings
        self.assertIn("4", self.command("feel good story", console))

    def testEmpty(self):
        """
        Test empty console instance
        """

        console = Console()
        self.assertIn("AttributeError", self.command("search", console))

    def testHighlight(self):
        """
        Test .highlight command
        """

        self.assertIn("highlight", self.command(".highlight"))
        self.assertIn("wins", self.command("feel good story"))
        self.assertIn("Taiwan", self.command("asia"))

    def testPreloop(self):
        """
        Test preloop
        """

        self.assertIn("txtai console", self.preloop())

    def testWorkflow(self):
        """
        Test .workflow command
        """

        self.command(f".load {self.apppath}")
        self.assertIn("echo", self.command(".workflow test echo"))

    def command(self, command, console=None):
        """
        Runs a console command.

        Args:
            command: command to run
            console: console instance, defaults to self.console

        Returns:
            command output
        """

        # Run info
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            if not console:
                console = self.console

            console.onecmd(command)

        return output.getvalue()

    def preloop(self):
        """
        Runs console.preloop and redirects stdout.

        Returns:
            preloop output
        """

        # Run info
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            self.console.preloop()

        return output.getvalue()



================================================
FILE: test/python/testembeddings.py
================================================
"""
Embeddings module tests
"""

import json
import os
import tempfile
import unittest

from unittest.mock import patch

import numpy as np

from txtai.embeddings import Embeddings, Reducer
from txtai.serialize import SerializeFactory


# pylint: disable=R0904
class TestEmbeddings(unittest.TestCase):
    """
    Embeddings tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def testAutoId(self):
        """
        Test auto id generation
        """

        # Default sequence id
        embeddings = Embeddings()
        embeddings.index(self.data)

        uid = embeddings.search(self.data[4], 1)[0][0]
        self.assertEqual(uid, 4)

        # UUID
        embeddings = Embeddings(autoid="uuid4")
        embeddings.index(self.data)

        uid = embeddings.search(self.data[4], 1)[0][0]
        self.assertEqual(len(uid), 36)

    def testColumns(self):
        """
        Test custom text/object columns
        """

        embeddings = Embeddings({"keyword": True, "columns": {"text": "value"}})
        data = [{"value": x} for x in self.data]
        embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

        # Run search
        uid = embeddings.search("lottery", 1)[0][0]
        self.assertEqual(uid, 4)

    def testContext(self):
        """
        Test embeddings context manager
        """

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.context")

        with Embeddings() as embeddings:
            embeddings.index(self.data)
            embeddings.save(index)

        with Embeddings().load(index) as embeddings:
            uid = embeddings.search(self.data[4], 1)[0][0]
            self.assertEqual(uid, 4)

    def testDefaults(self):
        """
        Test default configuration
        """

        # Run index with no config which will fall back to default configuration
        embeddings = Embeddings()
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        self.assertEqual(embeddings.count(), 6)

    def testDelete(self):
        """
        Test delete
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Delete best match
        self.embeddings.delete([4])

        # Search for best match
        uid = self.embeddings.search("feel good story", 1)[0][0]

        self.assertEqual(self.embeddings.count(), 5)
        self.assertEqual(uid, 5)

    def testDense(self):
        """
        Test dense alias
        """

        # Dense flag is an alias for path
        embeddings = Embeddings(dense="sentence-transformers/nli-mpnet-base-v2")
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        self.assertEqual(embeddings.count(), 6)

    def testEmpty(self):
        """
        Test empty index
        """

        # Test search against empty index
        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})
        self.assertEqual(embeddings.search("test"), [])

        # Test index with no data
        embeddings.index([])
        self.assertIsNone(embeddings.ann)

        # Test upsert with no data
        embeddings.index([(0, "this is a test", None)])
        embeddings.upsert([])
        self.assertIsNotNone(embeddings.ann)

    def testEmptyString(self):
        """
        Test empty string indexing
        """

        # Test empty string
        self.embeddings.index([(0, "", None)])
        self.assertTrue(self.embeddings.search("test"))

        # Test empty string with dict
        self.embeddings.index([(0, {"text": ""}, None)])
        self.assertTrue(self.embeddings.search("test"))

    def testExternal(self):
        """
        Test embeddings backed by external vectors
        """

        def transform(data):
            embeddings = []
            for text in data:
                # Create dummy embedding using sum and mean of character ordinals
                ordinals = [ord(c) for c in text]
                embeddings.append(np.array([sum(ordinals), np.mean(ordinals)]))

            return embeddings

        # Index data using simple embeddings transform method
        embeddings = Embeddings({"method": "external", "transform": transform})
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Run search
        uid = embeddings.search(self.data[4], 1)[0][0]
        self.assertEqual(uid, 4)

    def testExternalPrecomputed(self):
        """
        Test embeddings backed by external pre-computed vectors
        """

        # Test with no transform function
        data = np.random.rand(5, 10).astype(np.float32)

        embeddings = Embeddings({"method": "external"})
        embeddings.index([(uid, row, None) for uid, row in enumerate(data)])

        # Run search
        uid = embeddings.search(data[4], 1)[0][0]
        self.assertEqual(uid, 4)

    def testHybrid(self):
        """
        Test hybrid search
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        # Index data with sparse + dense vectors
        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "hybrid": True})
        embeddings.index(data)

        # Run search
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.hybrid")

        # Test load/save
        embeddings.save(index)
        embeddings.load(index)

        # Run search
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Index data with sparse + dense vectors and unnormalized scores
        embeddings.config["scoring"]["normalize"] = False
        embeddings.index(data)

        # Run search
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Test upsert
        data[0] = (0, "Feel good story: baby panda born", None)
        embeddings.upsert([data[0]])

        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 0)

    def testIds(self):
        """
        Test legacy config ids loading
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.ids")

        # Save index
        self.embeddings.save(index)

        # Set ids on config to simulate legacy ids format
        with open(f"{index}/config.json", "r", encoding="utf-8") as handle:
            config = json.load(handle)
            config["ids"] = list(range(len(self.data)))

        with open(f"{index}/config.json", "w", encoding="utf-8") as handle:
            json.dump(config, handle, default=str, indent=2)

        # Reload index
        self.embeddings.load(index)

        # Run search
        uid = self.embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Check that ids is not in config
        self.assertTrue("ids" not in self.embeddings.config)

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testIdsPickle(self):
        """
        Test legacy pickle ids
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.idspickle")

        # Save index
        self.embeddings.save(index)

        # Create ids as pickle
        path = os.path.join(tempfile.gettempdir(), "embeddings.idspickle", "ids")
        serializer = SerializeFactory.create("pickle", allowpickle=True)
        serializer.save(self.embeddings.ids.ids, path)

        with self.assertWarns(RuntimeWarning):
            self.embeddings.load(index)

        # Run search
        uid = self.embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

    def testIndex(self):
        """
        Test index
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Search for best match
        uid = self.embeddings.search("feel good story", 1)[0][0]

        self.assertEqual(uid, 4)

    def testKeyword(self):
        """
        Test keyword only (sparse) search
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        # Index data with sparse keyword vectors
        embeddings = Embeddings({"keyword": True})
        embeddings.index(data)

        # Run search
        uid = embeddings.search("lottery ticket", 1)[0][0]
        self.assertEqual(uid, 4)

        # Test count method
        self.assertEqual(embeddings.count(), len(data))

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.keyword")

        # Test load/save
        embeddings.save(index)
        embeddings.load(index)

        # Run search
        uid = embeddings.search("lottery ticket", 1)[0][0]
        self.assertEqual(uid, 4)

        # Update data
        data[0] = (0, "Feel good story: baby panda born", None)
        embeddings.upsert([data[0]])

        # Search for best match
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 0)

    def testQuantize(self):
        """
        Test scalar quantization
        """

        for ann in ["faiss", "numpy", "torch"]:
            # Index data with 1-bit scalar quantization
            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "quantize": 1, "backend": ann})
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            uid = embeddings.search("feel good story", 1)[0][0]
            self.assertEqual(uid, 4)

    def testReducer(self):
        """
        Test reducer model
        """

        # Test model with single PCA component
        data = np.random.rand(5, 5).astype(np.float32)
        reducer = Reducer(data, 1)

        # Generate query and keep original data to ensure it changes
        query = np.random.rand(1, 5).astype(np.float32)
        original = query.copy()

        # Run test
        reducer(query)
        self.assertFalse(np.array_equal(query, original))

        # Test model with multiple PCA components
        reducer = Reducer(data, 3)

        # Generate query and keep original data to ensure it changes
        query = np.random.rand(5).astype(np.float32)
        original = query.copy()

        # Run test
        reducer(query)
        self.assertFalse(np.array_equal(query, original))

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testReducerLegacy(self):
        """
        Test reducer model with legacy model format
        """

        # Test model with single PCA component
        data = np.random.rand(5, 5).astype(np.float32)
        reducer = Reducer(data, 1)

        # Save legacy format
        path = os.path.join(tempfile.gettempdir(), "reducer")
        serializer = SerializeFactory.create("pickle", allowpickle=True)
        serializer.save(reducer.model, path)

        # Load legacy format
        reducer = Reducer()
        reducer.load(path)

        # Generate query and keep original data to ensure it changes
        query = np.random.rand(1, 5).astype(np.float32)
        original = query.copy()

        # Run test
        reducer(query)
        self.assertFalse(np.array_equal(query, original))

    def testSave(self):
        """
        Test save
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.base")

        self.embeddings.save(index)
        self.embeddings.load(index)

        # Search for best match
        uid = self.embeddings.search("feel good story", 1)[0][0]

        self.assertEqual(uid, 4)

        # Test offsets still work after save/load
        self.embeddings.upsert([(0, "Looking out into the dreadful abyss", None)])
        self.assertEqual(self.embeddings.count(), len(self.data))

    def testShortcuts(self):
        """
        Test embeddings creation shortcuts
        """

        tests = [
            ({"keyword": True}, ["scoring"]),
            ({"keyword": "sif"}, ["scoring"]),
            ({"sparse": True}, ["scoring"]),
            ({"dense": True}, ["ann"]),
            ({"hybrid": True}, ["ann", "scoring"]),
            ({"hybrid": "tfidf"}, ["ann", "scoring"]),
            ({"hybrid": "sparse"}, ["ann", "scoring"]),
            ({"graph": True}, ["graph"]),
        ]

        for config, checks in tests:
            embeddings = Embeddings(config)
            embeddings.index(["test"])

            for attr in checks:
                self.assertIsNotNone(getattr(embeddings, attr))

    def testSimilarity(self):
        """
        Test similarity
        """

        # Get best matching id
        uid = self.embeddings.similarity("feel good story", self.data)[0][0]

        self.assertEqual(uid, 4)

    def testSparse(self):
        """
        Test sparse vector search
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        # Index data with sparse vectors
        embeddings = Embeddings({"sparse": "sparse-encoder-testing/splade-bert-tiny-nq"})
        embeddings.index(data)

        # Run search
        uid = embeddings.search("lottery ticket", 1)[0][0]
        self.assertEqual(uid, 4)

        # Test count method
        self.assertEqual(embeddings.count(), len(data))

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.sparse")

        # Test load/save
        embeddings.save(index)
        embeddings.load(index)

        # Run search
        uid = embeddings.search("lottery ticket", 1)[0][0]
        self.assertEqual(uid, 4)

        # Test similarity
        uid = embeddings.similarity("lottery ticket", self.data)[0][0]
        self.assertEqual(uid, 4)

        # Update data
        data[0] = (0, "Feel good story: baby panda born", None)
        embeddings.upsert([data[0]])

        # Search for best match
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 0)

    def testSubindex(self):
        """
        Test subindex
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        # Disable top-level indexing and create subindex
        embeddings = Embeddings({"defaults": False, "indexes": {"index1": {"path": "sentence-transformers/nli-mpnet-base-v2"}}})
        embeddings.index(data)

        # Test transform
        self.assertEqual(embeddings.transform("feel good story").shape, (768,))
        self.assertEqual(embeddings.transform("feel good story", index="index1").shape, (768,))
        with self.assertRaises(KeyError):
            embeddings.transform("feel good story", index="index2")

        # Run search
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.subindex")

        # Test load/save
        embeddings.save(index)
        embeddings.load(index)

        # Run search
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

        # Update data
        data[0] = (0, "Feel good story: baby panda born", None)
        embeddings.upsert([data[0]])

        # Search for best match
        uid = embeddings.search("feel good story", 10)[0][0]
        self.assertEqual(uid, 0)

        # Check missing text is set to id when top-level indexing is disabled
        embeddings.upsert([(embeddings.count(), {"content": "empty text"}, None)])
        uid = embeddings.search(f"{embeddings.count() - 1}", 1)[0][0]
        self.assertEqual(uid, embeddings.count() - 1)

        # Close embeddings
        embeddings.close()

    def testTruncate(self):
        """
        Test dimensionality truncation
        """

        # Truncate vectors to a specified number of dimensions
        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "dimensionality": 750, "vectors": {"revision": "main"}})
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Search for best match
        uid = embeddings.search("feel good story", 1)[0][0]
        self.assertEqual(uid, 4)

    def testUpsert(self):
        """
        Test upsert
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        # Reset embeddings for test
        self.embeddings.ann = None
        self.embeddings.ids = None

        # Create an index for the list of text
        self.embeddings.upsert(data)

        # Update data
        data[0] = (0, "Feel good story: baby panda born", None)
        self.embeddings.upsert([data[0]])

        # Search for best match
        uid = self.embeddings.search("feel good story", 1)[0][0]

        self.assertEqual(uid, 0)

    @patch("os.cpu_count")
    def testWords(self, cpucount):
        """
        Test embeddings backed by word vectors
        """

        # Mock CPU count
        cpucount.return_value = 1

        # Create dataset
        data = [(x, row.split(), None) for x, row in enumerate(self.data)]

        # Create embeddings model, backed by word vectors
        embeddings = Embeddings({"path": "neuml/glove-6B-quantized", "scoring": "bm25", "pca": 3, "quantize": True})

        # Call scoring and index methods
        embeddings.score(data)
        embeddings.index(data)

        # Test search
        self.assertIsNotNone(embeddings.search("win", 1))

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "embeddings.wordvectors")

        # Test save/load
        embeddings.save(index)
        embeddings.load(index)

        # Test search
        self.assertIsNotNone(embeddings.search("win", 1))

    @patch("os.cpu_count")
    def testWordsUpsert(self, cpucount):
        """
        Test embeddings backed by word vectors with upserts
        """

        # Mock CPU count
        cpucount.return_value = 1

        # Create dataset
        data = [(x, row.split(), None) for x, row in enumerate(self.data)]

        # Create embeddings model, backed by word vectors
        embeddings = Embeddings({"path": "neuml/glove-6B/model.sqlite", "scoring": "bm25", "pca": 3})

        # Call scoring and index methods
        embeddings.score(data)
        embeddings.index(data)

        # Now upsert and override record
        data = [(0, "win win", None)]

        # Update scoring and run upsert
        embeddings.score(data)
        embeddings.upsert(data)

        # Test search after upsert
        uid = embeddings.search("win", 1)[0][0]
        self.assertEqual(uid, 0)



================================================
FILE: test/python/testgraph.py
================================================
"""
Graph module tests
"""

import os
import itertools
import tempfile
import unittest

from unittest.mock import patch

from txtai.archive import ArchiveFactory
from txtai.embeddings import Embeddings
from txtai.graph import Graph, GraphFactory
from txtai.serialize import SerializeFactory


# pylint: disable=R0904
class TestGraph(unittest.TestCase):
    """
    Graph tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.config = {
            "path": "sentence-transformers/nli-mpnet-base-v2",
            "content": True,
            "functions": [{"name": "graph", "function": "graph.attribute"}],
            "expressions": [
                {"name": "category", "expression": "graph(indexid, 'category')"},
                {"name": "topic", "expression": "graph(indexid, 'topic')"},
                {"name": "topicrank", "expression": "graph(indexid, 'topicrank')"},
            ],
            "graph": {"limit": 5, "minscore": 0.2, "batchsize": 4, "approximate": False, "topics": {"categories": ["News"], "stopwords": ["the"]}},
        }

        # Create embeddings instance
        cls.embeddings = Embeddings(cls.config)

    def testAnalysis(self):
        """
        Test analysis methods
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Graph centrality
        graph = self.embeddings.graph
        centrality = graph.centrality()
        self.assertEqual(list(centrality.keys())[0], 5)

        # Page Rank
        pagerank = graph.pagerank()
        self.assertEqual(list(pagerank.keys())[0], 5)

        # Path between nodes
        path = graph.showpath(4, 5)
        self.assertEqual(len(path), 2)

    def testCommunity(self):
        """
        Test community detection
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Get graph reference
        graph = self.embeddings.graph

        # Rebuild topics with updated graph settings
        graph.config = {"topics": {"algorithm": "greedy"}}
        graph.addtopics()
        self.assertEqual(sum((len(graph.topics[x]) for x in graph.topics)), 6)

        graph.config = {"topics": {"algorithm": "lpa"}}
        graph.addtopics()
        self.assertEqual(sum((len(graph.topics[x]) for x in graph.topics)), 4)

    def testCustomBackend(self):
        """
        Test resolving a custom backend
        """

        graph = GraphFactory.create({"backend": "txtai.graph.NetworkX"})
        graph.initialize()
        self.assertIsNotNone(graph)

    def testCustomBackendNotFound(self):
        """
        Test resolving an unresolvable backend
        """

        with self.assertRaises(ImportError):
            graph = GraphFactory.create({"backend": "notfound.graph"})
            graph.initialize()

    def testDatabase(self):
        """
        Test creating a Graph backed by a relational database
        """

        # Generate graph database
        path = os.path.join(tempfile.gettempdir(), "graph.sqlite")
        graph = GraphFactory.create({"backend": "rdbms", "url": f"sqlite:///{path}", "schema": "txtai"})

        # Initialize the graph
        graph.initialize()

        for x in range(5):
            graph.addnode(x, field=x)

        for x, y in itertools.combinations(range(5), 2):
            graph.addedge(x, y)

        # Test methods
        self.assertEqual(list(graph.scan()), [str(x) for x in range(5)])
        self.assertEqual(list(graph.scan(attribute="field")), [str(x) for x in range(5)])
        self.assertEqual(list(graph.filter([0]).scan()), [0])

        # Test save/load
        graph.save(None)
        graph.load(None)
        self.assertEqual(list(graph.scan()), [str(x) for x in range(5)])

        # Test remove node
        graph.delete([0])
        self.assertFalse(graph.hasnode(0))
        self.assertFalse(graph.hasedge(0))

        # Close graph
        graph.close()

    def testDefault(self):
        """
        Test embeddings default graph setting
        """

        embeddings = Embeddings(content=True, graph=True)
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        self.assertEqual(embeddings.graph.count(), len(self.data))

    def testDelete(self):
        """
        Test delete
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Delete row
        self.embeddings.delete([4])

        # Validate counts
        graph = self.embeddings.graph
        self.assertEqual(graph.count(), 5)
        self.assertEqual(graph.edgecount(), 1)
        self.assertEqual(sum((len(graph.topics[x]) for x in graph.topics)), 5)
        self.assertEqual(len(graph.categories), 6)

    def testEdges(self):
        """
        Test edges
        """

        # Create graph
        graph = GraphFactory.create({})
        graph.initialize()
        graph.addedge(0, 1)

        # Test edge exists
        self.assertTrue(graph.hasedge(0))
        self.assertTrue(graph.hasedge(0, 1))

    def testFilter(self):
        """
        Test creating filtered subgraphs
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Validate counts
        graph = self.embeddings.search("feel good story", graph=True)
        self.assertEqual(graph.count(), 3)
        self.assertEqual(graph.edgecount(), 2)

    def testFunction(self):
        """
        Test running graph functions with SQL
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Test function
        result = self.embeddings.search("select category, topic, topicrank from txtai where id = 0", 1)[0]

        # Check columns have a value
        self.assertIsNotNone(result["category"])
        self.assertIsNotNone(result["topic"])
        self.assertIsNotNone(result["topicrank"])

    def testFunctionReindex(self):
        """
        Test running graph functions with SQL after reindex
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Test functions reset with a reindex
        self.embeddings.reindex(self.embeddings.config)

        # Test function
        result = self.embeddings.search("select category, topic, topicrank from txtai where id = 0", 1)[0]

        # Check columns have a value
        self.assertIsNotNone(result["category"])
        self.assertIsNotNone(result["topic"])
        self.assertIsNotNone(result["topicrank"])

    def testIndex(self):
        """
        Test index
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Validate counts
        graph = self.embeddings.graph
        self.assertEqual(graph.count(), 6)
        self.assertEqual(graph.edgecount(), 2)
        self.assertEqual(len(graph.topics), 6)
        self.assertEqual(len(graph.categories), 6)

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testLegacy(self):
        """
        Test loading a legacy graph in TAR format
        """

        # Create graph
        graph = GraphFactory.create({})
        graph.initialize()
        graph.addedge(0, 1)

        categories = ["C1"]
        topics = {"T1": [0, 1]}

        serializer = SerializeFactory.create("pickle", allowpickle=True)

        # Save files to temporary directory and combine into TAR
        path = os.path.join(tempfile.gettempdir(), "graph.tar")
        with tempfile.TemporaryDirectory() as directory:
            # Save graph
            serializer.save(graph.backend, f"{directory}/graph")

            # Save categories, if necessary
            serializer.save(categories, f"{directory}/categories")

            # Save topics, if necessary
            serializer.save(topics, f"{directory}/topics")

            # Pack files
            archive = ArchiveFactory.create(directory)
            archive.save(path, "tar")

        # Load loading legacy format
        graph = GraphFactory.create({})
        graph.load(path)

        # Validate graph data is correct
        self.assertEqual(graph.count(), 2)
        self.assertEqual(graph.edgecount(), 1)
        self.assertEqual(graph.topics, topics)
        self.assertEqual(graph.categories, categories)

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        graph = Graph({})

        self.assertRaises(NotImplementedError, graph.create)
        self.assertRaises(NotImplementedError, graph.count)
        self.assertRaises(NotImplementedError, graph.scan, None)
        self.assertRaises(NotImplementedError, graph.node, None)
        self.assertRaises(NotImplementedError, graph.addnode, None)
        self.assertRaises(NotImplementedError, graph.addnodes, None)
        self.assertRaises(NotImplementedError, graph.removenode, None)
        self.assertRaises(NotImplementedError, graph.hasnode, None)
        self.assertRaises(NotImplementedError, graph.attribute, None, None)
        self.assertRaises(NotImplementedError, graph.addattribute, None, None, None)
        self.assertRaises(NotImplementedError, graph.removeattribute, None, None)
        self.assertRaises(NotImplementedError, graph.edgecount)
        self.assertRaises(NotImplementedError, graph.edges, None)
        self.assertRaises(NotImplementedError, graph.addedge, None, None)
        self.assertRaises(NotImplementedError, graph.addedges, None)
        self.assertRaises(NotImplementedError, graph.hasedge, None, None)
        self.assertRaises(NotImplementedError, graph.centrality)
        self.assertRaises(NotImplementedError, graph.pagerank)
        self.assertRaises(NotImplementedError, graph.showpath, None, None)
        self.assertRaises(NotImplementedError, graph.isquery, None)
        self.assertRaises(NotImplementedError, graph.parse, None)
        self.assertRaises(NotImplementedError, graph.search, None)
        self.assertRaises(NotImplementedError, graph.communities, None)
        self.assertRaises(NotImplementedError, graph.load, None)
        self.assertRaises(NotImplementedError, graph.save, None)
        self.assertRaises(NotImplementedError, graph.loaddict, None)
        self.assertRaises(NotImplementedError, graph.savedict)

    def testRelationships(self):
        """
        Test manually-provided relationships
        """

        # Create relationships for id 0
        relationships = [{"id": f"ID{x}"} for x in range(1, len(self.data))]

        # Test with content enabled
        self.embeddings.index({"id": f"ID{i}", "text": x, "relationships": relationships if i == 0 else None} for i, x in enumerate(self.data))
        self.assertEqual(len(self.embeddings.graph.edges(0)), len(self.data) - 1)

        # Test with content disabled
        config = self.config.copy()
        config["content"] = False

        embeddings = Embeddings(config)
        embeddings.index({"id": f"ID{i}", "text": x, "relationships": relationships if i == 0 else None} for i, x in enumerate(self.data))
        self.assertEqual(len(embeddings.graph.edges(0)), len(self.data) - 1)
        embeddings.close()

    def testRelationshipsInvalid(self):
        """
        Test manually-provided relationships with no matching id
        """

        # Create relationships for id 0
        relationships = [{"id": "INVALID"}]

        # Index with invalid relationship
        self.embeddings.index({"text": x, "relationships": relationships if i == 0 else None} for i, x in enumerate(self.data))

        # Validate only relationship is semantically-derived
        edges = list(self.embeddings.graph.edges(0))
        self.assertTrue(len(edges) == 1 and edges[0] != "INVALID")

    def testResetTopics(self):
        """
        Test resetting of topics
        """

        # Create an index for the list of text
        self.embeddings.index([(1, "text", None)])
        self.embeddings.upsert([(1, "graph", None)])
        self.assertEqual(list(self.embeddings.graph.topics.keys()), ["graph"])

    def testSave(self):
        """
        Test save
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "graph")

        # Save and reload index
        self.embeddings.save(index)
        self.embeddings.load(index)

        # Validate counts
        graph = self.embeddings.graph
        self.assertEqual(graph.count(), 6)
        self.assertEqual(graph.edgecount(), 2)
        self.assertEqual(sum((len(graph.topics[x]) for x in graph.topics)), 6)
        self.assertEqual(len(graph.categories), 6)

    def testSaveDict(self):
        """
        Test loading and saving to dictionaries
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Validate counts
        graph = self.embeddings.graph
        count, edgecount = graph.count(), graph.edgecount()

        # Save and reload graph as dict
        data = graph.savedict()
        graph.loaddict(data)

        # Validate counts
        self.assertEqual(graph.count(), count)
        self.assertEqual(graph.edgecount(), edgecount)

    def testSearch(self):
        """
        Test search
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Run standard search
        results = self.embeddings.search(
            """
            MATCH (A)-[]->(B)
            RETURN A, B
        """
        )
        self.assertEqual(len(results), 3)

        # Run path search
        results = self.embeddings.search(
            """
            MATCH P=()-[]->()
            RETURN P
        """
        )
        self.assertEqual(len(results), 3)

        # Run graph search
        g = self.embeddings.search(
            """
            MATCH (A)-[]->(B)
            RETURN A, ID(B)
        """,
            graph=True,
        )
        self.assertEqual(g.count(), 3)

        # Run path search
        results = self.embeddings.search(
            """
            MATCH P=()-[]->()
            RETURN P
        """,
            graph=True,
        )
        self.assertEqual(g.count(), 3)

        # Run similar search
        results = self.embeddings.search(
            """
            MATCH P=(A)-[]->()
            WHERE SIMILAR(A, "feel good story")
            RETURN A
            ORDER BY A.score DESC
            LIMIT 1
        """,
            graph=True,
        )
        self.assertEqual(list(results.scan())[0], 4)

    def testSearchBatch(self):
        """
        Test batch search
        """

        # Create an index for the list of text
        self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Run standard search
        results = self.embeddings.batchsearch(
            [
                """
            MATCH (A)-[]->(B)
            RETURN A, B
        """
            ]
        )
        self.assertEqual(len(results[0]), 3)

    def testSimple(self):
        """
        Test creating a simple graph
        """

        graph = GraphFactory.create({"topics": {}})

        # Initialize the graph
        graph.initialize()

        for x in range(5):
            graph.addnode(x)

        for x, y in itertools.combinations(range(5), 2):
            graph.addedge(x, y)

        # Validate counts
        self.assertEqual(graph.count(), 5)
        self.assertEqual(graph.edgecount(), 10)

        # Test missing edge
        self.assertIsNone(graph.edges(100))

        # Test topics with no text
        graph.addtopics()
        self.assertEqual(len(graph.topics), 5)

    def testSubindex(self):
        """
        Test subindex
        """

        # Build data array
        data = [(uid, text, None) for uid, text in enumerate(self.data)]

        embeddings = Embeddings(
            {
                "content": True,
                "functions": [{"name": "graph", "function": "indexes.index1.graph.attribute"}],
                "expressions": [
                    {"name": "category", "expression": "graph(indexid, 'category')"},
                    {"name": "topic", "expression": "graph(indexid, 'topic')"},
                    {"name": "topicrank", "expression": "graph(indexid, 'topicrank')"},
                ],
                "indexes": {
                    "index1": {
                        "path": "sentence-transformers/nli-mpnet-base-v2",
                        "graph": {
                            "limit": 5,
                            "minscore": 0.2,
                            "batchsize": 4,
                            "approximate": False,
                            "topics": {"categories": ["News"], "stopwords": ["the"]},
                        },
                    }
                },
            }
        )

        # Create an index for the list of text
        embeddings.index(data)

        # Test function
        result = embeddings.search("select id, category, topic, topicrank from txtai where id = 0", 1)[0]

        # Check columns have a value
        self.assertIsNotNone(result["category"])
        self.assertIsNotNone(result["topic"])
        self.assertIsNotNone(result["topicrank"])

        # Update data
        data[0] = (0, "Feel good story: lottery winner announced", None)
        embeddings.upsert([data[0]])

        # Test function
        result = embeddings.search("select id, category, topic, topicrank from txtai where id = 0", 1)[0]

        # Check columns have a value
        self.assertIsNotNone(result["category"])
        self.assertIsNotNone(result["topic"])
        self.assertIsNotNone(result["topicrank"])

    def testUpsert(self):
        """
        Test upsert
        """

        # Update data
        self.embeddings.upsert([(0, {"text": "Canadian ice shelf collapses".split()}, None)])

        # Validate counts
        graph = self.embeddings.graph
        self.assertEqual(graph.count(), 6)
        self.assertEqual(graph.edgecount(), 2)
        self.assertEqual(sum((len(graph.topics[x]) for x in graph.topics)), 6)
        self.assertEqual(len(graph.categories), 6)



================================================
FILE: test/python/testoptional.py
================================================
"""
Optional module tests
"""

import sys
import unittest

# pylint: disable=C0415,W0611,W0621
import timm
import txtai


class TestOptional(unittest.TestCase):
    """
    Optional tests. Simulates optional dependencies not being installed.
    """

    @classmethod
    def setUpClass(cls):
        """
        Simulate optional packages not being installed
        """

        modules = [
            "annoy",
            "bitsandbytes",
            "bs4",
            "chonkie",
            "croniter",
            "docling.document_converter",
            "duckdb",
            "fastapi",
            "ggml",
            "gliner",
            "grandcypher",
            "grand",
            "hnswlib",
            "imagehash",
            "libcloud.storage.providers",
            "litellm",
            "llama_cpp",
            "model2vec",
            "networkx",
            "nltk",
            "onnxmltools",
            "onnxruntime",
            "onnxruntime.quantization",
            "pandas",
            "peft",
            "pgvector",
            "PIL",
            "rich",
            "scipy",
            "scipy.sparse",
            "sentence_transformers",
            "sklearn.decomposition",
            "smolagents",
            "sounddevice",
            "soundfile",
            "sqlalchemy",
            "sqlite_vec",
            "staticvectors",
            "tika",
            "ttstokenizer",
            "xmltodict",
        ]

        # Get handle to all currently loaded txtai modules
        modules = modules + [key for key in sys.modules if key.startswith("txtai")]
        cls.modules = {module: None for module in modules}

        # Replace loaded modules with stubs. Save modules for later reloading
        for module in cls.modules:
            if module in sys.modules:
                cls.modules[module] = sys.modules[module]

            # Remove txtai modules. Set optional dependencies to None to prevent reloading.
            if "txtai" in module:
                if module in sys.modules:
                    del sys.modules[module]
            else:
                sys.modules[module] = None

    @classmethod
    def tearDownClass(cls):
        """
        Resets modules environment back to initial state.
        """

        # Reset replaced modules in setup
        for key, value in cls.modules.items():
            if value:
                sys.modules[key] = value
            else:
                del sys.modules[key]

    def testAgent(self):
        """
        Test missing agent dependencies
        """

        from txtai.agent import Agent

        with self.assertRaises(ImportError):
            Agent(llm="hf-internal-testing/tiny-random-LlamaForCausalLM", max_steps=1)

    def testANN(self):
        """
        Test missing ANN dependencies
        """

        from txtai.ann import ANNFactory, SparseANNFactory

        # Test dense methods
        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "annoy"})

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "ggml"})

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "hnsw"})

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "pgvector"})

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "sqlite"})

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "torch", "torch": {"quantize": True}})

        # Test sparse methods
        with self.assertRaises(ImportError):
            SparseANNFactory.create({"backend": "ivfsparse"})

        with self.assertRaises(ImportError):
            SparseANNFactory.create({"backend": "pgsparse"})

    def testApi(self):
        """
        Test missing api dependencies
        """

        with self.assertRaises(ImportError):
            import txtai.api

    def testConsole(self):
        """
        Test missing console dependencies
        """

        from txtai.console import Console

        with self.assertRaises(ImportError):
            Console()

    def testCloud(self):
        """
        Test missing cloud dependencies
        """

        from txtai.cloud import ObjectStorage

        with self.assertRaises(ImportError):
            ObjectStorage(None)

    def testDatabase(self):
        """
        Test missing database dependencies
        """

        from txtai.database import Client, DuckDB, ImageEncoder

        with self.assertRaises(ImportError):
            Client({})

        with self.assertRaises(ImportError):
            DuckDB({})

        with self.assertRaises(ImportError):
            ImageEncoder()

    def testGraph(self):
        """
        Test missing graph dependencies
        """

        from txtai.graph import GraphFactory, Query

        with self.assertRaises(ImportError):
            GraphFactory.create({"backend": "networkx"})

        with self.assertRaises(ImportError):
            GraphFactory.create({"backend": "rdbms"})

        with self.assertRaises(ImportError):
            Query()

    def testModel(self):
        """
        Test missing model dependencies
        """

        from txtai.embeddings import Reducer
        from txtai.models import OnnxModel

        with self.assertRaises(ImportError):
            Reducer()

        with self.assertRaises(ImportError):
            OnnxModel(None)

    def testPipeline(self):
        """
        Test missing pipeline dependencies
        """

        from txtai.pipeline import (
            AudioMixer,
            AudioStream,
            Caption,
            Entity,
            FileToHTML,
            HFOnnx,
            HFTrainer,
            HTMLToMarkdown,
            ImageHash,
            LiteLLM,
            LlamaCpp,
            Microphone,
            MLOnnx,
            Objects,
            Segmentation,
            Tabular,
            TextToAudio,
            TextToSpeech,
            Transcription,
            Translation,
        )

        with self.assertRaises(ImportError):
            AudioMixer()

        with self.assertRaises(ImportError):
            AudioStream()

        with self.assertRaises(ImportError):
            Caption()

        with self.assertRaises(ImportError):
            Entity("neuml/gliner-bert-tiny")

        with self.assertRaises(ImportError):
            FileToHTML(backend="docling")

        with self.assertRaises(ImportError):
            FileToHTML(backend="tika")

        with self.assertRaises(ImportError):
            HFOnnx()("google/bert_uncased_L-2_H-128_A-2", quantize=True)

        with self.assertRaises(ImportError):
            HFTrainer()(None, None, lora=True)

        with self.assertRaises(ImportError):
            HTMLToMarkdown()

        with self.assertRaises(ImportError):
            ImageHash()

        with self.assertRaises(ImportError):
            LiteLLM("huggingface/t5-small")

        with self.assertRaises(ImportError):
            LlamaCpp("TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/tinyllama-1.1b-chat-v0.3.Q2_K.gguf")

        with self.assertRaises(ImportError):
            Microphone()

        with self.assertRaises(ImportError):
            MLOnnx()

        with self.assertRaises(ImportError):
            Objects()

        with self.assertRaises(ImportError):
            Segmentation(sentences=True)

        with self.assertRaises(ImportError):
            Segmentation(chunker="token")

        with self.assertRaises(ImportError):
            Tabular()

        with self.assertRaises(ImportError):
            TextToAudio()

        with self.assertRaises(ImportError):
            TextToSpeech()

        with self.assertRaises(ImportError):
            Transcription()

        with self.assertRaises(ImportError):
            Translation().detect(["test"])

    def testScoring(self):
        """
        Test missing scoring dependencies
        """

        from txtai.scoring import ScoringFactory

        with self.assertRaises(ImportError):
            ScoringFactory.create({"method": "pgtext"})

    def testVectors(self):
        """
        Test missing vector dependencies
        """

        from txtai.vectors import SparseVectors, VectorsFactory, SparseVectorsFactory
        from txtai.util import SparseArray

        # Test dense vectors
        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "litellm", "path": "huggingface/sentence-transformers/all-MiniLM-L6-v2"}, None)

        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "llama.cpp", "path": "nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q2_K.gguf"}, None)

        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "model2vec", "path": "minishlab/M2V_base_output"}, None)

        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "sentence-transformers", "path": "sentence-transformers/nli-mpnet-base-v2"}, None)

        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "words"}, None)

        # Test default model
        model = VectorsFactory.create({"path": "sentence-transformers/all-MiniLM-L6-v2"}, None)
        self.assertIsNotNone(model)

        # Test sparse vectors
        with self.assertRaises(ImportError):
            SparseVectors(None, None, None)

        with self.assertRaises(ImportError):
            SparseVectorsFactory.create({"method": "sentence-transformers", "path": "sparse-encoder-testing/splade-bert-tiny-nq"}, None)

        with self.assertRaises(ImportError):
            SparseArray()

    def testWorkflow(self):
        """
        Test missing workflow dependencies
        """

        from txtai.workflow import ExportTask, ImageTask, ServiceTask, StorageTask, Workflow

        with self.assertRaises(ImportError):
            ExportTask()

        with self.assertRaises(ImportError):
            ImageTask()

        with self.assertRaises(ImportError):
            ServiceTask()

        with self.assertRaises(ImportError):
            StorageTask()

        with self.assertRaises(ImportError):
            Workflow([], workers=1).schedule(None, [])



================================================
FILE: test/python/testserialize.py
================================================
"""
Serialize module tests
"""

import os
import unittest

from unittest.mock import patch

from txtai.serialize import Serialize, SerializeFactory


class TestSerialize(unittest.TestCase):
    """
    Serialize tests.
    """

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        serialize = Serialize()

        self.assertRaises(NotImplementedError, serialize.loadstream, None)
        self.assertRaises(NotImplementedError, serialize.savestream, None, None)
        self.assertRaises(NotImplementedError, serialize.loadbytes, None)
        self.assertRaises(NotImplementedError, serialize.savebytes, None)

    def testMessagePack(self):
        """
        Test MessagePack encoder
        """

        serializer = SerializeFactory.create()
        self.assertEqual(serializer.loadbytes(serializer.savebytes("test")), "test")

    def testPickleDisabled(self):
        """
        Test disabled pickle serialization
        """

        # Validate an error is raised
        with self.assertRaises(ValueError):
            serializer = SerializeFactory.create("pickle", allowpickle=True)
            data = serializer.savebytes("Test")

            serializer = SerializeFactory.create("pickle")
            serializer.loadbytes(data)

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testPickleEnabled(self):
        """
        Test enabled pickle serialization
        """

        # Validate a warning is raised
        with self.assertWarns(RuntimeWarning):
            serializer = SerializeFactory.create("pickle")
            data = serializer.savebytes("Test")
            serializer.loadbytes(data)



================================================
FILE: test/python/testworkflow.py
================================================
"""
Workflow module tests
"""

import contextlib
import glob
import io
import os
import tempfile
import sys
import unittest

import numpy as np
import torch

from txtai.api import API
from txtai.embeddings import Documents, Embeddings
from txtai.pipeline import Nop, Segmentation, Summary, Translation, Textractor
from txtai.workflow import (
    Workflow,
    Task,
    ConsoleTask,
    ExportTask,
    FileTask,
    ImageTask,
    RagTask,
    RetrieveTask,
    StorageTask,
    TemplateTask,
    WorkflowTask,
)

# pylint: disable=C0411
from utils import Utils


# pylint: disable=R0904
class TestWorkflow(unittest.TestCase):
    """
    Workflow tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        # Default YAML workflow configuration
        cls.config = """
        # Embeddings index
        writable: true
        embeddings:
            scoring: bm25
            path: google/bert_uncased_L-2_H-128_A-2
            content: true

        # Text segmentation
        segmentation:
            sentences: true

        # Workflow definitions
        workflow:
            index:
                tasks:
                    - action: segmentation
                    - action: index
            search:
                tasks:
                    - search
            transform:
                tasks:
                    - transform
        """

    def testBaseWorkflow(self):
        """
        Test a basic workflow
        """

        translate = Translation()

        # Workflow that translate text to Spanish
        workflow = Workflow([Task(lambda x: translate(x, "es"))])

        results = list(workflow(["The sky is blue", "Forest through the trees"]))

        self.assertEqual(len(results), 2)

    def testChainWorkflow(self):
        """
        Test a chain of workflows
        """

        workflow1 = Workflow([Task(lambda x: [y * 2 for y in x])])
        workflow2 = Workflow([Task(lambda x: [y - 1 for y in x])], batch=4)

        results = list(workflow2(workflow1([1, 2, 4, 8, 16, 32])))
        self.assertEqual(results, [1, 3, 7, 15, 31, 63])

    def testComplexWorkflow(self):
        """
        Test a complex workflow
        """

        textractor = Textractor(paragraphs=True, minlength=150, join=True)
        summary = Summary("t5-small")

        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})
        documents = Documents()

        def index(x):
            documents.add(x)
            return x

        # Extract text and summarize articles
        articles = Workflow([FileTask(textractor), Task(lambda x: summary(x, maxlength=15))])

        # Complex workflow that extracts text, runs summarization then loads into an embeddings index
        tasks = [WorkflowTask(articles, r".\.pdf$"), Task(index, unpack=False)]

        data = ["file://" + Utils.PATH + "/article.pdf", "Workflows can process audio files, documents and snippets"]

        # Convert file paths to data tuples
        data = [(x, element, None) for x, element in enumerate(data)]

        # Execute workflow, discard results as they are streamed
        workflow = Workflow(tasks)
        data = list(workflow(data))

        # Build the embeddings index
        embeddings.index(documents)

        # Cleanup temporary storage
        documents.close()

        # Run search and validate result
        index, _ = embeddings.search("search text", 1)[0]
        self.assertEqual(index, 0)
        self.assertEqual(data[0][1], "txtai builds an AI-powered index over sections")

    def testConcurrentWorkflow(self):
        """
        Test running concurrent task actions
        """

        nop = Nop()

        workflow = Workflow([Task([nop, nop], concurrency="thread")])
        results = list(workflow([2, 4]))
        self.assertEqual(results, [(2, 2), (4, 4)])

        workflow = Workflow([Task([nop, nop], concurrency="process")])
        results = list(workflow([2, 4]))
        self.assertEqual(results, [(2, 2), (4, 4)])

        workflow = Workflow([Task([nop, nop], concurrency="unknown")])
        results = list(workflow([2, 4]))
        self.assertEqual(results, [(2, 2), (4, 4)])

    def testConsoleWorkflow(self):
        """
        Test a console task
        """

        # Excel export
        workflow = Workflow([ConsoleTask()])

        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            list(workflow([{"id": 1, "text": "Sentence 1"}, {"id": 2, "text": "Sentence 2"}]))

        self.assertIn("Sentence 2", output.getvalue())

    def testExportWorkflow(self):
        """
        Test an export task
        """

        # Excel export
        path = os.path.join(tempfile.gettempdir(), "export.xlsx")
        workflow = Workflow([ExportTask(output=path)])
        list(workflow([{"id": 1, "text": "Sentence 1"}, {"id": 2, "text": "Sentence 2"}]))
        self.assertGreater(os.path.getsize(path), 0)

        # Export CSV
        path = os.path.join(tempfile.gettempdir(), "export.csv")
        workflow = Workflow([ExportTask(output=path)])
        list(workflow([{"id": 1, "text": "Sentence 1"}, {"id": 2, "text": "Sentence 2"}]))
        self.assertGreater(os.path.getsize(path), 0)

        # Export CSV with timestamp
        path = os.path.join(tempfile.gettempdir(), "export-timestamp.csv")
        workflow = Workflow([ExportTask(output=path, timestamp=True)])
        list(workflow([{"id": 1, "text": "Sentence 1"}, {"id": 2, "text": "Sentence 2"}]))

        # Find timestamped file and ensure it has data
        path = glob.glob(os.path.join(tempfile.gettempdir(), "export-timestamp*.csv"))[0]
        self.assertGreater(os.path.getsize(path), 0)

    def testExtractWorkflow(self):
        """
        Test column extraction tasks
        """

        workflow = Workflow([Task(lambda x: x, unpack=False, column=0)], batch=1)

        results = list(workflow([(0, 1)]))
        self.assertEqual(results[0], 0)

        results = list(workflow([(0, (1, 2), None)]))
        self.assertEqual(results[0], (0, 1, None))

        results = list(workflow([1]))
        self.assertEqual(results[0], 1)

    def testImageWorkflow(self):
        """
        Test an image task
        """

        workflow = Workflow([ImageTask()])

        results = list(workflow([Utils.PATH + "/books.jpg"]))

        self.assertEqual(results[0].size, (1024, 682))

    def testInvalidWorkflow(self):
        """
        Test task with invalid parameters
        """

        with self.assertRaises(TypeError):
            Task(invalid=True)

    def testMergeWorkflow(self):
        """
        Test merge tasks
        """

        task = Task([lambda x: [pow(y, 2) for y in x], lambda x: [pow(y, 3) for y in x]], merge="hstack")

        # Test hstack (column-wise) merge
        workflow = Workflow([task])
        results = list(workflow([2, 4]))
        self.assertEqual(results, [(4, 8), (16, 64)])

        # Test vstack (row-wise) merge
        task.merge = "vstack"
        results = list(workflow([2, 4]))
        self.assertEqual(results, [4, 8, 16, 64])

        # Test concat (values joined into single string) merge
        task.merge = "concat"
        results = list(workflow([2, 4]))
        self.assertEqual(results, ["4. 8", "16. 64"])

        # Test no merge
        task.merge = None
        results = list(workflow([2, 4, 6]))
        self.assertEqual(results, [[4, 16, 36], [8, 64, 216]])

        # Test generated (id, data, tag) tuples are properly returned
        workflow = Workflow([Task(lambda x: [(0, y, None) for y in x])])
        results = list(workflow([(1, "text", "tags")]))
        self.assertEqual(results[0], (0, "text", None))

    def testMergeUnbalancedWorkflow(self):
        """
        Test merge tasks with unbalanced outputs (i.e. one action produce more output than another for same input).
        """

        nop = Nop()
        segment1 = Segmentation(sentences=True)

        task = Task([nop, segment1])

        # Test hstack
        workflow = Workflow([task])
        results = list(workflow(["This is a test sentence. And another sentence to split."]))
        self.assertEqual(
            results, [("This is a test sentence. And another sentence to split.", ["This is a test sentence.", "And another sentence to split."])]
        )

        # Test vstack
        task.merge = "vstack"
        workflow = Workflow([task])
        results = list(workflow(["This is a test sentence. And another sentence to split."]))
        self.assertEqual(
            results, ["This is a test sentence. And another sentence to split.", "This is a test sentence.", "And another sentence to split."]
        )

    def testNumpyWorkflow(self):
        """
        Test a numpy workflow
        """

        task = Task([lambda x: np.power(x, 2), lambda x: np.power(x, 3)], merge="hstack")

        # Test hstack (column-wise) merge
        workflow = Workflow([task])
        results = list(workflow(np.array([2, 4])))
        self.assertTrue(np.array_equal(np.array(results), np.array([[4, 8], [16, 64]])))

        # Test vstack (row-wise) merge
        task.merge = "vstack"
        results = list(workflow(np.array([2, 4])))
        self.assertEqual(results, [4, 8, 16, 64])

        # Test no merge
        task.merge = None
        results = list(workflow(np.array([2, 4, 6])))
        self.assertTrue(np.array_equal(np.array(results), np.array([[4, 16, 36], [8, 64, 216]])))

    def testRetrieveWorkflow(self):
        """
        Test a retrieve task
        """

        # Test retrieve with generated temporary directory
        workflow = Workflow([RetrieveTask()])
        results = list(workflow(["file://" + Utils.PATH + "/books.jpg"]))
        self.assertTrue(results[0].endswith("books.jpg"))

        # Test retrieve with specified temporary directory
        workflow = Workflow([RetrieveTask(directory=os.path.join(tempfile.gettempdir(), "retrieve"))])
        results = list(workflow(["file://" + Utils.PATH + "/books.jpg"]))
        self.assertTrue(results[0].endswith("books.jpg"))

        # Test with directory structures
        workflow = Workflow([RetrieveTask(flatten=False)])
        results = list(workflow(["file://" + Utils.PATH + "/books.jpg"]))
        self.assertTrue(results[0].endswith("books.jpg") and "txtai" in results[0])

    def testScheduleWorkflow(self):
        """
        Test workflow schedules
        """

        # Test workflow schedule with Python
        workflow = Workflow([Task()])
        workflow.schedule("* * * * * *", ["test"], 1)
        self.assertEqual(len(workflow.tasks), 1)

        # Test workflow schedule with YAML
        workflow = """
        segmentation:
            sentences: true
        workflow:
            segment:
                schedule:
                    cron: '* * * * * *'
                    elements:
                        - a sentence to segment
                    iterations: 1
                tasks:
                    - action: segmentation
                      task: console
        """

        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            app = API(workflow)
            app.wait()

        self.assertIn("a sentence to segment", output.getvalue())

    def testScheduleErrorWorkflow(self):
        """
        Test workflow schedules with errors
        """

        def action(elements):
            raise FileNotFoundError

        # Test workflow proceeds after exception raised
        with self.assertLogs() as logs:
            workflow = Workflow([Task(action=action)])
            workflow.schedule("* * * * * *", ["test"], 1)

        self.assertIn("FileNotFoundError", " ".join(logs.output))

    def testStorageWorkflow(self):
        """
        Test a storage task
        """

        workflow = Workflow([StorageTask()])

        results = list(workflow(["local://" + Utils.PATH, "test string"]))

        self.assertEqual(len(results), 22)

    def testTemplateInput(self):
        """
        Test template task input
        """

        workflow = Workflow([TemplateTask(template="This is a {text}")])

        # Test with string inputs
        results = list(workflow(["prompt"]))
        self.assertEqual(results[0], "This is a prompt")

        # Test with dict inputs
        results = list(workflow([{"text": "prompt"}]))
        self.assertEqual(results[0], "This is a prompt")

        # Test with tuple inputs
        workflow = Workflow([TemplateTask(template="This is a {arg0}", unpack=False)])
        results = list(workflow([("prompt",)]))
        self.assertEqual(results[0], "This is a prompt")

        # Test invalid inputs
        with self.assertRaises(KeyError):
            workflow = Workflow([TemplateTask(template="No variables")])
            results = list(workflow([{"unused": "prompt"}]))

        # Test no template
        workflow = Workflow([TemplateTask()])
        results = list(workflow(["prompt"]))
        self.assertEqual(results[0], "prompt")

    def testTemplateRules(self):
        """
        Test template task rules
        """

        # Test rule applied
        workflow = Workflow([TemplateTask(template="This is a {text}", rules={"text": "Test skip"})])
        results = list(workflow([{"text": "Test skip"}]))
        self.assertEqual(results[0], "Test skip")

        # Test rule not applied
        results = list(workflow([{"text": "prompt"}]))
        self.assertEqual(results[0], "This is a prompt")

    def testTemplateRag(self):
        """
        Test rag template task
        """

        # Test outputs
        workflow = Workflow([RagTask(template="This is a {text}")])
        results = list(workflow(["prompt"]))
        self.assertEqual(results[0], {"query": "prompt", "question": "This is a prompt"})

        # Test partial outputs
        workflow = Workflow([RagTask(template="This is a {text}")])
        results = list(workflow([{"query": "query", "question": "prompt"}]))
        self.assertEqual(results[0], {"query": "query", "question": "This is a prompt"})

        # Test additional template parameters
        workflow = Workflow([RagTask(template="This is a {text} with another {param}")])
        results = list(workflow([{"query": "query", "question": "prompt", "param": "value"}]))
        self.assertEqual(results[0], {"query": "query", "question": "This is a prompt with another value", "param": "value"})

    def testTensorTransformWorkflow(self):
        """
        Test a tensor workflow with list transformations
        """

        # Test one-one list transformation
        task = Task(lambda x: x.tolist())
        workflow = Workflow([task])
        results = list(workflow(np.array([2])))
        self.assertEqual(results, [2])

        # Test one-many list transformation
        task = Task(lambda x: [x.tolist() * 2])
        workflow = Workflow([task])
        results = list(workflow(np.array([2])))
        self.assertEqual(results, [2, 2])

    def testTorchWorkflow(self):
        """
        Test a torch workflow
        """

        # pylint: disable=E1101,E1102
        task = Task([lambda x: torch.pow(x, 2), lambda x: torch.pow(x, 3)], merge="hstack")

        # Test hstack (column-wise) merge
        workflow = Workflow([task])
        results = np.array([x.numpy() for x in workflow(torch.tensor([2, 4]))])
        self.assertTrue(np.array_equal(results, np.array([[4, 8], [16, 64]])))

        # Test vstack (row-wise) merge
        task.merge = "vstack"
        results = list(workflow(torch.tensor([2, 4])))
        self.assertEqual(results, [4, 8, 16, 64])

        # Test no merge
        task.merge = None
        results = np.array([x.numpy() for x in workflow(torch.tensor([2, 4, 6]))])
        self.assertTrue(np.array_equal(np.array(results), np.array([[4, 16, 36], [8, 64, 216]])))

    def testYamlFunctionWorkflow(self):
        """
        Test YAML workflow with a function action
        """

        # Create function and add to module
        def action(elements):
            return [x * 2 for x in elements]

        sys.modules[__name__].action = action

        workflow = """
        workflow:
            run:
                tasks:
                    - testworkflow.action
        """

        app = API(workflow)
        self.assertEqual(list(app.workflow("run", [1, 2])), [2, 4])

    def testYamlIndexWorkflow(self):
        """
        Test reading a YAML index workflow in Python.
        """

        app = API(self.config)
        self.assertEqual(
            list(app.workflow("index", ["This is a test sentence. And another sentence to split."])),
            ["This is a test sentence.", "And another sentence to split."],
        )

        # Read from file
        path = os.path.join(tempfile.gettempdir(), "workflow.yml")
        with open(path, "w", encoding="utf-8") as f:
            f.write(self.config)

        app = API(path)
        self.assertEqual(
            list(app.workflow("index", ["This is a test sentence. And another sentence to split."])),
            ["This is a test sentence.", "And another sentence to split."],
        )

        # Read from YAML object
        app = API(API.read(self.config))
        self.assertEqual(
            list(app.workflow("index", ["This is a test sentence. And another sentence to split."])),
            ["This is a test sentence.", "And another sentence to split."],
        )

    def testYamlSearchWorkflow(self):
        """
        Test reading a YAML search workflow in Python.
        """

        # Test search
        app = API(self.config)
        list(app.workflow("index", ["This is a test sentence. And another sentence to split."]))
        self.assertEqual(
            list(app.workflow("search", ["another"]))[0]["text"],
            "And another sentence to split.",
        )

    def testYamlWorkflowTask(self):
        """
        Test YAML workflow with a workflow task
        """

        # Create function and add to module
        def action(elements):
            return [x * 2 for x in elements]

        sys.modules[__name__].action = action

        workflow = """
        workflow:
            run:
                tasks:
                    - testworkflow.action
            flow:
                tasks:
                    - run
        """

        app = API(workflow)
        self.assertEqual(list(app.workflow("flow", [1, 2])), [2, 4])

    def testYamlTransformWorkflow(self):
        """
        Test reading a YAML transform workflow in Python.
        """

        # Test search
        app = API(self.config)
        self.assertEqual(len(list(app.workflow("transform", ["text"]))[0]), 128)

    def testYamlError(self):
        """
        Test reading a YAML workflow with errors.
        """

        # Read from string
        config = """
        # Workflow definitions
        workflow:
            error:
                tasks:
                    - action: error
        """

        with self.assertRaises(KeyError):
            API(config)



================================================
FILE: test/python/utils.py
================================================
"""
Utils module
"""


class Utils:
    """
    Utility constants and methods
    """

    PATH = "/tmp/txtai"



================================================
FILE: test/python/testann/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testann/testdense.py
================================================
"""
Dense ANN module tests
"""

import os
import platform
import sys
import tempfile
import unittest

from unittest.mock import patch

import numpy as np

from txtai.ann import ANNFactory, ANN
from txtai.serialize import SerializeFactory


# pylint: disable=R0904
class TestDense(unittest.TestCase):
    """
    Dense ANN tests.
    """

    def testAnnoy(self):
        """
        Test Annoy backend
        """

        self.runTests("annoy", None, False)

    def testAnnoyCustom(self):
        """
        Test Annoy backend with custom settings
        """

        # Test with custom settings
        self.runTests("annoy", {"annoy": {"ntrees": 2, "searchk": 1}}, False)

    def testCustomBackend(self):
        """
        Test resolving a custom backend
        """

        self.runTests("txtai.ann.Faiss")

    def testCustomBackendNotFound(self):
        """
        Test resolving an unresolvable backend
        """

        with self.assertRaises(ImportError):
            ANNFactory.create({"backend": "notfound.ann"})

    def testFaiss(self):
        """
        Test Faiss backend
        """

        self.runTests("faiss")

    def testFaissCustom(self):
        """
        Test Faiss backend with custom settings
        """

        # Test with custom settings
        self.runTests("faiss", {"faiss": {"nprobe": 2, "components": "PCA16,IDMap,SQ8", "sample": 1.0}}, False)
        self.runTests("faiss", {"faiss": {"components": "IVF,SQ8"}}, False)

    @patch("platform.system")
    def testFaissMacOS(self, system):
        """
        Test Faiss backend with macOS
        """

        # Run test
        system.return_value = "Darwin"

        # pylint: disable=C0415, W0611
        # Force reload of class
        name = "txtai.ann.dense.faiss"
        module = sys.modules[name]
        del sys.modules[name]
        import txtai.ann.dense.faiss

        # Run tests
        self.runTests("faiss")

        # Restore original module
        sys.modules[name] = module

    @unittest.skipIf(os.name == "nt", "mmap not supported on Windows")
    def testFaissMmap(self):
        """
        Test Faiss backend with mmap enabled
        """

        # Test to with mmap enabled
        self.runTests("faiss", {"faiss": {"mmap": True}}, False)

    def testGGML(self):
        """
        Test GGML backend
        """

        self.runTests("ggml")

    def testGGMLQuantization(self):
        """
        Test GGML backend with quantization enabled
        """

        ann = ANNFactory.create({"backend": "ggml", "ggml": {"quantize": "Q4_0"}})

        # Generate and index dummy data
        data = np.random.rand(100, 256).astype(np.float32)
        ann.index(data)

        # Test save and load
        index = os.path.join(tempfile.gettempdir(), "ggml.q4_0.v1")
        ann.save(index)
        ann.load(index)

        # Generate query vector and test search
        query = np.random.rand(256).astype(np.float32)
        self.normalize(query)
        self.assertGreater(ann.search(np.array([query]), 1)[0][0][1], 0)

        # Validate count
        self.assertEqual(ann.count(), 100)

        # Test delete
        ann.delete([0])
        self.assertEqual(ann.count(), 99)

        # Save updated index with deletes and reload
        index = os.path.join(tempfile.gettempdir(), "ggml.q4_0.v2")
        ann.save(index)
        ann.load(index)
        ann.index(data)

    def testGGMLInvalid(self):
        """
        Test invalid GGML configurations
        """

        data = np.random.rand(100, 240).astype(np.float32)

        with self.assertRaises(ValueError):
            ann = ANNFactory.create({"backend": "ggml", "ggml": {"quantize": "NOEXIST", "gpu": False}})
            ann.index(data)

        with self.assertRaises(ValueError):
            ann = ANNFactory.create({"backend": "ggml", "ggml": {"quantize": "Q4_K"}})
            ann.index(data)

    def testHnsw(self):
        """
        Test Hnswlib backend
        """

        self.runTests("hnsw")

    def testHnswCustom(self):
        """
        Test Hnswlib backend with custom settings
        """

        # Test with custom settings
        self.runTests("hnsw", {"hnsw": {"efconstruction": 100, "m": 4, "randomseed": 0, "efsearch": 5}})

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        ann = ANN({})

        self.assertRaises(NotImplementedError, ann.load, None)
        self.assertRaises(NotImplementedError, ann.index, None)
        self.assertRaises(NotImplementedError, ann.append, None)
        self.assertRaises(NotImplementedError, ann.delete, None)
        self.assertRaises(NotImplementedError, ann.search, None, None)
        self.assertRaises(NotImplementedError, ann.count)
        self.assertRaises(NotImplementedError, ann.save, None)

    def testNumPy(self):
        """
        Test NumPy backend
        """

        self.runTests("numpy")

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testNumPyLegacy(self):
        """
        Test NumPy backend with legacy pickled data
        """

        serializer = SerializeFactory.create("pickle", allowpickle=True)

        # Create output directory
        output = os.path.join(tempfile.gettempdir(), "ann.npy")
        path = os.path.join(output, "embeddings")
        os.makedirs(output, exist_ok=True)

        # Generate data and save as pickle
        data = np.random.rand(100, 240).astype(np.float32)
        serializer.save(data, path)

        ann = ANNFactory.create({"backend": "numpy"})
        ann.load(path)

        # Validate count
        self.assertEqual(ann.count(), 100)

    def testNumPySafetensors(self):
        """
        Test NumPy backend with safetensors storage
        """

        ann = ANNFactory.create({"backend": "numpy", "numpy": {"safetensors": True}})

        # Generate and index dummy data
        data = np.random.rand(100, 240).astype(np.float32)
        ann.index(data)

        # Test save and load
        index = os.path.join(tempfile.gettempdir(), "numpy.safetensors")
        ann.save(index)
        ann.load(index)

        # Generate query vector and test search
        query = np.random.rand(240).astype(np.float32)
        self.normalize(query)
        self.assertGreater(ann.search(np.array([query]), 1)[0][0][1], 0)

        # Validate count
        self.assertEqual(ann.count(), 100)

    @patch("sqlalchemy.orm.Query.limit")
    def testPGVector(self, query):
        """
        Test PGVector backend
        """

        # Generate test record
        data = np.random.rand(1, 240).astype(np.float32)

        # Mock database query
        query.return_value = [(x, -1.0) for x in range(data.shape[0])]

        configs = [
            ("full", {"dimensions": 240}, {}, data),
            ("half", {"dimensions": 240}, {"precision": "half"}, data),
            ("binary", {"quantize": 1, "dimensions": 240 * 8}, {}, data.astype(np.uint8)),
        ]

        # Create ANN
        for name, config, pgvector, data in configs:
            path = os.path.join(tempfile.gettempdir(), f"pgvector.{name}.sqlite")
            ann = ANNFactory.create(
                {**{"backend": "pgvector", "pgvector": {**{"url": f"sqlite:///{path}", "schema": "txtai"}, **pgvector}}, **config}
            )

            # Test indexing
            ann.index(data)
            ann.append(data)

            # Validate search results
            self.assertEqual(ann.search(data, 1), [[(0, 1.0)]])

            # Validate save/load/delete
            ann.save(None)
            ann.load(None)

            # Validate count
            self.assertEqual(ann.count(), 2)

            # Test delete
            ann.delete([0])
            self.assertEqual(ann.count(), 1)

            # Close ANN
            ann.close()

    @unittest.skipIf(platform.system() == "Darwin", "SQLite extensions not supported on macOS")
    def testSQLite(self):
        """
        Test SQLite backend
        """

        self.runTests("sqlite")

    @unittest.skipIf(platform.system() == "Darwin", "SQLite extensions not supported on macOS")
    def testSQLiteCustom(self):
        """
        Test SQLite backend with custom settings
        """

        # Test with custom settings
        self.runTests("sqlite", {"sqlite": {"quantize": 1}})
        self.runTests("sqlite", {"sqlite": {"quantize": 8}})

        # Test saving to a new path
        model = self.backend("sqlite")
        expected = model.count() - 1

        # Test save variations
        index = os.path.join(tempfile.gettempdir(), "ann.sqlite")
        new = os.path.join(tempfile.gettempdir(), "ann.sqlite.new")

        # Save new
        model.save(index)

        # Save to same path
        model.save(index)

        # Delete id
        model.delete([0])

        # Save to another path
        model.load(index)
        model.save(new)

        self.assertEqual(model.count(), expected)

    def testTorch(self):
        """
        Test Torch backend
        """

        self.runTests("torch")

    @unittest.skipIf(platform.system() == "Darwin", "Torch quantization not supported on macOS")
    def testTorchQuantization(self):
        """
        Test Torch backend with quantization enabled
        """

        for qtype in ["fp4", "nf4", "int8"]:
            ann = ANNFactory.create({"backend": "torch", "torch": {"quantize": {"type": qtype}}})

            # Generate and index dummy data
            data = np.random.rand(100, 240).astype(np.float32)
            ann.index(data)

            # Test save and load
            index = os.path.join(tempfile.gettempdir(), f"{qtype}.safetensors")
            ann.save(index)
            ann.load(index)

            # Generate query vector and test search
            query = np.random.rand(240).astype(np.float32)
            self.normalize(query)
            self.assertGreater(ann.search(np.array([query]), 1)[0][0][1], 0)

            # Validate count
            self.assertEqual(ann.count(), 100)

            # Test delete
            ann.delete([0])
            self.assertEqual(ann.count(), 99)

    def runTests(self, name, params=None, update=True):
        """
        Runs a series of standard backend tests.

        Args:
            name: backend name
            params: additional config parameters
            update: If append/delete options should be tested
        """

        self.assertEqual(self.backend(name, params).config["backend"], name)
        self.assertEqual(self.save(name, params).count(), 10000)

        if update:
            self.assertEqual(self.append(name, params, 500).count(), 10500)
            self.assertEqual(self.delete(name, params, [0, 1]).count(), 9998)
            self.assertEqual(self.delete(name, params, [100000]).count(), 10000)

        self.assertGreater(self.search(name, params), 0)

    def backend(self, name, params=None, length=10000):
        """
        Test a backend.

        Args:
            name: backend name
            params: additional config parameters
            length: number of rows to generate

        Returns:
            ANN model
        """

        # Generate test data
        data = np.random.rand(length, 240).astype(np.float32)
        self.normalize(data)

        config = {"backend": name, "dimensions": data.shape[1]}
        if params:
            config.update(params)

        model = ANNFactory.create(config)
        model.index(data)

        return model

    def append(self, name, params=None, length=500):
        """
        Appends new data to index.

        Args:
            name: backend name
            params: additional config parameters
            length: number of rows to generate

        Returns:
            ANN model
        """

        # Initial model
        model = self.backend(name, params)

        # Generate test data
        data = np.random.rand(length, 240).astype(np.float32)
        self.normalize(data)

        model.append(data)

        return model

    def delete(self, name, params=None, ids=None):
        """
        Deletes data from index.

        Args:
            name: backend name
            params: additional config parameters
            ids: ids to delete

        Returns:
            ANN model
        """

        # Initial model
        model = self.backend(name, params)
        model.delete(ids)

        return model

    def save(self, name, params=None):
        """
        Test save/load.

        Args:
            name: backend name
            params: additional config parameters

        Returns:
            ANN model
        """

        model = self.backend(name, params)

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "ann")

        # Save and close index
        model.save(index)
        model.close()

        # Reload index
        model.load(index)

        return model

    def search(self, name, params=None):
        """
        Test ANN search.

        Args:
            name: backend name
            params: additional config parameters

        Returns:
            search results
        """

        # Generate ANN index
        model = self.backend(name, params)

        # Generate query vector
        query = np.random.rand(240).astype(np.float32)
        self.normalize(query)

        # Ensure top result has similarity > 0
        return model.search(np.array([query]), 1)[0][0][1]

    def normalize(self, embeddings):
        """
        Normalizes embeddings using L2 normalization. Operation applied directly on array.

        Args:
            embeddings: input embeddings matrix
        """

        # Calculation is different for matrices vs vectors
        if len(embeddings.shape) > 1:
            embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]
        else:
            embeddings /= np.linalg.norm(embeddings)



================================================
FILE: test/python/testann/testsparse.py
================================================
"""
Sparse ANN module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from scipy.sparse import random
from sklearn.preprocessing import normalize

from txtai.ann import SparseANNFactory


class TestSparse(unittest.TestCase):
    """
    Sparse ANN tests.
    """

    def testCustomBackend(self):
        """
        Test resolving a custom backend
        """

        self.assertIsNotNone(SparseANNFactory.create({"backend": "txtai.ann.IVFSparse"}))

    def testCustomBackendNotFound(self):
        """
        Test resolving an unresolvable backend
        """

        with self.assertRaises(ImportError):
            SparseANNFactory.create({"backend": "notfound.ann"})

    def testIVFSparse(self):
        """
        Test IVFSparse backend
        """

        # Generate test record
        insert = self.generate(500, 30522)
        append = self.generate(500, 30522)

        # Count of records
        count = insert.shape[0] + append.shape[0]

        # Create ANN
        path = os.path.join(tempfile.gettempdir(), "ivfsparse")
        ann = SparseANNFactory.create({"backend": "ivfsparse", "ivfsparse": {"nlist": 2, "nprobe": 2, "sample": 1.0}})

        # Test indexing
        ann.index(insert)
        ann.append(append)

        # Validate search results
        results = [x[0] for x in ann.search(insert[5], 10)[0]]
        self.assertIn(5, results)

        # Validate save/load/delete
        ann.save(path)
        ann.load(path)

        # Validate count
        self.assertEqual(ann.count(), count)

        # Test delete
        ann.delete([0])
        self.assertEqual(ann.count(), count - 1)

        # Re-validate search results
        results = [x[0] for x in ann.search(append[0], 10)[0]]
        self.assertIn(insert.shape[0], results)

        # Close ANN
        ann.close()

        # Test cluster pruning
        ann = SparseANNFactory.create({"backend": "ivfsparse", "ivfsparse": {"nlist": 15, "nprobe": 1, "sample": 1.0}})
        ann.index(insert)
        self.assertLess(len(ann.blocks), 15)
        ann.close()

    @patch("sqlalchemy.orm.Query.limit")
    def testPGSparse(self, query):
        """
        Test Sparse Postgres backend
        """

        # Generate test record
        data = self.generate(1, 30522)

        # Mock database query
        query.return_value = [(x, -1.0) for x in range(data.shape[0])]

        # Create ANN
        path = os.path.join(tempfile.gettempdir(), "pgsparse.sqlite")
        ann = SparseANNFactory.create({"backend": "pgsparse", "dimensions": 30522, "pgsparse": {"url": f"sqlite:///{path}", "schema": "txtai"}})

        # Test indexing
        ann.index(data)
        ann.append(data)

        # Validate search results
        self.assertEqual(ann.search(data, 1), [[(0, 1.0)]])

        # Validate save/load/delete
        ann.save(None)
        ann.load(None)

        # Validate count
        self.assertEqual(ann.count(), 2)

        # Test delete
        ann.delete([0])
        self.assertEqual(ann.count(), 1)

        # Test > 1000 dimensions
        data = random(1, 30522, format="csr", density=0.1)
        ann.index(data)
        self.assertEqual(ann.count(), 1)

        # Close ANN
        ann.close()

    def generate(self, m, n):
        """
        Generates random normalized sparse data.

        Args:
            m, n: shape of the matrix

        Returns:
            csr matrix
        """

        # Generate random csr matrix
        data = random(m, n, format="csr")

        # Normalize and return
        return normalize(data)



================================================
FILE: test/python/testapi/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testapi/testapiagent.py
================================================
"""
Agent API module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import API, application

# Configuration for agents
AGENTS = """
agent:
    test:
        max_iterations: 1
        tools:
            - name: testtool
              description: Test tool
              target: testapi.testapiagent.TestTool

llm:
    path: hf-internal-testing/tiny-random-LlamaForCausalLM
"""


# pylint: disable=R0904
class TestAgent(unittest.TestCase):
    """
    API tests for agents.
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(AGENTS)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        # Patch LLM to generate answer
        agent = application.get().agents["test"]
        agent.process.model.llm = lambda *args, **kwargs: 'Action:\n{"name": "final_answer", "arguments": "Hi"}'

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestAgent.start()

    def testAgent(self):
        """
        Test agent via API
        """

        results = self.client.post("agent", json={"name": "test", "text": "Hello"}).json()
        self.assertEqual(results, "Hi")

    def testEmpty(self):
        """
        Test empty API configuration
        """

        api = API({})

        self.assertIsNone(api.agent("junk", "test"))


class TestTool:
    """
    Class to test agent tools
    """

    def __call__(self):
        pass



================================================
FILE: test/python/testapi/testapiembeddings.py
================================================
"""
Embeddings API module tests
"""

import os
import tempfile
import unittest
import urllib.parse

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import API, application

# Configuration for a read/write embeddings index
INDEX = """
# Index file path
path: %s

# Allow indexing of documents
writable: True

# Questions settings
questions:
    path: distilbert-base-cased-distilled-squad

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2

# Extractor settings
extractor:
    path: questions
"""

# Configuration for a read-only embeddings index
READONLY = """
# Index file path
path: %s

# Allow indexing of documents
writable: False

# Embeddings settings
embeddings:
"""

# Configuration for an index with custom functions
FUNCTIONS = """
# Ignore existing index
pathignore: %s

# Allow indexing of documents
writable: True

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2
    content: True
    functions:
        - testapi.testapiembeddings.Elements
        - name: length
          argcount: 1
          function: testapi.testapiembeddings.length
        - name: ann
          function: ann
    transform: testapi.testapiembeddings.transform
"""

# Configuration for RAG
RAG = """
# Ignore existing index
pathignore: %s

# Allow indexing of documents
writable: True

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2
    content: True

# LLM
llm:
    path: hf-internal-testing/tiny-random-gpt2
    task: language-generation

# RAG settings
rag:
    path: llm
    output: flatten
"""

# Configuration for reranker
RERANK = """
# Index file path
path: %s

# Allow indexing of documents
writable: True

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2
    content: True

# Similarity and Reranking settings
similarity:
    path: neuml/colbert-bert-tiny
    lateencode: True

reranker:
"""


class TestEmbeddings(unittest.TestCase):
    """
    API tests for embeddings indices.
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start(yaml):
        """
        Starts a mock FastAPI client.

        Args:
            yaml: input configuration
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")
        index = os.path.join(tempfile.gettempdir(), "testapi")

        with open(config, "w", encoding="utf-8") as output:
            output.write(yaml % index)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestEmbeddings.start(INDEX)

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Index data
        cls.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(cls.data)])
        cls.client.get("index")

    def testCount(self):
        """
        Test count via API
        """

        self.assertEqual(self.client.get("count").json(), 6)

    def testDelete(self):
        """
        Test delete via API
        """

        # Delete best match
        ids = self.client.post("delete", json=[4]).json()
        self.assertEqual(ids, [4])

        # Search for best match
        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]

        self.assertEqual(self.client.get("count").json(), 5)
        self.assertEqual(uid, 5)

        # Reset data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

    def testEmpty(self):
        """
        Test empty API configuration
        """

        api = API({"writable": True})

        self.assertIsNone(api.search("test", None))
        self.assertIsNone(api.batchsearch(["test"], None))
        self.assertIsNone(api.delete(["test"]))
        self.assertIsNone(api.count())
        self.assertIsNone(api.similarity("test", ["test"]))
        self.assertIsNone(api.batchsimilarity(["test"], ["test"]))
        self.assertIsNone(api.explain("test"))
        self.assertIsNone(api.batchexplain(["test"]))
        self.assertIsNone(api.transform("test"))
        self.assertIsNone(api.batchtransform(["test"]))
        self.assertIsNone(api.extract(["test"], ["test"]))

    def testExtractor(self):
        """
        Test qa extraction via API
        """

        data = [
            "Giants hit 3 HRs to down Dodgers",
            "Giants 5 Dodgers 4 final",
            "Dodgers drop Game 2 against the Giants, 5-4",
            "Blue Jays beat Red Sox final score 2-1",
            "Red Sox lost to the Blue Jays, 2-1",
            "Blue Jays at Red Sox is over. Score: 2-1",
            "Phillies win over the Braves, 5-0",
            "Phillies 5 Braves 0 final",
            "Final: Braves lose to the Phillies in the series opener, 5-0",
            "Lightning goaltender pulled, lose to Flyers 4-1",
            "Flyers 4 Lightning 1 final",
            "Flyers win 4-1",
        ]

        questions = ["What team won the game?", "What was score?"]

        # pylint: disable=C3001
        execute = lambda query: self.client.post(
            "extract",
            json={"queue": [{"name": question, "query": query, "question": question, "snippet": False} for question in questions], "texts": data},
        ).json()

        answers = execute("Red Sox - Blue Jays")
        self.assertEqual("Blue Jays", answers[0]["answer"])
        self.assertEqual("2-1", answers[1]["answer"])

        # Ad-hoc questions
        question = "What hockey team won?"

        answers = self.client.post(
            "extract", json={"queue": [{"name": question, "query": question, "question": question, "snippet": False}], "texts": data}
        ).json()
        self.assertEqual("Flyers", answers[0]["answer"])

    def testReindex(self):
        """
        Test reindex via API
        """

        # Reindex data
        self.client.post("reindex", json={"config": {"path": "sentence-transformers/nli-mpnet-base-v2"}})

        # Search for best match
        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]

        self.assertEqual(uid, 4)

        # Reset data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

    def testSearch(self):
        """
        Test search via API
        """

        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]
        self.assertEqual(uid, 4)

    def testSearchBatch(self):
        """
        Test batch search via API
        """

        results = self.client.post("batchsearch", json={"queries": ["feel good story", "climate change"], "limit": 1}).json()

        uids = [result[0]["id"] for result in results]
        self.assertEqual(uids, [4, 1])

    def testSimilarity(self):
        """
        Test similarity via API
        """

        uid = self.client.post("similarity", json={"query": "feel good story", "texts": self.data}).json()[0]["id"]

        self.assertEqual(uid, 4)

    def testSimilarityBatch(self):
        """
        Test batch similarity via API
        """

        results = self.client.post("batchsimilarity", json={"queries": ["feel good story", "climate change"], "texts": self.data}).json()

        uids = [result[0]["id"] for result in results]
        self.assertEqual(uids, [4, 1])

    def testTransform(self):
        """
        Test embeddings transform via API
        """

        self.assertEqual(len(self.client.get("transform?text=testembed").json()), 768)

    def testTransformBatch(self):
        """
        Test batch embeddings transform via API
        """

        embeddings = self.client.post("batchtransform", json=self.data).json()

        self.assertEqual(len(embeddings), len(self.data))
        self.assertEqual(len(embeddings[0]), 768)

    def testUpsert(self):
        """
        Test upsert via API
        """

        # Update data
        self.client.post("add", json=[{"id": 0, "text": "Feel good story: baby panda born"}])
        self.client.get("upsert")

        # Search for best match
        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]

        self.assertEqual(uid, 0)

        # Reset data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

    def testViewOnly(self):
        """
        Test read-only API instance
        """

        # Re-create read-only model
        self.client = TestEmbeddings.start(READONLY)

        # Test search
        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]
        self.assertEqual(uid, 4)

        # Test similarity
        uid = self.client.post("similarity", json={"query": "feel good story", "texts": self.data}).json()[0]["id"]
        self.assertEqual(uid, 4)

        # Test errors raised for write operations
        self.assertEqual(self.client.post("add", json=[{"id": 0, "text": "test"}]).status_code, 403)
        self.assertEqual(self.client.get("index").status_code, 403)
        self.assertEqual(self.client.get("upsert").status_code, 403)
        self.assertEqual(self.client.post("delete", json=[0]).status_code, 403)
        self.assertEqual(self.client.post("reindex", json={"config": {"path": "sentence-transformers/nli-mpnet-base-v2"}}).status_code, 403)

    def testXFunctions(self):
        """
        Test API instance with custom functions
        """

        # Re-create model with custom functions
        self.client = TestEmbeddings.start(FUNCTIONS)

        # Index data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

        query = urllib.parse.quote("select elements('text') length from txtai limit 1")
        self.assertEqual(self.client.get(f"search?query={query}").json()[0]["length"], 4)

        query = urllib.parse.quote("select length('text') length from txtai limit 1")
        self.assertEqual(self.client.get(f"search?query={query}").json()[0]["length"], 4)

    def testXPlain(self):
        """
        Test API instance with explain methods
        """

        results = self.client.post("explain", json={"query": "feel good story", "limit": 1}).json()

        self.assertEqual(results[0]["text"], self.data[4])
        self.assertIsNotNone(results[0].get("tokens"))

    def testXPlainBatch(self):
        """
        Test batch query explain via API
        """

        results = self.client.post("batchexplain", json={"queries": ["feel good story", "climate change"], "limit": 1}).json()

        text = [result[0]["text"] for result in results]
        self.assertEqual(text, [self.data[4], self.data[1]])
        self.assertIsNotNone(results[0][0].get("tokens"))

    def testXRAG(self):
        """
        Test RAG via API
        """

        # Re-create model with custom functions
        self.client = TestEmbeddings.start(RAG)

        # Index data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

        response = self.client.get("rag?query=bear").json()
        self.assertIsInstance(response, str)

        response = self.client.post("batchrag", json={"queries": ["bear", "bear"]}).json()
        self.assertEqual(len(response), 2)

    def testXRerank(self):
        """
        Test rerank via API
        """

        # Re-create model with custom functions
        self.client = TestEmbeddings.start(RERANK)

        # Index data
        self.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(self.data)])
        self.client.get("index")

        uid = self.client.get("rerank?query=bear").json()[0]["id"]
        self.assertEqual(uid, "3")

        results = self.client.post("batchrerank", json={"queries": ["bear", "bear"]}).json()

        uids = [result[0]["id"] for result in results]
        self.assertEqual(uids, ["3", "3"])


class Elements:
    """
    Custom SQL function as callable object.
    """

    def __call__(self, text):
        return length(text)


def transform(document):
    """
    Custom transform function.
    """

    return document


def length(text):
    """
    Custom SQL function.
    """

    return len(text)



================================================
FILE: test/python/testapi/testapipipeline.py
================================================
"""
Pipeline API module tests
"""

import os
import tempfile
import unittest
import urllib

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import API, application

# pylint: disable=C0411
from utils import Utils

# Configuration for pipelines
PIPELINES = """
# Image captions
caption:

# Entity extraction
entity:
    path: dslim/bert-base-NER

# Extractor settings
extractor:
    similarity: similarity
    path: llm

# Label settings
labels:
    path: prajjwal1/bert-medium-mnli

# LLM settings
llm:
    path: hf-internal-testing/tiny-random-gpt2
    task: language-generation

# Image objects
objects:

# Text segmentation
segmentation:
    sentences: true

# Enable pipeline similarity backed by zero shot classifier
similarity:

# Summarization
summary:
    path: t5-small

# Tabular
tabular:

# Text extraction
textractor:

# Text to speech
texttospeech:

# Transcription
transcription:

# Translation:
translation:

# Enable file uploads
upload:
"""


# pylint: disable=R0904
class TestPipeline(unittest.TestCase):
    """
    API tests for pipelines.
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(PIPELINES)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestPipeline.start()

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.text = (
            "Search is the base of many applications. Once data starts to pile up, users want to be able to find it. It's the foundation "
            "of the internet and an ever-growing challenge that is never solved or done. The field of Natural Language Processing (NLP) is "
            "rapidly evolving with a number of new developments. Large-scale general language models are an exciting new capability "
            "allowing us to add amazing functionality quickly with limited compute and people. Innovation continues with new models "
            "and advancements coming in at what seems a weekly basis. This article introduces txtai, an AI-powered search engine "
            "that enables Natural Language Understanding (NLU) based search in any application."
        )

    def testCaption(self):
        """
        Test caption via API
        """

        caption = self.client.get(f"caption?file={Utils.PATH}/books.jpg").json()

        self.assertEqual(caption, "a book shelf filled with books and a stack of books")

    def testCaptionBatch(self):
        """
        Test batch caption via API
        """

        path = Utils.PATH + "/books.jpg"

        captions = self.client.post("batchcaption", json=[path, path]).json()
        self.assertEqual(captions, ["a book shelf filled with books and a stack of books"] * 2)

    def testEntity(self):
        """
        Test entity extraction via API
        """

        entities = self.client.get(f"entity?text={self.data[1]}").json()
        self.assertEqual([e[0] for e in entities], ["Canada", "Manhattan"])

    def testEntityBatch(self):
        """
        Test batch entity via API
        """

        entities = self.client.post("batchentity", json=[self.data[1]]).json()
        self.assertEqual([e[0] for e in entities[0]], ["Canada", "Manhattan"])

    def testEmpty(self):
        """
        Test empty API configuration
        """

        api = API({})

        self.assertIsNone(api.label("test", ["test"]))
        self.assertIsNone(api.pipeline("junk", "test"))

    def testLabel(self):
        """
        Test label via API
        """

        labels = self.client.post("label", json={"text": "this is the best sentence ever", "labels": ["positive", "negative"]}).json()

        self.assertEqual(labels[0]["id"], 0)

    def testLabelBatch(self):
        """
        Test batch label via API
        """

        labels = self.client.post(
            "batchlabel", json={"texts": ["this is the best sentence ever", "This is terrible"], "labels": ["positive", "negative"]}
        ).json()

        results = [l[0]["id"] for l in labels]
        self.assertEqual(results, [0, 1])

    def testLLM(self):
        """
        Test LLM inference via API
        """

        response = self.client.get("llm?text=test").json()
        self.assertIsInstance(response, str)

    def testLLMBatch(self):
        """
        Test batch LLM inference via API
        """

        response = self.client.post("batchllm", json={"texts": ["test", "test"]}).json()
        self.assertEqual(len(response), 2)

    def testObjects(self):
        """
        Test objects via API
        """

        objects = self.client.get(f"objects?file={Utils.PATH}/books.jpg").json()

        self.assertEqual(objects[0][0], "book")

    def testObjectsBatch(self):
        """
        Test batch objects via API
        """

        path = Utils.PATH + "/books.jpg"

        objects = self.client.post("batchobjects", json=[path, path]).json()
        self.assertEqual([o[0][0] for o in objects], ["book"] * 2)

    def testSegment(self):
        """
        Test segmentation via API
        """

        text = self.client.get("segment?text=This is a test. And another test.").json()

        # Check array length is 2
        self.assertEqual(len(text), 2)

    def testSegmentBatch(self):
        """
        Test batch segmentation via API
        """

        text = "This is a test. And another test."
        texts = self.client.post("batchsegment", json=[text, text]).json()

        # Check array length is 2 and first element length is 2
        self.assertEqual(len(texts), 2)
        self.assertEqual(len(texts[0]), 2)

    def testSimilarity(self):
        """
        Test similarity via API
        """

        uid = self.client.post("similarity", json={"query": "feel good story", "texts": self.data}).json()[0]["id"]

        self.assertEqual(self.data[uid], self.data[4])

    def testSimilarityBatch(self):
        """
        Test batch similarity via API
        """

        results = self.client.post("batchsimilarity", json={"queries": ["feel good story", "climate change"], "texts": self.data}).json()

        uids = [result[0]["id"] for result in results]
        self.assertEqual(uids, [4, 1])

    def testSummary(self):
        """
        Test summary via API
        """

        summary = self.client.get(f"summary?text={urllib.parse.quote(self.text)}&minlength=15&maxlength=15").json()
        self.assertEqual(summary, "the field of natural language processing (NLP) is rapidly evolving")

    def testSummaryBatch(self):
        """
        Test batch summary via API
        """

        summaries = self.client.post("batchsummary", json={"texts": [self.text, self.text], "minlength": 15, "maxlength": 15}).json()
        self.assertEqual(summaries, ["the field of natural language processing (NLP) is rapidly evolving"] * 2)

    def testTabular(self):
        """
        Test tabular via API
        """

        results = self.client.get(f"tabular?file={Utils.PATH}/tabular.csv").json()

        # Check length of results is as expected
        self.assertEqual(len(results), 6)

    def testTabularBatch(self):
        """
        Test batch tabular via API
        """

        path = Utils.PATH + "/tabular.csv"

        results = self.client.post("batchtabular", json=[path, path]).json()
        self.assertEqual((len(results[0]), len(results[1])), (6, 6))

    def testTextractor(self):
        """
        Test textractor via API
        """

        text = self.client.get(f"textract?file={Utils.PATH}/article.pdf").json()

        # Check length of text is as expected
        self.assertEqual(len(text), 2471)

    def testTextractorBatch(self):
        """
        Test batch textractor via API
        """

        path = Utils.PATH + "/article.pdf"

        texts = self.client.post("batchtextract", json=[path, path]).json()
        self.assertEqual((len(texts[0]), len(texts[1])), (2471, 2471))

    def testTextToSpeech(self):
        """
        Test text to speech
        """

        # Generate audio and check for WAV signature
        audio = self.client.get("texttospeech?text=hello&encoding=wav").content
        self.assertTrue(audio[0:4] == b"RIFF")

    def testTranscribe(self):
        """
        Test transcribe via API
        """

        text = self.client.get(f"transcribe?file={Utils.PATH}/Make_huge_profits.wav").json()

        # Check length of text is as expected
        self.assertEqual(text, "Make huge profits without working make up to one hundred thousand dollars a day")

    def testTranscribeBatch(self):
        """
        Test batch transcribe via API
        """

        path = Utils.PATH + "/Make_huge_profits.wav"

        texts = self.client.post("batchtranscribe", json=[path, path]).json()
        self.assertEqual(texts, ["Make huge profits without working make up to one hundred thousand dollars a day"] * 2)

    def testTranslate(self):
        """
        Test translate via API
        """

        translation = self.client.get(f"translate?text={urllib.parse.quote('This is a test translation into Spanish')}&target=es").json()
        self.assertEqual(translation, "Esta es una traducciÃ³n de prueba al espaÃ±ol")

    def testTranslateBatch(self):
        """
        Test batch translate via API
        """

        text = "This is a test translation into Spanish"
        translations = self.client.post("batchtranslate", json={"texts": [text, text], "target": "es"}).json()
        self.assertEqual(translations, ["Esta es una traducciÃ³n de prueba al espaÃ±ol"] * 2)

    def testUpload(self):
        """
        Test file upload
        """

        path = Utils.PATH + "/article.pdf"
        with open(path, "rb") as f:
            path = self.client.post("upload", files={"files": f}).json()[0]
            self.assertTrue(os.path.exists(path))



================================================
FILE: test/python/testapi/testapiworkflow.py
================================================
"""
Workflow API module tests
"""

import json
import os
import tempfile
import unittest

from http.server import HTTPServer, BaseHTTPRequestHandler
from multiprocessing.pool import ThreadPool
from threading import Thread
from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import API, application

# Configuration for workflows
WORKFLOWS = """
# Embeddings index
writable: true
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2

# Labels
labels:
    path: prajjwal1/bert-medium-mnli

nop:

# Text segmentation
segmentation:
    sentences: true

# Workflow definitions
workflow:
    labels:
        tasks:
            - action: labels
              args: [[positive, negative]]
    multiaction:
        tasks:
            - action:
                - labels
                - nop
              initialize: testapi.testapiworkflow.TestInitFinal
              finalize: testapi.testapiworkflow.TestInitFinal
              merge: concat
              args:
                - [[positive, negative], false, True]
                - null
    schedule:
        schedule:
            cron: '* * * * * *'
            elements:
                - This is a test sentence. And another sentence to split.
            iterations: 1
        tasks:
            - action: segmentation
    segment:
        tasks:
            - action: segmentation
            - action: index
    get:
        tasks:
            - task: service
              url: http://127.0.0.1:8001/testget
              method: get
              params:
                text:
    post:
        tasks:
            - task: service
              url: http://127.0.0.1:8001/testpost
              params:

    xml:
        tasks:
            - task: service
              url: http://127.0.0.1:8001/xml
              method: get
              batch: false
              extract: row
              params:
                text:
"""


class RequestHandler(BaseHTTPRequestHandler):
    """
    Test HTTP handler.
    """

    def do_GET(self):
        """
        GET request handler.
        """

        self.send_response(200)

        if self.path.startswith("/xml"):
            response = "<row><text>test</text></row>".encode("utf-8")
            mime = "application/xml"
        else:
            response = '[{"text": "test"}]'.encode("utf-8")
            mime = "application/json"

        self.send_header("content-type", mime)
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()

    def do_POST(self):
        """
        POST request handler.
        """

        length = int(self.headers["content-length"])
        data = json.loads(self.rfile.read(length))

        response = json.dumps([[y for y in x.split(".") if y] for x in data]).encode("utf-8")

        self.send_response(200)
        self.send_header("content-type", "application/json")
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()


class TestWorkflow(unittest.TestCase):
    """
    API tests for workflows.
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(WORKFLOWS)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestWorkflow.start()

        cls.httpd = HTTPServer(("127.0.0.1", 8001), RequestHandler)

        server = Thread(target=cls.httpd.serve_forever, daemon=True)
        server.start()

    @classmethod
    def tearDownClass(cls):
        """
        Shutdown mock http server.
        """

        cls.httpd.shutdown()

    def testAPICleanup(self):
        """
        Test API threadpool closed when __del__ called.
        """

        api = API({})
        api.pool = ThreadPool()

        # pylint: disable=C2801
        api.__del__()

        self.assertIsNone(api.pool)

    def testServiceGet(self):
        """
        Test workflow with ServiceTask GET via API
        """

        text = "This is a test sentence. And another sentence to split."
        results = self.client.post("workflow", json={"name": "get", "elements": [text]}).json()

        self.assertEqual(len(results), 1)
        self.assertEqual(len(results[0]), 1)

    def testServicePost(self):
        """
        Test workflow with ServiceTask POST via API
        """

        text = "This is a test sentence. And another sentence to split."
        results = self.client.post("workflow", json={"name": "post", "elements": [text]}).json()

        self.assertEqual(len(results), 1)
        self.assertEqual(len(results[0]), 2)

    def testServiceXml(self):
        """
        Test workflow with ServiceTask GET via API and XML response
        """

        text = "This is a test sentence. And another sentence to split."
        results = self.client.post("workflow", json={"name": "xml", "elements": [text]}).json()

        self.assertEqual(len(results), 1)
        self.assertEqual(len(results[0]), 1)

    def testWorkflowLabels(self):
        """
        Test workflow with labels via API
        """

        text = "This is the best"

        results = self.client.post("workflow", json={"name": "labels", "elements": [text]}).json()
        self.assertEqual(results[0][0], 0)

        results = self.client.post("workflow", json={"name": "multiaction", "elements": [text]}).json()
        self.assertEqual(results[0], "['positive']. This is the best")

    def testWorkflowSegment(self):
        """
        Test workflow with segmentation via API
        """

        text = "This is a test sentence. And another sentence to split."

        results = self.client.post("workflow", json={"name": "segment", "elements": [text]}).json()
        self.assertEqual(len(results), 2)

        results = self.client.post("workflow", json={"name": "segment", "elements": [[0, text]]}).json()
        self.assertEqual(len(results), 2)


class TestInitFinal:
    """
    Class to test task initialize and finalize calls.
    """

    def __call__(self):
        pass



================================================
FILE: test/python/testapi/testauthorization.py
================================================
"""
Authorization module tests
"""

import hashlib
import os
import tempfile
import unittest

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import application


class TestAuthorization(unittest.TestCase):
    """
    API tests for token authorization.
    """

    @staticmethod
    @patch.dict(
        os.environ,
        {
            "CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"),
            "DEPENDENCIES": "txtai.api.Authorization",
            "TOKEN": hashlib.sha256("token".encode("utf-8")).hexdigest(),
        },
    )
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write("embeddings:\n")

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestAuthorization.start()

    def testInvalid(self):
        """
        Test invalid authorization
        """

        response = self.client.get("search?query=test")
        self.assertEqual(response.status_code, 401)

        response = self.client.get("search?query=test", headers={"Authorization": "Bearer invalid"})
        self.assertEqual(response.status_code, 401)

    def testValid(self):
        """
        Test valid authorization
        """

        results = self.client.get("search?query=test", headers={"Authorization": "Bearer token"}).json()
        self.assertEqual(results, [])



================================================
FILE: test/python/testapi/testcluster.py
================================================
"""
Cluster API module tests
"""

import json
import os
import tempfile
import unittest
import urllib.parse

from http.server import HTTPServer, BaseHTTPRequestHandler
from threading import Thread
from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import application

# Configuration for an embeddings cluster
CLUSTER = """
cluster:
    shards:
        - http://127.0.0.1:8002
        - http://127.0.0.1:8003
"""


class RequestHandler(BaseHTTPRequestHandler):
    """
    Test HTTP handler.
    """

    def do_GET(self):
        """
        GET request handler.
        """

        if self.path == "/count":
            response = 26
        elif self.path.startswith("/search?query=select"):
            if "group+by+id" in self.path:
                response = [{"count(*)": 26}]
            elif "group+by+text" in self.path:
                response = [{"count(*)": 12, "text": "This is a test"}, {"count(*)": 14, "text": "And another test"}]
            elif "group+by+txt" in self.path:
                response = [{"count(*)": 12, "txt": "This is a test"}, {"count(*)": 14, "txt": "And another test"}]
            else:
                if self.server.server_port == 8002:
                    response = [{"count(*)": 12, "min(indexid)": 0, "max(indexid)": 11, "avg(indexid)": 6.3}]
                else:
                    response = [{"count(*)": 16, "min(indexid)": 2, "max(indexid)": 14, "avg(indexid)": 6.7}]
        elif self.path.startswith("/search"):
            response = [{"id": 4, "score": 0.40}]
        else:
            response = {"result": "ok"}

        # Convert response to string
        response = json.dumps(response).encode("utf-8")

        self.send_response(200)
        self.send_header("content-type", "application/json")
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()

    def do_POST(self):
        """
        POST request handler.
        """

        if self.path.startswith("/batchsearch"):
            response = [[{"id": 4, "score": 0.40}], [{"id": 1, "score": 0.40}]]
        elif self.path.startswith("/delete"):
            if self.server.server_port == 8002:
                response = [0]
            else:
                response = []
        else:
            response = {"result": "ok"}

        response = json.dumps(response).encode("utf-8")

        self.send_response(200)
        self.send_header("content-type", "application/json")
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()


@unittest.skipIf(os.name == "nt", "TestCluster skipped on Windows")
class TestCluster(unittest.TestCase):
    """
    API tests for embeddings clusters
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(CLUSTER)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestCluster.start()

        cls.httpd1 = HTTPServer(("127.0.0.1", 8002), RequestHandler)

        server1 = Thread(target=cls.httpd1.serve_forever, daemon=True)
        server1.start()

        cls.httpd2 = HTTPServer(("127.0.0.1", 8003), RequestHandler)

        server2 = Thread(target=cls.httpd2.serve_forever, daemon=True)
        server2.start()

        # Index data
        cls.client.post("add", json=[{"id": 0, "text": "test"}])
        cls.client.get("index")

    @classmethod
    def tearDownClass(cls):
        """
        Shutdown mock http server.
        """

        cls.httpd1.shutdown()
        cls.httpd2.shutdown()

    def testCount(self):
        """
        Test cluster count
        """

        self.assertEqual(self.client.get("count").json(), 52)

    def testDelete(self):
        """
        Test cluster delete
        """

        self.assertEqual(self.client.post("delete", json=[0]).json(), [0])

    def testDeleteString(self):
        """
        Test cluster delete with string id
        """

        self.assertEqual(self.client.post("delete", json=["0"]).json(), [0])

    def testIds(self):
        """
        Test id configurations
        """

        # String ids
        self.client.post("add", json=[{"id": "0", "text": "test"}])
        self.assertEqual(self.client.get("index").status_code, 200)

        # Auto ids
        self.client.post("add", json=[{"text": "test"}])
        self.assertEqual(self.client.get("index").status_code, 200)

    def testReindex(self):
        """
        Test cluster reindex
        """

        self.assertEqual(self.client.post("reindex", json={"config": {"path": "sentence-transformers/nli-mpnet-base-v2"}}).status_code, 200)

    def testSearch(self):
        """
        Test cluster search
        """

        # Encode parameters
        params = json.dumps({"x": 1})

        query = urllib.parse.quote("feel good story")
        uid = self.client.get(f"search?query={query}&limit=1&weights=0.5&index=default&parameters={params}&graph=False").json()[0]["id"]
        self.assertEqual(uid, 4)

    def testSearchBatch(self):
        """
        Test cluster batch search
        """

        results = self.client.post(
            "batchsearch",
            json={
                "queries": ["feel good story", "climate change"],
                "limit": 1,
                "weights": 0.5,
                "index": "default",
                "parameters": [{"x": 1}, {"x": 2}],
                "graph": False,
            },
        ).json()

        uids = [result[0]["id"] for result in results]
        self.assertEqual(uids, [4, 1])

    def testSQL(self):
        """
        Test cluster SQL statement
        """

        query = urllib.parse.quote("select count(*), min(indexid), max(indexid), avg(indexid) from txtai where text='This is a test'")
        self.assertEqual(
            self.client.get(f"search?query={query}").json(), [{"count(*)": 28, "min(indexid)": 0, "max(indexid)": 14, "avg(indexid)": 6.5}]
        )

        query = urllib.parse.quote("select count(*), text txt from txtai group by txt order by count(*) desc")
        self.assertEqual(
            self.client.get(f"search?query={query}").json(),
            [{"count(*)": 28, "txt": "And another test"}, {"count(*)": 24, "txt": "This is a test"}],
        )

        query = urllib.parse.quote("select count(*), text from txtai group by text order by count(*) asc")
        self.assertEqual(
            self.client.get(f"search?query={query}").json(),
            [{"count(*)": 24, "text": "This is a test"}, {"count(*)": 28, "text": "And another test"}],
        )

        query = urllib.parse.quote("select count(*) from txtai group by id order by count(*)")
        self.assertEqual(self.client.get(f"search?query={query}").json(), [{"count(*)": 52}])

    def testUpsert(self):
        """
        Test cluster upsert
        """

        # Update data
        self.client.post("add", json=[{"id": 4, "text": "Feel good story: baby panda born"}])
        self.client.get("upsert")

        # Search for best match
        query = "feel good story"
        uid = self.client.get(f"search?query={query}&limit=1").json()[0]["id"]

        self.assertEqual(uid, 4)



================================================
FILE: test/python/testapi/testencoding.py
================================================
"""
Encoding module tests
"""

import base64
import os
import tempfile
import unittest
import urllib.parse

from io import BytesIO
from unittest.mock import patch

import msgpack
import numpy as np
import PIL

from fastapi.testclient import TestClient

from txtai.api import application
from txtai.api.responses import JSONEncoder

# pylint: disable=C0411
from utils import Utils

# Configuration for image storage
INDEX = """
# Allow indexing of documents
writable: %s

embeddings:
    defaults: False
    content: True
    objects: %s
"""


class TestEncoding(unittest.TestCase):
    """
    API tests for response encoding
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start(yaml):
        """
        Starts a mock FastAPI client.

        Args:
            yaml: input configuration
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(yaml)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestEncoding.start(INDEX % ("True", "image"))

    def testImages(self):
        """
        Test image encoding
        """

        with open(Utils.PATH + "/books.jpg", "rb") as f:
            self.client.post("addimage", data={"uid": 0}, files={"data": f})
            self.client.get("index")

        query = urllib.parse.quote_plus("select id, object from txtai limit 1")
        results = self.client.get(f"search?query={query}").json()

        # Test reading image
        self.assertIsInstance(PIL.Image.open(BytesIO(base64.b64decode(results[0]["object"]))), PIL.Image.Image)

    def testInvalidInputs(self):
        """
        Test invalid parameter inputs
        """

        response = self.client.post("addimage", data={"uid": [0, 1]}, files={"data": b"123"})
        self.assertEqual(response.status_code, 422)

        response = self.client.post("addobject", data={"uid": [0, 1]}, files={"data": b"123"})
        self.assertEqual(response.status_code, 422)

    def testInvalidJSON(self):
        """
        Test that invalid JSON raises an exception
        """

        with self.assertRaises(TypeError):
            JSONEncoder().encode(np.random.rand(1, 1))

    def testMessagePack(self):
        """
        Test message pack encoding
        """

        # Validate binary encoding
        results = self.client.get("count", headers={"Accept": "application/msgpack"}).content
        self.assertEqual(results, b"\x01")

        # Validate query result
        query = urllib.parse.quote_plus("select id, object from txtai limit 1")
        results = self.client.get(f"search?query={query}", headers={"Accept": "application/msgpack"}).content
        results = msgpack.unpackb(results)

        # Test reading image
        self.assertIsInstance(PIL.Image.open(BytesIO(results[0]["object"])), PIL.Image.Image)

    def testObjects(self):
        """
        Test object encoding
        """

        # Recreate model with standard object encoding
        self.client = TestEncoding.start(INDEX % ("True", "True"))

        # Test various formats
        self.client.post("addobject", data={"uid": "id0"}, files={"data": b"1234"})
        self.client.post("addobject", files={"data": b"ABC"})
        self.client.post("addobject", data={"uid": "id1", "field": "object"}, files={"data": b"A1234"})
        self.client.get("index")

        query = urllib.parse.quote_plus("select id, object from txtai where id = 'id0' limit 1")
        results = self.client.get(f"search?query={query}").json()
        self.assertEqual(base64.b64decode(results[0]["object"]), b"1234")

        # Test with messagepack encoding
        results = self.client.get(f"search?query={query}", headers={"Accept": "application/msgpack"}).content
        results = msgpack.unpackb(results)
        self.assertEqual(results[0]["object"], b"1234")

        count = self.client.get("count").json()
        self.assertEqual(count, 3)

    def testReadOnly(self):
        """
        Test read only indexes
        """

        # Recreate model with standard object encoding
        self.client = TestEncoding.start(INDEX % ("False", "True"))

        # Test errors raised for write operations
        with open(Utils.PATH + "/books.jpg", "rb") as f:
            response = self.client.post("addimage", data={"uid": 0}, files={"data": f})
            self.assertEqual(response.status_code, 403)

        self.assertEqual(self.client.post("addobject", data={"uid": 0}, files={"data": b"1234"}).status_code, 403)



================================================
FILE: test/python/testapi/testextension.py
================================================
"""
Extension module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from fastapi import APIRouter
from fastapi.testclient import TestClient

from txtai.api import application, Extension
from txtai.pipeline import Pipeline

# Example pipeline extension
PIPELINES = """
testapi.testextension.SamplePipeline:
"""


class SampleRouter:
    """
    Sample API router.
    """

    router = APIRouter()

    @staticmethod
    @router.get("/sample")
    def sample(text: str):
        """
        Calls sample pipeline.

        Args:
            text: input text

        Returns:
            formatted text
        """

        return application.get().pipeline("testapi.testextension.SamplePipeline", (text,))


class SampleExtension(Extension):
    """
    Sample API extension.
    """

    def __call__(self, app):
        app.include_router(SampleRouter().router)


class SamplePipeline(Pipeline):
    """
    Sample pipeline.
    """

    def __call__(self, text):
        return text.lower()


class TestExtension(unittest.TestCase):
    """
    API tests for extensions.
    """

    @staticmethod
    @patch.dict(
        os.environ,
        {
            "CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"),
            "API_CLASS": "txtai.api.API",
            "EXTENSIONS": "testapi.testextension.SampleExtension",
        },
    )
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(PIPELINES)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestExtension.start()

    def testEmpty(self):
        """
        Test an empty extension
        """

        extension = Extension()
        self.assertIsNone(extension(None))

    def testExtension(self):
        """
        Test a pipeline extension
        """

        text = self.client.get("sample?text=Test%20String").json()
        self.assertEqual(text, "test string")



================================================
FILE: test/python/testapi/testmcp.py
================================================
"""
Agent API module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import application

# Configuration for agents
MCP = """
mcp: True
"""


# pylint: disable=R0904
class TestMCP(unittest.TestCase):
    """
    API tests for model context protocol (MCP)
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testapi.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testapi.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(MCP)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestMCP.start()

    def testMCP(self):
        """
        Test that application a /mcp route
        """

        self.assertTrue(any(route.path == "/mcp" for route in self.client.app.routes))



================================================
FILE: test/python/testapi/testopenai.py
================================================
"""
OpenAI API module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from fastapi.testclient import TestClient

from txtai.api import application

# pylint: disable=C0411
from utils import Utils

# API Configuration
CONFIG = """
# Enable OpenAI-compatible API
openai: True

# Allow indexing of documents
writable: True

# Agent configuration
agent:
    hello:
        max_iterations: 1

# Embeddings settings
embeddings:
    path: sentence-transformers/nli-mpnet-base-v2
    content: True        

# LLM configuration
llm:
    path: hf-internal-testing/tiny-random-LlamaForCausalLM

# Text segmentation
segmentation:

# Text to speech
texttospeech:

# Transcription
transcription:

# Workflow
workflow:
    echo:
        tasks:
            - task: console
"""


# pylint: disable=R0904
class TestOpenAI(unittest.TestCase):
    """
    Tests for OpenAI-compatible API endpoint for txtai.
    """

    @staticmethod
    @patch.dict(os.environ, {"CONFIG": os.path.join(tempfile.gettempdir(), "testopenai.yml"), "API_CLASS": "txtai.api.API"})
    def start():
        """
        Starts a mock FastAPI client.
        """

        config = os.path.join(tempfile.gettempdir(), "testopenai.yml")

        with open(config, "w", encoding="utf-8") as output:
            output.write(CONFIG)

        # Create new application and set on client
        application.app = application.create()
        client = TestClient(application.app)
        application.start()

        # Patch LLM to generate answer
        agent = application.get().agents["hello"]
        agent.process.model.llm = lambda *args, **kwargs: 'Action:\n{"name": "final_answer", "arguments": "Hi"}'

        return client

    @classmethod
    def setUpClass(cls):
        """
        Create API client on creation of class.
        """

        cls.client = TestOpenAI.start()

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Index data
        cls.client.post("add", json=[{"id": x, "text": row} for x, row in enumerate(cls.data)])
        cls.client.get("index")

    def testChatAgent(self):
        """
        Test a chat completion with an agent
        """

        response = self.client.post("/v1/chat/completions", json={"messages": [{"role": "user", "content": "Hello"}], "model": "hello"}).json()

        self.assertEqual(response["choices"][0]["message"]["content"], "Hi")

    def testChatLLM(self):
        """
        Test a chat completion with a LLM
        """

        response = self.client.post("/v1/chat/completions", json={"messages": [{"role": "user", "content": "Hello"}], "model": "llm"}).json()

        self.assertIsNotNone(response["choices"][0]["message"]["content"])

    def testChatPipeline(self):
        """
        Test a chat completion with a pipeline
        """

        response = self.client.post("/v1/chat/completions", json={"messages": [{"role": "user", "content": "Hello"}], "model": "segmentation"}).json()

        self.assertEqual(response["choices"][0]["message"]["content"], "Hello")

    def testChatSearch(self):
        """
        Test a chat completion with an embeddings search
        """

        response = self.client.post(
            "/v1/chat/completions", json={"messages": [{"role": "user", "content": "feel good story"}], "model": "embeddings"}
        ).json()

        self.assertEqual(response["choices"][0]["message"]["content"], self.data[4])

    def testChatStream(self):
        """
        Test a chat completion with a LLM
        """

        response = self.client.post("/v1/chat/completions", json={"messages": [{"role": "user", "content": "Hello"}], "model": "llm", "stream": True})

        self.assertGreater(len(response.text.split("\n\n")), 0)

    def testChatWorkflow(self):
        """
        Test a chat completion with a workflow
        """

        response = self.client.post("/v1/chat/completions", json={"messages": [{"role": "user", "content": "Hello"}], "model": "echo"}).json()

        self.assertEqual(response["choices"][0]["message"]["content"], "Hello")

    def testEmbeddings(self):
        """
        Test generating embeddings vectors
        """

        response = self.client.post("/v1/embeddings", json={"input": "text to embed", "model": "nli-mpnet-base-v2"}).json()

        self.assertEqual(len(response["data"][0]["embedding"]), 768)

    def testSpeech(self):
        """
        Test generating speech for input text
        """

        response = self.client.post(
            "/v1/audio/speech", json={"model": "tts", "input": "text to speak", "voice": "default", "response_format": "wav"}
        ).content

        self.assertTrue(response[0:4] == b"RIFF")

    def testTranscribe(self):
        """
        Test audio to text transcription
        """

        path = Utils.PATH + "/Make_huge_profits.wav"
        with open(path, "rb") as f:
            text = self.client.post("/v1/audio/transcriptions", files={"file": f}).json()["text"]
            self.assertEqual(text, "Make huge profits without working make up to one hundred thousand dollars a day")

    def testTranslate(self):
        """
        Test audio translation
        """

        path = Utils.PATH + "/Make_huge_profits.wav"
        with open(path, "rb") as f:
            text = self.client.post("/v1/audio/translations", files={"file": f}).json()["text"]
            self.assertEqual(text, "Make huge profits without working make up to one hundred thousand dollars a day")



================================================
FILE: test/python/testdatabase/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testdatabase/testclient.py
================================================
"""
Client module tests
"""

import os
import time
import tempfile

from txtai.embeddings import Embeddings

from .testrdbms import Common


# pylint: disable=R0904
class TestClient(Common.TestRDBMS):
    """
    Embeddings with content stored in a client RDBMS.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Content backend
        cls.backend = None

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def setUp(self):
        """
        Set unique database path for each test.
        """

        # Generate unique database path and set on embeddings
        path = os.path.join(tempfile.gettempdir(), f"{int(time.time() * 1000)}.sqlite")
        self.backend = f"sqlite:///{path}"

        self.embeddings.config["content"] = self.backend

    def testSchema(self):
        """
        Test database creation with a specified schema
        """

        # Default sequence id
        embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2", content=self.backend, schema="txtai")
        embeddings.index(self.data)

        result = embeddings.search("feel good story", 1)[0]
        self.assertEqual(result["text"], self.data[4])



================================================
FILE: test/python/testdatabase/testcustom.py
================================================
"""
Custom database tests
"""

import unittest

from txtai.database import DatabaseFactory


class TestCustom(unittest.TestCase):
    """
    Custom database backend tests.
    """

    def testCustomBackend(self):
        """
        Test resolving a custom backend
        """

        database = DatabaseFactory.create({"content": "txtai.database.SQLite"})
        self.assertIsNotNone(database)

    def testCustomBackendNotFound(self):
        """
        Test resolving an unresolvable backend
        """

        with self.assertRaises(ImportError):
            DatabaseFactory.create({"content": "notfound.database"})



================================================
FILE: test/python/testdatabase/testdatabase.py
================================================
"""
Database tests
"""

import unittest

from txtai.database import Database


class TestDatabase(unittest.TestCase):
    """
    Base database tests.
    """

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        database = Database({})

        self.assertRaises(NotImplementedError, database.load, None)
        self.assertRaises(NotImplementedError, database.insert, None)
        self.assertRaises(NotImplementedError, database.delete, None)
        self.assertRaises(NotImplementedError, database.reindex, None)
        self.assertRaises(NotImplementedError, database.save, None)
        self.assertRaises(NotImplementedError, database.close)
        self.assertRaises(NotImplementedError, database.ids, None)
        self.assertRaises(NotImplementedError, database.count)
        self.assertRaises(NotImplementedError, database.resolve, None, None)
        self.assertRaises(NotImplementedError, database.embed, None, None)
        self.assertRaises(NotImplementedError, database.query, None, None, None, None)



================================================
FILE: test/python/testdatabase/testduckdb.py
================================================
"""
DuckDB module tests
"""

import os
import unittest

from txtai.embeddings import Embeddings

from .testrdbms import Common


# pylint: disable=R0904
class TestDuckDB(Common.TestRDBMS):
    """
    Embeddings with content stored in DuckDB.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Content backend
        cls.backend = "duckdb"

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": cls.backend})

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    @unittest.skipIf(os.name == "nt", "testArchive skipped on Windows")
    def testArchive(self):
        """
        Test embeddings index archiving
        """

        super().testArchive()



================================================
FILE: test/python/testdatabase/testencoder.py
================================================
"""
Test encoding/decoding database objects
"""

import glob
import os
import unittest
import tempfile

from unittest.mock import patch

from io import BytesIO

from PIL import Image

from txtai.embeddings import Embeddings

# pylint: disable=C0411
from utils import Utils


class TestEncoder(unittest.TestCase):
    """
    Encoder tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = []
        for path in glob.glob(Utils.PATH + "/*jpg"):
            cls.data.append((path, {"object": Image.open(path)}, None))

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings(
            {"method": "sentence-transformers", "path": "sentence-transformers/clip-ViT-B-32", "content": True, "objects": "image"}
        )

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def testDefault(self):
        """
        Test an index with default encoder
        """

        try:
            # Set default encoder
            self.embeddings.config["objects"] = True

            # Test all database providers
            for content in ["duckdb", "sqlite"]:
                self.embeddings.config["content"] = content

                data = [(0, {"object": bytearray([1, 2, 3]), "text": "default test"}, None)]

                # Create an index
                self.embeddings.index(data)

                result = self.embeddings.search("select object from txtai limit 1")[0]

                self.assertEqual(result["object"].getvalue(), bytearray([1, 2, 3]))
        finally:
            self.embeddings.config["objects"] = "image"
            self.embeddings.config["content"] = True

    def testImages(self):
        """
        Test an index with image encoder
        """

        # Create an index for the list of images
        self.embeddings.index(self.data)

        result = self.embeddings.search("select id, object from txtai where similar('universe') limit 1")[0]

        self.assertTrue(result["id"].endswith("stars.jpg"))
        self.assertTrue(isinstance(result["object"], Image.Image))

    @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
    def testPickle(self):
        """
        Test an index with pickle encoder
        """

        try:
            # Set pickle encoder
            self.embeddings.config["objects"] = "pickle"
            data = [(0, {"object": [1, 2, 3, 4, 5], "text": "default test"}, None)]

            # Create an index
            self.embeddings.index(data)

            result = self.embeddings.search("select object from txtai limit 1")[0]

            self.assertEqual(result["object"], [1, 2, 3, 4, 5])
        finally:
            self.embeddings.config["objects"] = "image"

    def testReindex(self):
        """
        Test reindex with objects
        """

        # Create an index for the list of images
        self.embeddings.index(self.data)

        # Reindex images
        self.embeddings.reindex({"method": "sentence-transformers", "path": "sentence-transformers/clip-ViT-B-32"})

        result = self.embeddings.search("select id, object from txtai where similar('universe') limit 1")[0]

        self.assertTrue(result["id"].endswith("stars.jpg"))
        self.assertTrue(isinstance(result["object"], Image.Image))

    def testReindexFunction(self):
        """
        Test reindex with objects and a function
        """

        try:
            # Streaming function that loads images on the fly
            def prepare(documents):
                for uid, data, tags in documents:
                    yield (uid, Image.open(data), tags)

            # Create an index for the list of images
            self.embeddings.index(self.data)

            # Set default encoder and use function to load images
            self.embeddings.config["objects"] = True

            # Save and load index to force default encoder
            index = os.path.join(tempfile.gettempdir(), "objects")
            self.embeddings.save(index)
            self.embeddings.load(index)

            # Reindex images
            self.embeddings.reindex({"method": "sentence-transformers", "path": "sentence-transformers/clip-ViT-B-32"}, function=prepare)

            result = self.embeddings.search("select id, object from txtai where similar('universe') limit 1")[0]

            self.assertTrue(result["id"].endswith("stars.jpg"))
            self.assertTrue(isinstance(result["object"], BytesIO))
        finally:
            self.embeddings.config["objects"] = "image"



================================================
FILE: test/python/testdatabase/testrdbms.py
================================================
"""
Common file database module tests
"""

import contextlib
import io
import os
import tempfile
import unittest

from unittest.mock import patch

from txtai.embeddings import Embeddings, IndexNotFoundError
from txtai.database import Embedded, RDBMS, SQLError


class Common:
    """
    Wraps common file database tests to prevent unit test discovery for this class.
    """

    # pylint: disable=R0904
    class TestRDBMS(unittest.TestCase):
        """
        Embeddings with content stored in a file database tests.
        """

        @classmethod
        def setUpClass(cls):
            """
            Initialize test data.
            """

            cls.data = [
                "US tops 5 million confirmed virus cases",
                "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
                "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
                "The National Park Service warns against sacrificing slower friends in a bear attack",
                "Maine man wins $1M from $25 lottery ticket",
                "Make huge profits without work, earn up to $100,000 a day",
            ]

            # Content backend
            cls.backend = None

            # Create embeddings model, backed by sentence-transformers & transformers
            cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": cls.backend})

        @classmethod
        def tearDownClass(cls):
            """
            Cleanup data.
            """

            if cls.embeddings:
                cls.embeddings.close()

        def testArchive(self):
            """
            Test embeddings index archiving
            """

            for extension in ["tar.bz2", "tar.gz", "tar.xz", "zip"]:
                # Create an index for the list of text
                self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

                # Generate temp file path
                index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.{extension}")

                self.embeddings.save(index)
                self.embeddings.load(index)

                # Search for best match
                result = self.embeddings.search("feel good story", 1)[0]

                self.assertEqual(result["text"], self.data[4])

                # Test offsets still work after save/load
                self.embeddings.upsert([(0, "Looking out into the dreadful abyss", None)])
                self.assertEqual(self.embeddings.count(), len(self.data))

        def testAutoId(self):
            """
            Test auto id generation
            """

            # Default sequence id
            embeddings = Embeddings(path="sentence-transformers/nli-mpnet-base-v2", content=self.backend)
            embeddings.index(self.data)

            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

            # UUID
            embeddings.config["autoid"] = "uuid4"
            embeddings.index(self.data)

            result = embeddings.search(self.data[4], 1)[0]
            self.assertEqual(len(result["id"]), 36)

        def testCheckpoint(self):
            """
            Test embeddings index checkpoints
            """

            # Checkpoint directory
            checkpoint = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.checkpoint")

            # Save embeddings checkpoint
            self.embeddings.index(self.data, checkpoint=checkpoint)

            # Reindex with checkpoint
            self.embeddings.index(self.data, checkpoint=checkpoint)

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

        def testColumns(self):
            """
            Test custom text/object columns
            """

            embeddings = Embeddings({"keyword": True, "content": self.backend, "columns": {"text": "value"}})
            data = [{"value": x} for x in self.data]
            embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

            # Run search
            result = embeddings.search("lottery", 1)[0]
            self.assertEqual(result["text"], self.data[4])

        def testClose(self):
            """
            Test embeddings close
            """

            embeddings = None

            # Create index twice to test open/close and ensure resources are freed
            for _ in range(2):
                embeddings = Embeddings(
                    {"path": "sentence-transformers/nli-mpnet-base-v2", "scoring": {"method": "bm25", "terms": True}, "content": self.backend}
                )

                # Add record to index
                embeddings.index([(0, "Close test", None)])

                # Save index
                index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.close")
                embeddings.save(index)

                # Close index
                embeddings.close()

            # Test embeddings is empty
            self.assertIsNone(embeddings.ann)
            self.assertIsNone(embeddings.database)

        def testData(self):
            """
            Test content storage and retrieval
            """

            data = self.data + [{"date": "2021-01-01", "text": "Baby panda", "flag": 1}]

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(data)])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[-1]["text"])

        def testDelete(self):
            """
            Test delete
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Delete best match
            self.embeddings.delete([4])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(self.embeddings.count(), 5)
            self.assertEqual(result["text"], self.data[5])

        def testEmpty(self):
            """
            Test empty index
            """

            # Test search against empty index
            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": self.backend})
            self.assertEqual(embeddings.search("test"), [])

            # Test index with no data
            embeddings.index([])
            self.assertIsNone(embeddings.ann)

            # Test upsert with no data
            embeddings.index([(0, "this is a test", None)])
            embeddings.upsert([])
            self.assertIsNotNone(embeddings.ann)

        def testEmptyString(self):
            """
            Test empty string indexing
            """

            # Test empty string
            self.embeddings.index([(0, "", None)])
            self.assertTrue(self.embeddings.search("test"))

            # Test empty string with dict
            self.embeddings.index([(0, {"text": ""}, None)])
            self.assertTrue(self.embeddings.search("test"))

        def testExplain(self):
            """
            Test query explain
            """

            # Test explain with similarity
            result = self.embeddings.explain("feel good story", self.data)[0]
            self.assertEqual(result["text"], self.data[4])
            self.assertEqual(len(result.get("tokens")), 8)

        def testExplainBatch(self):
            """
            Test query explain batch
            """

            # Test explain with query
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            result = self.embeddings.batchexplain(["feel good story"], limit=1)[0][0]
            self.assertEqual(result["text"], self.data[4])
            self.assertEqual(len(result.get("tokens")), 8)

        def testExplainEmpty(self):
            """
            Test query explain with no filtering criteria
            """

            self.assertEqual(self.embeddings.explain("select * from txtai limit 1")[0]["id"], "0")

        def testGenerator(self):
            """
            Test index with a generator
            """

            def documents():
                for uid, text in enumerate(self.data):
                    yield (uid, text, None)

            # Create an index for the list of text
            self.embeddings.index(documents())

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testHybrid(self):
            """
            Test hybrid search
            """

            # Build data array
            data = [(uid, text, None) for uid, text in enumerate(self.data)]

            # Index data with sparse + dense vectors.
            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "hybrid": True, "content": self.backend})
            embeddings.index(data)

            # Run search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.hybrid")

            # Test load/save
            embeddings.save(index)
            embeddings.load(index)

            # Run search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Index data with sparse + dense vectors and unnormalized scores.
            embeddings.config["scoring"]["normalize"] = False
            embeddings.index(data)

            # Run search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Test upsert
            data[0] = (0, "Feel good story: baby panda born", None)
            embeddings.upsert([data[0]])

            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[0][1])

        def testIndex(self):
            """
            Test index
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testIndexTokens(self):
            """
            Test index with tokens
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text.split(), None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testInfo(self):
            """
            Test info
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            output = io.StringIO()
            with contextlib.redirect_stdout(output):
                self.embeddings.info()

            self.assertIn("txtai", output.getvalue())

        def testInstructions(self):
            """
            Test indexing with instruction prefixes.
            """

            embeddings = Embeddings(
                {
                    "path": "sentence-transformers/nli-mpnet-base-v2",
                    "content": self.backend,
                    "instructions": {"query": "query: ", "data": "passage: "},
                }
            )

            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testInvalidData(self):
            """
            Test invalid JSON data
            """

            # Test invalid JSON value
            with self.assertRaises(ValueError):
                self.embeddings.index([(0, {"text": "This is a test", "flag": float("NaN")}, None)])

        def testKeyword(self):
            """
            Test keyword only (sparse) search
            """

            # Build data array
            data = [(uid, text, None) for uid, text in enumerate(self.data)]

            # Index data with sparse keyword vectors
            embeddings = Embeddings({"keyword": True, "content": self.backend})
            embeddings.index(data)

            # Run search
            result = embeddings.search("lottery ticket", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Test count method
            self.assertEqual(embeddings.count(), len(data))

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.keyword")

            # Test load/save
            embeddings.save(index)
            embeddings.load(index)

            # Run search
            result = embeddings.search("lottery ticket", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Update data
            data[0] = (0, "Feel good story: baby panda born", None)
            embeddings.upsert([data[0]])

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[0][1])

        def testMultiData(self):
            """
            Test indexing with multiple data types (text, documents)
            """

            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": self.backend, "batch": len(self.data)})

            # Create an index using mixed data (text and documents)
            data = []
            for uid, text in enumerate(self.data):
                data.append((uid, text, None))
                data.append((uid, {"content": text}, None))

            embeddings.index(data)

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testMultiSave(self):
            """
            Test multiple successive saves
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Save original index
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.insert")
            self.embeddings.save(index)

            # Modify index
            self.embeddings.upsert([(0, "Looking out into the dreadful abyss", None)])

            # Save to a different location
            indexupdate = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.update")
            self.embeddings.save(indexupdate)

            # Save to same location
            self.embeddings.save(index)

            # Test all indexes match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

            self.embeddings.load(index)
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

            self.embeddings.load(indexupdate)
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

        def testNoIndex(self):
            """
            Test an embeddings instance with no available indexes
            """

            # Disable top-level indexing
            embeddings = Embeddings(
                {
                    "content": self.backend,
                    "defaults": False,
                }
            )
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            with self.assertRaises(IndexNotFoundError):
                embeddings.search("select id, text, score from txtai where similar('feel good story')")

        def testNotImplemented(self):
            """
            Test exceptions for non-implemented methods
            """

            db = RDBMS({})

            self.assertRaises(NotImplementedError, db.connect, None)
            self.assertRaises(NotImplementedError, db.getcursor)
            self.assertRaises(NotImplementedError, db.jsonprefix)
            self.assertRaises(NotImplementedError, db.jsoncolumn, None)
            self.assertRaises(NotImplementedError, db.rows)
            self.assertRaises(NotImplementedError, db.addfunctions)

            db = Embedded({})
            self.assertRaises(NotImplementedError, db.copy, None)

        def testObject(self):
            """
            Test object field
            """

            # Encode object
            embeddings = Embeddings({"defaults": False, "content": self.backend, "objects": True})
            embeddings.index([{"object": "binary data".encode("utf-8")}])

            # Decode and test extracted object
            obj = embeddings.search("select object from txtai where id = 0")[0]["object"]
            self.assertEqual(str(obj.getvalue(), "utf-8"), "binary data")

        @patch.dict(os.environ, {"ALLOW_PICKLE": "True"})
        def testPickle(self):
            """
            Test pickle configuration
            """

            embeddings = Embeddings(
                {
                    "format": "pickle",
                    "path": "sentence-transformers/nli-mpnet-base-v2",
                    "content": self.backend,
                }
            )

            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.pickle")

            embeddings.save(index)

            # Check that config exists
            self.assertTrue(os.path.exists(os.path.join(index, "config")))

            # Check that index can be reloaded
            embeddings.load(index)
            self.assertEqual(embeddings.count(), 6)

        def testQuantize(self):
            """
            Test scalar quantization
            """

            # Index data with 1-bit scalar quantization
            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "quantize": 1, "content": self.backend})
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

        def testQueryModel(self):
            """
            Test index
            """

            embeddings = Embeddings(
                {"path": "sentence-transformers/nli-mpnet-base-v2", "content": self.backend, "query": {"path": "neuml/t5-small-txtsql"}}
            )

            # Create an index for the list of text
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = embeddings.search("feel good story with win in text", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testReindex(self):
            """
            Test reindex
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Delete records to test indexids still match
            self.embeddings.delete(([0, 1]))

            # Reindex
            self.embeddings.reindex({"path": "sentence-transformers/nli-mpnet-base-v2"})

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testSave(self):
            """
            Test save
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}")

            self.embeddings.save(index)
            self.embeddings.load(index)

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

            # Test offsets still work after save/load
            self.embeddings.upsert([(0, "Looking out into the dreadful abyss", None)])
            self.assertEqual(self.embeddings.count(), len(self.data))

        def testSettings(self):
            """
            Test custom SQLite settings
            """

            # Index with write-ahead logging enabled
            embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": self.backend, "sqlite": {"wal": True}})

            # Create an index for the list of text
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]

            self.assertEqual(result["text"], self.data[4])

        def testSQL(self):
            """
            Test running a SQL query
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, {"text": text, "length": len(text), "attribute": f"ID{uid}"}, None) for uid, text in enumerate(self.data)])

            # Test similar
            result = self.embeddings.search(
                "select text, score from txtai where similar('feel good story') group by text, score having count(*) > 0 order by score desc", 1
            )[0]
            self.assertEqual(result["text"], self.data[4])

            # Test similar with limits
            result = self.embeddings.search("select * from txtai where similar('feel good story', 1) limit 1")[0]
            self.assertEqual(result["text"], self.data[4])

            # Test similar with offset
            result = self.embeddings.search("select * from txtai where similar('feel good story') offset 1")[0]
            self.assertEqual(result["text"], self.data[5])

            # Test where
            result = self.embeddings.search("select * from txtai where text like '%iceberg%'", 1)[0]
            self.assertEqual(result["text"], self.data[1])

            # Test count
            result = self.embeddings.search("select count(*) from txtai")[0]
            self.assertEqual(list(result.values())[0], len(self.data))

            # Test columns
            result = self.embeddings.search("select id, text, length, data, entry from txtai")[0]
            self.assertEqual(sorted(result.keys()), ["data", "entry", "id", "length", "text"])

            # Test column filtering
            result = self.embeddings.search("select text from txtai where attribute = 'ID4'", 1)[0]
            self.assertEqual(result["text"], self.data[4])

            # Test SQL parse error
            with self.assertRaises(SQLError):
                self.embeddings.search("select * from txtai where bad,query")

        def testSQLBind(self):
            """
            Test SQL statements with bind parameters
            """

            # Create an index for the list of text
            self.embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Test similar clause bind parameters
            result = self.embeddings.search("select id, text, score from txtai where similar(:x)", parameters={"x": "feel good story"})[0]
            self.assertEqual(result["text"], self.data[4])

            # Test similar clause bind and non-bind parameters
            result = self.embeddings.search("select id, text, score from txtai where similar(:x, 0.5)", parameters={"x": "feel good story"})[0]
            self.assertEqual(result["text"], self.data[4])

            # Test where filtering with bind parameters
            result = self.embeddings.search("select * from txtai where text like :x", parameters={"x": "%iceberg%"})[0]
            self.assertEqual(result["text"], self.data[1])

        def testSparse(self):
            """
            Test sparse vector search
            """

            # Build data array
            data = [(uid, text, None) for uid, text in enumerate(self.data)]

            # Index data with sparse vectors
            embeddings = Embeddings({"sparse": "sparse-encoder-testing/splade-bert-tiny-nq", "content": self.backend})
            embeddings.index(data)

            # Run search
            result = embeddings.search("lottery ticket", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Test count method
            self.assertEqual(embeddings.count(), len(data))

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.sparse")

            # Test load/save
            embeddings.save(index)
            embeddings.load(index)

            # Run search
            result = embeddings.search("lottery ticket", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Update data
            data[0] = (0, "Feel good story: baby panda born", None)
            embeddings.upsert([data[0]])

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[0][1])

        def testSubindex(self):
            """
            Test subindex
            """

            # Build data array
            data = [(uid, text, None) for uid, text in enumerate(self.data)]

            # Disable top-level indexing and create subindex
            embeddings = Embeddings(
                {"content": self.backend, "defaults": False, "indexes": {"index1": {"path": "sentence-transformers/nli-mpnet-base-v2"}}}
            )
            embeddings.index(data)

            # Test transform
            self.assertEqual(embeddings.transform("feel good story").shape, (768,))

            # Run search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Run SQL search
            result = embeddings.search("select id, text, score from txtai where similar('feel good story', 10, 0.5)")[0]
            self.assertEqual(result["text"], data[4][1])

            # Test missing index
            with self.assertRaises(IndexNotFoundError):
                embeddings.search("select id, text, score from txtai where similar('feel good story', 'notindex')")

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.subindex")

            # Test load/save
            embeddings.save(index)
            embeddings.load(index)

            # Run search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1])

            # Update data
            data[0] = (0, "Feel good story: baby panda born", None)
            embeddings.upsert([data[0]])

            # Search for best match
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[0][1])

            # Check missing text is set to id when top-level indexing is disabled
            embeddings.upsert([(embeddings.count(), {"content": "empty text"}, None)])
            result = embeddings.search(f"{embeddings.count() - 1}", 1)[0]
            self.assertEqual(result["text"], str(embeddings.count() - 1))

            # Close embeddings
            embeddings.close()

        def testSubindexEmpty(self):
            """
            Test loading an empty subindex
            """

            # Build data array
            data = [(uid, {"column1": text}, None) for uid, text in enumerate(self.data)]

            # Disable top-level indexing and create subindexes
            embeddings = Embeddings(
                {
                    "content": self.backend,
                    "defaults": False,
                    "indexes": {
                        "index1": {"path": "sentence-transformers/nli-mpnet-base-v2", "columns": {"text": "column1"}},
                        "index2": {"path": "sentence-transformers/nli-mpnet-base-v2", "columns": {"text": "column2"}},
                    },
                }
            )
            embeddings.index(data)

            # Generate temp file path
            index = os.path.join(tempfile.gettempdir(), f"embeddings.{self.category()}.subindexempty")

            # Save index
            embeddings.save(index)

            # Test exists
            self.assertTrue(embeddings.exists(index))

            # Load index
            embeddings.load(index)

            # Test search
            result = embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[4][1]["text"])

        def testTerms(self):
            """
            Test extracting keyword terms from query
            """

            result = self.embeddings.terms("select * from txtai where similar('keyword terms')")
            self.assertEqual(result, "keyword terms")

        def testTruncate(self):
            """
            Test dimensionality truncation
            """

            # Truncate vectors to a specified number of dimensions
            embeddings = Embeddings(
                {"path": "sentence-transformers/nli-mpnet-base-v2", "dimensionality": 750, "content": self.backend, "vectors": {"revision": "main"}}
            )
            embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], self.data[4])

        def testUpsert(self):
            """
            Test upsert
            """

            # Build data array
            data = [(uid, text, None) for uid, text in enumerate(self.data)]

            # Reset embeddings for test
            self.embeddings.ann = None
            self.embeddings.database = None

            # Create an index for the list of text
            self.embeddings.upsert(data)

            # Update data
            data[0] = (0, "Feel good story: baby panda born", None)
            self.embeddings.upsert([data[0]])

            # Search for best match
            result = self.embeddings.search("feel good story", 1)[0]
            self.assertEqual(result["text"], data[0][1])

        def testUpsertBatch(self):
            """
            Test upsert batch
            """

            try:
                # Build data array
                data = [(uid, text, None) for uid, text in enumerate(self.data)]

                # Reset embeddings for test
                self.embeddings.ann = None
                self.embeddings.database = None

                # Create an index for the list of text
                self.embeddings.upsert(data)

                # Set batch size to 1
                self.embeddings.config["batch"] = 1

                # Update data
                data[0] = (0, "Feel good story: baby panda born", None)
                data[1] = (0, "Not good news", None)
                self.embeddings.upsert([data[0], data[1]])

                # Search for best match
                result = self.embeddings.search("feel good story", 1)[0]

                self.assertEqual(result["text"], data[0][1])
            finally:
                del self.embeddings.config["batch"]

        def category(self):
            """
            Content backend category.

            Returns:
                category
            """

            return self.__class__.__name__.lower().replace("test", "")



================================================
FILE: test/python/testdatabase/testsql.py
================================================
"""
SQL module tests
"""

import unittest

from txtai.database import DatabaseFactory, SQL, SQLError


class TestSQL(unittest.TestCase):
    """
    Test SQL parsing and generation.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        # Create SQL parser for SQLite
        cls.db = DatabaseFactory.create({"content": True})
        cls.db.initialize()

        cls.sql = SQL(cls.db)

    def testAlias(self):
        """
        Test alias clauses
        """

        self.assertSql("select", "select a as a1 from txtai", "json_extract(data, '$.a') as a1")
        self.assertSql("select", "select a 'a1' from txtai", "json_extract(data, '$.a') 'a1'")
        self.assertSql("select", 'select a "a1" from txtai', "json_extract(data, '$.a') \"a1\"")
        self.assertSql("select", "select a a1 from txtai", "json_extract(data, '$.a') a1")
        self.assertSql(
            "select",
            "select a, b as b1, c, d + 1 as 'd1' from txtai",
            "json_extract(data, '$.a') as \"a\", json_extract(data, '$.b') as b1, "
            + "json_extract(data, '$.c') as \"c\", json_extract(data, '$.d') + 1 as 'd1'",
        )
        self.assertSql("select", "select id as myid from txtai", "s.id as myid")
        self.assertSql("select", "select length(a) t from txtai", "length(json_extract(data, '$.a')) t")

        self.assertSql("where", "select id as myid from txtai where myid != 3 and a != 1", "myid != 3 and json_extract(data, '$.a') != 1")
        self.assertSql("where", "select txt T from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")
        self.assertSql("where", "select txt 'T' from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")
        self.assertSql("where", "select txt \"T\" from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")
        self.assertSql("where", "select txt as T from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")
        self.assertSql("where", "select txt as 'T' from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")
        self.assertSql("where", "select txt as \"T\" from txtai where t LIKE '%abc%'", "t LIKE '%abc%'")

        self.assertSql("groupby", "select id as myid, count(*) from txtai group by myid, a", "myid, json_extract(data, '$.a')")
        self.assertSql("orderby", "select id as myid from txtai order by myid, a", "myid, json_extract(data, '$.a')")

    def testBadSQL(self):
        """
        Test invalid SQL
        """

        with self.assertRaises(SQLError):
            self.db.search("select * from txtai where order by")

        with self.assertRaises(SQLError):
            self.db.search("select * from txtai where groupby order by")

        with self.assertRaises(SQLError):
            self.db.search("select * from txtai where a(1)")

        with self.assertRaises(SQLError):
            self.db.search("select a b c from txtai where id match id")

    def testBracket(self):
        """
        Test bracket expressions
        """

        self.assertSql("select", "select [a] from txtai", "json_extract(data, '$.a') as \"a\"")
        self.assertSql("select", "select [a] ab from txtai", "json_extract(data, '$.a') ab")
        self.assertSql("select", "select [abc] from txtai", "json_extract(data, '$.abc') as \"abc\"")
        self.assertSql("select", "select [id], text, score from txtai", "s.id, text, score")
        self.assertSql("select", "select [ab cd], text, score from txtai", "json_extract(data, '$.ab cd') as \"ab cd\", text, score")
        self.assertSql("select", "select [a[0]] from txtai", "json_extract(data, '$.a[0]') as \"a[0]\"")
        self.assertSql("select", "select [a[0].ab] from txtai", "json_extract(data, '$.a[0].ab') as \"a[0].ab\"")
        self.assertSql("select", "select [a[0].c[0]] from txtai", "json_extract(data, '$.a[0].c[0]') as \"a[0].c[0]\"")
        self.assertSql("select", "select avg([a]) from txtai", "avg(json_extract(data, '$.a')) as \"avg([a])\"")

        self.assertSql("where", "select * from txtai where [a b] < 1 or a > 1", "json_extract(data, '$.a b') < 1 or json_extract(data, '$.a') > 1")
        self.assertSql("where", "select [a[0].c[0]] a from txtai where a < 1", "a < 1")
        self.assertSql("groupby", "select * from txtai group by [a]", "json_extract(data, '$.a')")
        self.assertSql("orderby", "select * from txtai where order by [a]", "json_extract(data, '$.a')")

    def testDistinct(self):
        """
        Test distinct expressions
        """

        # Attributes
        self.assertSql("select", "select distinct id from txtai", "distinct s.id")
        self.assertSql("select", "select distinct id as myid from txtai", "distinct s.id as myid")
        self.assertSql("select", "select distinct a from txtai", "distinct json_extract(data, '$.a') as \"a\"")
        self.assertSql("select", "select distinct a.b from txtai", "distinct json_extract(data, '$.a.b') as \"a.b\"")

        # Bracket expression
        self.assertSql("select", "select distinct [ab cd] from txtai", "distinct json_extract(data, '$.ab cd') as \"distinct[ab cd]\"")

        # Function expression
        self.assertSql("select", "select distinct(id) from txtai", 'distinct(s.id) as "distinct(id)"')
        self.assertSql("select", "select count(distinct id) from txtai", 'count(distinct s.id) as "count(distinct id)"')
        self.assertSql("select", "select count(distinct a) from txtai", "count(distinct json_extract(data, '$.a')) as \"count(distinct a)\"")
        self.assertSql("select", "select count(distinct avg(id)) from txtai", 'count(distinct avg(s.id)) as "count(distinct avg(id))"')
        self.assertSql(
            "select", "select count(distinct avg(a)) from txtai", "count(distinct avg(json_extract(data, '$.a'))) as \"count(distinct avg(a))\""
        )

        # Compound expression
        self.assertSql("select", "select distinct a/1 from txtai", "distinct json_extract(data, '$.a') / 1 as \"a / 1\"")
        self.assertSql("select", "select distinct(a/1) from txtai", "distinct(json_extract(data, '$.a') / 1) as \"distinct(a / 1)\"")

    def testGroupby(self):
        """
        Test group by clauses
        """

        prefix = "select count(*), flag from txtai "

        self.assertSql("groupby", prefix + "group by text", "text")
        self.assertSql("groupby", prefix + "group by distinct(a)", "distinct(json_extract(data, '$.a'))")
        self.assertSql("groupby", prefix + "where a > 1 group by text", "text")

    def testHaving(self):
        """
        Test having clauses
        """

        prefix = "select count(*), flag from txtai "

        self.assertSql("having", prefix + "group by text having count(*) > 1", "count(*) > 1")
        self.assertSql("having", prefix + "where flag = 1 group by text having count(*) > 1", "count(*) > 1")

    def testIsSQL(self):
        """
        Test SQL detection method.
        """

        self.assertTrue(self.sql.issql("select text from txtai where id = 1"))
        self.assertFalse(self.sql.issql(1234))

    def testLimit(self):
        """
        Test limit clauses
        """

        prefix = "select count(*) from txtai "

        self.assertSql("limit", prefix + "limit 100", "100")

    def testOffset(self):
        """
        Test offset clauses
        """

        prefix = "select count(*) from txtai "

        self.assertSql("offset", prefix + "limit 100 offset 50", "50")
        self.assertSql("offset", prefix + "offset 50", "50")

    def testOrderby(self):
        """
        Test order by clauses
        """

        prefix = "select * from txtai "

        self.assertSql("orderby", prefix + "order by id", "s.id")
        self.assertSql("orderby", prefix + "order by id, text", "s.id, text")
        self.assertSql("orderby", prefix + "order by id asc", "s.id asc")
        self.assertSql("orderby", prefix + "order by id desc", "s.id desc")
        self.assertSql("orderby", prefix + "order by id asc, text desc", "s.id asc, text desc")

    def testSelectBasic(self):
        """
        Test basic select clauses
        """

        self.assertSql("select", "select id, indexid, tags from txtai", "s.id, s.indexid, s.tags")
        self.assertSql("select", "select id, indexid, flag from txtai", "s.id, s.indexid, json_extract(data, '$.flag') as \"flag\"")
        self.assertSql("select", "select id, indexid, a.b.c from txtai", "s.id, s.indexid, json_extract(data, '$.a.b.c') as \"a.b.c\"")
        self.assertSql("select", "select 'id', [id], (id) from txtai", "'id', s.id, (s.id)")
        self.assertSql("select", "select * from txtai", "*")

    def testSelectCompound(self):
        """
        Test compound select clauses
        """

        self.assertSql("select", "select a + 1 from txtai", "json_extract(data, '$.a') + 1 as \"a + 1\"")
        self.assertSql("select", "select 1 * a from txtai", "1 * json_extract(data, '$.a') as \"1 * a\"")
        self.assertSql("select", "select a/1 from txtai", "json_extract(data, '$.a') / 1 as \"a / 1\"")
        self.assertSql("select", "select avg(a-b) from txtai", "avg(json_extract(data, '$.a') - json_extract(data, '$.b')) as \"avg(a - b)\"")
        self.assertSql("select", "select distinct(text) from txtai", "distinct(text)")
        self.assertSql("select", "select id, score, (a/2)*3 from txtai", "s.id, score, (json_extract(data, '$.a') / 2) * 3 as \"(a / 2) * 3\"")
        self.assertSql("select", "select id, score, (a/2*3) from txtai", "s.id, score, (json_extract(data, '$.a') / 2 * 3) as \"(a / 2 * 3)\"")
        self.assertSql(
            "select",
            "select func(func2(indexid + 1), a) from txtai",
            "func(func2(s.indexid + 1), json_extract(data, '$.a')) as \"func(func2(indexid + 1), a)\"",
        )
        self.assertSql("select", "select func(func2(indexid + 1), a) a from txtai", "func(func2(s.indexid + 1), json_extract(data, '$.a')) a")
        self.assertSql("select", "select 'prefix' || id from txtai", "'prefix' || s.id as \"'prefix' || id\"")
        self.assertSql("select", "select 'prefix' || id id from txtai", "'prefix' || s.id id")
        self.assertSql("select", "select 'prefix' || a a from txtai", "'prefix' || json_extract(data, '$.a') a")

    def testSimilar(self):
        """
        Test similar functions
        """

        prefix = "select * from txtai "

        self.assertSql("where", prefix + "where similar('abc')", "__SIMILAR__0")
        self.assertSql("similar", prefix + "where similar('abc')", [["abc"]])

        self.assertSql("where", prefix + "where similar('abc') AND id = 1", "__SIMILAR__0 AND s.id = 1")
        self.assertSql("similar", prefix + "where similar('abc')", [["abc"]])

        self.assertSql("where", prefix + "where similar('abc') and similar('def')", "__SIMILAR__0 and __SIMILAR__1")
        self.assertSql("similar", prefix + "where similar('abc') and similar('def')", [["abc"], ["def"]])

        self.assertSql("where", prefix + "where similar('abc', 1000)", "__SIMILAR__0")
        self.assertSql("similar", prefix + "where similar('abc', 1000)", [["abc", "1000"]])

        self.assertSql("where", prefix + "where similar('abc', 1000) and similar('def', 10)", "__SIMILAR__0 and __SIMILAR__1")
        self.assertSql("similar", prefix + "where similar('abc', 1000) and similar('def', 10)", [["abc", "1000"], ["def", "10"]])

        self.assertSql("where", prefix + "where coalesce(similar('abc'), similar('abc'))", "coalesce(__SIMILAR__0, __SIMILAR__1)")
        self.assertSql("similar", prefix + "where coalesce(similar('abc'), similar('abc'))", [["abc"], ["abc"]])

    def testUpper(self):
        """
        Test SQL statements are case insensitive.
        """

        self.assertSql("groupby", "SELECT * FROM TXTAI WHERE a = 1 GROUP BY id", "s.id")
        self.assertSql("orderby", "SELECT * FROM TXTAI WHERE a = 1 ORDER BY id", "s.id")

    def testWhereBasic(self):
        """
        Test basic where clauses
        """

        prefix = "select * from txtai "

        self.assertSql("where", prefix + "where a = b", "json_extract(data, '$.a') = json_extract(data, '$.b')")
        self.assertSql("where", prefix + "where abc = def", "json_extract(data, '$.abc') = json_extract(data, '$.def')")
        self.assertSql("where", prefix + "where a = b.value", "json_extract(data, '$.a') = json_extract(data, '$.b.value')")
        self.assertSql("where", prefix + "where a = 1", "json_extract(data, '$.a') = 1")
        self.assertSql("where", prefix + "WHERE 1 = a", "1 = json_extract(data, '$.a')")
        self.assertSql("where", prefix + "WHERE a LIKE 'abc'", "json_extract(data, '$.a') LIKE 'abc'")
        self.assertSql("where", prefix + "WHERE a NOT LIKE 'abc'", "json_extract(data, '$.a') NOT LIKE 'abc'")
        self.assertSql("where", prefix + "WHERE a IN (1, 2, 3, b)", "json_extract(data, '$.a') IN (1, 2, 3, json_extract(data, '$.b'))")
        self.assertSql("where", prefix + "WHERE a is not null", "json_extract(data, '$.a') is not null")
        self.assertSql("where", prefix + "WHERE score >= 0.15", "score >= 0.15")

    def testWhereCompound(self):
        """
        Test compound where clauses
        """

        prefix = "select * from txtai "

        self.assertSql("where", prefix + "where a > (b + 1)", "json_extract(data, '$.a') > (json_extract(data, '$.b') + 1)")
        self.assertSql("where", prefix + "where a > func('abc')", "json_extract(data, '$.a') > func('abc')")
        self.assertSql(
            "where", prefix + "where (id = 1 or id = 2) and a like 'abc'", "(s.id = 1 or s.id = 2) and json_extract(data, '$.a') like 'abc'"
        )
        self.assertSql(
            "where",
            prefix + "where a > f(d(b, c, 1),1)",
            "json_extract(data, '$.a') > f(d(json_extract(data, '$.b'), json_extract(data, '$.c'), 1), 1)",
        )
        self.assertSql("where", prefix + "where (id = 1 AND id = 2) OR indexid = 3", "(s.id = 1 AND s.id = 2) OR s.indexid = 3")
        self.assertSql("where", prefix + "where f(id) = b(id)", "f(s.id) = b(s.id)")
        self.assertSql("where", prefix + "WHERE f(id)", "f(s.id)")

    def assertSql(self, clause, query, expected):
        """
        Helper method to assert a query clause is as expected.

        Args:
            clause: clause to select
            query: input query
            expected: expected transformed query value
        """

        self.assertEqual(self.sql(query)[clause], expected)



================================================
FILE: test/python/testdatabase/testsqlite.py
================================================
"""
SQLite module tests
"""

from txtai.embeddings import Embeddings

from .testrdbms import Common


# pylint: disable=R0904
class TestSQLite(Common.TestRDBMS):
    """
    Embeddings with content stored in SQLite tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        # Content backend
        cls.backend = "sqlite"

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": cls.backend})

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def testFunction(self):
        """
        Test custom functions
        """

        embeddings = Embeddings(
            {
                "path": "sentence-transformers/nli-mpnet-base-v2",
                "content": self.backend,
                "functions": [{"name": "length", "function": "testdatabase.testsqlite.length"}],
            }
        )

        # Create an index for the list of text
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        # Search for best match
        result = embeddings.search("select length(text) length from txtai where id = 0", 1)[0]

        self.assertEqual(result["length"], 39)


def length(text):
    """
    Custom SQL function.
    """

    return len(text)



================================================
FILE: test/python/testmodels/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testmodels/testmodels.py
================================================
"""
Models module tests
"""

import unittest

from unittest.mock import patch

import torch

from txtai.models import Models


class TestModels(unittest.TestCase):
    """
    Models tests.
    """

    @patch("torch.cuda.is_available")
    def testDeviceid(self, cuda):
        """
        Test the deviceid method
        """

        cuda.return_value = True
        self.assertEqual(Models.deviceid(True), 0)
        self.assertEqual(Models.deviceid(False), -1)
        self.assertEqual(Models.deviceid(0), 0)
        self.assertEqual(Models.deviceid(1), 1)

        # Test direct torch device
        # pylint: disable=E1101
        self.assertEqual(Models.deviceid(torch.device("cpu")), torch.device("cpu"))

        cuda.return_value = False
        self.assertEqual(Models.deviceid(True), -1)
        self.assertEqual(Models.deviceid(False), -1)
        self.assertEqual(Models.deviceid(0), -1)
        self.assertEqual(Models.deviceid(1), -1)

    def testDevice(self):
        """
        Test the device method
        """

        # pylint: disable=E1101
        self.assertEqual(Models.device("cpu"), torch.device("cpu"))
        self.assertEqual(Models.device(torch.device("cpu")), torch.device("cpu"))



================================================
FILE: test/python/testmodels/testpooling.py
================================================
"""
Pooling module tests
"""

import unittest

from txtai.models import Models, ClsPooling, MeanPooling, PoolingFactory


class TestPooling(unittest.TestCase):
    """
    Pooling tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize device
        """

        # Device id
        cls.device = Models.deviceid(True)

    def testCLS(self):
        """
        Test CLS pooling
        """

        # Test CLS pooling
        pooling = PoolingFactory.create({"path": "flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-cls_dot", "device": self.device})
        self.assertEqual(type(pooling), ClsPooling)

        pooling = PoolingFactory.create({"method": "clspooling", "path": "sentence-transformers/nli-mpnet-base-v2", "device": self.device})
        self.assertEqual(type(pooling), ClsPooling)

        # Test CLS pooling encoding
        self.assertEqual(pooling.encode(["test"])[0].shape, (768,))

    def testLength(self):
        """
        Test pooling with max_seq_length
        """

        # Test reading max_seq_length parmaeter
        pooling = PoolingFactory.create({"path": "sentence-transformers/nli-mpnet-base-v2", "device": self.device, "maxlength": True})
        self.assertEqual(pooling.maxlength, 75)

        # Test specified maxlength
        pooling = PoolingFactory.create({"path": "sentence-transformers/nli-mpnet-base-v2", "device": self.device, "maxlength": 256})
        self.assertEqual(pooling.maxlength, 256)

        # Test max_seq_length is ignored when parameter is omitted
        pooling = PoolingFactory.create({"path": "sentence-transformers/nli-mpnet-base-v2", "device": self.device})
        self.assertEqual(pooling.maxlength, 512)

        # Test maxlength when max_seq_length not present
        pooling = PoolingFactory.create({"path": "hf-internal-testing/tiny-random-gpt2", "device": self.device, "maxlength": True})
        self.assertEqual(pooling.maxlength, 1024)

    def testMean(self):
        """
        Test mean pooling
        """

        # Test mean pooling
        pooling = PoolingFactory.create({"path": "sentence-transformers/nli-mpnet-base-v2", "device": self.device})
        self.assertEqual(type(pooling), MeanPooling)

        pooling = PoolingFactory.create(
            {"method": "meanpooling", "path": "flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-cls_dot", "device": self.device}
        )
        self.assertEqual(type(pooling), MeanPooling)

    def testMuvera(self):
        """
        Test late pooling with MUVERA fixed dimensional encoding
        """

        # Test MUVERA encoding
        for model in ["neuml/colbert-bert-tiny", "neuml/pylate-bert-tiny"]:
            # Test defaults
            pooling = PoolingFactory.create({"path": model, "device": self.device})
            self.assertEqual(pooling.encode(["test"], category="query").shape, (1, 10240))

            # Test custom settings
            pooling = PoolingFactory.create(
                {"path": model, "device": self.device, "modelargs": {"muvera": {"repetitions": 5, "hashes": 2, "projection": 8}}}
            )
            self.assertEqual(pooling.encode(["test"], category="data").shape, (1, 160))



================================================
FILE: test/python/testpipeline/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testaudio/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testaudio/testaudiomixer.py
================================================
"""
AudioMixer module tests
"""

import unittest

import numpy as np

from txtai.pipeline import AudioMixer


class TestAudioStream(unittest.TestCase):
    """
    AudioStream tests.
    """

    def testAudioStream(self):
        """
        Test mixing audio streams
        """

        audio1 = np.random.rand(2, 5000), 100
        audio2 = np.random.rand(2, 5000), 100

        mixer = AudioMixer()
        audio, rate = mixer((audio1, audio2))

        self.assertEqual(audio.shape, (2, 5000))
        self.assertEqual(rate, 100)



================================================
FILE: test/python/testpipeline/testaudio/testaudiostream.py
================================================
"""
AudioStream module tests
"""

import unittest

from unittest.mock import patch

import soundfile as sf

from txtai.pipeline import AudioStream

# pylint: disable=C0411
from utils import Utils


class TestAudioStream(unittest.TestCase):
    """
    AudioStream tests.
    """

    @patch("sounddevice.play")
    def testAudioStream(self, play):
        """
        Test playing audio
        """

        play.return_value = True

        # Read audio data
        audio, rate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

        stream = AudioStream()
        self.assertIsNotNone(stream([(audio, rate), AudioStream.COMPLETE]))

        # Wait for completion
        stream.wait()



================================================
FILE: test/python/testpipeline/testaudio/testmicrophone.py
================================================
"""
Microphone module tests
"""

import unittest

from unittest.mock import patch

import numpy as np
import soundfile as sf

from txtai.pipeline import Microphone

# pylint: disable=C0411
from utils import Utils


class TestMicrophone(unittest.TestCase):
    """
    Microphone tests.
    """

    # pylint: disable=C0115,C0116
    @patch("sounddevice.RawInputStream")
    def testMicrophone(self, inputstream):
        """
        Test listening to microphone
        """

        class RawInputStream:
            def __init__(self, **kwargs):
                self.args = kwargs

                # Read audio data
                self.index, self.passes = 0, 0
                audio, self.samplerate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

                # Convert data to PCM
                self.audio = self.int16(audio)

                # Start with random data to test that speech is not detected
                self.data = np.concatenate((self.audio * 50, np.zeros(shape=self.audio.shape, dtype=np.int16)))

            def start(self):
                pass

            def stop(self):
                pass

            def read(self, size):
                # Get chunk
                chunk = self.data[self.index : self.index + size]
                self.index += size

                # Initial pass is random data, 2nd pass is speech data
                if self.index > len(self.data):
                    if not self.passes:
                        self.index, self.passes = 0, self.passes + 1
                        self.data = self.audio
                    elif self.index >= len(self.audio) * 10:
                        # Break out of loop if speech continues to not be detected
                        raise IOError("Data exhausted")

                return chunk, False

            def int16(self, data):
                i = np.iinfo(np.int16)
                absmax = 2 ** (i.bits - 1)
                offset = i.min + absmax
                return (data * absmax + offset).clip(i.min, i.max).astype(np.int16)

        # Mock input stream
        inputstream.side_effect = RawInputStream

        # Create microphone pipeline and read data
        pipeline = Microphone()
        data, rate = pipeline()

        # Validate sample rate and length of data
        self.assertEqual(len(data), 91220)
        self.assertEqual(rate, 16000)



================================================
FILE: test/python/testpipeline/testaudio/testtexttoaudio.py
================================================
"""
TextToAudio module tests
"""

import unittest

from txtai.pipeline import TextToAudio


class TestTextToAudio(unittest.TestCase):
    """
    TextToAudio tests.
    """

    def testTextToAudio(self):
        """
        Test generating audio for text
        """

        tta = TextToAudio("hf-internal-testing/tiny-random-MusicgenForConditionalGeneration")

        # Check that data is generated
        audio, rate = tta("This is a test")

        self.assertGreater(len(audio), 0)
        self.assertEqual(rate, 24000)



================================================
FILE: test/python/testpipeline/testaudio/testtexttospeech.py
================================================
"""
TextToSpeech module tests
"""

import unittest

from unittest.mock import patch

from txtai.pipeline import TextToSpeech


class TestTextToSpeech(unittest.TestCase):
    """
    TextToSpeech tests.
    """

    def testESPnet(self):
        """
        Test generating speech for text with an ESPnet model
        """

        tts = TextToSpeech()

        # Check that data is generated
        speech, rate = tts("This is a test")

        self.assertGreater(len(speech), 0)
        self.assertEqual(rate, 22050)

    def testKokoro(self):
        """
        Test generating speech for text with a Kokoro model
        """

        tts = TextToSpeech("neuml/kokoro-int8-onnx", maxtokens=2)

        # Check that data is generated
        speech, rate = tts("This is a test")

        self.assertGreater(len(speech), 0)
        self.assertEqual(rate, 22050)

    @patch("onnxruntime.get_available_providers")
    @patch("torch.cuda.is_available")
    def testProviders(self, cuda, providers):
        """
        Test that GPU provider is detected
        """

        # Test CUDA and onnxruntime-gpu installed
        cuda.return_value = True
        providers.return_value = ["CUDAExecutionProvider", "CPUExecutionProvider"]

        tts = TextToSpeech()
        self.assertEqual(tts.providers()[0][0], "CUDAExecutionProvider")

    def testSpeechT5(self):
        """
        Test generating speech for text with a SpeechT5 model
        """

        tts = TextToSpeech("neuml/txtai-speecht5-onnx")

        # Check that data is generated
        speech, rate = tts("This is a test")

        self.assertGreater(len(speech), 0)
        self.assertEqual(rate, 22050)

    def testStreaming(self):
        """
        Test streaming speech generation
        """

        tts = TextToSpeech()

        # Check that data is generated
        speech, rate = list(tts("This is a test. And another".split(), stream=True))[0]

        # Check that data is generated
        self.assertGreater(len(speech), 0)
        self.assertEqual(rate, 22050)



================================================
FILE: test/python/testpipeline/testaudio/testtranscription.py
================================================
"""
Transcription module tests
"""

import unittest

import numpy as np
import soundfile as sf

from scipy import signal

from txtai.pipeline import Transcription

# pylint: disable=C0411
from utils import Utils


class TestTranscription(unittest.TestCase):
    """
    Transcription tests.
    """

    def testArray(self):
        """
        Test audio data to text transcription
        """

        transcribe = Transcription()

        # Read audio data
        raw, samplerate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

        self.assertEqual(transcribe((raw, samplerate)), "Make huge profits without working make up to one hundred thousand dollars a day")
        self.assertEqual(transcribe(raw, samplerate), "Make huge profits without working make up to one hundred thousand dollars a day")

    def testChunks(self):
        """
        Test splitting transcription into chunks
        """

        transcribe = Transcription()

        result = transcribe(Utils.PATH + "/Make_huge_profits.wav", join=False)[0]

        self.assertIsInstance(result["raw"], np.ndarray)
        self.assertIsNotNone(result["rate"])
        self.assertEqual(result["text"], "Make huge profits without working make up to one hundred thousand dollars a day")

    def testFile(self):
        """
        Test audio file to text transcription
        """

        transcribe = Transcription()

        self.assertEqual(
            transcribe(Utils.PATH + "/Make_huge_profits.wav"), "Make huge profits without working make up to one hundred thousand dollars a day"
        )

    def testGenerateArguments(self):
        """
        Test transcription with generation keyword arguments
        """

        transcribe = Transcription()

        # Read audio data
        raw, samplerate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

        self.assertEqual(
            transcribe(raw, samplerate, language="English", task="transcribe"),
            "Make huge profits without working make up to one hundred thousand dollars a day",
        )

    def testResample(self):
        """
        Test resampled audio file to text transcription
        """

        transcribe = Transcription()

        # Read audio data
        raw, samplerate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

        # Resample for testing
        samples = round(len(raw) * float(22050) / samplerate)
        raw, samplerate = signal.resample(raw, samples), 22050

        self.assertEqual(transcribe(raw, samplerate), "Make huge profits without working make up to one hundred thousand dollars a day")

    def testStereo(self):
        """
        Test audio file in stereo to text transcription
        """

        transcribe = Transcription()

        # Read audio data
        raw, samplerate = sf.read(Utils.PATH + "/Make_huge_profits.wav")

        # Convert mono to stereo
        raw = np.column_stack((raw, raw))

        self.assertEqual(transcribe(raw, samplerate), "Make huge profits without working make up to one hundred thousand dollars a day")



================================================
FILE: test/python/testpipeline/testdata/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testdata/testfiletohtml.py
================================================
"""
FileToHTML module tests
"""

import os
import unittest

from unittest.mock import patch

from txtai.pipeline.data.filetohtml import Tika


class TestFileToHTML(unittest.TestCase):
    """
    FileToHTML tests.
    """

    @patch.dict(os.environ, {"TIKA_JAVA": "1112444abc"})
    def testTika(self):
        """
        Test the Tika.available returns False when Java is not available
        """

        self.assertFalse(Tika.available())



================================================
FILE: test/python/testpipeline/testdata/testtabular.py
================================================
"""
Tabular module tests
"""

import unittest

from txtai.pipeline import Tabular

# pylint: disable=C0411
from utils import Utils


class TestTabular(unittest.TestCase):
    """
    Tabular tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single tabular instance
        """

        cls.tabular = Tabular("id", ["text"])

    def testContent(self):
        """
        Test parsing additional content
        """

        tabular = Tabular("id", ["text"], True)

        row = {"id": 0, "text": "This is a test", "flag": 1}

        # When content is enabled, both (uid, text, tags) and (uid, data, tags) rows are generated
        # given that data doesn't necessarily include the text to index
        rows = tabular([row])
        uid, data, _ = rows[1]

        # Data should contain the entire input row
        self.assertEqual(uid, 0)
        self.assertEqual(data, row)

        # Only select flag field
        tabular.content = ["flag"]
        rows = tabular([row])
        uid, data, _ = rows[1]

        # Data should only contain a single field, flag
        self.assertEqual(uid, 0)
        self.assertTrue(list(data.keys()) == ["flag"])
        self.assertEqual(data["flag"], 1)

    def testCSV(self):
        """
        Test parsing a CSV file
        """

        rows = self.tabular([Utils.PATH + "/tabular.csv"])
        uid, text, _ = rows[0][0]

        self.assertEqual(uid, 0)
        self.assertEqual(text, "The first sentence")

    def testDict(self):
        """
        Test parsing a dict
        """

        rows = self.tabular([{"id": 0, "text": "This is a test"}])
        uid, text, _ = rows[0]

        self.assertEqual(uid, 0)
        self.assertEqual(text, "This is a test")

    def testList(self):
        """
        Test parsing a list
        """

        rows = self.tabular([[{"id": 0, "text": "This is a test"}]])
        uid, text, _ = rows[0][0]

        self.assertEqual(uid, 0)
        self.assertEqual(text, "This is a test")

    def testMissingColumns(self):
        """
        Test rows with uneven or missing columns
        """

        tabular = Tabular("id", ["text"], True)

        rows = tabular([{"id": 0, "text": "This is a test", "metadata": "meta"}, {"id": 1, "text": "This is a test"}])

        # When content is enabled both (id, text, tag) and (id, data, tag) tuples are generated given that
        # data doesn't necessarily include the text to index
        _, data, _ = rows[3]

        self.assertIsNone(data["metadata"])

    def testNoColumns(self):
        """
        Test creating text without specifying columns
        """

        tabular = Tabular("id")
        rows = tabular([{"id": 0, "text": "This is a test", "summary": "Describes text in more detail"}])
        uid, text, _ = rows[0]

        self.assertEqual(uid, 0)
        self.assertEqual(text, "This is a test. Describes text in more detail")



================================================
FILE: test/python/testpipeline/testdata/testtextractor.py
================================================
"""
Textractor module tests
"""

import platform
import unittest

from txtai.pipeline import Textractor

# pylint: disable=C0411
from utils import Utils


class TestTextractor(unittest.TestCase):
    """
    Textractor tests.
    """

    def testClean(self):
        """
        Test text cleaning method
        """

        # Default text cleaning
        textractor = Textractor()
        self.assertEqual(textractor(" a  b  c "), "a b c")

        # Require text to be minlength
        textractor = Textractor(minlength=10)
        self.assertEqual(textractor(" a  b  c "), None)

        # Disable text cleaning
        textractor = Textractor(cleantext=False, minlength=10)
        self.assertEqual(textractor(" a  b  c "), " a  b  c ")

    def testChonkie(self):
        """
        Test a chonkie chunker
        """

        # Test chonkie chunking
        textractor = Textractor(chunker="sentence", chunk_size=5, chunk_overlap=0)
        self.assertEqual(textractor("This is a test. And another test."), ["This is a test.", "And another test."])

        # Test bad chunker throws an exception
        with self.assertRaises(AttributeError):
            textractor = Textractor(chunker="badchunker")

    def testDefault(self):
        """
        Test default text extraction
        """

        # Text input
        textractor = Textractor(backend=None)
        text = textractor(Utils.PATH + "/tabular.csv")
        self.assertEqual(len(text), 125)

        # Markdown input
        textractor = Textractor(sections=True)
        sections = textractor("# Heading 1\nText1\n\n# Heading 2\nText2\n")

        # Check number of sections is as expected
        self.assertEqual(len(sections), 2)

    @unittest.skipIf(platform.system() == "Darwin", "Docling skipped on macOS to avoid MPS issues")
    def testDocling(self):
        """
        Test docling backend
        """

        textractor = Textractor(backend="docling")

        # Extract text and check for Markdown formatting
        text = textractor(Utils.PATH + "/article.pdf")
        self.assertTrue("## Introducing txtai" in text)

    def testLines(self):
        """
        Test extraction to lines
        """

        textractor = Textractor(lines=True)

        # Extract text as lines
        lines = textractor(Utils.PATH + "/article.pdf")

        # Check number of lines is as expected
        self.assertEqual(len(lines), 35)

    def testHTML(self):
        """
        Test HTML to Markdown
        """

        # Headings
        self.assertMarkdown("<h1>This is a test</h1>", "# This is a test")
        self.assertMarkdown("<h6>This is a test</h6>", "###### This is a test")

        # Blockquotes
        self.assertMarkdown("<blockquote>This is a test</blockquote>", "> This is a test")

        # Lists
        self.assertMarkdown("<ul><li>Test1</li><li>Test2</li></ul>", "- Test1\n- Test2")
        self.assertMarkdown("<ol><li>Test1</li><li>Test2</li></ol>", "1. Test1\n2. Test2")

        # Code
        self.assertMarkdown("<code>This is a test</code>", "```\nThis is a test\n```")
        self.assertMarkdown("<pre>This is a test</pre>", "```\nThis is a test\n```")

        # Tables
        self.assertMarkdown(
            "<table><tr><th>Header1</th><th>Header2</th></tr><tr><td>Test1</td><td>Test2</td></tr></table>",
            "|Header1|Header2|\n|---|---|\n|Test1|Test2|",
        )

        # Ignore list
        self.assertMarkdown("<aside>This is a test</aside>", "")

        # Text formatting
        self.assertMarkdown("<p>This is a test</p>", "This is a test")
        self.assertMarkdown("<p>This is a <b>test</b</p>", "This is a **test**")
        self.assertMarkdown("<p>This is a <strong>test</strong></p>", "This is a **test**")
        self.assertMarkdown("<p>This is a <i>test</i></p>", "This is a *test*")
        self.assertMarkdown("<p>This is a <em>test</em></p>", "This is a *test*")
        self.assertMarkdown("<p>This is a <a href='link'>test</a>", "This is a [test](link)")

        # Collapse to outer tag
        self.assertMarkdown("<p>This is a <strong><em>test</em></strong></p>", "This is a **test**")
        self.assertMarkdown("<p>This is a <em><strong>test</strong></em></p>", "This is a *test*")

    def testParagraphs(self):
        """
        Test extraction to paragraphs
        """

        textractor = Textractor(paragraphs=True)

        # Extract text as paragraphs
        paragraphs = textractor(Utils.PATH + "/article.pdf")

        # Check number of paragraphs is as expected
        self.assertEqual(len(paragraphs), 11)

    def testSections(self):
        """
        Test extraction to sections
        """

        textractor = Textractor(sections=True)

        # Extract as sections
        sections = textractor(Utils.PATH + "/document.pdf")

        # Check number of sections is as expected
        self.assertEqual(len(sections), 3)

    def testSentences(self):
        """
        Test extraction to sentences
        """

        textractor = Textractor(sentences=True)

        # Extract text as sentences
        sentences = textractor(Utils.PATH + "/article.pdf")

        # Check number of sentences is as expected
        self.assertEqual(len(sentences), 17)

    def testSingle(self):
        """
        Test a single extraction with no tokenization of the results
        """

        textractor = Textractor()

        # Extract text as a single block
        text = textractor(Utils.PATH + "/article.pdf")

        # Check length of text is as expected
        self.assertEqual(len(text), 2471)

    def testTable(self):
        """
        Test table extraction
        """

        textractor = Textractor()

        # Extract text as a single block
        for name in ["document.docx", "spreadsheet.xlsx"]:
            text = textractor(f"{Utils.PATH}/{name}")

            # Check for table header
            self.assertTrue("|---|" in text)

    def testTikaFlag(self):
        """
        Test legacy tika flag
        """

        textractor = Textractor(tika=True)
        self.assertIsNotNone(textractor.html)

        textractor = Textractor(tika=False)
        self.assertIsNone(textractor.html)

    def testTuples(self):
        """
        Test output tuples
        """

        # Default text cleaning
        textractor = Textractor(tuples=True)

        path, text = textractor(Utils.PATH + "/article.pdf")
        self.assertEqual(path, Utils.PATH + "/article.pdf")
        self.assertEqual(len(text), 2471)

    def testURL(self):
        """
        Test parsing a remote URL
        """

        # Test parsing URLs for each backend
        for backend in ["docling", "tika"]:
            textractor = Textractor(backend=backend)
            text = textractor("https://github.com/neuml/txtai")
            self.assertTrue("txtai is an all-in-one AI framework" in text)

    def assertMarkdown(self, html, expected):
        """
        Helper method to assert generated markdown is as expected.

        Args:
            html: input html snippet
            expected: expected markdown text
        """

        textractor = Textractor()
        self.assertEqual(textractor(f"<html><body>{html}</body></html>"), expected)



================================================
FILE: test/python/testpipeline/testdata/testtokenizer.py
================================================
"""
Tokenizer module tests
"""

import unittest

from txtai.pipeline import Tokenizer


class TestTokenizer(unittest.TestCase):
    """
    Tokenizer tests.
    """

    def testAlphanumTokenize(self):
        """
        Test alphanumeric tokenization
        """

        # Alphanumeric tokenization through backwards compatible static method
        self.assertEqual(Tokenizer.tokenize("Y this is a test!"), ["test"])
        self.assertEqual(Tokenizer.tokenize("abc123 ABC 123"), ["abc123", "abc"])

    def testEmptyTokenize(self):
        """
        Test handling empty and None inputs
        """

        # Test that parser can handle empty or None strings
        self.assertEqual(Tokenizer.tokenize(""), [])
        self.assertEqual(Tokenizer.tokenize(None), None)

    def testStandardTokenize(self):
        """
        Test standard tokenization
        """

        # Default standard tokenizer parameters
        tokenizer = Tokenizer()

        # Define token tests
        tests = [
            ("Y this is a test!", ["y", "this", "is", "a", "test"]),
            ("abc123 ABC 123", ["abc123", "abc", "123"]),
            ("Testing hy-phenated words", ["testing", "hy", "phenated", "words"]),
            ("111-111-1111", ["111", "111", "1111"]),
            ("Test.1234", ["test", "1234"]),
        ]

        # Run through tests
        for test, result in tests:
            # Unicode Text Segmentation per Unicode Annex #29
            self.assertEqual(tokenizer(test), result)



================================================
FILE: test/python/testpipeline/testimage/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testimage/testcaption.py
================================================
"""
Caption module tests
"""

import unittest

from PIL import Image

from txtai.pipeline import Caption

# pylint: disable=C0411
from utils import Utils


class TestCaption(unittest.TestCase):
    """
    Caption tests.
    """

    def testCaption(self):
        """
        Test captions
        """

        caption = Caption()
        self.assertEqual(caption(Image.open(Utils.PATH + "/books.jpg")), "a book shelf filled with books and a stack of books")



================================================
FILE: test/python/testpipeline/testimage/testimagehash.py
================================================
"""
ImageHash module tests
"""

import unittest

from PIL import Image

from txtai.pipeline import ImageHash

# pylint: disable=C0411
from utils import Utils


class TestImageHash(unittest.TestCase):
    """
    ImageHash tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Caches an image to hash
        """

        cls.image = Image.open(Utils.PATH + "/books.jpg")

    def testArray(self):
        """
        Test numpy return type
        """

        ihash = ImageHash(strings=False)
        self.assertEqual(ihash(self.image).shape, (64,))

    def testAverage(self):
        """
        Test average hash
        """

        ihash = ImageHash("average")
        self.assertIn(ihash(self.image), ["0859dd04bfbfbf00", "0859dd04ffbfbf00"])

    def testColor(self):
        """
        Test color hash
        """

        ihash = ImageHash("color")
        self.assertIn(ihash(self.image), ["1ffffe02000e000c0e0000070000", "1ff8fe03000e00070e0000070000"])

    def testDifference(self):
        """
        Test difference hash
        """

        ihash = ImageHash("difference")
        self.assertEqual(ihash(self.image), "d291996d6969686a")

    def testPerceptual(self):
        """
        Test perceptual hash
        """

        ihash = ImageHash("perceptual")
        self.assertEqual(ihash(self.image), "8be8418577b331b9")

    def testWavelet(self):
        """
        Test wavelet hash
        """

        ihash = ImageHash("wavelet")
        self.assertEqual(ihash(Utils.PATH + "/books.jpg"), "68015d85bfbf3f00")



================================================
FILE: test/python/testpipeline/testimage/testobjects.py
================================================
"""
Objects module tests
"""

import unittest

from txtai.pipeline import Objects

# pylint: disable=C0411
from utils import Utils


class TestObjects(unittest.TestCase):
    """
    Object detection tests.
    """

    def testClassification(self):
        """
        Test object detection using an image classification model
        """

        objects = Objects(classification=True, threshold=0.3)
        self.assertEqual(objects(Utils.PATH + "/books.jpg")[0][0], "library")

    def testDetection(self):
        """
        Test object detection using an object detection model
        """

        objects = Objects()
        self.assertEqual(objects(Utils.PATH + "/books.jpg")[0][0], "book")

    def testFlatten(self):
        """
        Test object detection using an object detection model, flatten to return only objects
        """

        objects = Objects()
        self.assertEqual(objects(Utils.PATH + "/books.jpg", flatten=True)[0], "book")



================================================
FILE: test/python/testpipeline/testllm/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testllm/testgenerator.py
================================================
"""
Generator module tests
"""

import unittest

from txtai.pipeline import Generator


class TestGenerator(unittest.TestCase):
    """
    Sequences tests.
    """

    def testGeneration(self):
        """
        Test text pipeline generation
        """

        model = Generator("hf-internal-testing/tiny-random-gpt2")
        start = "Hello, how are"

        # Test that text is generated
        self.assertIsNotNone(model(start))



================================================
FILE: test/python/testpipeline/testllm/testlitellm.py
================================================
"""
LiteLLM module tests
"""

import json
import os
import time
import unittest
import uuid

from unittest.mock import patch

from http.server import HTTPServer, BaseHTTPRequestHandler
from threading import Thread

from txtai.pipeline import LLM


class RequestHandler(BaseHTTPRequestHandler):
    """
    Test HTTP handler.
    """

    def do_POST(self):
        """
        POST request handler.
        """

        # Parse input headers
        length = int(self.headers["content-length"])
        data = json.loads(self.rfile.read(length))

        if data.get("stream"):
            # Mock streaming response
            content = "application/octet-stream"
            response = (
                "data: "
                + json.dumps(
                    {
                        "id": str(uuid.uuid4()),
                        "object": "chat.completion.chunk",
                        "created": int(time.time() * 1000),
                        "model": "test",
                        "choices": [{"id": 0, "delta": {"content": "blue"}}],
                    }
                )
                + "\n\ndata: [DONE]\n\n"
            )
        else:
            # Mock standard response
            content = "application/json"
            response = json.dumps(
                {
                    "id": str(uuid.uuid4()),
                    "object": "chat.completion",
                    "created": int(time.time() * 1000),
                    "model": "test",
                    "choices": [{"id": 0, "message": {"role": "assistant", "content": "blue"}, "finish_reason": "stop"}],
                }
            )

        # Encode response as bytes
        response = response.encode("utf-8")

        self.send_response(200)
        self.send_header("content-type", content)
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()


class TestLiteLLM(unittest.TestCase):
    """
    LiteLLM tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create mock http server.
        """

        cls.httpd = HTTPServer(("127.0.0.1", 8000), RequestHandler)

        server = Thread(target=cls.httpd.serve_forever, daemon=True)
        server.start()

    @classmethod
    def tearDownClass(cls):
        """
        Shutdown mock http server.
        """

        cls.httpd.shutdown()

    @patch.dict(os.environ, {"OPENAI_API_KEY": "test"})
    def testGeneration(self):
        """
        Test generation with LiteLLM
        """

        # Test model generation with LiteLLM
        model = LLM("openai/gpt-4o", api_base="http://127.0.0.1:8000")
        self.assertEqual(model("The sky is"), "blue")

        # Test default role
        self.assertEqual(model("The sky is", defaultrole="user"), "blue")

        # Test streaming
        self.assertEqual(" ".join(x for x in model("The sky is", stream=True)), "blue")

        # Test vision
        self.assertEqual(model.isvision(), False)



================================================
FILE: test/python/testpipeline/testllm/testllama.py
================================================
"""
Llama module tests
"""

import unittest

from unittest.mock import patch

from txtai.pipeline import LLM


class TestLlama(unittest.TestCase):
    """
    llama.cpp tests.
    """

    @patch("llama_cpp.Llama")
    def testContext(self, llama):
        """
        Test n_ctx with llama.cpp
        """

        class Llama:
            """
            Mock llama.cpp instance to test invalid context
            """

            def __init__(self, **kwargs):
                if kwargs.get("n_ctx") == 0 or kwargs.get("n_ctx", 0) >= 10000:
                    raise ValueError("Failed to create context")

                # Save parameters
                self.params = kwargs

        # Mock llama.cpp instance
        llama.side_effect = Llama

        # Model to test
        path = "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/tinyllama-1.1b-chat-v0.3.Q2_K.gguf"

        # Test omitting n_ctx falls back to default settings
        llm = LLM(path)
        self.assertNotIn("n_ctx", llm.generator.llm.params)

        # Test n_ctx=0 falls back to default settings
        llm = LLM(path, n_ctx=0)
        self.assertNotIn("n_ctx", llm.generator.llm.params)

        # Test n_ctx manually set
        llm = LLM(path, n_ctx=1024)
        self.assertEqual(llm.generator.llm.params["n_ctx"], 1024)

        # Mock a value for n_ctx that's too big
        with self.assertRaises(ValueError):
            llm = LLM(path, n_ctx=10000)

    def testGeneration(self):
        """
        Test generation with llama.cpp
        """

        # Test model generation with llama.cpp
        model = LLM("TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/tinyllama-1.1b-chat-v0.3.Q2_K.gguf", chat_format="chatml")

        # Test with prompt
        self.assertEqual(model("2 + 2 = ", maxlength=10, seed=0, stop=["."])[0], "4")

        # Test with list of messages
        messages = [{"role": "system", "content": "You are a helpful assistant. You answer math problems."}, {"role": "user", "content": "2+2?"}]
        self.assertIsNotNone(model(messages, maxlength=10, seed=0, stop=["."]))

        # Test default role
        self.assertIsNotNone(model("2 + 2 = ", maxlength=10, seed=0, stop=["."], defaultrole="user"))

        # Test streaming
        self.assertEqual(" ".join(x for x in model("2 + 2 = ", maxlength=10, stream=True, seed=0, stop=["."]))[0], "4")



================================================
FILE: test/python/testpipeline/testllm/testllm.py
================================================
"""
LLM module tests
"""

import unittest

import torch

from transformers import AutoModelForCausalLM, AutoTokenizer

from txtai.pipeline import LLM, Generation

# pylint: disable=C0411
from utils import Utils


class TestLLM(unittest.TestCase):
    """
    LLM tests.
    """

    def testArguments(self):
        """
        Test pipeline keyword arguments
        """

        start = "Hello, how are"

        # Test that text is generated with custom parameters
        model = LLM("hf-internal-testing/tiny-random-gpt2", task="language-generation", torch_dtype="torch.float32")
        self.assertIsNotNone(model(start))

        model = LLM("hf-internal-testing/tiny-random-gpt2", task="language-generation", torch_dtype=torch.float32)
        self.assertIsNotNone(model(start))

    def testBatchSize(self):
        """
        Test batch size
        """

        model = LLM("sshleifer/tiny-gpt2")
        self.assertIsNotNone(model(["Hello, how are"] * 2, batch_size=2))

    def testCustom(self):
        """
        Test custom LLM framework
        """

        model = LLM("hf-internal-testing/tiny-random-gpt2", task="language-generation", method="txtai.pipeline.HFGeneration")
        self.assertIsNotNone(model("Hello, how are"))

    def testCustomNotFound(self):
        """
        Test resolving an unresolvable LLM framework
        """

        with self.assertRaises(ImportError):
            LLM("hf-internal-testing/tiny-random-gpt2", method="notfound.generation")

    def testDefaultRole(self):
        """
        Test default role
        """

        model = LLM("hf-internal-testing/tiny-random-LlamaForCausalLM")
        self.assertIsNotNone(model("Hello, how are", defaultrole="user"))

    def testExternal(self):
        """
        Test externally loaded model
        """

        model = AutoModelForCausalLM.from_pretrained("hf-internal-testing/tiny-random-gpt2")
        tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/tiny-random-gpt2")

        model = LLM((model, tokenizer), template="{text}")
        start = "Hello, how are"

        # Test that text is generated
        self.assertIsNotNone(model(start))

    def testMaxLength(self):
        """
        Test max length
        """

        model = LLM("sshleifer/tiny-gpt2")
        self.assertIsInstance(model("Hello, how are", maxlength=10), str)

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        generation = Generation()
        self.assertRaises(NotImplementedError, generation.stream, None, None, None, None)

    def testStop(self):
        """
        Test stop strings
        """

        model = LLM("sshleifer/tiny-gpt2")
        self.assertIsNotNone(model("Hello, how are", stop=["you"]))

    def testStream(self):
        """
        Test streaming generation
        """

        model = LLM("sshleifer/tiny-gpt2")
        self.assertIsInstance(" ".join(x for x in model("Hello, how are", stream=True)), str)

    def testStripThink(self):
        """
        Test stripthink parameter
        """

        # pylint: disable=W0613
        def execute1(*args, **kwargs):
            return ["<think>test</think>you"]

        def execute2(*args, **kwargs):
            return ["<|channel|>final<|message|> you"]

        model = LLM("hf-internal-testing/tiny-random-LlamaForCausalLM")

        for method in [execute1, execute2]:
            # Override execute method
            model.generator.execute = method
            self.assertEqual(model("Hello, how are", stripthink=True), "you")

    def testStripThinkStream(self):
        """
        Test stripthink parameter with streaming output
        """

        # pylint: disable=W0613
        def execute1(*args, **kwargs):
            yield from "<think>test</think>you"

        def execute2(*args, **kwargs):
            yield from "<|channel|>final<|message|>you"

        model = LLM("hf-internal-testing/tiny-random-LlamaForCausalLM")

        for method in [execute1, execute2]:
            # Override execute method
            model.generator.execute = method
            self.assertEqual("".join(model("Hello, how are", stripthink=True, stream=True)), "you")

    def testVision(self):
        """
        Test vision LLM
        """

        model = LLM("neuml/tiny-random-qwen2vl")
        result = model(
            [{"role": "user", "content": [{"type": "text", "text": "What is in this image?"}, {"type": "image", "image": Utils.PATH + "/books.jpg"}]}]
        )

        self.assertIsNotNone(result)



================================================
FILE: test/python/testpipeline/testllm/testrag.py
================================================
"""
RAG module tests
"""

import platform
import unittest

from txtai.embeddings import Embeddings
from txtai.pipeline import Questions, RAG, Similarity


class TestRAG(unittest.TestCase):
    """
    RAG tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single rag instance.
        """

        cls.data = [
            "Giants hit 3 HRs to down Dodgers",
            "Giants 5 Dodgers 4 final",
            "Dodgers drop Game 2 against the Giants, 5-4",
            "Blue Jays beat Red Sox final score 2-1",
            "Red Sox lost to the Blue Jays, 2-1",
            "Blue Jays at Red Sox is over. Score: 2-1",
            "Phillies win over the Braves, 5-0",
            "Phillies 5 Braves 0 final",
            "Final: Braves lose to the Phillies in the series opener, 5-0",
            "Lightning goaltender pulled, lose to Flyers 4-1",
            "Flyers 4 Lightning 1 final",
            "Flyers win 4-1",
        ]

        # Create embeddings model, backed by sentence-transformers & transformers
        cls.embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

        # Create rag instance
        cls.rag = RAG(cls.embeddings, "distilbert-base-cased-distilled-squad")

    @classmethod
    def tearDownClass(cls):
        """
        Cleanup data.
        """

        if cls.embeddings:
            cls.embeddings.close()

    def testAnswer(self):
        """
        Test qa extraction with an answer
        """

        questions = ["What team won the game?", "What was score?"]

        # pylint: disable=C3001
        execute = lambda query: self.rag([(question, query, question, False) for question in questions], self.data)

        answers = execute("Red Sox - Blue Jays")
        self.assertEqual("Blue Jays", answers[0][1])
        self.assertEqual("2-1", answers[1][1])

        # Ad-hoc questions
        question = "What hockey team won?"

        answers = self.rag([(question, question, question, False)], self.data)
        self.assertEqual("Flyers", answers[0][1])

    def testEmptyQuery(self):
        """
        Test an empty queries list
        """

        self.assertEqual(self.rag.query(None, None), [])

    def testNoAnswer(self):
        """
        Test qa extraction with no answer
        """

        question = ""

        answers = self.rag([(question, question, question, False)], self.data)
        self.assertIsNone(answers[0][1])

        question = "abcdef"
        answers = self.rag([(question, question, question, False)], self.data)
        self.assertIsNone(answers[0][1])

    @unittest.skipIf(platform.system() == "Darwin", "Quantized models not supported on macOS")
    def testQuantize(self):
        """
        Test qa extraction backed by a quantized model
        """

        rag = RAG(self.embeddings, "distilbert-base-cased-distilled-squad", True)

        question = "How many home runs?"

        answers = rag([(question, question, question, True)], self.data)
        self.assertTrue(answers[0][1].startswith("Giants hit 3 HRs"))

    def testOutputs(self):
        """
        Test output formatting rules
        """

        question = "How many home runs?"

        # Test flatten to list of answers
        rag = RAG(self.embeddings, "distilbert-base-cased-distilled-squad", output="flatten")
        answers = rag([(question, question, question, True)], self.data)
        self.assertTrue(answers[0].startswith("Giants hit 3 HRs"))

        # Test reference field
        rag = RAG(self.embeddings, "distilbert-base-cased-distilled-squad", output="reference")
        answers = rag([(question, question, question, True)], self.data)
        self.assertTrue(self.data[answers[0][2]].startswith("Giants hit 3 HRs"))

    def testPrompt(self):
        """
        Test a user prompt with templating
        """

        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        rag = RAG(
            embeddings,
            "google/flan-t5-small",
            template="""
              Answer the following question and return a number.
              Question: {question}
              Context:{context}""",
            output="flatten",
        )

        self.assertEqual(rag("How many HRs"), "3")

    def testPromptTemplates(self):
        """
        Test system and user prompt templates
        """

        rag = RAG(
            self.embeddings,
            "sshleifer/tiny-gpt2",
            system="You are a friendly assistant",
            template="""
              Answer the following question and return a number.
              Question: {question}
              Context:{context}""",
        )

        prompts = rag.prompts(["How many HRs?"], [self.data])[0]
        self.assertEqual([x["role"] for x in prompts], ["system", "user"])

    def testSearch(self):
        """
        Test qa extraction with an embeddings search for context
        """

        embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2", "content": True})
        embeddings.index([(uid, text, None) for uid, text in enumerate(self.data)])

        rag = RAG(embeddings, "distilbert-base-cased-distilled-squad")

        question = "How many home runs?"

        answers = rag([(question, question, question, True)])
        self.assertTrue(answers[0][1].startswith("Giants hit 3 HRs"))

    def testSimilarity(self):
        """
        Test qa extraction using a Similarity pipeline to build context
        """

        # Create rag instance
        rag = RAG(Similarity("prajjwal1/bert-medium-mnli"), Questions("distilbert-base-cased-distilled-squad"))

        question = "How many home runs?"

        answers = rag([(question, "HRs", question, True)], self.data)
        self.assertTrue(answers[0][1].startswith("Giants hit 3 HRs"))

    def testSnippet(self):
        """
        Test qa extraction with a full answer snippet
        """

        question = "How many home runs?"

        answers = self.rag([(question, question, question, True)], self.data)
        self.assertTrue(answers[0][1].startswith("Giants hit 3 HRs"))

    def testSnippetEmpty(self):
        """
        Test snippet method can handle empty parameters
        """

        self.assertEqual(self.rag.snippets(["name"], [None], [None], [None]), [("name", None)])

    def testStringInput(self):
        """
        Test with single string input
        """

        result = self.rag("How many home runs?", self.data)
        self.assertEqual(result["answer"], "3")

    def testTasks(self):
        """
        Test loading models with task parameter
        """

        for task, model in [
            ("language-generation", "hf-internal-testing/tiny-random-gpt2"),
            ("sequence-sequence", "hf-internal-testing/tiny-random-t5"),
        ]:
            rag = RAG(self.embeddings, model, task=task)
            self.assertIsNotNone(rag)



================================================
FILE: test/python/testpipeline/testllm/testsequences.py
================================================
"""
Sequences module tests
"""

import unittest

from txtai.pipeline import Sequences


class TestSequences(unittest.TestCase):
    """
    Sequences tests.
    """

    def testGeneration(self):
        """
        Test text2text pipeline generation
        """

        model = Sequences("t5-small")
        self.assertEqual(model("Testing the model", prefix="translate English to German: "), "Das Modell zu testen")



================================================
FILE: test/python/testpipeline/testtext/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testtext/testentity.py
================================================
"""
Entity module tests
"""

import unittest

from txtai.pipeline import Entity


class TestEntity(unittest.TestCase):
    """
    Entity tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create entity instance.
        """

        cls.entity = Entity("dslim/bert-base-NER")

    def testEntity(self):
        """
        Test entity
        """

        # Run entity extraction
        entities = self.entity("Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg")
        self.assertEqual([e[0] for e in entities], ["Canada", "Manhattan"])

    def testEntityFlatten(self):
        """
        Test entity with flattened output
        """

        # Test flatten
        entities = self.entity("Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg", flatten=True)
        self.assertEqual(entities, ["Canada", "Manhattan"])

        # Test flatten with join
        entities = self.entity(
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg", flatten=True, join=True
        )
        self.assertEqual(entities, "Canada Manhattan")

    def testEntityTypes(self):
        """
        Test entity type filtering
        """

        # Run entity extraction
        entities = self.entity("Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg", labels=["PER"])
        self.assertFalse(entities)

    def testGliner(self):
        """
        Test entity pipeline with a GLiNER model
        """

        entity = Entity("neuml/gliner-bert-tiny")
        entities = entity("My name is John Smith.", flatten=True)
        self.assertEqual(entities, ["John Smith"])



================================================
FILE: test/python/testpipeline/testtext/testlabels.py
================================================
"""
Labels module tests
"""

import unittest

from txtai.pipeline import Labels


class TestLabels(unittest.TestCase):
    """
    Labels tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single labels instance.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.labels = Labels("prajjwal1/bert-medium-mnli")

    def testLabel(self):
        """
        Test labels with single text input
        """

        self.assertEqual(self.labels("This is the best sentence ever", ["positive", "negative"])[0][0], 0)

    def testLabelFlatten(self):
        """
        Test labels with single text input, flattened to top text labels
        """

        self.assertEqual(self.labels("This is the best sentence ever", ["positive", "negative"], flatten=True)[0], "positive")

    def testLabelBatch(self):
        """
        Test labels with multiple text inputs
        """

        results = [l[0][0] for l in self.labels(["This is the best sentence ever", "This is terrible"], ["positive", "negative"])]
        self.assertEqual(results, [0, 1])

    def testLabelBatchFlatten(self):
        """
        Test labels with multiple text inputs, flattened to top text labels
        """

        results = [l[0] for l in self.labels(["This is the best sentence ever", "This is terrible"], ["positive", "negative"], flatten=True)]
        self.assertEqual(results, ["positive", "negative"])

    def testLabelFixed(self):
        """
        Test labels with a fixed label text classification model
        """

        labels = Labels(dynamic=False)

        # Get index of "POSITIVE" label
        index = labels.labels().index("POSITIVE")

        # Verify results
        self.assertEqual(labels("This is the best sentence ever")[0][0], index)
        self.assertEqual(labels("This is the best sentence ever", multilabel=True)[0][0], index)

    def testLabelFixedFlatten(self):
        """
        Test labels with a fixed label text classification model, flattened to top text labels
        """

        labels = Labels(dynamic=False)

        # Verify results
        self.assertEqual(labels("This is the best sentence ever", flatten=True)[0], "POSITIVE")
        self.assertEqual(labels("This is the best sentence ever", multilabel=True, flatten=True)[0], "POSITIVE")



================================================
FILE: test/python/testpipeline/testtext/testreranker.py
================================================
"""
Reranker module tests
"""

import unittest

from txtai import Embeddings
from txtai.pipeline import Reranker, Similarity


class TestReranker(unittest.TestCase):
    """
    Reranker tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single labels instance.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

    def testRanker(self):
        """
        Test re-ranking pipeline
        """

        embeddings = Embeddings(content=True)
        embeddings.index(self.data)

        similarity = Similarity("neuml/colbert-bert-tiny", lateencode=True)

        ranker = Reranker(embeddings, similarity)
        self.assertEqual(ranker("lottery winner")[0]["id"], "4")



================================================
FILE: test/python/testpipeline/testtext/testsimilarity.py
================================================
"""
Similarity module tests
"""

import unittest

from txtai.pipeline import Similarity


class TestSimilarity(unittest.TestCase):
    """
    Similarity tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single labels instance.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.similarity = Similarity("prajjwal1/bert-medium-mnli")

    def testCrossEncoder(self):
        """
        Test cross-encoder similarity model
        """

        similarity = Similarity("cross-encoder/ms-marco-MiniLM-L-2-v2", crossencode=True)
        uid = similarity("Who won the lottery?", self.data)[0][0]
        self.assertEqual(self.data[uid], self.data[4])

    def testCrossEncoderBatch(self):
        """
        Test cross-encoder similarity model with multiple inputs
        """

        similarity = Similarity("cross-encoder/ms-marco-MiniLM-L-2-v2", crossencode=True)
        results = [r[0][0] for r in similarity(["Who won the lottery?", "Where did an iceberg collapse?"], self.data)]
        self.assertEqual(results, [4, 1])

    def testLateEncoder(self):
        """
        Test late-encoder similarity model
        """

        similarity = Similarity("neuml/pylate-bert-tiny", lateencode=True)
        uid = similarity("Who won the lottery?", self.data)[0][0]
        self.assertEqual(self.data[uid], self.data[4])

        # Test encode method
        # pylint: disable=E1101
        self.assertEqual(similarity.encode(["Who won the lottery?"], "data").shape, (1, 8, 128))

    def testLateEncoderBatch(self):
        """
        Test late-encoder similarity model with multiple inputs
        """

        similarity = Similarity("neuml/colbert-bert-tiny", lateencode=True)
        results = [r[0][0] for r in similarity(["Who won the lottery?", "Where did an iceberg collapse?"], self.data)]
        self.assertEqual(results, [4, 1])

    def testSimilarity(self):
        """
        Test similarity with single query
        """

        uid = self.similarity("feel good story", self.data)[0][0]
        self.assertEqual(self.data[uid], self.data[4])

    def testSimilarityBatch(self):
        """
        Test similarity with multiple queries
        """

        results = [r[0][0] for r in self.similarity(["feel good story", "climate change"], self.data)]
        self.assertEqual(results, [4, 1])

    def testSimilarityFixed(self):
        """
        Test similarity with a fixed label text classification model
        """

        similarity = Similarity(dynamic=False)

        # Test with query as label text and label id
        self.assertLessEqual(similarity("negative", ["This is the best sentence ever"])[0][1], 0.1)
        self.assertLessEqual(similarity("0", ["This is the best sentence ever"])[0][1], 0.1)

    def testSimilarityLong(self):
        """
        Test similarity with long text
        """

        uid = self.similarity("other", ["Very long text " * 1000, "other text"])[0][0]
        self.assertEqual(uid, 1)



================================================
FILE: test/python/testpipeline/testtext/testsummary.py
================================================
"""
Summary module tests
"""

import unittest

from txtai.pipeline import Summary


class TestSummary(unittest.TestCase):
    """
    Summary tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single summary instance.
        """

        cls.text = (
            "Search is the base of many applications. Once data starts to pile up, users want to be able to find it. It's the foundation "
            "of the internet and an ever-growing challenge that is never solved or done. The field of Natural Language Processing (NLP) is "
            "rapidly evolving with a number of new developments. Large-scale general language models are an exciting new capability "
            "allowing us to add amazing functionality quickly with limited compute and people. Innovation continues with new models "
            "and advancements coming in at what seems a weekly basis. This article introduces txtai, an AI-powered search engine "
            "that enables Natural Language Understanding (NLU) based search in any application."
        )

        cls.summary = Summary("t5-small")

    def testSummary(self):
        """
        Test summarization of text
        """

        self.assertEqual(self.summary(self.text, minlength=15, maxlength=15), "the field of natural language processing (NLP) is rapidly evolving")

    def testSummaryBatch(self):
        """
        Test batch summarization of text
        """

        summaries = self.summary([self.text, self.text], maxlength=15)
        self.assertEqual(len(summaries), 2)

    def testSummaryNoLength(self):
        """
        Test summary with no max length set
        """

        self.assertEqual(
            self.summary(self.text + self.text),
            "search is the base of many applications. Once data starts to pile up, users want to be able to find it. "
            + "Large-scale general language models are an exciting new capability allowing us to add amazing functionality quickly "
            + "with limited compute and people.",
        )

    def testSummaryShort(self):
        """
        Test that summarization is skipped
        """

        self.assertEqual(self.summary("Text", maxlength=15), "Text")



================================================
FILE: test/python/testpipeline/testtext/testtranslation.py
================================================
"""
Translation module tests
"""

import unittest
import time

import requests

from txtai.pipeline import Translation


class TestTranslation(unittest.TestCase):
    """
    Translation tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create single translation instance.
        """

        cls.translate = Translation()

        # Preload list of models. Handle HF Hub errors.
        complete, wait = False, 1
        while not complete:
            try:
                cls.translate.lookup("en", "es")
                complete = True
            except requests.exceptions.HTTPError:
                # Exponential backoff
                time.sleep(wait)

                # Wait up to 16 seconds
                wait = min(wait * 2, 16)

    def testDetect(self):
        """
        Test language detection
        """

        test = ["This is a test language detection."]
        language = self.translate.detect(test)

        self.assertListEqual(language, ["en"])

    def testDetectWithCustomFunc(self):
        """
        Test language detection with custom function
        """

        def dummy_func(text):
            return ["en" for x in text]

        translate = Translation(langdetect=dummy_func)

        test = ["This is a test language detection."]
        language = translate.detect(test)

        self.assertListEqual(language, ["en"])

    def testLongTranslation(self):
        """
        Test a translation longer than max tokenization length
        """

        text = "This is a test translation to Spanish. " * 100
        translation = self.translate(text, "es")

        # Validate translation text
        self.assertIsNotNone(translation)

    def testM2M100Translation(self):
        """
        Test a translation using M2M100 models
        """

        text = self.translate("This is a test translation to Croatian", "hr")

        # Validate translation text
        self.assertEqual(text, "Ovo je testni prijevod na hrvatski")

    def testMarianTranslation(self):
        """
        Test a translation using Marian models
        """

        text = "This is a test translation into Spanish"
        translation = self.translate(text, "es")

        # Validate translation text
        self.assertEqual(translation, "Esta es una traducciÃ³n de prueba al espaÃ±ol")

        # Validate translation back
        translation = self.translate(translation, "en")
        self.assertEqual(translation, text)

    def testNoLang(self):
        """
        Test no matching language id
        """

        self.assertIsNone(self.translate.langid([], "zz"))

    def testNoModel(self):
        """
        Test no known available model found
        """

        self.assertEqual(self.translate.modelpath("zz", "en"), "Helsinki-NLP/opus-mt-mul-en")

    def testNoTranslation(self):
        """
        Test translation skipped when text already in destination language
        """

        text = "This is a test translation to English"
        translation = self.translate(text, "en")

        # Validate no translation
        self.assertEqual(text, translation)

    def testTranslationWithShowmodels(self):
        """
        Test a translation using Marian models and showmodels flag to return
        model and language.
        """

        text = "This is a test translation into Spanish"
        result = self.translate(text, "es", showmodels=True)

        translation, language, modelpath = result
        # Validate translation text
        self.assertEqual(translation, "Esta es una traducciÃ³n de prueba al espaÃ±ol")
        # Validate detected language
        self.assertEqual(language, "en")
        # Validate model
        self.assertEqual(modelpath, "Helsinki-NLP/opus-mt-en-es")

        # Validate translation back
        result = self.translate(translation, "en", showmodels=True)

        translation, language, modelpath = result
        self.assertEqual(translation, text)
        # Validate detected language
        self.assertEqual(language, "es")
        # Validate model
        self.assertEqual(modelpath, "Helsinki-NLP/opus-mt-es-en")



================================================
FILE: test/python/testpipeline/testtrain/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testpipeline/testtrain/testonnx.py
================================================
"""
ONNX module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

from txtai.embeddings import Embeddings
from txtai.models import OnnxModel
from txtai.pipeline import HFOnnx, HFTrainer, Labels, MLOnnx, Questions


class TestOnnx(unittest.TestCase):
    """
    ONNX tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create default datasets.
        """

        cls.data = [{"text": "Dogs", "label": 0}, {"text": "dog", "label": 0}, {"text": "Cats", "label": 1}, {"text": "cat", "label": 1}] * 100

    def testDefault(self):
        """
        Test exporting an ONNX model with default parameters
        """

        # Export model to ONNX, use default parameters
        onnx = HFOnnx()
        model = onnx("google/bert_uncased_L-2_H-128_A-2")

        # Validate model has data
        self.assertGreater(len(model), 0)

        # Validate model device properly works
        self.assertEqual(OnnxModel(model).device, -1)

    def testClassification(self):
        """
        Test exporting a classification model to ONNX and running inference
        """

        path = "google/bert_uncased_L-2_H-128_A-2"

        trainer = HFTrainer()
        model, tokenizer = trainer(path, self.data)

        # Output file path
        output = os.path.join(tempfile.gettempdir(), "onnx")

        # Export model to ONNX
        onnx = HFOnnx()
        model = onnx((model, tokenizer), "text-classification", output, True)

        # Test classification
        labels = Labels((model, path), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    @patch("onnxruntime.get_available_providers")
    @patch("torch.cuda.is_available")
    def testPooling(self, cuda, providers):
        """
        Test exporting a pooling model to ONNX and running inference
        """

        path = "sentence-transformers/paraphrase-MiniLM-L3-v2"

        # Export model to ONNX
        onnx = HFOnnx()
        model = onnx(path, "pooling", quantize=True)

        # Test no CUDA and onnxruntime installed
        cuda.return_value = False
        providers.return_value = ["CPUExecutionProvider"]

        embeddings = Embeddings({"path": model, "tokenizer": path})
        self.assertEqual(embeddings.similarity("animal", ["dog", "book", "rug"])[0][0], 0)

        # Test no CUDA and onnxruntime-gpu installed
        cuda.return_value = False
        providers.return_value = ["CUDAExecutionProvider", "CPUExecutionProvider"]

        embeddings = Embeddings({"path": model, "tokenizer": path})
        self.assertIsNotNone(embeddings)

        # Test CUDA and only onnxruntime installed
        cuda.return_value = True
        providers.return_value = ["CPUExecutionProvider"]

        embeddings = Embeddings({"path": model, "tokenizer": path})
        self.assertIsNotNone(embeddings)

        # Test CUDA and onnxruntime-gpu installed
        cuda.return_value = True
        providers.return_value = ["CUDAExecutionProvider", "CPUExecutionProvider"]

        embeddings = Embeddings({"path": model, "tokenizer": path})
        self.assertIsNotNone(embeddings)

    def testQA(self):
        """
        Test exporting a QA model to ONNX and running inference
        """

        path = "distilbert-base-cased-distilled-squad"

        # Export model to ONNX
        onnx = HFOnnx()
        model = onnx(path, "question-answering")

        questions = Questions((model, path))
        self.assertEqual(questions(["What is the price?"], ["The price is $30"])[0], "$30")

    def testScikit(self):
        """
        Test exporting a scikit-learn model to ONNX and running inference
        """

        # pylint: disable=W0613
        def tokenizer(inputs, **kwargs):
            if isinstance(inputs, str):
                inputs = [inputs]

            return {"input_ids": [[x] for x in inputs]}

        # Train a scikit-learn model
        model = Pipeline([("tfidf", TfidfVectorizer()), ("lr", LogisticRegression())])
        model.fit([x["text"] for x in self.data], [x["label"] for x in self.data])

        # Export model to ONNX
        onnx = MLOnnx()
        model = onnx(model)

        # Test classification
        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    def testZeroShot(self):
        """
        Test exporting a zero shot classification model to ONNX and running inference
        """

        path = "prajjwal1/bert-medium-mnli"

        # Export model to ONNX
        onnx = HFOnnx()
        model = onnx(path, "zero-shot-classification", quantize=True)

        # Test zero shot classification
        labels = Labels((model, path))
        self.assertEqual(labels("That is great news", ["negative", "positive"])[0][0], 1)



================================================
FILE: test/python/testpipeline/testtrain/testquantization.py
================================================
"""
Quantization module tests
"""

import platform
import unittest

from transformers import AutoModel

from txtai.pipeline import HFModel, HFPipeline


class TestQuantization(unittest.TestCase):
    """
    Quantization tests.
    """

    @unittest.skipIf(platform.system() == "Darwin", "Quantized models not supported on macOS")
    def testModel(self):
        """
        Test quantizing a model through HFModel.
        """

        model = HFModel(quantize=True, gpu=False)
        model = model.prepare(AutoModel.from_pretrained("google/bert_uncased_L-2_H-128_A-2"))
        self.assertIsNotNone(model)

    @unittest.skipIf(platform.system() == "Darwin", "Quantized models not supported on macOS")
    def testPipeline(self):
        """
        Test quantizing a model through HFPipeline.
        """

        pipeline = HFPipeline("text-classification", "google/bert_uncased_L-2_H-128_A-2", True, False)
        self.assertIsNotNone(pipeline)



================================================
FILE: test/python/testpipeline/testtrain/testtrainer.py
================================================
"""
Trainer module tests
"""

import os
import unittest
import tempfile

from unittest.mock import patch

import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification

from txtai.data import Data
from txtai.pipeline import HFTrainer, Labels, Questions, Sequences


class TestTrainer(unittest.TestCase):
    """
    Trainer tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Create default datasets.
        """

        cls.data = [{"text": "Dogs", "label": 0}, {"text": "dog", "label": 0}, {"text": "Cats", "label": 1}, {"text": "cat", "label": 1}] * 100

    def testBasic(self):
        """
        Test training a model with basic parameters
        """

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", self.data)

        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    def testCLM(self):
        """
        Test training a model with causal language modeling
        """

        trainer = HFTrainer()
        model, _ = trainer("hf-internal-testing/tiny-random-gpt2", self.data, maxlength=16, task="language-generation")

        # Test model completed successfully
        self.assertIsNotNone(model)

    def testCustom(self):
        """
        Test training a model with custom parameters
        """

        # pylint: disable=E1120
        model = AutoModelForSequenceClassification.from_pretrained("google/bert_uncased_L-2_H-128_A-2")
        tokenizer = AutoTokenizer.from_pretrained("google/bert_uncased_L-2_H-128_A-2")

        trainer = HFTrainer()
        model, tokenizer = trainer(
            (model, tokenizer),
            self.data,
            self.data,
            columns=("text", "label"),
            do_eval=True,
            output_dir=os.path.join(tempfile.gettempdir(), "trainer"),
        )

        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    def testDataFrame(self):
        """
        Test training a model with a mock pandas DataFrame
        """

        class TestDataFrame:
            """
            Test DataFrame
            """

            def __init__(self, data):
                # Get list of columns
                self.columns = list(data[0].keys())

                # Build columnar data view
                self.data = {}
                for column in self.columns:
                    self.data[column] = Values([row[column] for row in data])

            def __getitem__(self, column):
                return self.data[column]

        class Values:
            """
            Test values list
            """

            def __init__(self, values):
                self.values = list(values)

            def __getitem__(self, index):
                return self.values[index]

            def unique(self):
                """
                Returns a list of unique values.

                Returns:
                    unique list of values
                """

                return set(self.values)

        # Mock DataFrame
        df = TestDataFrame(self.data)

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", df)

        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    def testDataset(self):
        """
        Test training a model with a mock Hugging Face Dataset
        """

        class TestDataset(torch.utils.data.Dataset):
            """
            Test Dataset
            """

            def __init__(self, data):
                self.data = data
                self.unique = lambda _: [0, 1]

            def __len__(self):
                return len(self.data)

            def __getitem__(self, index):
                return self.data[index]

            def column_names(self):
                """
                Returns column names for this dataset

                Returns:
                    list of columns
                """

                return ["text", "label"]

            # pylint: disable=W0613
            def map(self, fn, batched, num_proc, remove_columns):
                """
                Map each dataset row using fn.

                Args:
                    fn: function
                    batched: batch records

                Returns:
                    updated Dataset
                """

                self.data = [fn(x) for x in self.data]
                return self

        ds = TestDataset(self.data)

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", ds)

        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    def testEmpty(self):
        """
        Test an empty training data object
        """

        self.assertIsNone(Data(None, None, None).process(None))

    def testMLM(self):
        """
        Test training a model with masked language modeling.
        """

        trainer = HFTrainer()
        model, _ = trainer("hf-internal-testing/tiny-random-bert", self.data, task="language-modeling")

        # Test model completed successfully
        self.assertIsNotNone(model)

    def testMultiLabel(self):
        """
        Test training model with labels provided as a list
        """

        data = []
        for x in self.data:
            data.append({"text": x["text"], "label": [0.0, 1.0] if x["label"] else [1.0, 0.0]})

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", data)

        labels = Labels((model, tokenizer), dynamic=False)
        self.assertEqual(labels("cat")[0][0], 1)

    @patch("importlib.util.find_spec")
    def testPEFT(self, spec):
        """
        Test training a model with causal language modeling and PEFT
        """

        # Disable triton
        spec.return_value = None

        trainer = HFTrainer()
        model, _ = trainer(
            "hf-internal-testing/tiny-random-gpt2",
            self.data,
            maxlength=16,
            task="language-generation",
            quantize=True,
            lora=True,
        )

        # Test model completed successfully
        self.assertIsNotNone(model)

    def testQA(self):
        """
        Test training a QA model
        """

        # Training data
        data = [
            {"question": "What ingredient?", "context": "1 can whole tomatoes", "answers": "tomatoes"},
            {"question": "What ingredient?", "context": "Crush 1 tomato", "answers": "tomato"},
            {"question": "What ingredient?", "context": "1 yellow onion", "answers": "onion"},
            {"question": "What ingredient?", "context": "Unwrap 2 red onions", "answers": "onions"},
            {"question": "What ingredient?", "context": "1 red pepper", "answers": "pepper"},
            {"question": "What ingredient?", "context": "Clean 3 red peppers", "answers": "peppers"},
            {"question": "What ingredient?", "context": "1 clove garlic", "answers": "garlic"},
            {"question": "What ingredient?", "context": "Unwrap 3 cloves of garlic", "answers": "garlic"},
            {"question": "What ingredient?", "context": "3 pieces of ginger", "answers": "ginger"},
            {"question": "What ingredient?", "context": "Peel 1 orange", "answers": "orange"},
            {"question": "What ingredient?", "context": "1/2 lb beef", "answers": "beef"},
            {"question": "What ingredient?", "context": "Roast 3 lbs of beef", "answers": "beef"},
            {"question": "What ingredient?", "context": "1 pack of chicken", "answers": "chicken"},
            {"question": "What ingredient?", "context": "Forest through the trees", "answers": None},
        ]

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", data, data, task="question-answering", num_train_epochs=40)

        questions = Questions((model, tokenizer), gpu=True)
        self.assertEqual(questions(["What ingredient?"], ["Peel 1 onion"])[0], "onion")

    def testRegression(self):
        """
        Test training a model with a regression (continuous) output
        """

        data = []
        for x in self.data:
            data.append({"text": x["text"], "label": x["label"] + 0.1})

        trainer = HFTrainer()
        model, tokenizer = trainer("google/bert_uncased_L-2_H-128_A-2", data)

        labels = Labels((model, tokenizer), dynamic=False)

        # Regression tasks return a single entry with the regression output
        self.assertGreater(labels("cat")[0][1], 0.5)

    def testRTD(self):
        """
        Test training a language model with replaced token detection
        """

        # Save directory
        output = os.path.join(tempfile.gettempdir(), "trainer.rtd")

        trainer = HFTrainer()
        model, _ = trainer("hf-internal-testing/tiny-random-electra", self.data, task="token-detection", save_safetensors=False, output_dir=output)

        # Test model completed successfully
        self.assertIsNotNone(model)

        # Test output directories exist
        self.assertTrue(os.path.exists(os.path.join(output, "generator")))
        self.assertTrue(os.path.exists(os.path.join(output, "discriminator")))

    def testSeqSeq(self):
        """
        Test training a sequence-sequence model
        """

        data = [
            {"source": "Running again", "target": "Sleeping again"},
            {"source": "Run", "target": "Sleep"},
            {"source": "running", "target": "sleeping"},
        ]

        trainer = HFTrainer()
        model, tokenizer = trainer("t5-small", data, task="sequence-sequence", prefix="translate Run to Sleep: ", learning_rate=1e-3)

        # Run run-sleep translation
        sequences = Sequences((model, tokenizer))
        result = sequences("translate Run to Sleep: run")
        self.assertEqual(result.lower(), "sleep")



================================================
FILE: test/python/testscoring/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testscoring/testkeyword.py
================================================
"""
Keyword scoring tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from txtai.scoring import ScoringFactory, Scoring


# pylint: disable=R0904
class TestKeyword(unittest.TestCase):
    """
    Sparse keyword scoring tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "wins wins wins",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.data = [(uid, x, None) for uid, x in enumerate(cls.data)]

    def testBM25(self):
        """
        Test bm25
        """

        self.runTests("bm25")

    def testCustom(self):
        """
        Test custom method
        """

        self.runTests("txtai.scoring.BM25")

    def testCustomNotFound(self):
        """
        Test unresolvable custom method
        """

        with self.assertRaises(ImportError):
            ScoringFactory.create("notfound.scoring")

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        scoring = Scoring()

        self.assertRaises(NotImplementedError, scoring.insert, None, None)
        self.assertRaises(NotImplementedError, scoring.delete, None)
        self.assertRaises(NotImplementedError, scoring.weights, None)
        self.assertRaises(NotImplementedError, scoring.search, None, None)
        self.assertRaises(NotImplementedError, scoring.batchsearch, None, None, None)
        self.assertRaises(NotImplementedError, scoring.count)
        self.assertRaises(NotImplementedError, scoring.load, None)
        self.assertRaises(NotImplementedError, scoring.save, None)
        self.assertRaises(NotImplementedError, scoring.close)
        self.assertRaises(NotImplementedError, scoring.issparse)
        self.assertRaises(NotImplementedError, scoring.isnormalized)

    @patch("sqlalchemy.orm.Query.params")
    def testPGText(self, query):
        """
        Test PGText
        """

        # Mock database query
        query.return_value = [(3, 1.0)]

        # Create scoring
        path = os.path.join(tempfile.gettempdir(), "pgtext.sqlite")
        scoring = ScoringFactory.create({"method": "pgtext", "url": f"sqlite:///{path}", "schema": "txtai"})
        scoring.index((uid, {"text": text}, tags) for uid, text, tags in self.data)

        # Run search and validate correct result returned
        index, _ = scoring.search("bear", 1)[0]
        self.assertEqual(index, 3)

        # Run batch search
        index, _ = scoring.batchsearch(["bear"], 1)[0][0]
        self.assertEqual(index, 3)

        # Validate save/load/delete
        scoring.save(None)
        scoring.load(None)

        # Validate count
        self.assertEqual(scoring.count(), len(self.data))

        # Test delete
        scoring.delete([0])
        self.assertEqual(scoring.count(), len(self.data) - 1)

        # PGText is a normalized sparse index
        self.assertTrue(scoring.issparse() and scoring.isnormalized())
        self.assertIsNone(scoring.weights("This is a test".split()))

        # Close scoring
        scoring.close()

    def testSIF(self):
        """
        Test sif
        """

        self.runTests("sif")

    def testTFIDF(self):
        """
        Test tfidf
        """

        self.runTests("tfidf")

    def runTests(self, method):
        """
        Runs a series of tests for a scoring method.

        Args:
            method: scoring method
        """

        config = {"method": method}

        self.index(config)
        self.upsert(config)
        self.weights(config)
        self.search(config)
        self.delete(config)
        self.normalize(config)
        self.content(config)
        self.empty(config)
        self.copy(config)
        self.settings(config)

    def index(self, config, data=None):
        """
        Test scoring index method.

        Args:
            config: scoring config
            data: data to index with scoring method

        Returns:
            scoring
        """

        # Derive input data
        data = data if data else self.data

        scoring = ScoringFactory.create(config)
        scoring.index(data)

        keys = [k for k, v in sorted(scoring.idf.items(), key=lambda x: x[1])]

        # Test count
        self.assertEqual(scoring.count(), len(data))

        # Win should be lowest score
        self.assertEqual(keys[0], "wins")

        # Test save/load
        self.assertIsNotNone(self.save(scoring, config, f"scoring.{config['method']}.index"))

        # Test search returns none when terms disabled (default)
        self.assertIsNone(scoring.search("query"))

        return scoring

    def upsert(self, config):
        """
        Test scoring upsert method
        """

        scoring = ScoringFactory.create({**config, **{"tokenizer": {"alphanum": True, "stopwords": True}}})
        scoring.upsert(self.data)

        # Test count
        self.assertEqual(scoring.count(), len(self.data))

        # Test stop word is removed
        self.assertFalse("and" in scoring.idf)

    def save(self, scoring, config, name):
        """
        Test scoring index save/load.

        Args:
            scoring: scoring index
            config: scoring config
            name: output file name

        Returns:
            scoring
        """

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "scoring")
        os.makedirs(index, exist_ok=True)

        # Save scoring instance
        scoring.save(f"{index}/{name}")

        # Reload scoring instance
        scoring = ScoringFactory.create(config)
        scoring.load(f"{index}/{name}")

        return scoring

    def weights(self, config):
        """
        Test standard and tag weighted scores.

        Args:
            config: scoring config
        """

        document = (1, ["bear", "wins"], None)

        scoring = self.index(config)
        weights = scoring.weights(document[1])

        # Default weights
        self.assertNotEqual(weights[0], weights[1])

        data = self.data[:]

        uid, text, _ = data[3]
        data[3] = (uid, text, "wins")

        scoring = self.index(config, data)
        weights = scoring.weights(document[1])

        # Modified weights
        self.assertEqual(weights[0], weights[1])

    def search(self, config):
        """
        Test scoring search.

        Args:
            config: scoring config
        """

        # Create combined config
        config = {**config, **{"terms": True}}

        # Create scoring instance
        scoring = ScoringFactory.create(config)
        scoring.index(self.data)

        # Run search and validate correct result returned
        index, _ = scoring.search("bear", 1)[0]
        self.assertEqual(index, 3)

        # Run batch search
        index, _ = scoring.batchsearch(["bear"], 1)[0][0]
        self.assertEqual(index, 3)

        # Test save/reload
        self.save(scoring, config, f"scoring.{config['method']}.search")

        # Re-run search and validate correct result returned
        index, _ = scoring.search("bear", 1)[0]
        self.assertEqual(index, 3)

    def delete(self, config):
        """
        Test delete.
        """

        # Create combined config
        config = {**config, **{"terms": True, "content": True}}

        # Create scoring instance
        scoring = ScoringFactory.create(config)
        scoring.index(self.data)

        # Run search and validate correct result returned
        index = scoring.search("bear", 1)[0]["id"]

        # Delete result and validate the query no longer returns results
        scoring.delete([index])
        self.assertFalse(scoring.search("bear", 1))

        # Save and validate count
        self.save(scoring, config, f"scoring.{config['method']}.delete")
        self.assertEqual(scoring.count(), len(self.data) - 1)

    def normalize(self, config):
        """
        Test scoring search with normalized scores.

        Args:
            method: scoring method
        """

        scoring = ScoringFactory.create({**config, **{"terms": True, "normalize": True}})
        scoring.index(self.data)

        # Run search and validate correct result returned
        index, score = scoring.search(self.data[3][1], 1)[0]
        self.assertEqual(index, 3)
        self.assertEqual(score, 1.0)

    def content(self, config):
        """
        Test scoring search with content.

        Args:
            config: scoring config
        """

        scoring = ScoringFactory.create({**config, **{"terms": True, "content": True}})
        scoring.index(self.data)

        # Test text with content
        text = "Great news today"
        scoring.index([(scoring.total, text, None)])

        # Run search and validate correct result returned
        result = scoring.search("great news", 1)[0]["text"]
        self.assertEqual(result, text)

        # Test reading text from dictionary
        text = "Feel good story: baby panda born"
        scoring.index([(scoring.total, {"text": text}, None)])

        # Run search and validate correct result returned
        result = scoring.search("feel good story", 1)[0]["text"]
        self.assertEqual(result, text)

    def empty(self, config):
        """
        Test scoring index properly handles an index call when no data present.

        Args:
            config: scoring config
        """

        # Create scoring index with no data
        scoring = ScoringFactory.create(config)
        scoring.index([])

        # Assert index call returns and index has a count of 0
        self.assertEqual(scoring.total, 0)

    def copy(self, config):
        """
        Test scoring index copy method.
        """

        # Create scoring instance
        scoring = ScoringFactory.create({**config, **{"terms": True}})
        scoring.index(self.data)

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "scoring")
        os.makedirs(index, exist_ok=True)

        # Create file to test replacing existing file
        path = f"{index}/scoring.{config['method']}.copy"
        with open(f"{index}.terms", "w", encoding="utf-8") as f:
            f.write("TEST")

        # Save scoring instance
        scoring.save(path)
        self.assertTrue(os.path.exists(path))

    @patch("sys.byteorder", "big")
    def settings(self, config):
        """
        Test various settings.

        Args:
            config: scoring config
        """

        # Create combined config
        config = {**config, **{"terms": {"cachelimit": 0, "cutoff": 0.25, "wal": True}}}

        # Create scoring instance
        scoring = ScoringFactory.create(config)
        scoring.index(self.data)

        # Save/load index
        self.save(scoring, config, f"scoring.{config['method']}.settings")

        index, _ = scoring.search("bear bear bear wins", 1)[0]
        self.assertEqual(index, 3)

        # Save to same path
        self.save(scoring, config, f"scoring.{config['method']}.settings")

        # Save to different path
        self.save(scoring, config, f"scoring.{config['method']}.move")

        # Validate counts
        self.assertEqual(scoring.count(), len(self.data))



================================================
FILE: test/python/testscoring/testsparse.py
================================================
"""
Sparse module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

from txtai.scoring import ScoringFactory


# pylint: disable=R0904
class TestSparse(unittest.TestCase):
    """
    Sparse vector scoring tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Initialize test data.
        """

        cls.data = [
            "US tops 5 million confirmed virus cases",
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg",
            "Beijing mobilises invasion craft along coast as Taiwan tensions escalate",
            "The National Park Service warns against sacrificing slower friends in a bear attack",
            "Maine man wins $1M from $25 lottery ticket",
            "Make huge profits without work, earn up to $100,000 a day",
        ]

        cls.data = [(uid, x, None) for uid, x in enumerate(cls.data)]

    def testGeneral(self):
        """
        Test general sparse vector operations
        """

        # Models cache
        models = {}

        # Test sparse scoring
        scoring = ScoringFactory.create({"method": "sparse", "path": "sparse-encoder-testing/splade-bert-tiny-nq"}, models=models)
        scoring.index((uid, {"text": text}, tags) for uid, text, tags in self.data)

        # Run search and validate correct result returned
        index, _ = scoring.search("lottery ticket", 1)[0]
        self.assertEqual(index, 4)

        # Run batch search
        index, _ = scoring.batchsearch(["lottery ticket"], 1)[0][0]
        self.assertEqual(index, 4)

        # Validate count
        self.assertEqual(scoring.count(), len(self.data))

        # Test delete
        scoring.delete([4])
        self.assertEqual(scoring.count(), len(self.data) - 1)

        # Run search after delete
        index, _ = scoring.search("lottery ticket", 1)[0]
        self.assertEqual(index, 5)

        # Sparse vectors is a normalized sparse index
        self.assertTrue(scoring.issparse() and scoring.isnormalized())
        self.assertIsNone(scoring.weights("This is a test".split()))

        # Close scoring
        scoring.close()

        # Test model caching
        scoring = ScoringFactory.create({"method": "sparse", "path": "sparse-encoder-testing/splade-bert-tiny-nq"}, models=models)
        self.assertIsNotNone(scoring.model)
        scoring.close()

    def testEmpty(self):
        """
        Test empty sparse vectors
        """

        scoring = ScoringFactory.create({"method": "sparse", "path": "sparse-encoder-testing/splade-bert-tiny-nq"})
        scoring.upsert((uid, {"text": text}, tags) for uid, text, tags in self.data)
        self.assertEqual(scoring.count(), len(self.data))

    @patch("torch.cuda.device_count")
    def testGPU(self, count):
        """
        Test sparse vectors with GPU encoding
        """

        # Mock accelerator count
        count.return_value = 2

        # Test multiple gpus
        scoring = ScoringFactory.create({"method": "sparse", "path": "sparse-encoder-testing/splade-bert-tiny-nq", "gpu": "all"})
        self.assertIsNotNone(scoring)
        scoring.close()

    def testIVFFlat(self):
        """
        Test sparse vectors with IVFFlat clustering
        """

        # Expand dataset
        data = self.data * 1000

        # Test higher volume IVFFlat index with clustering
        config = {
            "method": "sparse",
            "vectormethod": "sentence-transformers",
            "path": "sparse-encoder-testing/splade-bert-tiny-nq",
            "ivfsparse": {"sample": 1.0},
        }
        scoring = ScoringFactory.create(config)
        scoring.index((uid, {"text": text}, tags) for uid, text, tags in data)

        # Generate temp file path
        index = os.path.join(tempfile.gettempdir(), "scoring")
        os.makedirs(index, exist_ok=True)

        # Save scoring instance
        scoring.save(f"{index}/scoring.sparse.index")

        # Reload scoring instance
        scoring = ScoringFactory.create(config)
        scoring.load(f"{index}/scoring.sparse.index")

        # Run search and validate correct result returned
        results = scoring.search("lottery ticket", 1)
        self.assertGreater(len(results), 0)
        scoring.close()



================================================
FILE: test/python/testvectors/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testvectors/testdense/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testvectors/testdense/testcustom.py
================================================
"""
Custom module tests
"""

import os
import unittest

import numpy as np

from txtai.vectors import VectorsFactory


class TestCustom(unittest.TestCase):
    """
    Custom vectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create custom vectors instance.
        """

        cls.model = VectorsFactory.create({"method": "txtai.vectors.HFVectors", "path": "sentence-transformers/nli-mpnet-base-v2"}, None)

    def testIndex(self):
        """
        Test transformers indexing
        """

        # Generate enough volume to test batching
        documents = [(x, "This is a test", None) for x in range(1000)]

        ids, dimension, batches, stream = self.model.index(documents)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 768)
        self.assertEqual(batches, 2)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (500, 768))

    def testNotFound(self):
        """
        Test unresolvable vector backend
        """

        with self.assertRaises(ImportError):
            VectorsFactory.create({"method": "notfound.vectors"})



================================================
FILE: test/python/testvectors/testdense/testexternal.py
================================================
"""
External module tests
"""

import os
import unittest

import numpy as np

from txtai.vectors import External, VectorsFactory


class TestExternal(unittest.TestCase):
    """
    External vectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create External vectors instance.
        """

        cls.model = VectorsFactory.create({"method": "external"}, None)

    def testIndex(self):
        """
        Test indexing with external vectors
        """

        # Generate dummy data
        data = np.random.rand(1000, 768).astype(np.float32)

        # Generate enough volume to test batching
        documents = [(x, data[x], None) for x in range(1000)]

        ids, dimension, batches, stream = self.model.index(documents)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 768)
        self.assertEqual(batches, 2)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (500, 768))

    def testMethod(self):
        """
        Test method is derived when transform function passed
        """

        model = VectorsFactory.create({"transform": lambda x: x}, None)
        self.assertTrue(isinstance(model, External))



================================================
FILE: test/python/testvectors/testdense/testhuggingface.py
================================================
"""
Huggingface module tests
"""

import os
import unittest

import numpy as np

from txtai.vectors import VectorsFactory


class TestHFVectors(unittest.TestCase):
    """
    HFVectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create HFVectors instance.
        """

        cls.model = VectorsFactory.create({"path": "sentence-transformers/nli-mpnet-base-v2"}, None)

    def testIndex(self):
        """
        Test transformers indexing
        """

        # Generate enough volume to test batching
        documents = [(x, "This is a test", None) for x in range(1000)]

        ids, dimension, batches, stream = self.model.index(documents)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 768)
        self.assertEqual(batches, 2)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (500, 768))

    def testText(self):
        """
        Test transformers text conversion
        """

        self.model.tokenize = True
        self.assertEqual(self.model.prepare("Y 123 This is a test!"), "test")
        self.assertEqual(self.model.prepare(["This", "is", "a", "test"]), "This is a test")

        self.model.tokenize = False
        self.assertEqual(self.model.prepare("Y 123 This is a test!"), "Y 123 This is a test!")
        self.assertEqual(self.model.prepare(["This", "is", "a", "test"]), "This is a test")

    def testTransform(self):
        """
        Test transformers transform
        """

        # Sample documents: one where tokenizer changes text and one with no changes to text
        documents = [(0, "This is a test and has no tokenization", None), (1, "test tokenization", None)]

        # Run with tokenization enabled
        self.model.tokenize = True
        embeddings1 = [self.model.transform(d) for d in documents]

        # Run with tokenization disabled
        self.model.tokenize = False
        embeddings2 = [self.model.transform(d) for d in documents]

        self.assertFalse(np.array_equal(embeddings1[0], embeddings2[0]))
        self.assertTrue(np.array_equal(embeddings1[1], embeddings2[1]))

    def testTransformArray(self):
        """
        Test transformers skips transforming NumPy arrays
        """

        # Generate data and run through vector model
        data1 = np.random.rand(5, 5).astype(np.float32)
        data2 = self.model.transform((0, data1, None))

        # Test transform method returns original data
        self.assertTrue(np.array_equal(data1, data2))

    def testTransformLong(self):
        """
        Test transformers transform on long text
        """

        # Sample documents: short text and longer text
        documents = [(0, "This is long text " * 512, None), (1, "This is short text", None)]

        # Run transform and ensure it completes without errors
        embeddings = [self.model.transform(d) for d in documents]
        self.assertIsNotNone(embeddings)



================================================
FILE: test/python/testvectors/testdense/testlitellm.py
================================================
"""
LiteLLM module tests
"""

import json
import os
import unittest

from http.server import HTTPServer, BaseHTTPRequestHandler
from threading import Thread

import numpy as np

from txtai.vectors import VectorsFactory


class RequestHandler(BaseHTTPRequestHandler):
    """
    Test HTTP handler.
    """

    def do_POST(self):
        """
        POST request handler.
        """

        # Generate mock response
        response = [[0.0] * 768]
        response = json.dumps(response).encode("utf-8")

        self.send_response(200)
        self.send_header("content-type", "application/json")
        self.send_header("content-length", len(response))
        self.end_headers()

        self.wfile.write(response)
        self.wfile.flush()


class TestLiteLLM(unittest.TestCase):
    """
    LiteLLM vectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create mock http server.
        """

        cls.httpd = HTTPServer(("127.0.0.1", 8004), RequestHandler)

        server = Thread(target=cls.httpd.serve_forever, daemon=True)
        server.start()

    @classmethod
    def tearDownClass(cls):
        """
        Shutdown mock http server.
        """

        cls.httpd.shutdown()

    def testIndex(self):
        """
        Test indexing with LiteLLM vectors
        """

        # LiteLLM vectors instance
        model = VectorsFactory.create(
            {"path": "huggingface/sentence-transformers/all-MiniLM-L6-v2", "vectors": {"api_base": "http://127.0.0.1:8004"}}, None
        )

        ids, dimension, batches, stream = model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 768)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 768))



================================================
FILE: test/python/testvectors/testdense/testllama.py
================================================
"""
Llama module tests
"""

import os
import unittest

import numpy as np

from txtai.vectors import VectorsFactory


class TestLlamaCpp(unittest.TestCase):
    """
    llama.cpp vectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create LlamaCpp instance.
        """

        cls.model = VectorsFactory.create({"path": "nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q2_K.gguf"}, None)

    def testIndex(self):
        """
        Test indexing with LlamaCpp vectors
        """

        ids, dimension, batches, stream = self.model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 768)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 768))



================================================
FILE: test/python/testvectors/testdense/testm2v.py
================================================
"""
Model2Vec module tests
"""

import os
import unittest

import numpy as np

from txtai.vectors import VectorsFactory


class TestModel2Vec(unittest.TestCase):
    """
    Model2vec vectors tests
    """

    @classmethod
    def setUpClass(cls):
        """
        Create Model2Vec instance.
        """

        cls.model = VectorsFactory.create({"path": "minishlab/potion-base-8M"}, None)

    def testIndex(self):
        """
        Test indexing with Model2Vec vectors
        """

        ids, dimension, batches, stream = self.model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 256)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 256))



================================================
FILE: test/python/testvectors/testdense/testsbert.py
================================================
"""
Sentence Transformers module tests
"""

import os
import unittest

from unittest.mock import patch

import numpy as np

from txtai.vectors import VectorsFactory


class TestSTVectors(unittest.TestCase):
    """
    STVectors tests
    """

    def testIndex(self):
        """
        Test indexing with sentence-transformers vectors
        """

        model = VectorsFactory.create({"method": "sentence-transformers", "path": "paraphrase-MiniLM-L3-v2"}, None)
        ids, dimension, batches, stream = model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 384)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 384))

    @patch("torch.cuda.device_count")
    def testMultiGPU(self, count):
        """
        Test multiple gpu encoding
        """

        # Mock accelerator count
        count.return_value = 2

        model = VectorsFactory.create({"method": "sentence-transformers", "path": "paraphrase-MiniLM-L3-v2", "gpu": "all"}, None)
        ids, dimension, batches, stream = model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 384)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 384))

        # Close the multiprocessing pool
        model.close()



================================================
FILE: test/python/testvectors/testdense/testvectors.py
================================================
"""
Vectors module tests
"""

import os
import tempfile
import unittest

import numpy as np

from txtai.vectors import Vectors, Recovery


class TestVectors(unittest.TestCase):
    """
    Vectors tests.
    """

    def testNotImplemented(self):
        """
        Test exceptions for non-implemented methods
        """

        vectors = Vectors(None, None, None)

        self.assertRaises(NotImplementedError, vectors.load, None)
        self.assertRaises(NotImplementedError, vectors.encode, None)

    def testNormalize(self):
        """
        Test batch normalize and single input normalize are equal
        """

        vectors = Vectors(None, None, None)

        # Generate data
        data1 = np.random.rand(5, 5).astype(np.float32)
        data2 = data1.copy()

        # Keep original data to ensure it changed
        original = data1.copy()

        # Normalize data
        vectors.normalize(data1)
        for x in data2:
            vectors.normalize(x)

        # Test both data arrays are the same and changed from original
        self.assertTrue(np.allclose(data1, data2))
        self.assertFalse(np.allclose(data1, original))

    def testRecovery(self):
        """
        Test vectors recovery failure
        """

        # Checkpoint directory
        checkpoint = os.path.join(tempfile.gettempdir(), "recovery")
        os.makedirs(checkpoint, exist_ok=True)

        # Create empty file
        # pylint: disable=R1732
        f = open(os.path.join(checkpoint, "id"), "w", encoding="utf-8")
        f.close()

        # Create the recovery instance with an empty checkpoint file
        recovery = Recovery(checkpoint, "id", np.load)
        self.assertIsNone(recovery())



================================================
FILE: test/python/testvectors/testdense/testwordvectors.py
================================================
"""
WordVectors module tests
"""

import os
import tempfile
import unittest

from unittest.mock import patch

import numpy as np

from huggingface_hub.errors import HFValidationError
from txtai.vectors import VectorsFactory
from txtai.vectors.dense.words import create, transform


class TestWordVectors(unittest.TestCase):
    """
    Vectors tests.
    """

    @classmethod
    def setUpClass(cls):
        """
        Sets the pretrained model to use
        """

        # Test with pretrained glove quantized vectors
        cls.path = "neuml/glove-6B-quantized"

    @patch("os.cpu_count")
    def testIndex(self, cpucount):
        """
        Test word vectors indexing
        """

        # Mock CPU count
        cpucount.return_value = 1

        # Generate data
        documents = [(x, "This is a test", None) for x in range(1000)]

        model = VectorsFactory.create({"path": self.path, "parallel": True}, None)

        ids, dimension, batches, stream = model.index(documents, 1)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 300)
        self.assertEqual(batches, 1000)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 300))

    @patch("os.cpu_count")
    def testIndexBatch(self, cpucount):
        """
        Test word vectors indexing with batch size set
        """

        # Mock CPU count
        cpucount.return_value = 1

        # Generate data
        documents = [(x, "This is a test", None) for x in range(1000)]

        model = VectorsFactory.create({"path": self.path, "parallel": True}, None)

        ids, dimension, batches, stream = model.index(documents, 512)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 300)
        self.assertEqual(batches, 2)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (512, 300))
            self.assertEqual(np.load(queue).shape, (488, 300))

    def testIndexSerial(self):
        """
        Test word vector indexing in single process mode
        """

        # Generate data
        documents = [(x, "This is a test", None) for x in range(1000)]

        model = VectorsFactory.create({"path": self.path, "parallel": False}, None)

        ids, dimension, batches, stream = model.index(documents, 1)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 300)
        self.assertEqual(batches, 1000)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (1, 300))

    def testIndexSerialBatch(self):
        """
        Test word vector indexing in single process mode with batch size set
        """

        # Generate data
        documents = [(x, "This is a test", None) for x in range(1000)]

        model = VectorsFactory.create({"path": self.path, "parallel": False}, None)

        ids, dimension, batches, stream = model.index(documents, 512)

        self.assertEqual(len(ids), 1000)
        self.assertEqual(dimension, 300)
        self.assertEqual(batches, 2)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(np.load(queue).shape, (512, 300))
            self.assertEqual(np.load(queue).shape, (488, 300))

    def testLookup(self):
        """
        Test word vector lookup
        """

        model = VectorsFactory.create({"path": self.path}, None)
        self.assertEqual(model.lookup(["txtai", "embeddings", "sentence"]).shape, (3, 300))

    def testMultiprocess(self):
        """
        Test multiprocess helper methods
        """

        create({"path": self.path}, None)

        uid, vector = transform((0, "test", None))
        self.assertEqual(uid, 0)
        self.assertEqual(vector.shape, (300,))

    def testNoExist(self):
        """
        Test loading model that doesn't exist
        """

        # Test non-existent path raises an exception
        with self.assertRaises((IOError, HFValidationError)):
            VectorsFactory.create({"method": "words", "path": os.path.join(tempfile.gettempdir(), "noexist")}, None)

    def testTransform(self):
        """
        Test word vector transform
        """

        model = VectorsFactory.create({"path": self.path}, None)
        self.assertEqual(len(model.transform((None, ["txtai"], None))), 300)



================================================
FILE: test/python/testvectors/testsparse/__init__.py
================================================
[Empty file]


================================================
FILE: test/python/testvectors/testsparse/testsbert.py
================================================
"""
Sparse Sentence Transformers module tests
"""

import os
import unittest

from txtai.vectors import SparseVectorsFactory
from txtai.util import SparseArray


class TestSparseSTVectors(unittest.TestCase):
    """
    SparseSTVectors tests
    """

    def testIndex(self):
        """
        Test indexing with sentence-transformers vectors
        """

        model = SparseVectorsFactory.create({"method": "sentence-transformers", "path": "sparse-encoder-testing/splade-bert-tiny-nq"})
        ids, dimension, batches, stream = model.index([(0, "test", None)])

        self.assertEqual(len(ids), 1)
        self.assertEqual(dimension, 30522)
        self.assertEqual(batches, 1)
        self.assertIsNotNone(os.path.exists(stream))

        # Test shape of serialized embeddings
        with open(stream, "rb") as queue:
            self.assertEqual(SparseArray().load(queue).shape, (1, 30522))



================================================
FILE: test/python/testvectors/testsparse/testvectors.py
================================================
"""
Sparse Vectors module tests
"""

import unittest

from txtai.vectors import SparseVectors, SparseVectorsFactory


class TestSparseVectors(unittest.TestCase):
    """
    Sparse Vectors tests.
    """

    def testCustom(self):
        """
        Test custom sparse vectors instance
        """

        self.assertIsNotNone(
            SparseVectorsFactory.create({"method": "txtai.vectors.SparseSTVectors", "path": "sparse-encoder-testing/splade-bert-tiny-nq"})
        )

    def testDefaultNormalize(self):
        """
        Test defaultnormalize method
        """

        vectors = SparseVectors(None, None, None)
        self.assertFalse(vectors.defaultnormalize())

    def testNotSupported(self):
        """
        Test exceptions for unsupported methods
        """

        vectors = SparseVectors(None, None, None)

        self.assertRaises(ValueError, vectors.truncate, None)
        self.assertRaises(ValueError, vectors.quantize, None)

    def testNotFound(self):
        """
        Test unresolvable vector backend
        """

        with self.assertRaises(ImportError):
            SparseVectorsFactory.create({"method": "notfound.vectors"})



================================================
FILE: .github/workflows/build.yml
================================================
# GitHub Actions build workflow
name: build

on: ["push", "pull_request"]

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]

    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Install Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.10"

      - name: Install Java
        uses: actions/setup-java@v5
        with:
          distribution: "zulu"
          java-version: 21

      - name: Install dependencies - Linux
        run: |
          sudo apt-get update
          sudo apt-get install libportaudio2 libsndfile1
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc
        if: matrix.os == 'ubuntu-latest'

      - name: Install dependencies - macOS
        run: |
          echo "OMP_NUM_THREADS=1" >> $GITHUB_ENV
          echo "PYTORCH_MPS_DISABLE=1" >> $GITHUB_ENV
          echo "LLAMA_NO_METAL=1" >> $GITHUB_ENV
          echo "TIKA_STARTUP_SLEEP=30" >> $GITHUB_ENV
          echo "TIKA_STARTUP_MAX_RETRY=10" >> $GITHUB_ENV
          brew install portaudio
          sudo xcode-select -s "/Applications/Xcode_16.app"
        if: matrix.os == 'macos-latest'

      - name: Install dependencies - Windows
        run: |
          "PYTHONIOENCODING=utf-8" >> $env:GITHUB_ENV
          choco install wget
        if: matrix.os == 'windows-latest'

      - name: Build
        run: |
          pip install -U wheel
          pip install gliner==0.2.21
          pip install .[all,dev]
          pip cache purge
          python -c "import nltk; nltk.download(['punkt', 'punkt_tab', 'averaged_perceptron_tagger_eng'])"
          python --version
          make data coverage
        env:
          HF_HUB_ETAG_TIMEOUT: 100
          HF_HUB_DOWNLOAD_TIMEOUT: 100
          HF_XET_CHUNK_CACHE_SIZE_BYTES: 0

      - uses: pre-commit/action@v3.0.1
        if: matrix.os == 'ubuntu-latest'

      - name: Test Coverage
        run: coveralls --service=github
        if: matrix.os == 'ubuntu-latest'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}



================================================
FILE: .github/workflows/docs.yml
================================================
name: docs
on:
  push:
    branches:
      - master
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-python@v6
        with:
          python-version: "3.10"
      - run: |
          pip install -U pip wheel
          pip install .[all,dev]
      - run: mkdocs gh-deploy --force
 

