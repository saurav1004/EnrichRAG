Directory structure:
â””â”€â”€ ruc-nlpir-flashrag/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CITATION.cff
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ _navbar.md
    â”‚   â”œâ”€â”€ _sidebar.md
    â”‚   â”œâ”€â”€ index.html
    â”‚   â”œâ”€â”€ sw.js
    â”‚   â”œâ”€â”€ .nojekyll
    â”‚   â”œâ”€â”€ en-us/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ .nojekyll
    â”‚   â”‚   â””â”€â”€ get_started/
    â”‚   â”‚       â””â”€â”€ introduction.md
    â”‚   â””â”€â”€ original_docs/
    â”‚       â”œâ”€â”€ baseline_details.md
    â”‚       â”œâ”€â”€ basic_usage.md
    â”‚       â”œâ”€â”€ benchmark_result.xlsx
    â”‚       â”œâ”€â”€ building-index.md
    â”‚       â”œâ”€â”€ chunk-doc-corpus.md
    â”‚       â”œâ”€â”€ configuration.md
    â”‚       â”œâ”€â”€ introduction_for_beginners_en.md
    â”‚       â”œâ”€â”€ multi_retriever_usage.md
    â”‚       â”œâ”€â”€ process-wiki.md
    â”‚       â””â”€â”€ reproduce_experiment.md
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ multi_turn.py
    â”‚   â”œâ”€â”€ run_refiner.py
    â”‚   â”œâ”€â”€ methods/
    â”‚   â”‚   â”œâ”€â”€ my_config.yaml
    â”‚   â”‚   â”œâ”€â”€ run_exp.py
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â””â”€â”€ get_lm_probs_dataset.py
    â”‚   â”œâ”€â”€ quick_start/
    â”‚   â”‚   â”œâ”€â”€ demo_en.py
    â”‚   â”‚   â”œâ”€â”€ simple_pipeline.py
    â”‚   â”‚   â””â”€â”€ dataset/
    â”‚   â”‚       â””â”€â”€ nq/
    â”‚   â”‚           â””â”€â”€ test.jsonl
    â”‚   â””â”€â”€ run_mm/
    â”‚       â”œâ”€â”€ build_index.sh
    â”‚       â”œâ”€â”€ my_config.yaml
    â”‚       â””â”€â”€ rum_mm_exp.py
    â”œâ”€â”€ flashrag/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ version.py
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ basic_config.yaml
    â”‚   â”‚   â””â”€â”€ config.py
    â”‚   â”œâ”€â”€ dataset/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ dataset.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ evaluator/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ _bleu.py
    â”‚   â”‚   â”œâ”€â”€ evaluator.py
    â”‚   â”‚   â”œâ”€â”€ metrics.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ generator/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ fid.py
    â”‚   â”‚   â”œâ”€â”€ generator.py
    â”‚   â”‚   â”œâ”€â”€ multimodal_generator.py
    â”‚   â”‚   â”œâ”€â”€ openai_generator.py
    â”‚   â”‚   â”œâ”€â”€ stop_word_criteria.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ judger/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ judger.py
    â”‚   â”œâ”€â”€ pipeline/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ branching_pipeline.py
    â”‚   â”‚   â”œâ”€â”€ mm_pipeline.py
    â”‚   â”‚   â”œâ”€â”€ pipeline.py
    â”‚   â”‚   â”œâ”€â”€ ReaRAG_utils.py
    â”‚   â”‚   â”œâ”€â”€ reasoning_pipeline.py
    â”‚   â”‚   â””â”€â”€ replug_utils.py
    â”‚   â”œâ”€â”€ prompt/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ base_prompt.py
    â”‚   â”‚   â”œâ”€â”€ coRAG_prompt.py
    â”‚   â”‚   â”œâ”€â”€ mm_prompt.py
    â”‚   â”‚   â””â”€â”€ selfask_examplars.py
    â”‚   â”œâ”€â”€ refiner/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ kg_refiner.py
    â”‚   â”‚   â”œâ”€â”€ refiner.py
    â”‚   â”‚   â””â”€â”€ selective_context_compressor.py
    â”‚   â”œâ”€â”€ retriever/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ __main__.py
    â”‚   â”‚   â”œâ”€â”€ encoder.py
    â”‚   â”‚   â”œâ”€â”€ index_builder.py
    â”‚   â”‚   â”œâ”€â”€ reranker.py
    â”‚   â”‚   â”œâ”€â”€ retriever.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ constants.py
    â”‚       â”œâ”€â”€ pred_parse.py
    â”‚       â””â”€â”€ utils.py
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ build_index.sh
    â”‚   â”œâ”€â”€ chunk_doc_corpus.py
    â”‚   â””â”€â”€ preprocess_wiki.py
    â””â”€â”€ .github/
        â””â”€â”€ scripts/
            â””â”€â”€ python/
                â””â”€â”€ update_version.py

================================================
FILE: README.md
================================================
# <div align="center">âš¡FlashRAG: A Python Toolkit for Efficient RAG Research<div>
\[ English | [ä¸­æ–‡](README_zh.md) \]
<div align="center">
<a href="https://arxiv.org/abs/2405.13576" target="_blank"><img src=https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv></a>
<a href="https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg></a>
<a href="https://www.modelscope.cn/datasets/hhjinjiajie/FlashRAG_Dataset" target="_blank"><img src=https://custom-icon-badges.demolab.com/badge/ModelScope%20Datasets-624aff?style=flat&logo=modelscope&logoColor=white></a>
<a href="https://deepwiki.com/RUC-NLPIR/FlashRAG"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="DeepWiki Document" height="20"/></a>
<a href="https://github.com/RUC-NLPIR/FlashRAG/blob/main/LICENSE"><img alt="License" src="https://img.shields.io/badge/LICENSE-MIT-green"></a>
<a><img alt="Static Badge" src="https://img.shields.io/badge/made_with-Python-blue"></a>
</div>

<h4 align="center">

<p>
<a href="#wrench-installation">Installation</a> |
<a href="#sparkles-features">Features</a> |
<a href="#rocket-quick-start">Quick-Start</a> |
<a href="#gear-components"> Components</a> |
<a href="#art-flashrag-ui"> FlashRAG-UI</a> |
<a href="#robot-supporting-methods"> Supporting Methods</a> |
<a href="#notebook-supporting-datasets--document-corpus"> Supporting Datasets</a> |
<a href="#raised_hands-additional-faqs"> FAQs</a>
</p>

</h4>


FlashRAG is a Python toolkit for the reproduction and development of Retrieval Augmented Generation (RAG) research. Our toolkit includes 36 pre-processed benchmark RAG datasets and **23 state-of-the-art RAG algorithms**, including **7 reasoning-based methods** that combine reasoning ability with retrieval.

<p align="center">
<img src="asset/framework.jpg">
</p>

With FlashRAG and provided resources, you can effortlessly reproduce existing SOTA works in the RAG domain or implement your custom RAG processes and components. Besides, we provide an easy-to-use UI:

https://github.com/user-attachments/assets/8ca00873-5df2-48a7-b853-89e7b18bc6e9

<p>
<a href="https://trendshift.io/repositories/10454" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10454" alt="RUC-NLPIR%2FFlashRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

## :link: Navigation
- [Features](#sparkles-features)
- [Roadmap](#mag_right-roadmap)
- [Changelog](#page_with_curl-changelog)
- [Installation](#wrench-installation)
- [Quick Start](#rocket-quick-start)
- [Components](#gear-components)
- [FlashRAG-UI](#art-flashrag-ui)
- [Supporting Methods](#robot-supporting-methods)
- [Supporting Datasets & Document Corpus](#notebook-supporting-datasets--document-corpus)
- [Additional FAQs](#raised_hands-additional-faqs)
- [License](#bookmark-license)
- [Citation](#star2-citation)

## :sparkles: Features

- **Extensive and Customizable Framework**: Includes essential components for RAG scenarios such as retrievers, rerankers, generators, and compressors, allowing for flexible assembly of complex pipelines.

- **Comprehensive Benchmark Datasets**: A collection of 36 pre-processed RAG benchmark datasets to test and validate RAG models' performances.

- **Pre-implemented Advanced RAG Algorithms**: Features **23 advancing RAG algorithms** with reported results, based on our framework. Easily reproducing results under different settings.

- **ðŸš€ Reasoning-based Methods**: **NEW!** We now support **7 reasoning-based methods** that combine reasoning ability with retrieval, achieving superior performance on complex multi-hop tasks.

- **Efficient Preprocessing Stage**: Simplifies the RAG workflow preparation by providing various scripts like corpus processing for retrieval, retrieval index building, and pre-retrieval of documents.

- **Optimized Execution**: The library's efficiency is enhanced with tools like vLLM, FastChat for LLM inference acceleration, and Faiss for vector index management.

- **Easy to Use UI** : We have developed a very easy to use UI to easily and quickly configure and experience the RAG baselines we have implemented, as well as run evaluation scripts on a visual interface.

## :mag_right: Roadmap

FlashRAG is still under development and there are many issues and room for improvement. We will continue to update. And we also sincerely welcome contributions on this open-source toolkit.

- [x] Support OpenAI models
- [x] Provdide instructions for each component
- [x] Integrate sentence Transformers
- [x] Support multimodal RAG
- [x] Support reasoning-based methods
- [ ] Inlcude more RAG approaches
- [ ] Enhance code adaptability and readability
- [ ] Add support for api-based retriever (vllm server)

## :page_with_curl: Changelog
[25/11/06] ðŸŽ¯ NEW Retriever! We have integrated a Web Search Engine-based Retriever, which seamlessly integrates with existing methods and can be enabled quickly with just a Serper API key! This enhancement significantly expands retrieval coverage and real-time capability, supporting dynamic information access and external knowledge augmentation. Experience a more flexible and powerful retrieval workflow now! 

[25/08/06] ðŸŽ¯ **NEW!** We have added support for **Reasoning Pipeline**, which is a new paradigm that combines reasoning ability and retrieval, representing work that includes [R1-Searcher](https://github.com/SsmallSong/R1-Searcher), [Search-R1](https://github.com/PeterGriffinJin/Search-R1),.... We evaluate the performance of the pipeline on various RAG benchmarks, it can achieve F1 scores close to 60 on multi hop inference datasets such as HotpotQA. See it in [**result table**](#robot-supporting-methods).

[25/03/21] ðŸš€ **Major Update!** We have expanded our toolkit to support **23 state-of-the-art RAG algorithms**, including **7 reasoning-based methods** that significantly improve performance on complex reasoning tasks. This represents a major milestone in our toolkit's evolution!

[25/02/24] ðŸ”¥ðŸ”¥ðŸ”¥ We have added support for **multimodal RAG**, including [**MLLMs like Llava, Qwen, InternVL**](https://ruc-nlpir.github.io/FlashRAG/#/zh-cn/component/generator?id=%e5%a4%9a%e6%a8%a1%e6%80%81%e7%94%9f%e6%88%90%e5%99%a8), and various [**multimodal retrievers with Clip architecture**](https://ruc-nlpir.github.io/FlashRAG/#/zh-cn/component/retriever?id=%e5%a4%9a%e6%a8%a1%e6%80%81%e6%a3%80%e7%b4%a2%e5%99%a8). More information can be found in our new version of arxiv article and our documentation. Try it!

[25/01/21] Our technical paper [FlashRAG: A Python Toolkit for Efficient RAG Research](https://arxiv.org/abs/2405.13576) is honored to have been accepted to the Resource Track of the 2025 **ACM Web Conference (WWW 2025)**. Please Check it out!

[25/01/12] Introduce <strong>FlashRAG-UI</strong>, an easy to use interface. You can easily and quickly configure and experience the supported RAG methods and evaluate them on the benchmarks.

[25/01/11] We have added support for a new method [<u>RQRAG</u>](https://arxiv.org/abs/2404.00610) method, see it in [**reproduce_experiment**](docs/original_docs/reproduce_experiment.md).

[25/01/07] We have currently support the aggregation of multiple retrievers, see it in [**multi retriever usage**](https://github.com/RUC-NLPIR/FlashRAG/blob/main/docs/original_docs/multi_retriever_usage.md).

[25/01/07] We have integrated a very flexible and lightweight corpus chunking library [**Chunkie**](https://github.com/chonkie-ai/chonkie?tab=readme-ov-file#usage), which supports various custom chunking methods (tokens, sentences, semantic, etc.). Use it in [<u>chunking doc corpus</u>](docs/original_docs/chunk-doc-corpus.md).

[24/10/21] We have released a version based on the Paddle framework that supports Chinese hardware platforms. Please refer to [FlashRAG Paddle](https://github.com/RUC-NLPIR/FlashRAG-Paddle) for details.

[24/10/13] A new in-domain dataset and corpus - [DomainRAG](https://arxiv.org/pdf/2406.05654) have been added to the dataset. The dataset is based on the internal enrollment data of Renmin University of China, covering seven types of tasks, which can be used for conducting domain-specific RAG testing.

[24/09/24] We have released a version based on the MindSpore framework that supports Chinese hardware platforms. Please refer to [FlashRAG MindSpore](https://github.com/RUC-NLPIR/FlashRAG-MindSpore) for details.

<details>
<summary>Show more</summary>

[24/09/18] Due to the complexity and limitations of installing Pyserini in certain environments, we have introduced a lightweight `BM25s` package as an alternative (faster and easier to use). The retriever based on Pyserini will be deprecated in future versions. To use retriever with `bm25s`, just set `bm25_backend` to `bm25s` in config.

[24/09/09] We add support for a new method [<u>Adaptive-RAG</u>](https://aclanthology.org/2024.naacl-long.389.pdf), which can automatically select the RAG process to execute based on the type of query. See it result in [<u>result table</u>](#robot-supporting-methods).

[24/08/02] We add support for a new method [<u>Spring</u>](https://arxiv.org/abs/2405.19670), significantly improve the performance of LLM by adding only a few token embeddings. See it result in [<u>result table</u>](#robot-supporting-methods).

[24/07/17] Due to some unknown issues with HuggingFace, our original dataset link has been invalid. We have updated it. Please check the [new link](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/) if you encounter any problems.

[24/07/06] We add support for a new method: [<u>Trace</u>](https://arxiv.org/abs/2406.11460), which refine text by constructing a knowledge graph. See it [<u>results</u>](#robot-supporting-methods) and [<u>details</u>](./docs/original_docs/baseline_details.md).

[24/06/19] We add support for a new method: [<u>IRCoT</u>](https://arxiv.org/abs/2212.10509), and update the [<u>result table</u>](#robot-supporting-methods).

[24/06/15] We provide a [<u>demo</u>](./examples/quick_start/demo_en.py) to perform the RAG process using our toolkit.

[24/06/11] We have integrated `sentence transformers` in the retriever module. Now it's easier to use the retriever without setting pooling methods.

[24/06/05] We have provided detailed document for reproducing existing methods (see [how to reproduce](./docs/original_docs/reproduce_experiment.md), [baseline details](./docs/original_docs/baseline_details.md)), and [<u>configurations settings</u>](./docs/original_docs/configuration.md).

[24/06/02] We have provided an introduction of FlashRAG for beginners, see [<u>an introduction to flashrag</u>](./docs/original_docs/introduction_for_beginners_en.md) ([<u>ä¸­æ–‡ç‰ˆ</u>](./docs/original_docs/introduction_for_beginners_zh.md) [<u>í•œêµ­ì–´</u>](./docs/original_docs/introduction_for_beginners_kr.md)).

[24/05/31] We supported Openai-series models as generator.

</details>

## :wrench: Installation
![PyPI - Version](https://img.shields.io/pypi/v/flashrag-dev) 
![PyPI - Downloads](https://img.shields.io/pypi/dw/flashrag-dev) 
![PyPI - Downloads](https://img.shields.io/pypi/dm/flashrag-dev)

To get started with FlashRAG, you can simply install it with pip:

```base
pip install flashrag-dev --pre
```

Or you can clone it from Github and install (requires Python 3.10+):

```bash
git clone https://github.com/RUC-NLPIR/FlashRAG.git
cd FlashRAG
pip install -e .
```

If you want to use vllm, sentence-transformers or pyserini, you can install the optional dependencies:

```bash
# Install all extra dependencies
pip install flashrag-dev[full]

# Install vllm for faster speed
pip install vllm>=0.4.1

# Install sentence-transformers
pip install sentence-transformers

# Install pyserini for bm25
pip install pyserini
```

Due to the incompatibility when installing `faiss` using `pip`, it is necessary to use the following conda command for installation.

```bash
# CPU-only version
conda install -c pytorch faiss-cpu=1.8.0

# GPU(+CPU) version
conda install -c pytorch -c nvidia faiss-gpu=1.8.0
```

Note: It is impossible to install the latest version of `faiss` on certain systems.

From the official Faiss repository ([source](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md)):

> - The CPU-only faiss-cpu conda package is currently available on Linux (x86_64 and arm64), OSX (arm64 only), and Windows (x86_64)
> - faiss-gpu, containing both CPU and GPU indices, is available on Linux (x86_64 only) for CUDA 11.4 and 12.1

## :rocket: Quick Start

### Corpus Construction
To build an index, you first need to save your corpus as a `jsonl` file with each line representing a document.

```jsonl
{"id": "0", "contents": "..."}
{"id": "1", "contents": "..."}
```

If you want to use Wikipedia as your corpus, you can refer to our documentation [Processing Wikipedia](./docs/original_docs/process-wiki.md) to convert it into an indexable format.

### Index Construction

You can use the following code to build your own index.

* For **dense retrieval methods**, especially popular embedding models, we use `faiss` to build the index.

* For **sparse retrieval methods (BM25)**, we use `Pyserini` or `bm25s` to build the corpus into a Lucene inverted index. The built index contains the original documents.

#### For Dense Retrieval Methods

Modify the parameters in the following code to your own.

```bash
python -m flashrag.retriever.index_builder \
  --retrieval_method e5 \
  --model_path /model/e5-base-v2/ \
  --corpus_path indexes/sample_corpus.jsonl \
  --save_dir indexes/ \
  --use_fp16 \
  --max_length 512 \
  --batch_size 256 \
  --pooling_method mean \
  --faiss_type Flat 
```

* ```--pooling_method```: If this parameter is not specified, we will automatically select it based on the model name and model file. However, since different embedding models use different pooling methods, **we may not have fully implemented them**. To ensure accuracy, you can **specify the pooling method corresponding to the retrieval model you are using** (`mean`, `pooler`, or `cls`).

* ```---instruction```: Some embedding models require additional instructions to be concatenated to the query before encoding, which can be specified here. Currently, we will automatically fill in the instructions for **E5** and **BGE** models, while other models need to be supplemented manually.

If the retrieval model supports the `sentence transformers` library, you can use the following code to build the index (**without considering the pooling method**).

```bash
python -m flashrag.retriever.index_builder \
  --retrieval_method e5 \
  --model_path /model/e5-base-v2/ \
  --corpus_path indexes/sample_corpus.jsonl \
  --save_dir indexes/ \
  --use_fp16 \
  --max_length 512 \
  --batch_size 256 \
  --pooling_method mean \
  --sentence_transformer \
  --faiss_type Flat 
```

#### For Sparse Retrieval Methods (BM25)

If building a bm25 index, there is no need to specify `model_path`.

##### Building Index with BM25s

```bash
python -m flashrag.retriever.index_builder \
  --retrieval_method bm25 \
  --corpus_path indexes/sample_corpus.jsonl \
  --bm25_backend bm25s \
  --save_dir indexes/ 
```

##### Building Index with Pyserini

```bash
python -m flashrag.retriever.index_builder \
  --retrieval_method bm25 \
  --corpus_path indexes/sample_corpus.jsonl \
  --bm25_backend pyserini \
  --save_dir indexes/ 
```

### For Sparse Neural Retrieval Methods (SPLADE)

##### Install Seismic Index:
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Install Rust for compiling
pip install pyseismic-lsr # Install Seismic
```

##### Then build the index with Seismic:
```bash
python -m flashrag.retriever.index_builder \ # builder
        --retrieval_method splade \ # Model name to trigger seismic index (splade only available)
        --model_path retriever/splade-v3 \ # Local path or repository path are both supported.
        --corpus_embedded_path data/ms_marco/ms_marco_embedded_corpus.jsonl \  # Use cached embedded corpus if corpus is already available in seismic expected format
        --corpus_path data/ms_marco/ms_marco_corpus.jsonl \ # Corpus path in format {id, contents} jsonl file to be embedded if not already built
        --save_dir indexes/ \ # save index directory
        --use_fp16 \ # tell to use fp16 for splade model
        --max_length 512 \ # max tokens for each document
        --batch_size 4 \ # batch size for splade model (4-5 seems the best size for Tesla T4 16GB)
        --n_postings 1000 \ # seismic number of posting lists
        --centroid_fraction 0.2 \ # seismic centroids
        --min_cluster_size 2 \ # seismic min cluster
        --summary_energy 0.4 \ # seismic energy
        --batched_indexing 10000000 # seismic batch
        --nknn 32 # Optional parameter. Tell to seismic to use also knn graph. if not present seismic will work without knn graph
```
### Using the ready-made pipeline

You can use the pipeline class we have already built (as shown in [<u>pipelines</u>](#pipelines)) to implement the RAG process inside. In this case, you just need to configure the config and load the corresponding pipeline.

Firstly, load the entire process's config, which records various hyperparameters required in the RAG process. You can input yaml files as parameters or directly as variables.

Please note that **variables as input take precedence over files**.

```python
from flashrag.config import Config

# hybrid load configs
config_dict = {'data_dir': 'dataset/'}
my_config = Config(
    config_file_path = 'my_config.yaml',
    config_dict = config_dict
```

We provide comprehensive guidance on how to set configurations, you can see our [<u>configuration guidance</u>](./docs/original_docs/configuration.md).
You can also refer to the [<u>basic yaml file</u>](./flashrag/config/basic_config.yaml) we provide to set your own parameters.

Next, load the corresponding dataset and initialize the pipeline. The components in the pipeline will be automatically loaded.

```python
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline
from flashrag.prompt import PromptTemplate
from flashrag.config import Config

config_dict = {'data_dir': 'dataset/'}
my_config = Config(
    config_file_path = 'my_config.yaml',
    config_dict = config_dict
)
all_split = get_dataset(my_config)
test_data = all_split['test']

pipeline = SequentialPipeline(my_config)
```

You can specify your own input prompt using `PromptTemplete`:

```python
prompt_templete = PromptTemplate(
    config,
    system_prompt = "Answer the question based on the given document. Only give me the answer and do not output any other words.\nThe following are given documents.\n\n{reference}",
    user_prompt = "Question: {question}\nAnswer:"
)
pipeline = SequentialPipeline(
  my_config,
  prompt_template = prompt_templete
)
```

Finally, execute `pipeline.run` to obtain the final result.

```python
output_dataset = pipeline.run(test_data, do_eval=True)
```

The `output_dataset` contains the intermediate results and metric scores for each item in the input dataset.
Meanwhile, the dataset with intermediate results and the overall evaluation score will also be saved as a file (if `save_intermediate_data` and `save_metric_score` are specified).

### Build your own pipeline!

Sometimes you may need to implement more complex RAG process, and you can build your own pipeline to implement it.
You just need to inherit `BasicPipeline`, initialize the components you need, and complete the `run` function.

```python
from flashrag.pipeline import BasicPipeline
from flashrag.utils import get_retriever, get_generator

class ToyPipeline(BasicPipeline):
  def __init__(self, config, prompt_templete=None):
    # Load your own components
    pass

  def run(self, dataset, do_eval=True):
    # Complete your own process logic

    # get attribute in dataset using `.`
    input_query = dataset.question
    ...
    # use `update_output` to save intermeidate data
    dataset.update_output("pred",pred_answer_list)
    dataset = self.evaluate(dataset, do_eval=do_eval)
    return dataset
```

Please first understand the input and output forms of the components you need to use from our [<u>documentation</u>](./docs/original_docs/basic_usage.md).

### Just use components

If you already have your own code and only want to use our components to embed the original code, you can refer to the [<u>basic introduction of the components</u>](./docs/original_docs/basic_usage.md) to obtain the input and output formats of each component.

## :gear: Components

In FlashRAG, we have built a series of common RAG components, including retrievers, generators, refiners, and more. Based on these components, we have assembled several pipelines to implement the RAG workflow, while also providing the flexibility to combine these components in custom arrangements to create your own pipeline.

#### RAG-Components

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Module</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="1">Judger</td>
      <td>SKR Judger</td>
      <td>Judging whether to retrieve using <a href="https://aclanthology.org/2023.findings-emnlp.691.pdf">SKR</a> method</td>
    </tr>
    <tr>
      <td rowspan="4">Retriever</td>
      <td>Dense Retriever</td>
      <td>Bi-encoder models such as dpr, bge, e5, using faiss for search</td>
    </tr>
    <tr>
      <td>BM25 Retriever</td>
      <td>Sparse retrieval method based on Lucene</td>
    </tr>
    <tr>
      <td>Bi-Encoder Reranker</td>
      <td>Calculate matching score using bi-Encoder</td>
    </tr>
    <tr>
      <td>Cross-Encoder Reranker</td>
      <td>Calculate matching score using cross-encoder</td>
    </tr>
    <tr>
      <td rowspan="5">Refiner</td>
      <td>Extractive Refiner</td>
      <td>Refine input by extracting important context</td>
    </tr>
    <tr>
      <td>Abstractive Refiner</td>
      <td>Refine input through seq2seq model</td>
    </tr>
    <tr>
      <td>LLMLingua Refiner</td>
      <td><a href="https://aclanthology.org/2023.emnlp-main.825/">LLMLingua-series</a> prompt compressor</td>
    </tr>
    <tr>
      <td>SelectiveContext Refiner</td>
      <td><a href="https://arxiv.org/abs/2310.06201">Selective-Context</a> prompt compressor</td>
    </tr>
    <tr>
      <td> KG Refiner </td>
      <td>Use <a hred='https://arxiv.org/abs/2406.11460'>Trace method to construct a knowledge graph</td>
    <tr>
      <td rowspan="4">Generator</td>
      <td>Encoder-Decoder Generator</td>
      <td>Encoder-Decoder model, supporting <a href="https://arxiv.org/abs/2007.01282">Fusion-in-Decoder (FiD)</a></td>
    </tr>
    <tr>
      <td>Decoder-only Generator</td>
      <td>Native transformers implementation</td>
    </tr>
    <tr>
      <td>FastChat Generator</td>
      <td>Accelerate with <a href="https://github.com/lm-sys/FastChat">FastChat</a></td>
    </tr>
    <tr>
      <td>vllm Generator</td>
      <td>Accelerate with <a href="https://github.com/vllm-project/vllm">vllm</a></td>
    </tr>
  </tbody>
</table>

#### Pipelines

Referring to a [<u>survey on retrieval-augmented generation</u>](https://arxiv.org/abs/2312.10997), we categorized RAG methods into four types based on their inference paths.

- **Sequential**: Sequential execuation of RAG process, like Query-(pre-retrieval)-retriever-(post-retrieval)-generator
- **Conditional**: Implements different paths for different types of input queries
- **Branching** : Executes multiple paths in parallel, merging the responses from each path
- **Loop**: Iteratively performs retrieval and generation

In each category, we have implemented corresponding common pipelines. Some pipelines have corresponding work papers.

<table>
    <thead>
        <tr>
            <th>Type</th>
            <th>Module</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="1">Sequential</td>
            <td>Sequential Pipeline</td>
            <td>Linear execution of query, supporting refiner, reranker</td>
        </tr>
        <tr>
            <td rowspan="1">Conditional</td>
            <td>Conditional Pipeline</td>
            <td>With a judger module, distinct execution paths for various query types</td>
        </tr>
        <tr>
            <td rowspan="2">Branching</td>
            <td>REPLUG Pipeline</td>
            <td>Generate answer by integrating probabilities in multiple generation paths</td>
        </tr>
          <td>SuRe Pipeline</td>
          <td>Ranking and merging generated results based on each document</td>
        </tr>
        <tr>
            <td rowspan="6">Loop</td>
            <td>Iterative Pipeline</td>
            <td>Alternating retrieval and generation</td>
        </tr>
        <tr>
            <td>Self-Ask Pipeline</td>
            <td>Decompose complex problems into subproblems using <a href="https://arxiv.org/abs/2210.03350">self-ask</a> </td>
        </tr>
        <tr>
            <td>Self-RAG Pipeline</td>
            <td>Adaptive retrieval, critique, and generation</td>
        </tr>
        <tr>
            <td>FLARE Pipeline</td>
            <td>Dynamic retrieval during the generation process</td>
        </tr>
        <tr>
            <td>IRCoT Pipeline</td>
            <td>Integrate retrieval process with CoT</td>
        </tr>
        <tr>
            <td>Reasoning Pipeline</td>
            <td>Reasoning with retrieval</td>
        </tr>
    </tbody>
</table>

## :art: FlashRAG-UI
<p>With <strong>FlashRAG-UI</strong>, you can easily and quickly configure and experience the supported RAG methods through our meticulously designed visual interface, and evaluate these methods on benchmarks, making complex research work more efficient!</p>

### :star2: Features
- **One-Click Configuration Loading**
  - You can load parameters and configuration files for various RAG methods through simple clicks, selections, and inputs.</li>
  - Supports preview interface for intuitive parameter settings.</li>
  - Provides save functionality to easily store configurations for future use.</li>
- **Quick Method Experience**
  - Quickly load corpora and index files to explore the characteristics and application scenarios of various RAG methods.</li>
  - Supports loading and switching different components and hyperparameters, seamlessly connecting different RAG Pipelines to quickly experience their performance and differences!</li>
- **Efficient Benchmark Reproduction**
  - Easily reproduce the built-in baseline methods and carefully collected benchmarks on FlashRAG-UI.</li>
  - Use cutting-edge research tools directly without complex settings, providing a smooth experience for your research work!</li>
  
<details>
<summary>Show more</summary>
<table align="center">
  <tr>
    <td align="center">
      <img src="./asset/demo_en1.jpg" alt="Image 1" width="505"/>
    </td>
    <td align="center">
      <img src="./asset/demo_en2.jpg" alt="Image 2" width="505"/>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="./asset/demo_en4.png" alt="Image 3" width="500"/>
    </td>
    <td align="center">
      <img src="./asset/demo_en3.jpg" alt="Image 4" width="500"/>
    </td>
  </tr>
</table>
</details>

#### Experience our meticulously designed FlashRAG-UIâ€”both user-friendly and visually appealing:
```bash
cd webui
python interface.py
```

## :robot: Supporting Methods

We have implemented **23 works** with a consistent setting of:

- **Generator:** LLAMA3-8B-instruct with input length of 2048
- **Retriever:** e5-base-v2 as embedding model, retrieve 5 docs per query
- **Prompt:** A consistent default prompt, template can be found in the [<u>method details</u>](./docs/original_docs/baseline_details.md).

For open-source methods, we implemented their processes using our framework. For methods where the author did not provide source code, we will try our best to follow the methods in the original paper for implementation.

For necessary settings and hyperparameters specific to some methods, we have documented them in the **specific settings** column. For more details, please consult our [<u>reproduce guidance</u>](./docs/original_docs/reproduce_experiment.md) and [<u>method details</u>](./docs/original_docs/baseline_details.md).

It's important to note that, to ensure consistency, we have utilized a uniform setting. However, this setting may differ from the original setting of the method, leading to variations in results compared to the original outcomes.

| Method                                                                                    | Type        | NQ (EM) | TriviaQA (EM) | Hotpotqa (F1) | 2Wiki (F1) | PopQA (F1) | WebQA(EM) | Specific setting                                |
| ----------------------------------------------------------------------------------------- | ----------- | ------- | ------------- | ------------- | ---------- | ---------- | --------- | ----------------------------------------------- |
| Naive Generation                                                                          | Sequential  | 22.6    | 55.7          | 28.4          | 33.9       | 21.7       | 18.8      |                                                 |
| Standard RAG                                                                              | Sequential  | 35.1    | 58.9          | 35.3          | 21.0       | 36.7       | 15.7      |                                                 |
| [AAR-contriever-kilt](https://aclanthology.org/2023.acl-long.136.pdf)                     | Sequential  | 30.1    | 56.8          | 33.4          | 19.8       | 36.1       | 16.1      |                                                 |
| [LongLLMLingua](https://arxiv.org/abs/2310.06839)                                         | Sequential  | 32.2    | 59.2          | 37.5          | 25.0       | 38.7       | 17.5      | Compress Ratio=0.5                              |
| [RECOMP-abstractive](https://arxiv.org/pdf/2310.04408)                                    | Sequential  | 33.1    | 56.4          | 37.5          | 32.4       | 39.9       | 20.2      |                                                 |
| [Selective-Context](https://arxiv.org/abs/2310.06201)                                     | Sequential  | 30.5    | 55.6          | 34.4          | 18.5       | 33.5       | 17.3      | Compress Ratio=0.5                              |
| [Trace](https://arxiv.org/abs/2406.11460)                                                 | Sequential  | 30.7    | 50.2          | 34.0          | 15.5       | 37.4       | 19.9      |                                                 |
| [Spring](https://arxiv.org/abs/2405.19670)                                                | Sequential  | 37.9    | 64.6          | 42.6          | 37.3       | 54.8       | 27.7      | Use Llama2-7B-chat with trained embedding table |
| [SuRe](https://arxiv.org/abs/2404.13081)                                                  | Branching   | 37.1    | 53.2          | 33.4          | 20.6       | 48.1       | 24.2      | Use provided prompt                             |
| [REPLUG](https://arxiv.org/abs/2301.12652)                                                | Branching   | 28.9    | 57.7          | 31.2          | 21.1       | 27.8       | 20.2      |                                                 |
| [SKR](https://aclanthology.org/2023.findings-emnlp.691.pdf)                               | Conditional | 33.2    | 56.0          | 32.4          | 23.4       | 31.7       | 17.0      | Use infernece-time training data                |
| [Adaptive-RAG](https://aclanthology.org/2024.naacl-long.389.pdf)                          | Conditional | 35.1    | 56.6          | 39.1          | 28.4       | 40.4       | 16.0      |                                                 |
| [Ret-Robust](https://arxiv.org/abs/2310.01558)                                            | Loop        | 42.9    | 68.2          | 35.8          | 43.4       | 57.2       | 33.7      | Use LLAMA2-13B with trained lora                |
| [Self-RAG](https://arxiv.org/abs/2310.11511)                                              | Loop        | 36.4    | 38.2          | 29.6          | 25.1       | 32.7       | 21.9      | Use trained selfrag-llama2-7B                   |
| [FLARE](https://arxiv.org/abs/2305.06983)                                                 | Loop        | 22.5    | 55.8          | 28.0          | 33.9       | 20.7       | 20.2      |                                                 |
| [Iter-Retgen](https://arxiv.org/abs/2305.15294), [ITRG](https://arxiv.org/abs/2310.05149) | Loop        | 36.8    | 60.1          | 38.3          | 21.6       | 37.9       | 18.2      |                                                 |
| [IRCoT](https://aclanthology.org/2023.acl-long.557.pdf)                                   | Loop        | 33.3    | 56.9          | 41.5          | 32.4       | 45.6       | 20.7      |                                                 |
| [RQRAG](https://arxiv.org/abs/2404.00610)                                   | Loop        | 32.6    | 52.5          | 33.5          | 35.8       | 46.4       | 26.2      |  Use trained rqrag-llama2-7B                                               | 

#### ðŸš€ Reasoning-based Methods (NEW!)

We now support **7 reasoning-based methods** that combine reasoning ability with retrieval, achieving superior performance on complex multi-hop tasks:

| Method                                                                                    | Type        | NQ (EM) | TriviaQA (EM) | PopQA (EM) | Hotpotqa (F1) | 2Wiki (F1) |  Musique (F1) | Bamboogle (F1) | Specific setting                             |
| ----------------------------------------------------------------------------------------- | ----------- | ------- | ------- | ------------- | ------------- | ---------- | ---------- | --------- | ----------------------------------------------- |
| [Search-R1](https://arxiv.org/abs/2503.09516) | Reasoning | 45.2 | 62.2 | 49.2 | 54.5 | 42.6 | 29.2 |  59.9 | SearchR1-nq_hotpotqa_train-qwen2.5-7b-em-ppo |
| [R1-Searcher](https://arxiv.org/pdf/2503.05592) | Reasoning | 36.9 | 61.6 | 42.0 | 49.0 | 49.1 | 24.7 | 57.7 | Qwen-2.5-7B-base-RAG-RL |
| [O2-Searcher](https://arxiv.org/pdf/2505.16582) | Reasoning | 41.4 | 51.4 | 46.8 | 43.4 | 48.6 | 19.0 | 47.6 | O2-Searcher-Qwen2.5-3B-GRPO |
| [AutoRefine](https://www.arxiv.org/pdf/2505.11277) | Reasoning | 43.8 | 59.8 | 32.4 | 54.0 | 50.3 | 23.6 | 46.6 | AutoRefine-Qwen2.5-3B-Base |
| [ReaRAG](https://arxiv.org/abs/2503.21729) | Reasoning | 26.3 | 51.8 | 24.6 | 42.9 | 41.6 | 21.2 | 41.9 | ReaRAG-9B |
| [CoRAG](https://arxiv.org/abs/2503.21729) | Reasoning | 40.9 | 63.1 | 36.0 | 56.6 | 60.7 | 31.9 | 54.1 | CoRAG-Llama3.1-8B-MultihopQA |
| [SimpleDeepSearcher](https://arxiv.org/pdf/2505.16834) | Reasoning | 36.1 | 61.6 | 42.0 | 49.0 | 49.1 | 24.7 | 57.7 | Qwen-7B-SimpleDeepSearcher |

## :notebook: Supporting Datasets & Document Corpus

### Datasets

We have collected and processed 36 datasets widely used in RAG research, pre-processing them to ensure a consistent format for ease of use. For certain datasets (such as Wiki-asp), we have adapted them to fit the requirements of RAG tasks according to the methods commonly used within the community. All datasets are available at [<u>Huggingface datasets</u>](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets).

For each dataset, we save each split as a `jsonl` file, and each line is a dict as follows:

```python
{
  'id': str,
  'question': str,
  'golden_answers': List[str],
  'metadata': dict
}
```

Below is the list of datasets along with the corresponding sample sizes:

| Task                      | Dataset Name    | Knowledge Source | # Train   | # Dev   | # Test |
| ------------------------- | --------------- | ---------------- | --------- | ------- | ------ |
| QA                        | NQ              | wiki             | 79,168    | 8,757   | 3,610  |
| QA                        | TriviaQA        | wiki & web       | 78,785    | 8,837   | 11,313 |
| QA                        | PopQA           | wiki             | /         | /       | 14,267 |
| QA                        | SQuAD           | wiki             | 87,599    | 10,570  | /      |
| QA                        | MSMARCO-QA      | web              | 808,731   | 101,093 | /      |
| QA                        | NarrativeQA     | books and story  | 32,747    | 3,461   | 10,557 |
| QA                        | WikiQA          | wiki             | 20,360    | 2,733   | 6,165  |
| QA                        | WebQuestions    | Google Freebase  | 3,778     | /       | 2,032  |
| QA                        | AmbigQA         | wiki             | 10,036    | 2,002   | /      |
| QA                        | SIQA            | -                | 33,410    | 1,954   | /      |
| QA                        | CommonSenseQA   | -                | 9,741     | 1,221   | /      |
| QA                        | BoolQ           | wiki             | 9,427     | 3,270   | /      |
| QA                        | PIQA            | -                | 16,113    | 1,838   | /      |
| QA                        | Fermi           | wiki             | 8,000     | 1,000   | 1,000  |
| multi-hop QA              | HotpotQA        | wiki             | 90,447    | 7,405   | /      |
| multi-hop QA              | 2WikiMultiHopQA | wiki             | 15,000    | 12,576  | /      |
| multi-hop QA              | Musique         | wiki             | 19,938    | 2,417   | /      |
| multi-hop QA              | Bamboogle       | wiki             | /         | /       | 125    |
| multi-hop QA              | StrategyQA      | wiki             | 2290      | /       | /      |
| Long-form QA              | ASQA            | wiki             | 4,353     | 948     | /      |
| Long-form QA              | ELI5            | Reddit           | 272,634   | 1,507   | /      |
| Long-form QA              | WikiPassageQA   | wiki             | 3,332     | 417     | 416    |
| Open-Domain Summarization | WikiASP         | wiki             | 300,636   | 37,046  | 37,368 |
| multiple-choice           | MMLU            | -                | 99,842    | 1,531   | 14,042 |
| multiple-choice           | TruthfulQA      | wiki             | /         | 817     | /      |
| multiple-choice           | HellaSWAG       | ActivityNet      | 39,905    | 10,042  | /      |
| multiple-choice           | ARC             | -                | 3,370     | 869     | 3,548  |
| multiple-choice           | OpenBookQA      | -                | 4,957     | 500     | 500    |
| multiple-choice           | QuaRTz          | -                | 2696      | 384     | 784    |
| Fact Verification         | FEVER           | wiki             | 104,966   | 10,444  | /      |
| Dialog Generation         | WOW             | wiki             | 63,734    | 3,054   | /      |
| Entity Linking            | AIDA CoNll-yago | Freebase & wiki  | 18,395    | 4,784   | /      |
| Entity Linking            | WNED            | Wiki             | /         | 8,995   | /      |
| Slot Filling              | T-REx           | DBPedia          | 2,284,168 | 5,000   | /      |
| Slot Filling              | Zero-shot RE    | wiki             | 147,909   | 3,724   | /      |
| In-domain QA              | DomainRAG       | Web pages of RUC | /         | /       | 485    |

### Document Corpus

Our toolkit supports jsonl format for retrieval document collections, with the following structure:

```jsonl
{"id":"0", "contents": "..."}
{"id":"1", "contents": "..."}
```

The `contents` key is essential for building the index. For documents that include both text and title, we recommend setting the value of `contents` to `{title}\n{text}`. The corpus file can also contain other keys to record additional characteristics of the documents.

In the academic research, Wikipedia and MS MARCO are the most commonly used retrieval document collections. For Wikipedia, we provide a [<u>comprehensive script</u>](./docs/original_docs/process-wiki.md) to process any Wikipedia dump into a clean corpus. Additionally, various processed versions of the Wikipedia corpus are available in many works, and we have listed some reference links.

For MS MARCO, it is already processed upon release and can be directly downloaded from its [<u>hosting link</u>](https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus) on Hugging Face.

### Index

To facilitate easier replication of the experiments, we now provide a preprocessed index available in the ModelScope dataset page: [FlashRAG_Dataset/retrieval_corpus/wiki18_100w_e5_index.zip](https://www.modelscope.cn/datasets/hhjinjiajie/FlashRAG_Dataset/file/view/master?id=47985&status=2&fileName=retrieval_corpus%252Fwiki18_100w_e5_index.zip).

The index was created using the e5-base-v2 retriever on our uploaded wiki18_100w dataset, which is consistent with the index used in our experiments.

## :lollipop: Awesome Work using FlashRAG

- [R1-Searcher](https://github.com/SsmallSong/R1-Searcher), a method that incentivizes the search capability in LLMs via reinforcement learning
- [ReSearch](https://github.com/Agent-RL/ReSearch), a method that learns to reason with search for LLMs via reinforcement learning
- [AutoCoA](https://github.com/ADaM-BJTU/AutoCoA), a method that internalizes chain-of-action generation into reasoning models

## :raised_hands: Additional FAQs

- [How should I set different experimental parameters?](./docs/original_docs/configuration.md)
- [How to build my own corpus, such as a specific segmented Wikipedia?](./docs/original_docs/process-wiki.md)
- [How to index my own corpus?](./docs/original_docs/building-index.md)
- [How to reproduce supporting methods?](./docs/original_docs/reproduce_experiment.md)

## :bookmark: License

FlashRAG is licensed under the [<u>MIT License</u>](./LICENSE).

## :star2: Citation

Please kindly cite our paper if helps your research:

```BibTex
@inproceedings{FlashRAG,
  author       = {Jiajie Jin and
                  Yutao Zhu and
                  Zhicheng Dou and
                  Guanting Dong and
                  Xinyu Yang and
                  Chenghao Zhang and
                  Tong Zhao and
                  Zhao Yang and
                  Ji{-}Rong Wen},
  editor       = {Guodong Long and
                  Michale Blumestein and
                  Yi Chang and
                  Liane Lewin{-}Eytan and
                  Zi Helen Huang and
                  Elad Yom{-}Tov},
  title        = {FlashRAG: {A} Modular Toolkit for Efficient Retrieval-Augmented Generation
                  Research},
  booktitle    = {Companion Proceedings of the {ACM} on Web Conference 2025, {WWW} 2025,
                  Sydney, NSW, Australia, 28 April 2025 - 2 May 2025},
  pages        = {737--740},
  publisher    = {{ACM}},
  year         = {2025},
  url          = {https://doi.org/10.1145/3701716.3715313},
  doi          = {10.1145/3701716.3715313}
}
```


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=RUC-NLPIR/FlashRAG&type=Date)](https://star-history.com/#RUC-NLPIR/FlashRAG&Date)



================================================
FILE: CITATION.cff
================================================
cff-version: 1.2.0
date-released: 2024-05
message: "If you use this software, please cite it as below."
authors:
- family-names: "Jin"
  given-names: "Jiajie"
- family-names: "Zhu"
  given-names: "Yutao"
- family-names: "Yang"
  given-names: "Xinyu"
- family-names: "Zhang"
  given-names: "Chenghao"
- family-names: "Dou"
  given-names: "Zhicheng"
- family-names: "Wen"
  given-names: "Ji-Rong"
title: "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research"
url: "https://arxiv.org/abs/2405.13576"
preferred-citation:
  type: article
  authors:
    - family-names: "Jin"
      given-names: "Jiajie"
    - family-names: "Zhu"
      given-names: "Yutao"
    - family-names: "Yang"
      given-names: "Xinyu"
    - family-names: "Zhang"
      given-names: "Chenghao"
    - family-names: "Dou"
      given-names: "Zhicheng"
    - family-names: "Wen"
      given-names: "Ji-Rong"
  title: "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research"
  journal: "CoRR"
  volume: "abs/2405.13576"
  year: 2024
  url: "https://arxiv.org/abs/2405.13576"
  eprinttype: "arXiv"
  eprint: "2405.13576"



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Jiajie Jin

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[tool.ruff.lint]
extend-select = ["W291", "W293"]



================================================
FILE: requirements.txt
================================================
datasets
base58
nltk
numpy
langid
openai
peft
PyYAML
rank_bm25
rouge
spacy
tiktoken
torch
tqdm
transformers>=4.40.0
bm25s[core]==0.2.1
fschat
streamlit
chonkie>=1.0.2, <1.1.0
gradio>=5.0.0
rouge-chinese
jieba



================================================
FILE: setup.py
================================================
from setuptools import setup, find_packages

with open("requirements.txt",encoding='utf-8') as fp:
    requirements = fp.read().splitlines()
with open("README.md", "r",encoding='utf-8') as fh:
    long_description = fh.read()
version = {}
with open("flashrag/version.py", encoding="utf8") as fp:
    exec(fp.read(), version)

extras_require = {
    'core': requirements,
    'retriever': ['pyserini', 'sentence-transformers>=3.0.1'],
    'generator': ['vllm'],
    'multimodal': ['timm', 'torchvision', 'pillow', 'qwen_vl_utils']
}
extras_require['full'] = sum(extras_require.values(), [])

setup(
    name="flashrag_dev",
    version=version['__version__'],
    packages=find_packages(),
    url="https://github.com/RUC-NLPIR/FlashRAG",
    license="MIT License",
    author="Jiajie Jin, Yutao Zhu, Chenghao Zhang, Xinyu Yang, Zhicheng Dou",
    author_email="jinjiajie@ruc.edu.cn",
    description="A library for efficient Retrieval-Augmented Generation research",
    long_description=long_description,
    long_description_content_type="text/markdown",
    package_data={"": ["*.yaml"]},
    include_package_data=True,
    install_requires=extras_require['core'],
    extras_require=extras_require,
    python_requires=">=3.9",
)



================================================
FILE: docs/_navbar.md
================================================
- Translations
    - [:cn: ä¸­æ–‡](/zh-cn/)
    - [:us: English](/en-us/)


================================================
FILE: docs/_sidebar.md
================================================
* Get Started
    * [Introduction](en-us/introduction.md)
    * [Installation](en-us/installation.md)
    * [Tutorial](en-us/tutorial.md)
    * [Webui](en-us/webui.md)

* Data Preparation
    * [Evaluation Datasets](en-us/data_preparation/evaluation-datasets.md)
    * [Build Corpus (Use Existing Corpus)](en-us/build-corpus.md)
    * [Build Index](en-us/build-index.md)

* Config Settings
    * [Overview](en-us/config-setting.md)
    * [Basic Settings](en-us/basic-settings.md)
    * [Retriever Settings](en-us/retriever-settings.md)
    * [Generator Settings](en-us/generator-settings.md)

* Components
    * [Retriever](en-us/retriever.md)
    * [Generator](en-us/generator.md)
    * [Judger](en-us/judger.md)
    * [Refiner](en-us/refiner.md)

* Prompt Template
    * [Overview](en-us/prompt-template.md)

* Pipelines
    * [Pipeline](en-us/pipeline.md)

* Evaluation
    * [Overview](en-us/evaluation.md)
    * [Evaluation Metrics](en-us/evaluation-metrics.md)

* Reproduce Experiments
    * [Results](en-us/results.md)
    * [Reproduce Process](en-us/reproduce-process.md)

* Api Reference
    * [flashrag.config](en-us/flashrag.config.md)
    * [flashrag.dataset](en-us/flashrag.dataset.md)
    * [flashrag.evaluator](en-us/flashrag.evaluator.md)
    * [flashrag.prompt](en-us/flashrag.prompt.md)
    * [flashrag.utils](en-us/flashrag.utils.md)
    * [flashrag.retriever](en-us/flashrag.retriever.md)
    * [flashrag.generator](en-us/flashrag.generator.md)
    * [flashrag.judger](en-us/flashrag.judger.md)
    * [flashrag.refiner](en-us/flashrag.refiner.md)
    * [flashrag.pipeline](en-us/flashrag.pipeline.md)
    


================================================
FILE: docs/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, minimum-scale=1.0, shrink-to-fit=no, viewport-fit=cover">

  <!-- Replace with your own title and description. -->
  <title>FlashRAG Documentation</title>
  <meta name="description" content="An efficient toolkit for Retrieval Augmented Generation.">
  <meta name="keywords" content="FlashRAG, Retrieval Augmented Generation, RAG, Toolkit, Documentation">
  <meta name="author" content="RUC-NLPIR FlashRAG Team">

  <!-- Default Theme (see https://docsify.js.org/#/themes) -->
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/docsify@4/lib/themes/vue.css">
  <!-- alert -->
  <link rel="stylesheet" href="//cdn.bootcss.com/sweetalert/1.1.3/sweetalert.min.css" type='text/css' media='all' />

</head>

<body>
  <div id="app"></div>

  <script>
    // Docsify Configuration (see https://docsify.js.org/#/configuration)
    window.$docsify = {
      name: 'FlashRAG Documentation',
      repo: 'https://github.com/RUC-NLPIR/FlashRAG',
      // Sidebar Configuration
      auto2top: true,
      loadNavbar: true,
      loadSidebar: true,
      maxLevel: 4,
      // Set subMaxLevel to 0 to remove automatic display of page table of contents (TOC) in Sidebar
      subMaxLevel: 3,
      externalLinkTarget: '_blank',
      homepage: 'en-us/README.md', //è®¾ç½®ä¸­æ–‡çš„README.mdä½œä¸ºé¦–é¡µ

      // Search Plugin Configuration
      search: {
        paths: 'auto',
        placeholder: {
          '/': 'ðŸ” Type to search',
          '/zh-cn/': 'ðŸ” æœç´¢',
          '/en-us/': 'ðŸ” Type to search'
        },
        //å¦‚æžœä¸­æ–‡å’Œè‹±æ–‡æ–‡æ¡£éƒ½æ²¡æœ‰å†…å®¹ï¼Œåˆ™è¿”å›ž
        noData: {
          '/': 'ðŸ˜’ No Results',
          '/zh-cn/': 'ðŸ˜’ æ‰¾ä¸åˆ°ç»“æžœ',
          '/en-us/': 'ðŸ˜’ No Results'
        },

        depth: 4,         //æœç´¢æ ‡é¢˜çš„æœ€å¤§å±‚çº§
        maxAge: 86400000, // è¿‡æœŸæ—¶é—´ï¼Œå•ä½æ¯«ç§’ï¼Œé»˜è®¤ä¸€å¤©
      },//å¢žåŠ æœç´¢æ¡†
      footer: {
        copy: 'Copyright &copy; 2024 ðŸ’– </span>',
        auth: ' <strong>-FlashRAG Team-</strong>',
        pre: '<hr/>',
        style: 'text-align: center;',
      },
      copyCode: {
        buttonText: 'Copy',
        errorText: 'Error',
        successText: 'OK!'
      },

      'flexible-alerts': {
        style: 'flat'
      }
    };
  </script>

  <!-- Required -->
  <script src="https://cdn.jsdelivr.net/npm/docsify@4/lib/docsify.min.js"></script>

  <!-- Recommended -->
  <script src="https://cdn.jsdelivr.net/npm/docsify@4/lib/plugins/zoom-image.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/docsify@4/lib/plugins/search.js"></script>
  <script src="//vox.cab/npm/docsify-sidebar-collapse/dist/docsify-sidebar-collapse.min.js"></script>
  <script src="//unpkg.com/docsify-pagination/dist/docsify-pagination.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/docsify-copy-code"></script>
  <!-- æ–‡æ¡£é¡µè„šå¢žå¼º -->
  <script src="//cdn.jsdelivr.net/npm/docsify-footer-enh/dist/docsify-footer.min.js"></script>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <!-- Markdown å¼•ç”¨æ ·å¼ç¾ŽåŒ– -->
  <script
    src="//cdn.jsdelivr.net/npm/docsify-plugin-flexible-alerts/dist/docsify-plugin-flexible-alerts.min.js"></script>

  <!-- å›¾ç‰‡ç¼©æ”¾ -->
  <script src="//cdn.jsdelivr.net/npm/docsify/lib/plugins/zoom-image.js"></script>
  <!-- ç½®é¡¶æ’ä»¶ -->
  <script src="style/jquery-1.11.3.min.js"></script>
  <script src="style/jquery.toTop.min.js"></script>
  <script>
    $('.to-top').toTop();
  </script>
  <!-- PWA ç¦»çº¿åŒ– -->
  <script>
    if (typeof navigator.serviceWorker !== 'undefined') {
      navigator.serviceWorker.register('sw.js')
    }
  </script>
  <!-- å¤åˆ¶æé†’ -->
  <script src="https://cdn.bootcss.com/sweetalert/1.1.3/sweetalert.min.js"></script>
  <script>
    document.body.oncopy = function () {
      swal("å¤åˆ¶æˆåŠŸ ðŸŽ‰",
        "success");
    };
  </script>

  <!-- C++ã€Javaã€pythonã€bashç­‰ç­‰è¯­æ³•é«˜äº® -->
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-bash.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-java.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-python.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-cpp.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-c.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-javascript.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-css.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-powershell.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-sql.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-yaml.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-properties.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/prismjs/components/prism-matlab.js"></script>

  <!-- CDN files for docsify-katex -->
  <script src="//cdn.jsdelivr.net/npm/docsify-katex@latest/dist/docsify-katex.js"></script>

</body>

</html>


================================================
FILE: docs/sw.js
================================================
/* ===========================================================
 * docsify sw.js
 * ===========================================================
 * Copyright 2016 @huxpro
 * Licensed under Apache 2.0
 * Register service worker.
 * ========================================================== */

const RUNTIME = 'docsify'
const HOSTNAME_WHITELIST = [
    self.location.hostname,
    'fonts.gstatic.com',
    'fonts.googleapis.com',
    'cdn.jsdelivr.net'
]

// The Util Function to hack URLs of intercepted requests
const getFixedUrl = (req) => {
    var now = Date.now()
    var url = new URL(req.url)

    // 1. fixed http URL
    // Just keep syncing with location.protocol
    // fetch(httpURL) belongs to active mixed content.
    // And fetch(httpRequest) is not supported yet.
    url.protocol = self.location.protocol

    // 2. add query for caching-busting.
    // Github Pages served with Cache-Control: max-age=600
    // max-age on mutable content is error-prone, with SW life of bugs can even extend.
    // Until cache mode of Fetch API landed, we have to workaround cache-busting with query string.
    // Cache-Control-Bug: https://bugs.chromium.org/p/chromium/issues/detail?id=453190
    if (url.hostname === self.location.hostname) {
        url.search += (url.search ? '&' : '?') + 'cache-bust=' + now
    }
    return url.href
}

/**
 *  @Lifecycle Activate
 *  New one activated when old isnt being used.
 *
 *  waitUntil(): activating ====> activated
 */
self.addEventListener('activate', event => {
    event.waitUntil(self.clients.claim())
})

/**
 *  @Functional Fetch
 *  All network requests are being intercepted here.
 *
 *  void respondWith(Promise<Response> r)
 */
self.addEventListener('fetch', event => {
    // Skip some of cross-origin requests, like those for Google Analytics.
    if (HOSTNAME_WHITELIST.indexOf(new URL(event.request.url).hostname) > -1) {
        // Stale-while-revalidate
        // similar to HTTP's stale-while-revalidate: https://www.mnot.net/blog/2007/12/12/stale
        // Upgrade from Jake's to Surma's: https://gist.github.com/surma/eb441223daaedf880801ad80006389f1
        const cached = caches.match(event.request)
        const fixedUrl = getFixedUrl(event.request)
        const fetched = fetch(fixedUrl, { cache: 'no-store' })
        const fetchedCopy = fetched.then(resp => resp.clone())

        // Call respondWith() with whatever we get first.
        // If the fetch fails (e.g disconnected), wait for the cache.
        // If thereâ€™s nothing in cache, wait for the fetch.
        // If neither yields a response, return offline pages.
        event.respondWith(
            Promise.race([fetched.catch(_ => cached), cached])
                .then(resp => resp || fetched)
                .catch(_ => { /* eat any errors */ })
        )

        // Update the cache with the version we fetched (only for ok status)
        event.waitUntil(
            Promise.all([fetchedCopy, caches.open(RUNTIME)])
                .then(([response, cache]) => response.ok && cache.put(event.request, response))
                .catch(_ => { /* eat any errors */ })
        )
    }
})


================================================
FILE: docs/.nojekyll
================================================
[Empty file]


================================================
FILE: docs/en-us/README.md
================================================
# Welcome to FlashRAG's documentation!

![](./asset/framework.jpg)

# Introduction

FlashRAG is a Python toolkit for the reproduction and development of Retrieval Augmented Generation (RAG) research. Our toolkit includes 36 pre-processed benchmark RAG datasets and 16 state-of-the-art RAG algorithms. Our features are as follows:

- **Extensive and Customizable Framework**: Includes essential components for RAG scenarios such as retrievers, rerankers, generators, and compressors, allowing for flexible assembly of complex pipelines.

- **Comprehensive Benchmark Datasets**: A collection of 36 pre-processed RAG benchmark datasets to test and validate RAG models' performances.

- **Pre-implemented Advanced RAG Algorithms**: Features 16 advancing RAG algorithms with reported results, based on our framework. Easily reproducing results under different settings.

- **Efficient Preprocessing Stage**: Simplifies the RAG workflow preparation by providing various scripts like corpus processing for retrieval, retrieval index building, and pre-retrieval of documents.

- **Optimized Execution**: The library's efficiency is enhanced with tools like vLLM, FastChat for LLM inference acceleration, and Faiss for vector index management.

- **Easy to Use UI** : We have developed a very easy to use UI to easily and quickly configure and experience the RAG baselines we have implemented, as well as run evaluation scripts on a visual interface.


# Documentation

- [Installation](docs/installation.md)
- [Features](docs/features.md)
- [Quick-Start](docs/quick-start.md)
- [Components](docs/components.md)
- [FlashRAG-UI](docs/flashrag-ui.md)
- [Supporting Methods](docs/supporting-methods.md)
- [Supporting Datasets](docs/supporting-datasets.md)
- [FAQs](docs/faqs.md)

# :bookmark: License

FlashRAG is licensed under the [<u>MIT License</u>](https://github.com/RUC-NLPIR/FlashRAG/blob/main/LICENSE).

# :star2: Citation

Please kindly cite our paper if helps your research:

```BibTex
@article{FlashRAG,
    author={Jiajie Jin and
            Yutao Zhu and
            Xinyu Yang and
            Chenghao Zhang and
            Zhicheng Dou},
    title={FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research},
    journal={CoRR},
    volume={abs/2405.13576},
    year={2024},
    url={https://arxiv.org/abs/2405.13576},
    eprinttype={arXiv},
    eprint={2405.13576}
}
```


================================================
FILE: docs/en-us/.nojekyll
================================================
[Empty file]


================================================
FILE: docs/en-us/get_started/introduction.md
================================================
[Empty file]


================================================
FILE: docs/original_docs/baseline_details.md
================================================
# Implementation Details for Benchmarking Experiments

We tested all implemented methods under a unified setting. Users only need to download the corresponding model and fill in the config to run the corresponding results using the script we provide [here](https://github.com/RUC-NLPIR/FlashRAG/blob/main/examples/methods/run_exp.py). This chapter details the implementation specifics of reproducing various algorithms using our toolkit, allowing users to effortlessly replicate the experiments and align their results with ours.

## Global Setting

**Retriever Setting**: In our main experiments, we utilize [E5-base-v2](https://huggingface.co/intfloat/e5-base-v2) as the retriever, retrieving five documents per query. We use the DPR version of the Wikipedia December 2018 dataset as our retrieval corpus, which can be downloaded from our dataset hosting page [here](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets). 
In subsequent retrieval experiments, we employ both BM25 and [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) as additional retrievers. The BM25 experiments are conducted using Pyserini. During index construction, the maximum padding length is set to 512, while the maximum query padding length is set to 128 during retrieval. The batch size for retrieval is 1024, with fp16 enabled. We employ the Faiss Flat index in indexing for accuracy.

**Generator Setting**: We employ [Llama3-8B-instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) as the generator in our main experiment, with a maximum input length of 2048 and a maximum output length of 32. Inference is performed using the vllm framework without sampling during generation. In subsequent generator experiments, we employ Qwen1.5-14B as additional generators. The experiments are conducted using four NVIDIA A100 80G GPUs.

**Prompt Setting**: We use a unified prompt to ensure fairness. Specifically, our system prompt is:

```
Answer the question based on the given document. Only give me the answer and do not output any other words. The following are given documents:{retrieval documents}
```
The retrieval documents are listed as follows:
```
Doc 1 (Title:{title}) {content} 
Doc 2 (Title:{title}) {content}
```
Our user prompt is:
```
Question:{question}
```
These prompts are combined using the `tokenizer.apply_chat_template` function, serving as the final input to the generator model.

**Dataset Setting**: For datasets with a test set, we use the test set for testing. Otherwise, we use the dev set for testing (i.e. nq, tqa, popqa, webq is test, and the rest is dev). All results used the first 1000 samples from each dataset (set `test_sample_num` to 1000 in config and turn off `random_sample`). We have tested on a full dataset and found that the results differ slightly from those of 1000 data points.

## Method Specific Setting

In addition to the general settings mentioned above, each method often has its own settings and configurations. For methods with specific settings, we will introduce each method's own settings below.

**AAR**: This work focuses on optimizing the retriever. For our experiments, we utilize the pre-trained retriever provided by the authors [here](https://huggingface.co/OpenMatch/AAR-Contriever-KILT). We use AAR-Contriever-KILT for our results and also support the AAR-ANCE implementation.

**LLMLingua**: In this method, we use Llama2-7B to compute perplexity and LongLLMLingua as the compressor, with a compression rate set to 0.55. Other parameters were set to default values. Unlike the original LongLLMLingua example, we use the retrieved text as input to the refiner (instead of the entire prompt). Our tests reveal that the Llama3 prompt requires special tokens; using the original setting caused these tokens to be omitted, resulting in degraded performance.

**RECOMP**: We use the abstractive model provided by RECOMP for our experiments [here](https://huggingface.co/fangyuan). For NQ, TQA, and HotpotQA, we use the corresponding models. For the remaining three datasets, there were no trained checkpoints available. Therefore, we use the HotpotQA checkpoint for 2WikiMultihopQA, and the NQ checkpoint for PopQA and WebQuestions. The maximum input length for the refiner is set to 1024, and the maximum output length to 512.

**Selective-Context**: We use GPT2 to output perplexity and set the compression rate to 0.5. Similar to LongLLMLingua, we use the retrieved documents as input to the refiner.

**Ret-Robust**: This method focuses on optimizing the generative model, training it using the Self-Ask prompt method. The authors provided LoRA models trained on NQ and 2WikiMultihopQA [here](https://huggingface.co/Ori/llama-2-13b-peft-nq-retrobust). Consequently, we test using the Llama2-13B model loaded with the corresponding LoRA. As there is no trained model for HotpotQA, we use the 2WikiMultihopQA LoRA. For the remaining datasets, we use the NQ LoRA. We set the maximum interaction rounds to 5 and the maximum output tokens to 100. For HotpotQA and 2WikiMultihopQA, we disable the `single_hop` setting to allow the process to automatically decompose complex queries into multiple iterations.

**SuRe**: This method prompts the model to generate candidate answers, scores, and ranks them, selecting the best one. To ensure consistency, we use the prompts provided in the original paper, which can be referenced alongside our code implementation.

**SKR**: We implement the SKR-knn method, which requires an encoder model and inference-time training data. Specifically, it identifies the most similar queries from the training data based on the input query, determining whether the input query needs retrieval. Our library includes the training data provided by the authors; the corresponding encoder model can be downloaded [here](https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased).

**Self-RAG**: We use the Llama2-7B checkpoint provided by Self-RAG [here](https://huggingface.co/selfrag/selfrag_llama2_7b), setting the max output tokens to 100 to ensure proper operation. The temperature is set to 0, and `top_p` is set to 1.

**IRCoT**: For all experiments, we used one shot example to add prompts. The example comes from [the demonstration file](https://github.com/StonyBrookNLP/ircot/blob/main/prompts/2wikimultihopqa/gold_with_3_distractors_context_cot_qa_codex.txt) provided by IRCoT. Max iter is set to 2.

**Trace**: This method requires first extracting triples from the search results and then constructing a reasoning chain. These two steps depend on the prompt of the feed shot for LLM. Follow the original work, we use Llama3-8B-instruct to do these steps, use 3 examplars in each prompt. For datasets that don't have examplars, we use the examplars from 2WikiMultihopQA as a substitute. Other hyperparameters follow default settings in our code.

**Spring**: This model needs to incorporate the embedding of virtual tokens for training on top of its own generator. Due to only training models from the llama2 series, we conducted experiments on `llama2-7B-chat`.


**Adaptive-RAG**: This method requires a classifier to classify the query. Since the author did not provide an official checkpoint, we used a checkpoint trained by others on Huggingface for the experiment (which may result in inconsistent results). If the official open-source checkpoint is released in the future, we will update the experimental results.


================================================
FILE: docs/original_docs/basic_usage.md
================================================
# RAG-Components Usage

## Retriever

The retriever takes a series of queries and retrieves the top k documents from a corpus corresponding to each query.

You can load the Retriever using `flashrag.utils.get_retriever`, which determines the type of retriever (BM25 or Dense) based on the `retrieval_method` specified in the config , and initializes the corpus and model needed internally by the retriever.

```python
retriever = get_retriever(config)
```

Then, you can use `retriever.search` (for a single query) or `retriever.batch_search` (for a list of queries) to perform retrieval.

```python
# if you set `return_score=True`
retrieval_results, scores = retriever.batch_search(input_query_list, return_score=True)

# if you set `return_score=False`
retrieval_results = retriever.batch_search(input_query_list, return_score=False)
```

When using `batch_search`, `retrieval_results` is a two-level nested list like:

```python
[
    [doc1, doc2, ...],  # doc items for query1
    [doc1, doc2, ....],  # doc items for query2
    ...
]
```

When using `search`, `retrieval_results` is a regular list containing the top k doc item for each query.

Each doc item is a dictionary containing `doc_id`, `title`, `contents`, etc. (similar to the contents in the corpus).

`scores` have the same format as `retrieval_results`, except that each `doc_item` is replaced by a `float` value representing the matching score between the document and the query provided by the retriever.

#### Additional features of the Retriever

The `search` and `batch_search` functions of the retriever implement three additional functionalities using two decorator functions:
- **Pre-load retrieval results**: Suitable for cases where you provide some retrieval results for queries yourself. If `use_retrieval_cache` is set in the config and a cache file is provided, it first checks whether the cache file contains the retrieval results for the corresponding queries and returns them if available.
- **Save retrieval results**: If `save_retrieval_cache` is set to True, the retriever will save the retrieval results for each query as a cache, making it easy to use the cache directly next time.
- **Rerank**: If `use_reranker=True`, the `search` function will integrate reranking to further sort the retrieval results.

## Generator


The Generator takes prompts as input and returns outputs corresponding to each prompt.

You can load the Generator using `flashrag.utils.get_generator`. Depending on the name of the input generator model, it will choose to load a different structure of the generator model.

```python
generator = get_generator(config)
```

Then, use the `generate` method for generation.

```python
input_list = ['who is taylor swift?', 'who is jack ma?']
result = generator.generate(input_list)
```

You can obtain the generation probability of each token by using `return_scores`.

```python
result, scores = generator.generate(input_list, return_scores=True)
```

The `generate` function can also accept parameters needed for generation, such as `topk`, `do_sample`, etc. These parameters can also be specified in the config, but the ones specified in `generate` take precedence.

```python
result = generator.generate(input_list, top_p=1.0, max_tokens=32)
```

## Config

`Config` class supports using `.yaml` files as input or variables as input. The priority of variables is higher than that of files. **All subsequent component settings depend on `Config`.**

If there are variables that need to be used that are not specified in these two places, default values will be loaded (`basic_config.yaml`).

```python
from flashrag.config import Config

config_dict = {"retrieval_method": "bge"}
config = Config('my_config.yaml', config_dict = config_dict)
print(config['retrieval_method'])
```





================================================
FILE: docs/original_docs/benchmark_result.xlsx
================================================
[Binary file]


================================================
FILE: docs/original_docs/building-index.md
================================================
## Indexing your own corpus


### Step1: Prepare corpus
To build an index, you first need to save your corpus in `jsonl` format as follows, each line is a document.  

```jsonl
{"id": "0", "contents": "contents for building index"}
{"id": "1", "contents": "contents for building index"}
```

If you want to use Wikipedia as a corpus, you can refer to our documentation for [process Wikipedia](./process-wiki.md) to convert it into an indexed format.


### Step2: Indexing

Then, use the following code to build your own index.


* For **dense retrieval methods**, especially the popular embedding models, we use `faiss` to build index. 

* For **sparse retrieval method (BM25)**, we construct corpus as Lucene inverted indexes based on `Pyserini` or `bm25s`. The constructed index contains the original doc.


#### For dense retrieval methods

Modify the parameters in the following code to yours.

```bash
python -m flashrag.retriever.index_builder \
    --retrieval_method e5 \
    --model_path /model/e5-base-v2/ \
    --corpus_path indexes/sample_corpus.jsonl \
    --save_dir indexes/ \
    --use_fp16 \
    --max_length 512 \
    --batch_size 256 \
    --pooling_method mean \
    --faiss_type Flat 
```

* ```--pooling_method```: If this is not specified, we will automatically select based on the model name and model file. However, due to the different pooling methods used by different embedding models, **we may not have fully implemented them**. To ensure accuracy, you can **specify the pooling method corresponding to the retrieval model** you are using (`mean`, `pooler` or `cls`).

* ```---instruction```: Some embedding models require additional instructions to concatenate the query before encoding, which can be specified here. At present, we will automatically fill in the instructions for **E5** and **BGE** models, while other models need to be manually supplemented.

If the retrieval model support `sentence transformers` library, you can use following code to build index (**no need to consider pooling method**).

```bash
python -m flashrag.retriever.index_builder \
    --retrieval_method e5 \
    --model_path /model/e5-base-v2/ \
    --corpus_path indexes/sample_corpus.jsonl \
    --save_dir indexes/ \
    --use_fp16 \
    --max_length 512 \
    --batch_size 256 \
    --pooling_method mean \
    --sentence_transformer \
    --faiss_type Flat 
```




#### For sparse retrieval method (BM25)

If building a bm25 index, there is no need to specify `model_path`.

##### Use BM25s to build index

```bash
python -m flashrag.retriever.index_builder \
    --retrieval_method bm25 \
    --corpus_path indexes/sample_corpus.jsonl \
    --bm25_backend bm25s \
    --save_dir indexes/ 
```

##### Use Pyserini to build index

```bash
python -m flashrag.retriever.index_builder \
    --retrieval_method bm25 \
    --corpus_path indexes/sample_corpus.jsonl \
    --bm25_backend pyserini \
    --save_dir indexes/ 
```







================================================
FILE: docs/original_docs/chunk-doc-corpus.md
================================================
# Chunking Document Corpus

You can chunk your document corpus into smaller chunks by following these steps. This is useful in building an index over a large corpus of long documents for RAG, or if you want to make sure that the document length is not too long for the model.

Given a Document Corpus JSONL file with the following format and `contents` field containing the `"{title}\n{text}"` format:

```jsonl
{ "id": 0, "contents": "..." }
{ "id": 1, "contents": "..." }
{ "id": 2, "contents": "..." }
...
```

You can run the following command:

```bash
cd scripts
python chunk_doc_corpus.py --input_path input.jsonl \
                          --output_path output.jsonl \
                          --chunk_by sentence \
                          --chunk_size 512
```

You will get a JSONL file with the following format:

```jsonl
{ "id": 0, "doc_id": 0, "title": ..., "contents": ... }
{ "id": 1, "doc_id": 0, "title": ..., "contents": ... }
{ "id": 2, "doc_id": 0, "title": ..., "contents": ... }
...
```

**NOTE:** That `doc_id` will be the same as the original document id, and `contents` will be the chunked document content in the new JSONL output.

## Parameters

- `input_path`: Path to the input JSONL file.
- `output_path`: Path to the output JSONL file.
- `chunk_by`: Chunking method to use. Can be `token`, `word`, `sentence`, or `recursive`.
- `chunk_size`: Size of chunks.
- `tokenizer_name_or_path`: Name or path of the tokenizer that used for chunking.



================================================
FILE: docs/original_docs/configuration.md
================================================
# Configuration and Parameters

FlashRAG enables comprehensive management of various parameters for controlling the experiment. FlashRAG supports two types of parameter configurations: **YAML config file** and **parameter dict**. The parameters are assigned via the `Config` module.

In practice, `config` can be used in a dictionary access manner, such as:
```python
from flashrag.config import Config
config_dict = {'generator_model': 'llama2-7B'}
config = Config(config_dict=config_dict)
model_name = config['generator_model']
```

In most cases, the majority of parameters do not need to be modified. Specific application examples can be found below in the [Example Configuration](#example-configuration).

## Operation Logic

##### Config File

Config File should be in YAML format, and users should fill in the corresponding parameters according to the YAML syntax. In our library, we provide a template file for users to refer to, with comments explaining the specific meaning of each parameter.

The code for using a config file is as follows:
```python 
from flashrag.config import Config
config = Config(config_file_path='myconfig.yaml')
```

##### Parameter Dict

Another way is to set up the configuration via a Python dictionary, where keys are parameter names and values are parameter values. This method is more flexible compared to using a file.

The code for using a parameter dict is as follows:
```python 
from flashrag.config import Config
config_dict = {'generator_model': 'llama2-7B'}
config = Config(config_dict=config_dict)
```

##### Priority

In FlashRAG, we support combining both methods.

The priority of configuration methods is: Parameter Dict > Config File > Default Settings

The default settings are recorded in [basic_config.yaml](../flashrag/config/basic_config.yaml).

##### Path Settings

To facilitate the management of various model paths and index paths, we introduce `model2path`, `model2pooling`, and `method2index`. Users only need to fill in the corresponding names and paths here to automatically load the paths to be used.

For example, after setting the path of `llama2-7B` in `model2path`, you can directly specify it as the corresponding model without rewriting the path.
```yaml
model2path:
    llama2-7B: 'my_path'
generator_model: 'llama2-7B'
```

If the corresponding model path is not found in the dictionary, it will look for the parameter corresponding to the model path (e.g., `generator_model_path`).

This rule applies to the following parameters:
- retrieval_method
- rerank_model_name
- generator_model
- index_path

## Example Configuration

An example configuration file can be found in [basic_config.yaml](../flashrag/config/basic_config.yaml). Below we will go through each section in the configuration file.

```yaml
# basic settings

# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to various models
model2path:
  e5: "intfloat/e5-base-v2"
  bge: "intfloat/e5-base-v2"
  contriever: "facebook/contriever"
  llama2-7B-chat: "meta-llama/Llama-2-7b-chat-hf"
  llama2-7B: "meta-llama/Llama-2-7b-hf"
  llama2-13B: "meta-llama/Llama-2-13b-hf"
  llama2-13B-chat: "meta-llama/Llama-2-13b-chat-hf"
  
# Pooling methods for each embedding model
model2pooling:
  e5: "mean"
  bge: "cls"
  contriever: "mean"
  jina: 'mean'
  dpr: cls

# Indexes path for retrieval models
method2index:
  e5: ~
  bm25: ~
  contriever: ~

# ------------------------------------------------Environment Settings------------------------------------------------#
# Directory paths for data and outputs
data_dir: "dataset/"
save_dir: "output/"

gpu_id: "0,1,2,3"
dataset_name: "nq" # name of the dataset in data_dir
split: ["test"]  # dataset split to load (e.g. train,dev,test)

# Sampling configurations for testing
test_sample_num: ~  # number of samples to test (only work in dev/test split), if None, test all samples
random_sample: False # whether to randomly sample the test samples

# Seed for reproducibility
seed: 2024

# Whether save intermediate data
save_intermediate_data: True
save_note: 'experiment'

# -------------------------------------------------Retrieval Settings------------------------------------------------#
# If set the name, the model path will be find in global paths
retrieval_method: "e5"  # name or path of the retrieval model. 
retrieval_model_path: ~ # path to the retrieval model
index_path: ~ # set automatically if not provided. 
faiss_gpu: False # whether use gpu to hold index
corpus_path: ~  # path to corpus in '.jsonl' format that store the documents

use_sentence_transformer: False # If set, the retriever will be load through `sentence transformer` library
retrieval_topk: 5 # number of retrieved documents
retrieval_batch_size: 256  # batch size for retrieval
retrieval_use_fp16: True  # whether to use fp16 for retrieval model
retrieval_query_max_length: 128  # max length of the query
save_retrieval_cache: True # whether to save the retrieval cache
use_retrieval_cache: False # whether to use the retrieval cache
retrieval_cache_path: ~ # path to the retrieval cache
retrieval_pooling_method: ~ # set automatically if not provided

use_reranker: False # whether to use reranker
rerank_model_name: ~ # same as retrieval_method
rerank_model_path: ~ # path to reranker model, path will be automatically find in `model2path`
rerank_pooling_method: ~
rerank_topk: 5  # number of remain documents after reranking
rerank_max_length: 512 
rerank_batch_size: 256 # batch size for reranker
rerank_use_fp16: True

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: fschat # inference frame work of LLM, supporting: 'hf','vllm','fschat', 'openai'
generator_model: "llama3-8B-instruct" # name or path of the generator model
# setting for openai model, only valid in openai framework
openai_setting:
  api_key: ~
  base_url: ~

generator_model_path: ~
generator_max_input_len: 1024  # max length of the input
generator_batch_size: 4 # batch size for generation, invalid for vllm
generation_params:  
  #do_sample: false
  max_tokens: 32
  #temperature: 1.0
  #top_p: 1.0
use_fid: False # whether to use FID, only valid in encoder-decoder model

# -------------------------------------------------Evaluation Settings------------------------------------------------#
# Metrics to evaluate the result
metrics: ['em','f1','acc,'precision','recall'] 
# Specify setting for metric, will be called within certain metrics
metric_setting: 
  retrieval_recall_topk: 5
save_metric_score: True #ã€€whether to save the metric score into txt file
```


### Global Settings

Here the paths to models, indexes, and pooling methods for each embedding model are saved. Later, you only need to specify the corresponding name to automatically load the path.

```yaml
# Paths to various models
model2path:
  e5: "intfloat/e5-base-v2"
  bge: "intfloat/e5-base-v2"
  contriever: "facebook/contriever"
  llama2-7B-chat: "meta-llama/Llama-2-7b-chat-hf"
  llama2-7B: "meta-llama/Llama-2-7b-hf"
  llama2-13B: "meta-llama/Llama-2-13b-hf"
  llama2-13B-chat: "meta-llama/Llama-2-13b-chat-hf"
  
# Pooling methods for each embedding model
model2pooling:
  e5: "mean"
  bge: "cls"
  contriever: "mean"
  jina: 'mean'
  dpr: cls

# Indexes path for retrieval models
method2index:
  e5: ~
  bm25: ~
  contriever: ~
```

### Environment Settings

Here mainly manage various configurations of the experiment.

```yaml
# Directory paths for data and outputs
data_dir: "dataset/"
save_dir: "output/"

gpu_id: "0,1,2,3"
dataset_name: "nq" # name of the dataset in data_dir
split: ["test"]  # dataset split to load (e.g. train,dev,test)

# Sampling configurations for testing
test_sample_num: ~  # number of samples to test (only work in dev/test split), if None, test all samples
random_sample: False # whether to randomly sample the test samples

# Seed for reproducibility
seed: 2024

# Whether save intermediate data
save_intermediate_data: True
save_note: 'experiment'
```

- `split`: Specifies the dataset split to load, multiple splits can be specified and used separately in the code.
- `save_note`, `save_dir`: Each experiment will create a new folder in `save_dir`, and add `save_note` as a marker in the folder name.
- `save_intermediate_data`: If enabled, intermediate results will be recorded in the experiment folder, including the retrieval content, generated content, and corresponding evaluation metrics for the content.
- `gpu_id`: Specifies which GPUs to use for the experiment, supports multiple GPUs.

If you do not want to use the entire dataset for testing, you can set 'test_sample_num' (invalid for the training set). If set, the corresponding number of samples at the front will be selected. If 'random_sample' is enabled, it will be randomly selected.

### Retrieval Settings

This section manages various parameters for the retriever and reranker.

```yaml
retrieval_method: "e5"  # name or path of the retrieval model. 
retrieval_model_path: ~ # path to the retrieval model
index_path: ~ # set automatically if not provided. 
faiss_gpu: False # whether use gpu to hold index
corpus_path: ~  # path to corpus in '.jsonl' format that store the documents

use_sentence_transformer: False # If set, the retriever will be load through `sentence transformer` library
retrieval_topk: 5 # number of retrieved documents
retrieval_batch_size: 256  # batch size for retrieval
retrieval_use_fp16: True  # whether to use fp16 for retrieval model
retrieval_query_max_length: 128  # max length of the query
save_retrieval_cache: True # whether to save the retrieval cache
use_retrieval_cache: False # whether to use the retrieval cache
retrieval_cache_path: ~ # path to the retrieval cache
retrieval_pooling_method: ~ # set automatically if not provided

use_reranker: False # whether to use reranker
rerank_model_name: ~ # same as retrieval_method
rerank_model_path: ~ # path to reranker model, path will be automatically find in `model2path`
rerank_pooling_method: ~
rerank_topk: 5  # number of remain documents after reranking
rerank_max_length: 512 
rerank_batch_size: 256 # batch size for reranker
rerank_use_fp16: True
```


If the paths in the previous dictionary are filled, only `retrieval_method` and `corpus_path` need to be modified (no need to modify `index_path` and `retrieval_model_path`).

FlashRAG supports saving and reusing retrieval results. When reusing, it will look in the cache to see if there is a query identical to the current one and read the corresponding results.
- `save_retrieval_cache`: If set to `True`, it will save the retrieval results as a JSON file, recording the retrieval results and scores for each query, enabling reuse next time.
- `retrieval_cache_path`: Set to the path of the previously saved retrieval cache.

To use a reranker, set `use_reranker` to `True` and fill in `rerank_model_name`. For Bi-Embedding type rerankers, the pooling method needs to be set, similar to the retrieval method.

If set `use_sentence_transformer` to `True`, there is no need to set consider pooling method.

### Generator Settings

This section records various settings for the generator.

```yaml
framework: fschat # inference frame work of LLM, supporting: 'hf','vllm','fschat', 'openai'
generator_model: "llama3-8B-instruct" # name or path of the generator model
# setting for openai model, only valid in openai framework
openai_setting:
  api_key: ~
  base_url: ~
generator_model_path: ~
generator_max_input_len: 1024  # max length of the input
generator_batch_size: 4 # batch size for generation, invalid for vllm
generation_params:  
  max_tokens: 32
use_fid: False # whether to use FID, only valid in encoder-decoder model
```

- `framework`: The base framework of the generator. It is recommended to use `vllm` for deployment.
- `generation_params`: Parameters needed during generation. The parameter names may need to be adjusted according to different frameworks. Refer to the function descriptions of vllm or huggingface generation for details.

### Evaluation Settings

This section sets various settings used during evaluation. If you use a custom evaluation metric, you can add your parameters in `metric_setting` and call them in the metric.

```yaml
# Metrics to evaluate the result
metrics: ['em','f1','acc','precision','recall'] 
# Specify setting for metric, will be called within certain metrics
metric_setting: 
  retrieval_recall_topk: 5
save_metric_score: True #ã€€whether to save the metric score into txt file
```

- `metrics`: The specific evaluation metrics to be used. The values are the `metric_name` of the evaluation metrics. Currently supported evaluation metrics can be found [<u>here</u>](../flashrag/evaluator/metrics.py).


================================================
FILE: docs/original_docs/introduction_for_beginners_en.md
================================================
## 1. Overview

This document aims to introduce various configurations and functionalities of this project using the Standard RAG process as an example. Additional documentation will be provided later to cover complex uses such as reproducing existing methods and detailed usage of individual components.

The Standard RAG process includes the following three steps:
1. Retrieve relevant documents from the knowledge base based on the user's query.
2. Incorporate the retrieved documents and the original query into a prompt.
3. Input the prompt into the generator.

This document will demonstrate the RAG process using `E5` as the retriever and `Llama2-7B-Chat` as the generator.

## 2. Prerequisites

To smoothly run the entire RAG process, you need to complete the following five preparations:

1. Install the project and its dependencies.
2. Download the required models.
3. Download the necessary datasets (a [toy dataset](../examples/quick_start/dataset/nq) is provided).
4. Download the document collection for retrieval (a [toy corpus](../examples/quick_start/indexes/general_knowledge.jsonl) is provided).
5. Build the index for retrieval (a [toy index](../examples/quick_start/indexes/e5_Flat.index) is provided).

To save time in getting started, we provide toy datasets, document collections, and corresponding indices. Therefore, you only need to complete the first two steps to successfully run the entire process.

### 2.1 Installing the Project and Dependencies

Install the project and its dependencies using the following commands.

Note that if you encounter issues installing the `vllm`, `fschat`, or `pyserini` packages, you can comment them out in the `requirement.txt` file. These packages are required for certain functionalities, but omitting them temporarily won't affect the workflow described in this document.

```bash
git clone https://github.com/RUC-NLPIR/FlashRAG.git
cd FlashRAG
pip install -e . 
```

### 2.2 Download Models

You need to download the following two models:

- E5-base-v2
- Llama2-7B-Chat

You can download the models from [Huggingface](https://huggingface.co/intfloat/e5-base-v2). If you are in China, it's recommended to use the mirror platform [hf-mirror](https://hf-mirror.com/) for downloading.

### 2.3 Download Datasets

The datasets include queries and corresponding standard answers, allowing us to evaluate the effectiveness of the RAG system.

For simplicity, we have sampled 17 pieces of data from NQ as a toy dataset, located at [examples/quick_start/dataset/nq](../examples/quick_start/dataset/nq/). The subsequent RAG process will be conducted on these questions.

Our repository also provides a large number of processed benchmark datasets. You can visit our  [huggingface datasets](https://huggingface.co/datasets/ignore/FlashRAG_datasets) to download and use them.

### 2.4 Downloading the Document Collection

The document collection contains a large number of segmented paragraphs, serving as the external knowledge source for the RAG system. Since commonly used document collections are often very large (~5G or more), we use a [general knowledge dataset](https://huggingface.co/datasets/MuskumPillerum/General-Knowledge) as a toy collection, located at  [examples/quick_start/indexes/general_knowledge.jsonl](../examples/quick_start/indexes/general_knowledge.jsonl)ã€‚

> Due to the small number of documents, many queries may not find relevant texts, which could affect the final retrieval results.


If you need to obtain the full document collection, you can visit our [huggingface dataset](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets) to download and use them.


### 2.5 Building the Retrieval Index

To improve retrieval efficiency, we often need to build the retrieval index in advance. For the BM25 method, the index is usually an inverted index (a directory in our project). For various embedding methods, the index is a Faiss database containing the embeddings of all texts in the document collection (an .index file). **Each index corresponds to a corpus and a retrieval method**, meaning that every time you want to use a new embedding model, you need to rebuild the index.

Here, we provide a [toy index](../examples/quick_start/indexes/e5_Flat.index), built using E5-base-v2 and the aforementioned toy corpus.

If you want to use your own retrieval model and documents, you can refer to our [index building document](./building-index.md) to build your index.


## 3. Running the RAG Process

In the following steps, we will break down each step and demonstrate the corresponding code. The complete code will be provided at the end, or you can refer to the [simple_pipeline.py](../examples/quick_start/simple_pipeline.py) file.


### 3.1 Loading the Config

First, we need to load the `Config` and fill in the paths of the previously downloaded items.

`Config` manages all the paths and hyperparameters in the experiment. In FlashRAG, various parameters can be passed into the Config via a yaml file or a Python dictionary. The passed parameters will replace the default internal parameters. For detailed parameter information and their default values, you can refer to our [`basic_config.yaml`](../flashrag/config/basic_config.yaml).


Here, we directly pass the paths via a dictionary.

```python
from flashrag.config import Config

config_dict = { 
    'data_dir': 'dataset/',
    'index_path': 'indexes/e5_Flat.index',
    'corpus_path': 'indexes/general_knowledge.jsonl',
    'model2path': {'e5': <retriever_path>, 'llama2-7B-chat': <generator_path>},
    'generator_model': 'llama2-7B-chat',
    'retrieval_method': 'e5',
    'metrics': ['em', 'f1', 'acc'],
    'retrieval_topk': 1,
    'save_intermediate_data': True
}

config = Config(config_dict=config_dict)
```

### 3.2 Loading the Dataset and Pipeline

Next, we need to load the dataset and pipeline.

The dataset can be automatically read through the previously set config; we only need to select the corresponding test set.

The pipeline loading requires us to select an appropriate pipeline based on the desired RAG process. Here, we choose `SequentialPipeline` for the Standard RAG process mentioned earlier.
The pipeline will automatically load the corresponding components (retriever and generator) and complete various initializations.

```python
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline

all_split = get_dataset(config)
test_data = all_split['test']
pipeline = SequentialPipeline(config)
```


### 3.3 Running the RAG Process

After completing the above steps, we only need to call the pipeline's `.run` method to run the RAG process on the dataset and generate evaluation results. This method returns a dataset containing intermediate results and final results, with the pred attribute containing the model's predictions.

Note that because we provided toy document collections and indices, the results might be relatively poor. Consider using your own document collections and indices for better results.

After the process completes, all results will be saved in a folder corresponding to the current experiment, including the retrieval and generation results for each query, overall evaluation scores, and more.

The complete code is as follows:

```python
from flashrag.config import Config
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline

config_dict = { 
                'data_dir': 'dataset/',
                'index_path': 'indexes/e5_Flat.index',
                'corpus_path': 'indexes/general_knowledge.jsonl',
                'model2path': {'e5': <retriever_path>, 'llama2-7B-chat': <generator_path>},
                'generator_model': 'llama2-7B-chat',
                'retrieval_method': 'e5',
                'metrics': ['em','f1','acc'],
                'retrieval_topk': 1,
                'save_intermediate_data': True
            }

config = Config(config_dict = config_dict)

all_split = get_dataset(config)
test_data = all_split['test']
pipeline = SequentialPipeline(config)

output_dataset = pipeline.run(test_data,do_eval=True)
print("---generation output---")
print(output_dataset.pred)
```




================================================
FILE: docs/original_docs/multi_retriever_usage.md
================================================
# Multi-Retriever Usage

## Configuration

Using multiple retrievers simultaneously requires configuration in the `multi_retrieve` section of the config file. Enable `use_multi_retriever` and configure it as follows:

```yaml
use_multi_retriever: True # whether to use multi retrievers
multi_retriever_setting:
  merge_method: "concat" # support 'concat', 'rrf', 'rerank'
  topk: 5 # final remaining documents, only used in 'rrf' and 'rerank' merge
  rerank_model_name: ~
  rerank_model_path: ~
  retriever_list:
    - retrieval_method: "e5"
      retrieval_topk: 5
      index_path: ~
      retrieval_model_path: ~
    - retrieval_method: "bm25"
      retrieval_topk: 5
      index_path: ~
      retrieval_model_path: ~
```

The `retriever_list` can include multiple retrievers, each with its own corresponding corpus and index. The configuration for each retriever follows the same settings as a single retriever.

Currently, three aggregation methods are supported:

1. **`concat`**: Directly concatenate the results from multiple retrievers.
2. **`rrf`**: Aggregate the results from retrievers using the RRF (Reciprocal Rank Fusion) algorithm.
3. **`rerank`**: Use a reranker to rerank all the results from the retrievers. This requires configuring the reranker.

For both `rrf` and `rerank`, only the top `k` results will be retained.

## Quick Usage

Below is a quick example of how to use the multi-retriever feature:

```python
from flashrag.utils import get_retriever
from flashrag.config import Config

config_dict = {
    "gpu_id": "1",
    "use_multi_retriever": True,
    "multi_retriever_setting": {
        "merge_method": "rerank",
        "topk": 5,
        'rerank_model_name': 'bge-reranker',
        'rerank_model_path': 'bge-reranker-m3',
        "retriever_list": [
            {
                "retrieval_method": "bm25",
                "corpus_path": "general_knowledge.jsonl",
                "index_path": "indexes/general_knowledge/bm25",
                "retrieval_topk": 3,
                "bm25_backend": "pyserini",
            },
            {
                "retrieval_method": "e5",
                "corpus_path": "general_knowledge.jsonl",
                "index_path": "indexes/general_knowledge/e5_Flat.index",
                "retrieval_topk": 1,
            },
        ],
    },
}
config = Config("my_config.yaml", config_dict=config_dict)
retriever = get_retriever(config)
query_list = ['who is the president of USA?']

output, score = retriever.batch_search(query_list, return_score=True)
for item,s in zip(output, score):
    print(item, s)
    print("----")
```



================================================
FILE: docs/original_docs/process-wiki.md
================================================
# Building Wiki Corpus

You can create your own wiki corpus by following these steps.

## Step1: Download Wiki dump

Download the Wikipedia dump you require in XML format. For instance: 

```bash
wget https://archive.org/download/enwiki-20181220/enwiki-20181220-pages-articles.xml.bz2
```

You can access other dumps from this [<u>website</u>](https://archive.org/search?query=Wikimedia+database+dump&sort=-downloads).

## Step2: Run process script

Execute the provided script to process the wiki dump into JSONL format. Adjust the corpus partitioning parameters as needed:

```bash
cd scripts
python preprocess_wiki.py --dump_path ../enwikinews-20240420-pages-articles.xml.bz2  \
                        --save_path ../test_sample.jsonl \
                        --chunk_by sentence \
                        --seg_size 6 \
                        --stride 1 \
                        --num_workers 1
```


We also provide the version we used for experiments. Download link: https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/tree/main/retrieval-corpus



================================================
FILE: docs/original_docs/reproduce_experiment.md
================================================
# Guidelines for Reproduction Methods

In this document, we will introduce how to reproduce the results of various methods listed in our table under a unified setting. For specific settings and explanations of each method, please refer to [implementation details](./baseline_details.md). It is recommended to have some basic understanding of our repository beforehand, which can be found in [introduction for beginners](./introduction_for_beginners_en.md).

## Preliminary

- Install FlashRAG and dependencies
- Download [Llama3-8B-instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [E5-base-v2](https://huggingface.co/intfloat/e5-base-v2)
- Download datasets (you can download from our repo: [here](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets))
- Download retrieval corpus (from [here](https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/tree/main/retrieval-corpus))
- Build index for retrieval, using E5 for embedding (see [how to build index?](./building-index.md))

## Reproduce Step

All the code used is based on the repository's [example/methods](../examples/methods/). We have set appropriate hyperparameters for various methods. If you need to adjust them yourself, you can refer to the config dictionary provided for each method and the original papers of each method.

### 1. Set Basic Config

First, you need to fill in the paths of various downloads in `my_config.yaml`. Specifically, you need to fill in the following four fields:
- **model2path**: Replace the paths of E5 and Llama3-8B-instruct models with your own paths
- **method2index**: Fill in the path of the index file built using E5
- **corpus_path**: Fill in the path of the Wikipedia corpus file in `jsonl` format
- **data_dir**: Change to the download path of your own dataset

### 2. Set Config for Specific Method

For some methods that require the use of additional models, extra steps are required. We will introduce the methods that need extra steps below. If you know that the method you want to run does not need these steps, you can skip directly to the third section.

Table of Contents:
- [AAR](#aar)
- [LongLLMLingua](#longllmlingua)
- [RECOMP](#recomp)
- [Selective-Context](#selective-context)
- [Ret-Robust](#ret-robust)
- [SKR](#skr)
- [Self-RAG](#self-rag)
- [Spring](#spring)
- [Adaptive-RAG](#adaptive-rag)
- [RQRAG](#rqrag)
- [R1-Searcher](#r1-searcher)

#### AAR

This method requires using a new retriever, so you need to download the retriever and build the index.

- Additional Step1: Download AAR-Contriever (from [here](https://huggingface.co/OpenMatch/AAR-Contriever-KILT))
- Additional Step2: Build the index for AAR-Contriever (note that the pooling method should be 'mean')
- Additional Step3: Modify the `index_path` and `model2path` in the `AAR` function in `run_exp.py`.

#### LongLLMLingua

This method requires downloading Llama2-7B.

- Additional Step1: Download Llama2-7B (from [here](https://huggingface.co/meta-llama/Llama-2-7b-hf))
- Additional Step2: Modify the `refiner_model_path` in the `llmlingua` function in `run_exp.py`

#### RECOMP

This method requires downloading three checkpoints trained by the authors (trained on NQ, TQA, and HotpotQA respectively).

- Additional Step1: Download the author's checkpoints ([NQ Model](https://huggingface.co/fangyuan/nq_abstractive_compressor), [TQA Model](https://huggingface.co/fangyuan/tqa_abstractive_compressor), [HotpotQA Model](https://huggingface.co/fangyuan/hotpotqa_abstractive))
- Additional Step2: Fill in the downloaded model paths in the `model_dict` of the `recomp` function

#### Selective-Context

This method requires downloading GPT2.

- Additional Step1: Download GPT2 (from [here](https://huggingface.co/openai-community/gpt2))
- Additional Step2: Modify the `refiner_model_path` in the `sc` function in `run_exp.py`

#### Ret-Robust

This method requires downloading the Lora trained by the authors and downloading the Llama2-13B model to load the Lora.

- Additional Step1: Download Llama2-13B (from [here](https://huggingface.co/meta-llama/Llama-2-13b-hf))
- Additional Step2: Download the author's trained Lora, trained on NQ (from [here](https://huggingface.co/Ori/llama-2-13b-peft-nq-retrobust)) and trained on 2WikiMultihopQA (from [here](https://huggingface.co/Ori/llama-2-13b-peft-2wikihop-retrobust))
- Additional Step3: Modify the corresponding Lora paths in the `model_dict` of the `retrobust` function and the Llama2-13B path in `my_config.yaml`

We recommend adjusting the `single_hop` parameter in the `SelfAskPipeline` according to different datasets, which controls whether to decompose the query. For `NQ, TQA, PopQA, WebQ`, we set `single_hop` to `True`.

#### SKR

This method requires an embedding model and training data used during the inference stage. We provide the training data given by the authors. If you wish to use your own training data, you can generate it according to the format of the training data and the original paper.

- Additional Step1: Download the embedding model (from [here](https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased))
- Additional Step2: Download the training data (from [here](../examples/methods/sample_data/skr_training.json))
- Additional Step3: Fill in the embedding model path in the `model_path` of the `skr` function
- Additional Step4: Fill in the training data path in the `training_data_path` of the `skr` function

#### Self-RAG

This method requires using a trained model and currently only supports running in the `vllm` framework.

- Additional Step1: Download the Self-RAG model (from [7B model](https://huggingface.co/selfrag/selfrag_llama2_7b), [13B model](https://huggingface.co/selfrag/selfrag_llama2_13b))
- Additional Step2: Modify the `generator_model_path` in the `selfrag` function.

#### Spring
This method requires a virtual token embedding file and currently only supports running in the `hf` framework.

- Additional Step1: Download virtual token embedding file from [official repo](https://huggingface.co/yutaozhu94/SPRING)
- Additional Step2: Modify the `token_embedding_path` in the `spring` function.

#### Adaptive-RAG

This method requires a classifier to classify the query. Since the author did not provide an official checkpoint, we used a checkpoint trained by others on Huggingface for the experiment (which may result in inconsistent results).

If the official open-source checkpoint is released in the future, we will update the experimental results.

- Additional Step1: Download classifier model from huggingface repo (**not official**): [illuminoplanet/combined_flan_t5_xl_classifier](https://huggingface.co/illuminoplanet/combined_flan_t5_xl_classifier)
- Additional Step2: Modify the `model_path` in `adaptive` function.

#### RQRAG

This method requires downloading the RQRAG model.

- Additional Step1: Download RQRAG model from huggingface repo: [zorowin123/rq_rag_llama2_7B](https://huggingface.co/zorowin123/rq_rag_llama2_7B)
- Additional Step2: Modify the `generator_model_path` in the `rqrag` function.

### 3. Run methods

Run the experiment on the NQ dataset using the following command.

```bash
python run_exp.py --method_name 'naive' \
                  --split 'test' \
                  --dataset_name 'nq' \
                  --gpu_id '0,1,2,3'
```

The method can be selected from the following:
```
naive zero-shot AAR-contriever llmlingua recomp selective-context sure replug skr flare iterretgen ircot trace
```


#### R1-Searcher

This method requires downloading the R1-Searcher model.

- Additional Step1: Download R1-Searcher model from huggingface repo: [XXsongLALA/Qwen-2.5-7B-base-RAG-RL](https://huggingface.co/XXsongLALA/Qwen-2.5-7B-base-RAG-RL)
- Additional Step2: Modify the `generator_model_path` in the `r1searcher` function.





================================================
FILE: examples/multi_turn.py
================================================
"""
A simple case to use generator for multi-turn interaction
"""
import argparse
from flashrag.config import Config
from flashrag.utils import get_generator, get_dataset, get_retriever
from flashrag.prompt import PromptTemplate

parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str)
args = parser.parse_args()
config_dict = {
    "data_dir": "dataset/",
    "model2path": {"llama3-8B-instruct": args.model_path},
    "generator_model": "llama3-8B-instruct",
    "retrieval_method": "e5",
    "metrics": ["em", "f1", "acc"],
    "retrieval_topk": 1,
    "save_intermediate_data": True,
}

config = Config(config_dict=config_dict)
generator = get_generator(config)
prompt_template = PromptTemplate(config)

messages = [{"role":"system", "content":"You are a helpful assistant, please follow the user's instructions to complete the task."}]

### turn 1
messages.append({"role": "user", "content":"Who is the wife of the current US President?"})
print(messages)
input_prompt = prompt_template.get_string(messages=messages)
print(f"#### Turn 1 input: {input_prompt}")
output = generator.generate(input_prompt)[0]
print(f"#### Turn 1 output: {output}")

### turn 2
messages.append({"role": "system", "content": output})
# add new input here
messages.append({"role": "user", "content": "Your answer is incorrect, can you provide a high-quality answer again?"})
input_prompt = prompt_template.get_string(messages=messages)
print(f"#### Turn 2 input: {input_prompt}")
output = generator.generate(input_prompt)[0]
print(f"#### Turn 2 output: {output}")




================================================
FILE: examples/run_refiner.py
================================================
from flashrag.config import Config
from flashrag.dataset import Dataset, Item


config_dict = {
    "save_note": 'test',
    'refiner_name': 'longllmlingua',
    'refiner_model_path': 'model/llama-2-7b-hf',
    'refiner_input_prompt_flag': True, # ç›´æŽ¥å¯¹promptè¿›è¡ŒåŽ‹ç¼©
    'llmlinuga_config': {
        'use_llmlingua2': False,
        "rate": 0.55,
        "condition_in_question": "after_condition",
        "reorder_context": "sort",
        "dynamic_context_compression_ratio": 0.3,
        "condition_compare": True,
        "context_budget": "+100",
        "rank_method": "longllmlingua",
    }
}

config = Config('my_config.yaml', config_dict)

from flashrag.refiner import LLMLinguaRefiner

refiner = LLMLinguaRefiner(config)

prompt = "Answer the question based on the given document. Only give me the answer and do not output any other words.\n\nThe following are given documents.\n\nDoc 1(Title: \"Polish-Russian War (film)\") Polish-Russian War (film) Polish-Russian War (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery \u017bu\u0142awski based on the novel Polish-Russian War under the white-red flag by Dorota Mas\u0142owska. The film's events take place over several days and they are set in the present time in a large Polish city. The main character is a bandit, a Polish dres (a Polish chav) called \"\"Strong\"\" (Borys Szyc), who does not work or study, and who frequently gets into conflict with the law and is in love with Magda (Roma G\u0105siorowska). The relationship is not going well. \n\nQuestion: Who is the mother of the director of film Polish-Russian War (Film)?"
dataset = Dataset(
    config = config, data = [Item({"prompt": prompt, "retrieval_result": ""})]
)
output = refiner.batch_run(dataset)[0]
print(output)


================================================
FILE: examples/methods/my_config.yaml
================================================
# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to models
model2path:
  e5: "intfloat/e5-base-v2"
  bge: "BAAI/bge-base-en-v1.5"
  contriever: "facebook/contriever"
  llama2-7B-chat: "meta-llama/Llama-2-7b-chat-hf"
  llama2-7B: "meta-llama/Llama-2-7b-hf"
  llama2-13B: "meta-llama/Llama-2-13b-hf"
  llama2-13B-chat: "meta-llama/Llama-2-13b-chat-hf"
  llama3-8B-instruct: "meta-llama/Meta-Llama-3-8B-Instruct"

# Pooling methods for each embedding model
model2pooling:
  e5: "mean"
  bge: "cls"
  contriever: "mean"
  jina: 'mean'
  dpr: cls

# Indexes path for retrieval models
method2index:
  e5: ~
  bm25: ~
  contriever: ~

# ------------------------------------------------Environment Settings------------------------------------------------#
# Directory paths for data and outputs
data_dir: "dataset/"
save_dir: "output/"

gpu_id: "0,1,2,3"
dataset_name: "nq" # name of the dataset in data_dir
split: [ "test" ]  # dataset split to load (e.g. train,dev,test)

# Sampling configurations for testing
test_sample_num: ~  # number of samples to test (only work in dev/test split), if None, test all samples
random_sample: False # whether to randomly sample the test samples

# Seed for reproducibility
seed: 2024

# Whether save intermediate data
save_intermediate_data: True
save_note: 'experiment'

# -------------------------------------------------Retrieval Settings------------------------------------------------#
# If set the name, the model path will be find in global paths
retrieval_method: "e5"  # name or path of the retrieval model. 
index_path: ~ # set automatically if not provided. 
faiss_gpu: False # whether use gpu to hold index
corpus_path: ~  # path to corpus in '.jsonl' format that store the documents

instruction: ~ # instruction for retrieval model
retrieval_topk: 5 # number of retrieved documents
retrieval_batch_size: 256  # batch size for retrieval
retrieval_use_fp16: True  # whether to use fp16 for retrieval model
retrieval_query_max_length: 128  # max length of the query
save_retrieval_cache: False # whether to save the retrieval cache
use_retrieval_cache: False # whether to use the retrieval cache
retrieval_cache_path: ~ # path to the retrieval cache
retrieval_pooling_method: ~ # set automatically if not provided

use_reranker: False # whether to use reranker
rerank_model_name: ~ # same as retrieval_method
rerank_model_path: ~ # path to reranker model, path will be automatically find in `retriever_model2path`
rerank_pooling_method: ~
rerank_topk: 5  # number of remain documents after reranking
rerank_max_length: 512
rerank_batch_size: 256 # batch size for reranker
rerank_use_fp16: True

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "llama3-8B-instruct" # name or path of the generator model
generator_max_input_len: 2048  # max length of the input
generator_batch_size: 2 # batch size for generation, invalid for vllm
generation_params:
  do_sample: False
  max_tokens: 32
use_fid: False # whether to use FID, only valid in encoder-decoder model


# -------------------------------------------------Refiner Settings------------------------------------------------#
# If set, the refiner will be used to refine the retrieval documents.
refiner_name: ~
refiner_model_path: ~

# Used for extractive method (e.g embedding models)
refiner_topk: 5 # number of remain sentence after refiner
refiner_pooling_method: 'mean' # pooling method of refiner model
refiner_encode_max_length: 256
# Used for abstractive method (e.g. generation models like bart-large-cnn)
refiner_max_input_length: 1024
refiner_max_output_length: 512

# Specify settings for llmlingua
llmlingua_config:
  rate: 0.55
  condition_in_question: 'after_condition'
  reorder_context: 'sort'
  dynamic_context_compression_ratio: 0.3
  condition_compare: True
  context_budget: "+100"
  rank_method: 'longllmlingua'
sc_config:
  'reduce_ratio': 0.5

# -------------------------------------------------Evaluation Settings------------------------------------------------#
# Metrics to evaluate the result
metrics: [ 'em','f1','acc','precision','recall']
# Specify setting for metric, will be called within certain metrics
metric_setting:
  retrieval_recall_topk: 5
save_metric_score: True #ã€€whether to save the metric score into txt file






================================================
FILE: examples/methods/run_exp.py
================================================
import os
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"
from flashrag.config import Config
from flashrag.utils import get_dataset
import argparse


def naive(args):
    save_note = "naive"
    config_dict = {"save_note": save_note, "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}

    from flashrag.pipeline import SequentialPipeline

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    pipeline = SequentialPipeline(config)

    result = pipeline.run(test_data)


def zero_shot(args):
    save_note = "zero-shot"
    config_dict = {"save_note": save_note, "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SequentialPipeline
    from flashrag.prompt import PromptTemplate

    templete = PromptTemplate(
        config=config,
        system_prompt="Answer the question based on your own knowledge. Only give me the answer and do not output any other words.",
        user_prompt="Question: {question}",
    )
    pipeline = SequentialPipeline(config, templete)
    result = pipeline.naive_run(test_data)


def aar(args):
    """
    Reference:
        Zichun Yu et al. "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In"
        in ACL 2023.
        Official repo: https://github.com/OpenMatch/Augmentation-Adapted-Retriever
    """
    # two types of checkpoint: ance / contriever
    # retrieval_method = "AAR-contriever"  # AAR-ANCE
    # index path of this retriever
    retrieval_method = args.method_name
    if "contriever" in retrieval_method:
        index_path = "aar-contriever_Flat.index"
    else:
        index_path = "aar-ance_Flat.index"

    model2path = {"AAR-contriever": "model/AAR-Contriever-KILT", "AAR-ANCE": "model/AAR-ANCE"}
    model2pooling = {"AAR-contriever": "mean", "AAR-ANCE": "cls"}
    save_note = retrieval_method
    config_dict = {
        "retrieval_method": retrieval_method,
        "model2path": model2path,
        "index_path": index_path,
        "model2pooling": model2pooling,
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SequentialPipeline

    pipeline = SequentialPipeline(config)
    # result = pipeline.run(test_data, pred_process_fun=pred_process_fun)
    result = pipeline.run(test_data)


def llmlingua(args):
    """
    Reference:
        Huiqiang Jiang et al. "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"
        in EMNLP 2023
        Huiqiang Jiang et al. "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"
        in ICLR MEFoMo 2024.
        Official repo: https://github.com/microsoft/LLMLingua
    """
    refiner_name = "longllmlingua"  #
    refiner_model_path = "model/llama-2-7b-hf"

    config_dict = {
        "refiner_name": refiner_name,
        "refiner_model_path": refiner_model_path,
        "llmlingua_config": {
            "rate": 0.55,
            "condition_in_question": "after_condition",
            "reorder_context": "sort",
            "dynamic_context_compression_ratio": 0.3,
            "condition_compare": True,
            "context_budget": "+100",
            "rank_method": "longllmlingua",
        },
        "refiner_input_prompt_flag": False,
        "save_note": "longllmlingua",
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SequentialPipeline

    pipeline = SequentialPipeline(config)
    result = pipeline.run(test_data)


def recomp(args):
    """
    Reference:
        Fangyuan Xu et al. "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"
        in ICLR 2024.
        Official repo: https://github.com/carriex/recomp
    """
    # ###### Specified parameters ######
    refiner_name = "recomp-abstractive"  # recomp-extractive
    model_dict = {
        "nq": "model/recomp_nq_abs",
        "triviaqa": "model/recomp_tqa_abs",
        "hotpotqa": "model/recomp_hotpotqa_abs",
    }

    refiner_model_path = model_dict.get(args.dataset_name, None)
    refiner_max_input_length = 1024
    refiner_max_output_length = 512
    # parameters for extractive compress
    refiner_topk = 5
    refiner_pooling_method = "mean"
    refiner_encode_max_length = 256

    config_dict = {
        "refiner_name": refiner_name,
        "refiner_model_path": refiner_model_path,
        "refiner_max_input_length": refiner_max_input_length,
        "refiner_max_output_length": refiner_max_output_length,
        "refiner_topk": 5,
        "refiner_pooling_method": refiner_pooling_method,
        "refiner_encode_max_length": refiner_encode_max_length,
        "save_note": refiner_name,
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SequentialPipeline

    pipeline = SequentialPipeline(config)
    result = pipeline.run(test_data)


def sc(args):
    """
    Reference:
        Yucheng Li et al. "Compressing Context to Enhance Inference Efficiency of Large Language Models"
        in EMNLP 2023.
        Official repo: https://github.com/liyucheng09/Selective_Context

    Note:
        Need to install spacy:
            ```python -m spacy download en_core_web_sm```
        or
            ```
            wget https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0.tar.gz
            pip install en_core_web_sm-3.6.0.tar.gz
            ```
    """
    refiner_name = "selective-context"
    refiner_model_path = "model/gpt2"

    config_dict = {
        "refiner_name": refiner_name,
        "refiner_model_path": refiner_model_path,
        "sc_config": {"reduce_ratio": 0.5},
        "save_note": "selective-context",
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SequentialPipeline

    pipeline = SequentialPipeline(config)
    result = pipeline.run(test_data)


def retrobust(args):
    """
    Reference:
        Ori Yoran et al. "Making Retrieval-Augmented Language Models Robust to Irrelevant Context"
        in ICLR 2024.
        Official repo: https://github.com/oriyor/ret-robust
    """
    model_dict = {
        "nq": "model/llama-2-13b-peft-nq-retrobust",
        "2wiki": "model/llama-2-13b-peft-2wikihop-retrobust",
    }
    if args.dataset_name in ["nq", "triviaqa", "popqa", "web_questions"]:
        lora_path = model_dict["nq"]
    elif args.dataset_name in ["hotpotqa", "2wikimultihopqa"]:
        lora_path = model_dict["2wiki"]
    else:
        print("Not use lora")
        lora_path = model_dict.get(args.dataset_name, None)
    config_dict = {
        "save_note": "Ret-Robust",
        "generator_model": "llama2-13B",
        "generator_lora_path": lora_path,
        "generation_params": {"max_tokens": 100},
        "gpu_id": args.gpu_id,
        "generator_max_input_len": 4096,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SelfAskPipeline
    from flashrag.utils import selfask_pred_parse

    pipeline = SelfAskPipeline(config, max_iter=5, single_hop=False)
    # use specify prediction parse function
    result = pipeline.run(test_data, pred_process_fun=selfask_pred_parse)


def sure(args):
    """
    Reference:
        Jaehyung Kim et al. "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs"
        in ICLR 2024
        Official repo: https://github.com/bbuing9/ICLR24_SuRe
    """
    config_dict = {"save_note": "SuRe", "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SuRePipeline

    pipeline = SuRePipeline(config)
    pred_process_fun = lambda x: x.split("\n")[0]
    result = pipeline.run(test_data)


def replug(args):
    """
    Reference:
        Weijia Shi et al. "REPLUG: Retrieval-Augmented Black-Box Language Models".
    """
    save_note = "replug"
    config_dict = {"save_note": save_note, "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    pred_process_fun = lambda x: x.split("\n")[0]

    from flashrag.pipeline import REPLUGPipeline

    pipeline = REPLUGPipeline(config)
    result = pipeline.run(test_data)


def skr(args):
    """
    Reference:
        Yile Wang et al. "Self-Knowledge Guided Retrieval Augmentation for Large Language Models"
        in EMNLP Findings 2023.
        Official repo: https://github.com/THUNLP-MT/SKR/

    Note:
        `skr-knn` need training data in inference stage to determain whether to retrieve. training data should in
        `.json` format in following format:
        format:
            [
                {
                    "question": ... ,  // question
                    "judgement": "ir_better" / "ir_worse" / "same",  // judgement result, can be obtained by comparing
                    ...
                },
                ...
            ]

    """
    judger_name = "skr"
    model_path = "model/sup-simcse-bert-base-uncased"
    training_data_path = "./sample_data/skr_training.json"

    config_dict = {
        "judger_name": judger_name,
        "judger_config": {
            "model_path": model_path,
            "training_data_path": training_data_path,
            "topk": 5,
            "batch_size": 64,
            "max_length": 128,
        },
        "save_note": "skr",
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import ConditionalPipeline

    pipeline = ConditionalPipeline(config)
    result = pipeline.run(test_data)


def selfrag(args):
    """
    Reference:
        Akari Asai et al. " SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection"
        in ICLR 2024.
        Official repo: https://github.com/AkariAsai/self-rag
    """
    config_dict = {
        "generator_model": "selfrag-llama2-7B",
        "generator_model_path": "model/selfrag_llama2_7b",
        "framework": "vllm",
        "save_note": "self-rag",
        "gpu_id": args.gpu_id,
        "generation_params": {
            "max_tokens": 100,
            "temperature": 0.0,
            "top_p": 1.0,
            "skip_special_tokens": False,
        },
        "dataset_name": args.dataset_name,
        "split": args.split,
    }
    config = Config("my_config.yaml", config_dict)

    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import SelfRAGPipeline

    pipeline = SelfRAGPipeline(
        config,
        threshold=0.2,
        max_depth=2,
        beam_width=2,
        w_rel=1.0,
        w_sup=1.0,
        w_use=1.0,
        use_grounding=True,
        use_utility=True,
        use_seqscore=True,
        ignore_cont=True,
        mode="adaptive_retrieval",
    )
    result = pipeline.run(test_data, long_form=False)


def flare(args):
    """
    Reference:
        Zhengbao Jiang et al. "Active Retrieval Augmented Generation"
        in EMNLP 2023.
        Official repo: https://github.com/bbuing9/ICLR24_SuRe

    """
    config_dict = {"save_note": "flare", "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import FLAREPipeline

    pipeline = FLAREPipeline(config)
    result = pipeline.run(test_data)


def iterretgen(args):
    """
    Reference:
        Zhihong Shao et al. "Enhancing Retrieval-Augmented Large Language Models with Iterative
                            Retrieval-Generation Synergy"
        in EMNLP Findings 2023.

        Zhangyin Feng et al. "Retrieval-Generation Synergy Augmented Large Language Models"
        in EMNLP Findings 2023.
    """
    iter_num = 3
    config_dict = {
        "save_note": "iter-retgen",
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }
    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import IterativePipeline

    pipeline = IterativePipeline(config, iter_num=iter_num)
    result = pipeline.run(test_data)


def ircot(args):
    """
    Reference:
        Harsh Trivedi et al. "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
        in ACL 2023
    """
    save_note = "ircot"
    config_dict = {"save_note": save_note, "gpu_id": args.gpu_id, "dataset_name": args.dataset_name, "split": args.split}

    from flashrag.pipeline import IRCOTPipeline

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    print(config["generator_model_path"])
    pipeline = IRCOTPipeline(config, max_iter=5)

    result = pipeline.run(test_data)


def trace(args):
    """
    Reference:
        Jinyuan Fang et al. "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation"
    """

    save_note = "trace"
    trace_config = {
        "num_examplars": 3,
        "max_chain_length": 4,
        "topk_triple_select": 5,  # num of candidate triples
        "num_choices": 20,
        "min_triple_prob": 1e-4,
        "num_beams": 5,  # number of selected prob at each step of constructing chain
        "num_chains": 20,  # number of generated chains
        "n_context": 5,  # number of used chains in generation
        "context_type": "triples",  # triples/triple-doc
    }
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "refiner_name": "kg-trace",
        "trace_config": trace_config,
        "framework": "hf",  # Trance only supports using Huggingface Transformers since it needs logits of outputs
        "split": args.split,
    }

    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    from flashrag.pipeline import SequentialPipeline

    pipeline = SequentialPipeline(config)

    result = pipeline.run(test_data)


def spring(args):
    """
    Reference:
        Yutao Zhu et al. "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models"
    """

    save_note = "spring"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "framework": "hf",
        "split": args.split,
    }
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    # download token embedding from: https://huggingface.co/yutaozhu94/SPRING
    token_embedding_path = "llama2.7b.chat.added_token_embeddings.pt"

    from flashrag.prompt import PromptTemplate
    from flashrag.pipeline import SequentialPipeline
    from flashrag.utils import get_generator, get_retriever

    # prepare prompt and generator for Spring method
    system_prompt = (
        "Answer the question based on the given document."
        "Only give me the answer and do not output any other words."
        "\nThe following are given documents.\n\n{reference}"
    )
    added_tokens = [f" [ref{i}]" for i in range(1, 51)]
    added_tokens = "".join(added_tokens)
    user_prompt = added_tokens + "Question: {question}\nAnswer:"
    prompt_template = PromptTemplate(config, system_prompt, user_prompt, enable_chat=False)

    generator = get_generator(config)
    generator.add_new_tokens(token_embedding_path, token_name_func=lambda idx: f"[ref{idx+1}]")

    pipeline = SequentialPipeline(config=config, prompt_template=prompt_template, generator=generator)
    result = pipeline.run(test_data)


def adaptive(args):
    judger_name = "adaptive-rag"
    model_path = "illuminoplanet/adaptive-rag-classifier"

    config_dict = {
        "judger_name": judger_name,
        "judger_config": {"model_path": model_path},
        "save_note": "adaptive-rag",
        "gpu_id": args.gpu_id,
        "dataset_name": args.dataset_name,
        "split": args.split,
    }
    # preparation
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import AdaptivePipeline

    pipeline = AdaptivePipeline(config)
    result = pipeline.run(test_data)

def rqrag(args):
    """
    Function to run the RQRAGPipeline.
    """
    save_note = "rqrag"
    max_depth = 3
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'generator_max_input_len': 4096,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path': 'zorowin123/rq_rag_llama2_7B',
        "dataset_name": args.dataset_name,
        "split": args.split,
        "max_depth": max_depth
    }

    config = Config("my_config.yaml", config_dict)
    
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import RQRAGPipeline
    pipeline = RQRAGPipeline(config, max_depth = max_depth)
    result = pipeline.run(test_data)


def r1searcher(args):
    """
    Function to run the R1-Searcher.
    """
    save_note = "r1-searcher"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'generator_max_input_len': 16384,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path': 'XXsongLALA/Qwen-2.5-7B-base-RAG-RL',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import ReasoningPipeline
    pipeline = ReasoningPipeline(config)
    result = pipeline.run(test_data)


def searchr1(args):
    """
    Function to run the search-r1.
    """
    save_note = "search-r1"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'generator_max_input_len': 16384,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path':'PeterJinGo/SearchR1-nq_hotpotqa_train-qwen2.5-7b-em-ppo',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import SearchR1Pipeline
    pipeline = SearchR1Pipeline(config)
    result = pipeline.run(test_data)
    
def autorefine(args):
    """
    Function to run the AutoRefine.
    """
    save_note = "autorefine"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'generator_max_input_len': 16384,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path': 'yrshi/AutoRefine-Qwen2.5-3B-Base',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import AutoRefinePipeline
    pipeline = AutoRefinePipeline(config)
    result = pipeline.run(test_data)


def o2searcher(args):
    """
    Function to run the O2searcher.
    """
    save_note = "O2searcher"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'retrieval_topk': 3,
        'generator_max_input_len': 16384,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path': 'Jianbiao/O2-Searcher-Qwen2.5-3B-GRPO',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import O2SearcherPipeline
    pipeline = O2SearcherPipeline(config)
    result = pipeline.run(test_data)
    
def rearag(args):
    """
    Function to run the rearag.
    """
    save_note = "rearag"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'is_reasoning': True,
        'generator_max_input_len': 8192 - 2 - 1024,
        'generation_params': {'max_tokens': 1024, 'do_sample': True},
        'generator_model_path': 'THU-KEG/ReaRAG-9B',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    from flashrag.pipeline import ReaRAGPipeline
    pipeline = ReaRAGPipeline(config)
    result = pipeline.run(test_data)
    
def corag(args):
    """
    Function to run the CoRAG.
    """
    save_note = "corag"
    task_desc = 'Given a search query, retrieve relevant documents that can help answer the query'
    if args.dataset_name in ['hotpotqa', 'musique', '2wikimultihopqa', 'bamboogle']:
        task_desc = 'Given a multi-hop question, retrieve documents that can help answer the question'
    if args.dataset_name in ['nq', 'triviaqa']:
        task_desc = 'Given a question, retrieve Wikipedia passages that answer the question'
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'task_desc': task_desc,
        'framework': 'vllm',
        'is_reasoning': True,
        'generator_max_input_len': 4096,
        'generation_params': {'max_tokens': 512, 'skip_special_tokens': False},
        'generator_model_path': 'corag/CoRAG-Llama3.1-8B-MultihopQA',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]

    from flashrag.pipeline import CoRAGPipeline
    pipeline = CoRAGPipeline(config)
    result = pipeline.run(test_data)
    
def simpledeepsearcher(args):
    """
    Function to run the SimpleDeepSearcher.
    """
    save_note = "simpledeepsearcher"
    config_dict = {
        "save_note": save_note,
        "gpu_id": args.gpu_id,
        'framework': 'vllm',
        'generator_max_input_len': 20480,
        'generation_params': {'max_tokens': 2048, 'skip_special_tokens': False},
        'generator_model_path': 'RUC-AIBOX/Qwen-7B-SimpleDeepSearcher',
        "dataset_name": args.dataset_name,
        "split": args.split,
    }

    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]   

    from flashrag.pipeline import SimpleDeepSearcherPipeline
    pipeline = SimpleDeepSearcherPipeline(config)
    result = pipeline.run(test_data)



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Running exp")
    parser.add_argument("--method_name", type=str)
    parser.add_argument("--split", type=str)
    parser.add_argument("--dataset_name", type=str)
    parser.add_argument("--gpu_id", type=str)

    
    func_dict = {
        "AAR-contriever": aar,
        "AAR-ANCE": aar,
        "naive": naive,
        "zero-shot": zero_shot,
        "llmlingua": llmlingua,
        "recomp": recomp,
        "selective-context": sc,
        "ret-robust": retrobust,
        "sure": sure,
        "replug": replug,
        "skr": skr,
        "selfrag": selfrag,
        "flare": flare,
        "iterretgen": iterretgen,
        "ircot": ircot,
        "trace": trace,
        "adaptive": adaptive,
        "rqrag": rqrag,
        "r1-searcher": r1searcher,
        "search-r1": searchr1,
        "autorefine":autorefine,
        "o2-searcher": o2searcher,
        "rearag": rearag,
        "corag": corag,
        "simpledeepsearcher": simpledeepsearcher,
    }

    args = parser.parse_args()
    func = func_dict[args.method_name]
    func(args)



================================================
FILE: examples/methods/utils/get_lm_probs_dataset.py
================================================
# This file is used for generating LM supervised dataset to finetune retrievers
# Implementation details are learned from REPLUG:https://arxiv.org/abs/2301.12652
import json
import random

import fire
import torch
from tqdm import tqdm

from flashrag.config import Config
from flashrag.prompt import PromptTemplate
from flashrag.utils import get_dataset
from flashrag.utils import get_retriever, get_generator


class LMProbCalculator:
    """
    Clculating the likelihood of the ground truth
    when LM uses every document retrieved from
    top-k retrieved ones in context
    """

    def __init__(self, config):
        # Load your own components
        self.retriever = get_retriever(config)
        self.generator = get_generator(config)
        self.prompt_template = PromptTemplate(config)

    def run(self, dataset):
        input_query = dataset.question
        answers = [answer[0] if len(answer) == 1 else random.choice(answer) for answer in dataset.golden_answers]
        retrieval_results = self.retriever.batch_search(input_query)
        data_ls = []
        N = len(answers)
        for i in tqdm(range(N)):
            q, res_list, answer = dataset.question[i], retrieval_results[i], answers[i]
            docs = []
            scores = []
            for res in res_list:
                input_prompt = self.prompt_template.get_string(question=q, retrieval_result=[res])
                score = self.calculate_prob(input_prompt, answer)
                scores.append(score)
                docs.append(res["contents"])
            scores = torch.softmax(torch.tensor(scores), dim=-1).tolist()
            data_ls.append({"query": q, "pos": docs, "scores": scores})
        return data_ls

    def calculate_prob(self, prompt, answer):
        _, answer_probs = self.generator.cal_gen_probs(prompt, answer)
        return float(answer_probs.mean())


def main(
    dataset_name="nq",  # qa dataset
    split="test",  # split
    num=4000,  # number of query-document pairs
    gpu_id="0",
    output="lmsft.jsonl",  # output path
    topk=20,  # number of retrieved documents
):
    config_dict = {
        "save_note": "replug_lsr",
        "gpu_id": gpu_id,
        "dataset_name": dataset_name,
        "test_sample_num": num,
        "split": ["test", "dev"],
        "retrieval_topk": topk,
        "framework": "hf",
    }
    config = Config("my_config.yaml", config_dict)
    all_split = get_dataset(config)
    test_data = all_split[split]
    lm_prob_calculator = LMProbCalculator(config)
    data_ls = lm_prob_calculator.run(test_data)
    with open(output, "w") as f:
        for data in data_ls:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")


if __name__ == "__main__":
    fire.Fire(main)



================================================
FILE: examples/quick_start/demo_en.py
================================================
import streamlit as st
from flashrag.config import Config
from flashrag.utils import get_retriever, get_generator
from flashrag.prompt import PromptTemplate


config_dict = {
    "save_note": "demo",
    "model2path": {"e5": "intfloat/e5-base-v2", "llama3-8B-instruct": "meta-llama/Meta-Llama-3-8B-Instruct"},
    "retrieval_method": "e5",
    "generator_model": "llama3-8B-instruct",
    "corpus_path": "indexes/general_knowledge.jsonl",
    "index_path": "indexes/e5_Flat.index",
}


@st.cache_resource
def load_retriever(_config):
    return get_retriever(_config)


@st.cache_resource
def load_generator(_config):
    return get_generator(_config)

if __name__ == '__main__':
    custom_theme = {
        "primaryColor": "#ff6347",
        "backgroundColor": "#f0f0f0",
        "secondaryBackgroundColor": "#d3d3d3",
        "textColor": "#121212",
        "font": "sans serif",
    }
    st.set_page_config(page_title="FlashRAG Demo", page_icon="âš¡")


    st.sidebar.title("Configuration")
    temperature = st.sidebar.slider("Temperature:", 0.01, 1.0, 0.5)
    topk = st.sidebar.slider("Number of retrieved documents:", 1, 10, 5)
    max_new_tokens = st.sidebar.slider("Max generation tokens:", 1, 2048, 256)


    st.title("âš¡FlashRAG Demo")
    st.write("This demo retrieves documents and generates responses based on user input.")

    query = st.text_area("Enter your prompt:")

    config = Config("my_config.yaml", config_dict=config_dict)
    generator = load_generator(config)
    retriever = load_retriever(config)


    system_prompt_rag = (
        "You are a friendly AI Assistant."
        "Respond to the input as a friendly AI assistant, generating human-like text, and follow the instructions in the input if applicable."
        "\nThe following are provided references. You can use them for answering question.\n\n{reference}"
    )
    system_prompt_no_rag = (
        "You are a friendly AI Assistant."
        "Respond to the input as a friendly AI assistant, generating human-like text, and follow the instructions in the input if applicable.\n"
    )
    base_user_prompt = "{question}"

    prompt_template_rag = PromptTemplate(config, system_prompt=system_prompt_rag, user_prompt=base_user_prompt)
    prompt_template_no_rag = PromptTemplate(config, system_prompt=system_prompt_no_rag, user_prompt=base_user_prompt)


    if st.button("Generate Responses"):
        with st.spinner("Retrieving and Generating..."):
            retrieved_docs = retriever.search(query, num=topk)

            st.subheader("References", divider="gray")
            for i, doc in enumerate(retrieved_docs):
                doc_title = doc.get("title", "No Title")
                doc_text = "\n".join(doc["contents"].split("\n")[1:])
                expander = st.expander(f"**[{i+1}]: {doc_title}**", expanded=False)
                with expander:
                    st.markdown(doc_text, unsafe_allow_html=True)

            st.subheader("Generated Responses:", divider="gray")

            input_prompt_with_rag = prompt_template_rag.get_string(question=query, retrieval_result=retrieved_docs)
            response_with_rag = generator.generate(
                input_prompt_with_rag, temperature=temperature, max_new_tokens=max_new_tokens
            )[0]
            st.subheader("Response with RAG:")
            st.write(response_with_rag)
            input_prompt_without_rag = prompt_template_no_rag.get_string(question=query)
            response_without_rag = generator.generate(
                input_prompt_without_rag, temperature=temperature, max_new_tokens=max_new_tokens
            )[0]
            st.subheader("Response without RAG:")
            st.markdown(response_without_rag)



================================================
FILE: examples/quick_start/simple_pipeline.py
================================================
import argparse
from flashrag.config import Config
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline
from flashrag.prompt import PromptTemplate

parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str)
parser.add_argument("--retriever_path", type=str)
args = parser.parse_args()

config_dict = {
    "data_dir": "dataset/",
    "index_path": "indexes/e5_Flat.index",
    "corpus_path": "indexes/general_knowledge.jsonl",
    "model2path": {"e5": args.retriever_path, "llama3-8B-instruct": args.model_path},
    "generator_model": "llama3-8B-instruct",
    "retrieval_method": "e5",
    "metrics": ["em", "f1", "acc"],
    "retrieval_topk": 1,
    "save_intermediate_data": True,
}

config = Config(config_dict=config_dict)

all_split = get_dataset(config)
test_data = all_split["test"]
prompt_templete = PromptTemplate(
    config,
    system_prompt="Answer the question based on the given document. \
                    Only give me the answer and do not output any other words. \
                    \nThe following are given documents.\n\n{reference}",
    user_prompt="Question: {question}\nAnswer:",
)


pipeline = SequentialPipeline(config, prompt_template=prompt_templete)


output_dataset = pipeline.run(test_data, do_eval=True)
print("---generation output---")
print(output_dataset.pred)



================================================
FILE: examples/quick_start/dataset/nq/test.jsonl
================================================
{"id": "test_0", "question": "who got the first nobel prize in physics", "golden_answers": ["Wilhelm Conrad R\u00f6ntgen"]}
{"id": "test_1", "question": "when is the next deadpool movie being released", "golden_answers": ["May 18, 2018"]}
{"id": "test_2", "question": "which mode is used for short wave broadcast service", "golden_answers": ["Olivia", "MFSK"]}
{"id": "test_3", "question": "the south west wind blows across nigeria between", "golden_answers": ["till September"]}
{"id": "test_4", "question": "what does hp mean in war and order", "golden_answers": ["hit points or health points"]}
{"id": "test_5", "question": "who wrote the first declaration of human rights", "golden_answers": ["Cyrus"]}
{"id": "test_6", "question": "who is the owner of reading football club", "golden_answers": ["Xiu Li Dai", "Dai Xiuli", "Dai Yongge", "Yongge Dai"]}
{"id": "test_7", "question": "when is the next scandal episode coming out", "golden_answers": ["February\u00a01,\u00a02018"]}
{"id": "test_8", "question": "when is the last time the philadelphia won the superbowl", "golden_answers": ["Super Bowl LII,", "2017"]}
{"id": "test_9", "question": "who was the first lady nominated member of the rajya sabha", "golden_answers": ["Mary Kom"]}
{"id": "test_10", "question": "what is the most current adobe flash player version", "golden_answers": ["28.0.0.137"]}
{"id": "test_11", "question": "swan lake the sleeping beauty and the nutcracker are three famous ballets by", "golden_answers": ["Pyotr Ilyich Tchaikovsky"]}
{"id": "test_12", "question": "how many episodes are there in dragon ball z", "golden_answers": ["291 episodes", "291"]}
{"id": "test_13", "question": "cast of law & order special victim unit", "golden_answers": ["Kelli Giddish", "Richard Belzer", "Stephanie March", "Diane Neal", "Ice-T", "Danny Pino", "Dann Florek", "Tamara Tunie", "Michaela McManus", "Mariska Hargitay", "Adam Beach", "B. D. Wong", "Christopher Meloni", "Ra\u00fal Esparza", "Michelle Hurd", "Peter Scanavino"]}
{"id": "test_14", "question": "who designed the garden city of new earswick", "golden_answers": ["planner Raymond Unwin", "architect Barry Parker", "Raymond Unwin"]}
{"id": "test_15", "question": "what is the first step in the evolution of the eye", "golden_answers": ["photoreceptor proteins that sense light", "eyespots"]}
{"id": "test_16", "question": "where is the tv show the curse of oak island filmed", "golden_answers": ["Oak Island"]}


================================================
FILE: examples/run_mm/build_index.sh
================================================
CUDA_VISIBLE_DEVICES=0 python -m flashrag.retriever.index_builder \
    --retrieval_method openai-clip-vit-large-patch14-336 \
    --model_path openai/clip-vit-large-patch14-336 \
    --corpus_path datasets/mmqa/train.parquet \
    --save_dir indexes/mmqa \
    --max_length 512 \
    --batch_size 512 \
    --faiss_type Flat \
    --index_modal all


# CUDA_VISIBLE_DEVICES=0 python -m flashrag.retriever.index_builder \
#     --retrieval_method bm25 \
#     --corpus_path datasets/mmqa/train.parquet \
#     --save_dir indexes/mmqa \
#     --max_length 512 \
#     --batch_size 512 \
#     --bm25_backend bm25s



================================================
FILE: examples/run_mm/my_config.yaml
================================================
# ----Global Paths----
# Paths to retrieval models
model2path:
  e5: intfloat/e5-base-v2
  bge: BAAI/bge-base-en-v1.5
  bge-zh: BAAI/bge-large-zh-v1.5
  jina: jinaai/jina-embeddings-v2-base-en
  contriever: facebook/contriever-msmarco
  llama2-13B: meta-llama/Llama-2-13b-hf
  llama2-13B-chat: meta-llama/Llama-2-13b-chat-hf
  llama2-7B: meta-llama/Llama-2-7b-hf
  llama2-7B-chat: meta-llama/Llama-2-7b-chat-hf
  selfrag-llama2-7B: selfrag/selfrag-llama2-7b
  llama3-8B-instruct: meta-llama/Llama-2-8b-instruct-hf
  phi-3: microsoft/phi-3-mini-4k-instruct
  qwen-14B: Qwen/Qwen1.5-14B-Chat
  qwen2-7B-instruct: Qwen/Qwen2-7B-Instruct
  gemma-7B: google/gemma-7b
  baichuan2-7B-chat: baichuan-inc/Baichuan2-7B-Chat
  proposition: google/flan-t5-large
  mistral-7B-instruct: mistralai/Mistral-7B-Instruct-v0.3
  bge-m3: BAAI/bge-m3
  glm-4-9b-chat: THUDM/glm-4-9b-chat
  qwen2.5-7B-instruct: Qwen/Qwen2.5-7B-Instruct
  llama3.1-8B-instruct: meta-llama/Llama-3.1-8B-Instruct
  qwen2-vl-2B: Qwen/Qwen2-VL-2B-Instruct
  qwen2-vl-7B: Qwen/Qwen2-VL-7B-Instruct
  internvl2-2B: OpenGVLab/InternVL2-2B
  internvl2-8B: OpenGVLab/InternVL2-8B
  internvl2.5-8B: OpenGVLab/InternVL2.5-8B
  llava-7B: liuhaotian/llava-v1.6-mistral-7b
  llava-7B-onevision-ov: liuhaotian/llava-onevision-7b-ov
  openai-clip: openai/clip-vit-large-patch14
  openai-clip-336: openai/clip-vit-large-patch14-336
  chinese-clip: OFA-Sys/chinese-clip-vit-large-patch14
  jina-clip-v2: jinaai/jina-clip-v2

# Pooling methods for each embedding model
model2pooling:
  default: "pooler"
  e5: "mean"
  bge: "cls"
  bge-zh: "cls"
  contriever: "mean"
  bge-m3: "cls"

# Indexes path for retrieval models
method2index:
  e5: ~
  bm25: ~
  contriever: ~
  bge: ~
  bge-m3: ~

# ----Environment Settings----
gpu_id: "1,2"
dataset_name: "nq"
split: ["dev",'test']

# Sampling configurations for testing
test_sample_num: 1000
random_sample: False
save_intermediate_data: True
# Seed for reproducibility
seed: 2024

# Directory paths for data and outputs
data_dir: "datasets/"
#save_dir: "/data00/jiajie_jin/test_project/output"
save_dir: "result/"

# ----Retrieval Settings----
retrieval_method: "e5" # name or path of the retrieval model
index_path: ~ # Set automatically if not provided
corpus_path: ~
retrieval_pooling_method: ~

retrieval_topk: 5
retrieval_batch_size: 256
retrieval_use_fp16: True
retrieval_query_max_length: 128
save_retrieval_cache: False
use_retrieval_cache: False
retrieval_cache_path: ~

use_reranker: False
rerank_model_name: e5
rerank_model_path: ~
rerank_pooling_method: ~
rerank_use_fp16: True
rerank_topk: 5
rerank_max_length: 512
rerank_batch_size: 256

# ----Generator Settings----
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "llama3-8B-instruct"  # name or path of the generator
generator_max_input_len: 4096
generator_batch_size: 4
generation_params:
  do_sample: False
  max_tokens: 200
  temperature: 0.1
  top_p: 1.0
vllm_gpu_memory_utilization: 0.95

# ----Evaluation Settings----
# Metrics to evaluate the result
metrics: ['em','f1','acc','precision','recall','input_tokens'] 
# Specify setting for metric, will be called within certain metrics
metric_setting: 
  retrieval_recall_topk: 5
  tokenizer_name: 'gpt-4'
save_metric_score: True #ã€€whether to save the metric score into txt file




================================================
FILE: examples/run_mm/rum_mm_exp.py
================================================
from flashrag.config import Config
from flashrag.utils import get_dataset
import argparse


def mathvista(args):
    clip_num = args.clip_num
    bm25_num = args.bm25_num
    mode = args.mode
    model_name = args.model_name
    config_dict = {
        "gpu_id": args.gpu_id,
        'dataset_name': 'mathvista',
        'test_sample_num': 10,
        'save_dir': 'result',
        'save_note': f'{model_name}-{mode}',
        'generator_model': model_name,
        'generation_params':{'max_new_tokens':256},
        'generator_max_input_len': 8192,
        'generator_batch_size': 1,
        'data_dir': "datasets",
        "use_multi_retriever": True,
        "multi_retriever_setting": {
            "merge_method": "concat",
            "retriever_list": [
                {
                    "retrieval_method": "bm25",
                    "corpus_path": "datasets/mathvista/train.parquet",
                    "index_path": "indexes/mathvista/bm25",
                    "retrieval_topk": bm25_num,
                    "bm25_backend": "bm25s",
                },
                {
                    "retrieval_method": "openai-clip-336",
                    "corpus_path": "datasets/mathvista/train.parquet",
                    "multimodal_index_path_dict": {
                        "image": "indexes/mathvista/openai-clip-vit-large-patch14-336_Flat_image.index",
                        "text": "indexes/mathvista/openai-clip-vit-large-patch14-336_Flat_text.index",
                    },
                    "retrieval_topk": clip_num,
                },
            ],
        },
        'metrics': ['acc', 'f1', 'em']
    }
    config = Config("./my_config.yaml", config_dict=config_dict)
    dataset = get_dataset(config)['test']

    from flashrag.pipeline import MMSequentialPipeline
    from flashrag.prompt import  MathVistaPromptTemplate

    if mode == 'no-ret':
        base_prompt_template = MathVistaPromptTemplate(config)
        pipeline = MMSequentialPipeline(config, prompt_template=base_prompt_template)
        dataset = pipeline.naive_run(dataset)
    else:
        base_prompt_template = MathVistaPromptTemplate(config)
        pipeline = MMSequentialPipeline(config, prompt_template=base_prompt_template)
        # dataset = pipeline.run(dataset, perform_modality_dict={'text': ['text']})
        dataset = pipeline.run(dataset)


def gaokao_mm(args):
    clip_num = args.clip_num
    bm25_num = args.bm25_num
    mode = args.mode
    model_name = args.model_name
    config_dict = {
        "gpu_id": args.gpu_id,
        'dataset_name': 'gaokao_mm',
        #'test_sample_num': 5,
        'save_dir': 'result',
        'save_note': f'{model_name}-{mode}',
        'generator_model': model_name,
        'generation_params':{'max_new_tokens':128},
        'generator_max_input_len': 4096,
        'generator_batch_size': 1,
        'data_dir': "datasets",
        "use_multi_retriever": True,
        "multi_retriever_setting": {
            "merge_method": "concat",
            "retriever_list": [
                {
                    "retrieval_method": "bm25",
                    "corpus_path": "datasets/gaokao_mm/train.parquet",
                    "index_path": "indexes/gaokao_mm/bm25/bm25",
                    "retrieval_topk": bm25_num,
                    "bm25_backend": "pyserini",
                },
                {
                    "retrieval_method": "chinese-clip",
                    "corpus_path": "datasets/gaokao_mm/train.parquet",
                    "multimodal_index_path_dict": {
                        "image": "indexes/gaokao_mm/chinese-clip-vit-large-patch14_Flat_image.index",
                        "text": "indexes/gaokao_mm/chinese-clip-vit-large-patch14_Flat_text.index",
                    },
                    "retrieval_topk": clip_num,
                },
            ],
        },
        'metrics': ['gaokao_acc']
    }
    config = Config("my_config.yaml", config_dict=config_dict)
    dataset = get_dataset(config)['test']

    from flashrag.pipeline import MMSequentialPipeline
    from flashrag.prompt import GAOKAOMMPromptTemplate
    from flashrag.utils import gaokaomm_pred_parse
    if mode == 'no-ret':
        zero_shot_prompt_template =  GAOKAOMMPromptTemplate(config, user_prompt="è¯·ä½ åšä¸€é“{subject}é€‰æ‹©é¢˜\nè¯·ä½ ç»“åˆæ–‡å­—å’Œå›¾ç‰‡ä¸€æ­¥ä¸€æ­¥æ€è€ƒ,å¹¶å°†æ€è€ƒè¿‡ç¨‹å†™åœ¨ã€è§£æžã€‘å’Œ<eoe>ä¹‹é—´ã€‚{instruction}\nä¾‹å¦‚ï¼š{example}\nè¯·ä½ ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ä½œç­”ã€‚\né¢˜ç›®å¦‚ä¸‹ï¼š{question}")
        pipeline = MMSequentialPipeline(config, prompt_template=zero_shot_prompt_template)
        dataset = pipeline.naive_run(dataset, pred_process_func=gaokaomm_pred_parse)
    else:
        # base_prompt_template = NewPromptTemplate(config)
        base_prompt_template = GAOKAOMMPromptTemplate(config)
        pipeline = MMSequentialPipeline(config, prompt_template=base_prompt_template)
        # dataset = pipeline.run(dataset, pred_process_func=gaokaomm_pred_parse)
        dataset = pipeline.run(dataset, pred_process_func=gaokaomm_pred_parse,perform_modality_dict={'text': ['text']})
        
    
def mmqa(args):
    clip_num = args.clip_num
    bm25_num = args.bm25_num
    mode = args.mode
    model_name = args.model_name
    config_dict = {
        "gpu_id": args.gpu_id,
        'dataset_name': 'mmqa',
        #'test_sample_num': 5,
        'save_dir': 'result',
        'save_note': f'{model_name}-{mode}',
        'generator_model': model_name,
        'generation_params':{'max_new_tokens':128},
        'generator_max_input_len': 8192,
        'generator_batch_size': 1,
        'data_dir': "datasets",
        "use_multi_retriever": True,
        "multi_retriever_setting": {
            "merge_method": "concat",
            "retriever_list": [
                {
                    "retrieval_method": "bm25",
                    "corpus_path": "datasets/mmqa/train.parquet",
                    "index_path": "indexes/mmqa/bm25",
                    "retrieval_topk": bm25_num,
                    "bm25_backend": "pyserini",
                },
                {
                    "retrieval_method": "openai-clip",
                    "corpus_path": "datasets/mmqa/train.parquet",
                    "multimodal_index_path_dict": {
                        "image": "indexes/mmqa/openai-clip-vit-large-patch14_Flat_image.index",
                        "text": "indexes/mmqa/openai-clip-vit-large-patch14_Flat_text.index",
                    },
                    "retrieval_topk": clip_num,
                },
            ],
        },
        'metrics': ['acc', 'f1', 'em']
    }
    config = Config("my_config.yaml", config_dict=config_dict)
    dataset = get_dataset(config)['dev']

    from flashrag.pipeline import MMSequentialPipeline
    from flashrag.prompt import  MMPromptTemplate
    from flashrag.utils import gaokaomm_pred_parse

    if mode == 'no-ret':
        base_prompt_template = MMPromptTemplate(config)
        #zero_shot_prompt_template =  MMPromptTemplate(config, user_prompt="Answer the following question. Only give me the final answer.\nQuestion: {question}\nAnswer: ")
        pipeline = MMSequentialPipeline(config, prompt_template=base_prompt_template)
        dataset = pipeline.naive_run(dataset)
    else:
        base_prompt_template = MMPromptTemplate(config)
        pipeline = MMSequentialPipeline(config, prompt_template=base_prompt_template)
        # dataset = pipeline.run(dataset, perform_modality_dict={'text': ['text']})
        dataset = pipeline.run(dataset)
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Running mm exp")
    parser.add_argument("--model_name", type=str)
    parser.add_argument("--dataset_name", type=str,default='mmqa')
    parser.add_argument("--gpu_id", type=str,default='0,1')
    parser.add_argument("--mode", type=str,default="no-ret")
    parser.add_argument("--clip_num", type=int,default=1)
    parser.add_argument("--bm25_num", type=int,default=1)

    func_dict = {
        "mathvista":mathvista,
        "gaokao_mm":gaokao_mm,
        "mmqa":mmqa
    }
    args = parser.parse_args()

    func = func_dict[args.dataset_name]
    func(args)



================================================
FILE: flashrag/__init__.py
================================================
[Empty file]


================================================
FILE: flashrag/version.py
================================================
__version__ = "0.3.0dev0"



================================================
FILE: flashrag/config/__init__.py
================================================
from flashrag.config.config import Config




================================================
FILE: flashrag/config/basic_config.yaml
================================================
# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to various models
model2path:
  e5: "intfloat/e5-base-v2"
  bge: "BAAI/bge-base-en-v1.5"
  contriever: "facebook/contriever"
  llama2-7B-chat: "meta-llama/Llama-2-7b-chat-hf"
  llama2-7B: "meta-llama/Llama-2-7b-hf"
  llama2-13B: "meta-llama/Llama-2-13b-hf"
  llama2-13B-chat: "meta-llama/Llama-2-13b-chat-hf"

# Pooling methods for each embedding model
model2pooling:
  e5: "mean"
  bge: "cls"
  contriever: "mean"
  jina: "mean"
  dpr: "pooler"

# Indexes path for retrieval models
method2index:
  e5: ~
  bm25: ~
  contriever: ~
  clip:
    "text": "path/to/text_index"
    "image": "path/to/image_index"

# ------------------------------------------------Environment Settings------------------------------------------------#
# Directory paths for data and outputs
data_dir: "dataset/"
save_dir: "output/"

gpu_id: "0,1,2,3"
dataset_name: "nq" # name of the dataset in data_dir
split: ["test"] # dataset split to load (e.g. train,dev,test)

# Sampling configurations for testing
test_sample_num: ~ # number of samples to test (only work in dev/test split), if None, test all samples
random_sample: False # whether to randomly sample the test samples

# Seed for reproducibility
seed: 2024

# Whether save intermediate data
save_intermediate_data: True
save_note: "experiment"

# -------------------------------------------------Retrieval Settings------------------------------------------------#
# If set the name, the model path will be find in global paths
retrieval_method: "e5" # name or path of the retrieval model.
retrieval_model_path: ~ # path to the retrieval model
index_path: ~ # set automatically if not provided.
multimodal_index_path_dict: ~ # use for multimodal retreiver, example format: {'text': 'path/to/text_index' or None, 'image': 'path/to/image_index' or None}
faiss_gpu: False # whether use gpu to hold index
corpus_path: ~ # path to corpus in '.jsonl' format that store the documents

instruction: ~ # instruction for the retrieval model
retrieval_topk: 5 # number of retrieved documents
retrieval_batch_size: 256 # batch size for retrieval
retrieval_use_fp16: True # whether to use fp16 for retrieval model
retrieval_query_max_length: 128 # max length of the query
save_retrieval_cache: False # whether to save the retrieval cache
use_retrieval_cache: False # whether to use the retrieval cache
retrieval_cache_path: ~ # path to the retrieval cache
retrieval_pooling_method: ~ # set automatically if not provided
bm25_backend: bm25s # pyserini, bm25s
use_sentence_transformer: False
silent_retrieval: True # whether to silent the retrieval process

seismic_query_cut: 10 # parameters for seismic. See seismic paper for full details
seismic_heap_factor: 0.8 # parameters for seismic. See seismic paper for full details
# -------------------------------------------------Reranker Settings------------------------------------------------#
use_reranker: False # whether to use reranker
rerank_model_name: ~ # same as retrieval_method
rerank_model_path: ~ # path to reranker model, path will be automatically find in `model2path`
rerank_pooling_method: ~
rerank_topk: 5 # number of remain documents after reranking
rerank_max_length: 512
rerank_batch_size: 256 # batch size for reranker
rerank_use_fp16: True

# If you want to use multi retrievers, you can set the following parameters
use_multi_retriever: False # whether to use multi retrievers
multi_retriever_setting:
  merge_method: "concat" # support 'concat', 'rrf', 'rerank'
  topk: 5 # final remain documents, only used in 'rrf' and 'rerank' merge
  rerank_model_name: ~
  rerank_model_path: ~
  retriever_list:
    - retrieval_method: "e5"
      retrieval_topk: 5
      index_path: ~
      retrieval_model_path: ~
    - retrieval_method: "bm25"
      retrieval_topk: 5
      index_path: ~
      retrieval_model_path: ~

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: fschat # inference frame work of LLM, supporting: 'hf','vllm','fschat', 'openai'
generator_model: "llama3-8B-instruct" # name or path of the generator model
# setting for openai model, only valid in openai framework
openai_setting:
  api_key: ~
  base_url: ~

generator_model_path: ~
generator_max_input_len: 1024 # max length of the input
generator_batch_size: 4 # batch size for generation, invalid for vllm
generation_params:
  #do_sample: false
  max_tokens: 32
  #temperature: 1.0
  #top_p: 1.0
use_fid: False # whether to use FID, only valid in encoder-decoder model
gpu_memory_utilization: 0.85 # ratio of gpu's memory usage for generator

# -------------------------------------------------Evaluation Settings------------------------------------------------#
# Metrics to evaluate the result
metrics: ["em", "f1", "acc", "precision", "recall", "input_tokens"]

# Specify setting for metric, will be called within certain metrics
metric_setting:
  retrieval_recall_topk: 5
  tokenizer_name: "gpt-4"
save_metric_score: True #ã€€whether to save the metric score into txt file




================================================
FILE: flashrag/config/config.py
================================================
import re
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import yaml
import random
import datetime


class Config:
    def __init__(self, config_file_path=None, config_dict={}):

        self.yaml_loader = self._build_yaml_loader()
        self.file_config = self._load_file_config(config_file_path)
        self.variable_config = config_dict

        self.external_config = self._merge_external_config()

        self.internal_config = self._get_internal_config()

        self.final_config = self._get_final_config()

        self._check_final_config()
        self._set_additional_key()

        self._init_device()
        self._set_seed()
        if not self.final_config.get('disable_save', False):
            self._prepare_dir()

    def _build_yaml_loader(self):
        loader = yaml.FullLoader
        loader.add_implicit_resolver(
            "tag:yaml.org,2002:float",
            re.compile(
                """^(?:
             [-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
            |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
            |\\.[0-9_]+(?:[eE][-+][0-9]+)?
            |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
            |[-+]?\\.(?:inf|Inf|INF)
            |\\.(?:nan|NaN|NAN))$""",
                re.X,
            ),
            list("-+0123456789."),
        )
        return loader

    def _load_file_config(self, config_file_path: str):
        file_config = dict()
        if config_file_path:
            with open(config_file_path, "r", encoding="utf-8") as f:
                file_config.update(yaml.load(f.read(), Loader=self.yaml_loader))
        return file_config

    @staticmethod
    def _update_dict(old_dict: dict, new_dict: dict):
        # Update the original update method of the dictionary:
        # If there is the same key in `old_dict` and `new_dict`, and value is of type dict, update the key in dict

        same_keys = []
        for key, value in new_dict.items():
            if key in old_dict and isinstance(value, dict):
                same_keys.append(key)
        for key in same_keys:
            old_item = old_dict[key]
            new_item = new_dict[key]
            old_item.update(new_item)
            new_dict[key] = old_item

        old_dict.update(new_dict)
        return old_dict

    def _merge_external_config(self):
        external_config = dict()
        external_config = self._update_dict(external_config, self.file_config)
        external_config = self._update_dict(external_config, self.variable_config)

        return external_config

    def _get_internal_config(self):
        current_path = os.path.dirname(os.path.realpath(__file__))
        init_config_path = os.path.join(current_path, "basic_config.yaml")
        internal_config = self._load_file_config(init_config_path)

        return internal_config

    def _get_final_config(self):
        final_config = dict()
        final_config = self._update_dict(final_config, self.internal_config)
        final_config = self._update_dict(final_config, self.external_config)

        return final_config

    def _check_final_config(self):
        # check split
        split = self.final_config["split"]
        if split is None:
            split = ["train", "dev", "test"]
        if isinstance(split, str):
            split = [split]
        self.final_config["split"] = split

    def _init_device(self):
        gpu_id = self.final_config["gpu_id"]
        if gpu_id is not None:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        try:
            # import pynvml 
            # pynvml.nvmlInit()
            # gpu_num = pynvml.nvmlDeviceGetCount()
            import torch
            gpu_num = torch.cuda.device_count()
        except:
            gpu_num = 0
        self.final_config['gpu_num'] = gpu_num
        if gpu_num > 0:
            self.final_config["device"] = "cuda"
        else:
            self.final_config['device'] = 'cpu'

    def _set_additional_key(self):
        def set_pooling_method(method, model2pooling):
            for key, value in model2pooling.items():
                if key.lower() in method.lower():
                    return value
            return "mean"

        def set_retrieval_keys(model2path, model2pooling, method2index, config):
            retrieval_method = config["retrieval_method"]
            if config["index_path"] is None:
                try:
                    config["index_path"] = method2index[retrieval_method]
                except:
                    print("Index is empty!!")

            if config.get("retrieval_model_path") is None:
                config["retrieval_model_path"] = model2path.get(retrieval_method, retrieval_method)

            if config.get("retrieval_pooling_method") is None:
                config["retrieval_pooling_method"] = set_pooling_method(retrieval_method, model2pooling)

            rerank_model_name = config.get("rerank_model_name", None)
            if config.get("rerank_model_path", None) is None:
                if rerank_model_name is not None:
                    config["rerank_model_path"] = model2path.get(rerank_model_name, rerank_model_name)
            if config.get("rerank_pooling_method", None) is None:
                if rerank_model_name is not None:
                    config["rerank_pooling_method"] = set_pooling_method(rerank_model_name, model2pooling)
            return config

        # set dataset
        dataset_name = self.final_config["dataset_name"]
        data_dir = self.final_config["data_dir"]
        self.final_config["dataset_path"] = os.path.join(data_dir, dataset_name)

        # set retrieval-related keys
        model2path = self.final_config["model2path"]
        model2pooling = self.final_config["model2pooling"]
        method2index = self.final_config["method2index"]
        self.final_config = set_retrieval_keys(model2path, model2pooling, method2index, self.final_config)
        # set keys for multi retriever
        if "multi_retriever_setting" in self.final_config:
            multi_retriever_config = self.final_config["multi_retriever_setting"]
            retriever_config_list = multi_retriever_config.get("retriever_list", [])
            # set for reranker merge method
            assert multi_retriever_config['merge_method'] in ['concat', 'rrf', 'rerank', None]
            if multi_retriever_config['merge_method'] == 'rerank':
                rerank_model_name = multi_retriever_config.get("rerank_model_name", None)
                assert rerank_model_name is not None
                multi_retriever_config['rerank_max_length'] = multi_retriever_config.get("rerank_max_length", 512)
                multi_retriever_config['rerank_batch_size'] = multi_retriever_config.get("rerank_batch_size", 256)
                multi_retriever_config['rerank_use_fp16'] = multi_retriever_config.get("rerank_use_fp16", True)
                
                if multi_retriever_config.get("rerank_model_path", None) is None:
                    if rerank_model_name is not None:
                        multi_retriever_config["rerank_model_path"] = model2path.get(rerank_model_name, rerank_model_name)
                if multi_retriever_config.get("rerank_pooling_method", None) is None:
                    if rerank_model_name is not None:
                        multi_retriever_config["rerank_pooling_method"] = set_pooling_method(rerank_model_name, model2pooling)
            
            # set config for each retriever
            for retriever_config in retriever_config_list:
                if "instruction" not in retriever_config:
                    retriever_config["instruction"] = None
                if "bm25_backend" not in retriever_config:
                    retriever_config["bm25_backend"] = "bm25s"
                if "use_reranker" not in retriever_config:
                    retriever_config["use_reranker"] = False
                if "index_path" not in retriever_config:
                    retriever_config["index_path"] = None
                if "corpus_path" not in retriever_config:
                    retriever_config["corpus_path"] = None
                if "use_sentence_transformer" not in retriever_config:
                    retriever_config["use_sentence_transformer"] = False
                retriever_config = set_retrieval_keys(model2path, model2pooling, method2index, retriever_config)
                
                # set other necessary keys as base setting
                keys = [
                    "retrieval_use_fp16",
                    "retrieval_query_max_length",
                    "faiss_gpu",
                    "retrieval_topk",
                    "retrieval_batch_size",
                    "use_reranker",
                    "rerank_model_name",
                    "rerank_model_path",
                    "retrieval_cache_path",
                ]
                for key in keys:
                    if key not in retriever_config:
                        retriever_config[key] = self.final_config.get(key, None)
                retriever_config["save_retrieval_cache"] = False
                retriever_config["use_retrieval_cache"] = False
        
        # set model path
        generator_model = self.final_config["generator_model"]

        if self.final_config.get("generator_model_path") is None:
            self.final_config["generator_model_path"] = model2path.get(generator_model, generator_model)

        if "refiner_name" in self.final_config:
            refiner_model = self.final_config["refiner_name"]
            if "refiner_model_path" not in self.final_config or self.final_config["refiner_model_path"] is None:
                self.final_config["refiner_model_path"] = model2path.get(refiner_model, None)
        if "instruction" not in self.final_config:
            self.final_config["instruction"] = None

        # set model path in metric setting
        metric_setting = self.final_config["metric_setting"]
        metric_tokenizer_name = metric_setting.get("tokenizer_name", None)
        from flashrag.utils.constants import OPENAI_MODEL_DICT

        if metric_tokenizer_name not in OPENAI_MODEL_DICT:
            metric_tokenizer_name = model2path.get(metric_tokenizer_name, metric_tokenizer_name)
            metric_setting["tokenizer_name"] = metric_tokenizer_name
            self.final_config["metric_setting"] = metric_setting

    def _prepare_dir(self):
        save_note = self.final_config["save_note"]
        save_dir = self.final_config['save_dir']
        if not save_dir.endswith("/"):
            save_dir += "/"

        current_time = datetime.datetime.now()

        self.final_config["save_dir"] = os.path.join(
            save_dir,
            f"{self.final_config['dataset_name']}_{current_time.strftime('%Y_%m_%d_%H_%M')}_{save_note}",
        )
        os.makedirs(self.final_config["save_dir"], exist_ok=True)
        # save config parameters
        config_save_path = os.path.join(self.final_config["save_dir"], "config.yaml")
        with open(config_save_path, "w") as f:
            yaml.dump(self.final_config, f, indent=4, sort_keys=False)

    def _set_seed(self):
        import torch
        import numpy as np
        seed = self.final_config['seed']
        try:
            seed = int(seed)
        except:
            seed = 2025
        self.final_config['seed'] = seed
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

    def __setitem__(self, key, value):
        if not isinstance(key, str):
            raise TypeError("index must be a str.")
        self.final_config[key] = value

    def __getattr__(self, item):
        if "final_config" not in self.__dict__:
            raise AttributeError("'Config' object has no attribute 'final_config'")
        if item in self.final_config:
            return self.final_config[item]
        raise AttributeError(f"'Config' object has no attribute '{item}'")

    def __getitem__(self, item):
        return self.final_config.get(item)

    def __contains__(self, key):
        if not isinstance(key, str):
            raise TypeError("index must be a str.")
        return key in self.final_config

    def __repr__(self):
        return self.final_config.__str__()



================================================
FILE: flashrag/dataset/__init__.py
================================================
from flashrag.dataset.dataset import *



================================================
FILE: flashrag/dataset/dataset.py
================================================
import os
import json
import random
import warnings
import datasets
from typing import List, Dict, Any, Optional, Generator
import numpy as np

class Item:
    """A container class used to store and manipulate a sample within a dataset.
    Information related to this sample during training/inference will be stored in `self.output`.
    Each attribute of this class can be used like a dict key (also for key in `self.output`).
    """

    def __init__(self, item_dict: Dict[str, Any]) -> None:
        self.id: Optional[str] = item_dict.get("id", None)
        self.question: Optional[str] = item_dict.get("question", None)
        self.golden_answers: List[str] = item_dict.get("golden_answers", [])
        self.choices: List[str] = item_dict.get("choices", [])
        self.metadata: Dict[str, Any] = item_dict.get("metadata", {})
        self.output: Dict[str, Any] = item_dict.get("output", {})
        self.data: Dict[str, Any] = item_dict

    def update_output(self, key: str, value: Any) -> None:
        """Update the output dict and keep a key in self.output can be used as an attribute."""
        if key in ["id", "question", "golden_answers", "output", "choices"]:
            raise AttributeError(f"{key} should not be changed")
        else:
            self.output[key] = value

    def update_evaluation_score(self, metric_name: str, metric_score: float) -> None:
        """Update the evaluation score of this sample for a metric."""
        if "metric_score" not in self.output:
            self.output["metric_score"] = {}
        self.output["metric_score"][metric_name] = metric_score

    def __getattr__(self, attr_name: str) -> Any:
        predefined_attrs = ["id", "question", "golden_answers", "metadata", "output", "choices"]
        if attr_name in predefined_attrs:
            return super().__getattribute__(attr_name)
        else:
            output = self.output
            if attr_name in output:
                return output[attr_name]
            else:
                try:
                    return self.data[attr_name]
                except AttributeError:
                    raise AttributeError(f"Attribute `{attr_name}` not found")

    def __setattr__(self, attr_name: str, value: Any) -> None:
        predefined_attrs = ["id", "question", "golden_answers", "metadata", "output", "choices", 'data']
        if attr_name in predefined_attrs:
            super().__setattr__(attr_name, value)
        else:
            self.update_output(attr_name, value)

    def to_dict(self) -> Dict[str, Any]:
        """Convert all information within the data sample into a dict. Information generated
        during the inference will be saved into output field.
        """
        from flashrag.dataset.utils import convert_numpy, remove_images, clean_prompt_image

        
        output = remove_images(self.data)

        # clean base64 image
        if 'prompt' in self.output:
            self.output['prompt'] = clean_prompt_image(self.output['prompt'])
            
        output['output'] = remove_images(convert_numpy(self.output))
        if self.metadata:
            output["metadata"] = remove_images(self.metadata)

        return output

    def __str__(self) -> str:
        """Return a string representation of the item with its main attributes."""
        return json.dumps(self.to_dict(), indent=4, ensure_ascii=False)


class Dataset:
    """A container class used to store the whole dataset. Inside the class, each data sample will be stored
    in `Item` class. The properties of the dataset represent the list of attributes corresponding to each item in the dataset.
    """

    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        dataset_path: Optional[str] = None,
        data: Optional[List[Dict[str, Any]]] = None,
        sample_num: Optional[int] = None,
        random_sample: bool = False,
    ) -> None:
        if config is not None:
            self.config = config
            dataset_name = config['dataset_name'] if 'dataset_name' in config else 'defalut_dataset'
        else:
            self.config = None
            warnings.warn("dataset_name is not in config, set it as default.")
            dataset_name = "default_dataset"
        self.dataset_name = dataset_name
        self.dataset_path = dataset_path

        self.sample_num = sample_num
        self.random_sample = random_sample

        if data is None:
            self.data = self._load_data(self.dataset_name, self.dataset_path)
        else:
            print("Load data from provided data")
            if isinstance(data[0], dict):
                self.data = [Item(item_dict) for item_dict in data]
            else:
                assert isinstance(data[0], Item)
                self.data = data

    def _load_data(self, dataset_name: str, dataset_path: str) -> List[Item]:
        """Load data from the provided dataset_path or directly download the file(TODO)."""
        if not os.path.exists(dataset_path):
            # TODO: auto download: self._download(self.dataset_name, dataset_path)
            raise FileNotFoundError(f"Dataset file {dataset_path} not found.")

        data = []
        if dataset_path.endswith(".jsonl") or dataset_path.endswith(".json"):
            with open(dataset_path, "r", encoding="utf-8") as f:
                for line in f:
                    item_dict = json.loads(line)
                    item = Item(item_dict)
                    data.append(item)
        elif dataset_path.endswith('parquet'):
            hf_data = datasets.load_dataset('parquet', data_files=dataset_path, split="train")
            hf_data = hf_data.cast_column('image', datasets.Image())
            for item in hf_data:
                item = Item(item)
                data.append(item)
        else:
            raise NotImplementedError
        
        if self.sample_num is not None:
            self.sample_num = int(self.sample_num)
            if self.random_sample:
                print(f"Random sample {self.sample_num} items in test set.")
                data = random.sample(data, self.sample_num)
            else:
                data = data[: self.sample_num]

        return data

    def update_output(self, key: str, value_list: List[Any]) -> None:
        """Update the overall output field for each sample in the dataset."""
        assert len(self.data) == len(value_list)
        for item, value in zip(self.data, value_list):
            item.update_output(key, value)

    @property
    def question(self) -> List[Optional[str]]:
        return [item.question for item in self.data]

    @property
    def golden_answers(self) -> List[List[str]]:
        return [item.golden_answers for item in self.data]

    @property
    def id(self) -> List[Optional[str]]:
        return [item.id for item in self.data]

    @property
    def output(self) -> List[Dict[str, Any]]:
        return [item.output for item in self.data]

    def get_batch_data(self, attr_name: str, batch_size: int) -> Generator[List[Any], None, None]:
        """Get an attribute of dataset items in batch."""
        for i in range(0, len(self.data), batch_size):
            batch_items = self.data[i : i + batch_size]
            yield [item[attr_name] for item in batch_items]

    def __getattr__(self, attr_name: str) -> List[Any]:
        return [item.__getattr__(attr_name) for item in self.data]

    def get_attr_data(self, attr_name: str) -> List[Any]:
        """For the attributes constructed later (not implemented using property),
        obtain a list of this attribute in the entire dataset.
        """
        return [item[attr_name] for item in self.data]

    def __getitem__(self, index: int) -> Item:
        return self.data[index]

    def __len__(self) -> int:
        return len(self.data)

    def save(self, save_path: str) -> None:
        """Save the dataset into the original format."""

        save_data = [item.to_dict() for item in self.data]
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=4, ensure_ascii=False)


    def __str__(self) -> str:
        """Return a string representation of the dataset with a summary of items."""
        return f"Dataset '{self.dataset_name}' with {len(self)} items"



================================================
FILE: flashrag/dataset/utils.py
================================================
from typing import Dict, Any, Union
import numpy as np
from flashrag.dataset import Dataset


def convert_numpy(data: Any) -> Any:
    if isinstance(data, dict):
        return {key: convert_numpy(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_numpy(element) for element in data]
    elif isinstance(data, np.ndarray):
        return data.tolist()
    elif isinstance(data, (np.integer,)):
        return int(data)
    elif isinstance(data, (np.floating,)):
        return float(data)
    elif isinstance(data, (np.bool_)):
        return bool(data)
    elif isinstance(data, (np.str_)):
        return str(data)
    else:
        return data

def filter_dataset(dataset: Dataset, filter_func=None):
    if filter_func is None:
        return dataset
    data = dataset.data
    for item in data:
        if not filter_func(item):
            data.remove(item)
    return Dataset(config=dataset.config, data=data)


def split_dataset(dataset: Dataset, split_symbol: list):
    assert len(split_symbol) == len(dataset)

    data = dataset.data
    data_split = {symbol: [] for symbol in set(split_symbol)}
    for symbol in set(split_symbol):
        symbol_data = [x for x, x_symbol in zip(data, split_symbol) if x_symbol == symbol]
        data_split[symbol] = Dataset(config=dataset.config, data=symbol_data)

    return data_split


def merge_dataset(dataset_split: dict, split_symbol: list):
    assert len(split_symbol) == sum([len(data) for data in dataset_split.values()])
    dataset_split_iter = {symbol: iter(dataset.data) for symbol, dataset in dataset_split.items()}

    final_data = []
    for item_symbol in split_symbol:
        final_data.append(next(dataset_split_iter[item_symbol]))
    final_dataset = Dataset(config=list(dataset_split.values())[0].config, data=final_data)

    return final_dataset


def get_batch_dataset(dataset: Dataset, batch_size=16):
    data = dataset.data
    for idx in range(0, len(data), batch_size):
        batched_data = data[idx : idx + batch_size]
        batch_dataset = Dataset(config=dataset.config, data=batched_data)
        yield batch_dataset


def merge_batch_dataset(dataset_list: Dataset):
    dataset = dataset_list[0]
    total_data = []
    for batch_dataset in dataset_list:
        total_data.extend(batch_dataset.data)
    dataset = Dataset(config=dataset.config, data=total_data)
    return dataset
def remove_images(data: Any) -> Any:
    from PIL import Image
    from typing import Any
    if isinstance(data, dict):
        return {key: remove_images(value) 
                for key, value in data.items()
                if not isinstance(value, Image.Image)}
    elif isinstance(data, list):
        return [remove_images(element) 
                for element in data 
                if not isinstance(element, Image.Image)]
    elif isinstance(data, tuple):
        return tuple(remove_images(element) 
                     for element in data 
                     if not isinstance(element, Image.Image))
    elif isinstance(data, set):
        return {remove_images(element) 
                for element in data 
                if not isinstance(element, Image.Image)}
    else:
        return data


def clean_prompt_image(input):
    try:
        for message in input:
            if isinstance(message.get("content"), list):
                message["content"] = [item for item in message["content"] if item.get("type") != "image"]
        return input
    except:
        return input


================================================
FILE: flashrag/evaluator/__init__.py
================================================
from flashrag.evaluator.evaluator import *
from flashrag.evaluator.metrics import *



================================================
FILE: flashrag/evaluator/_bleu.py
================================================
# Source: https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/tokenizers/tokenizer_13a.py
# Copyright 2020 SacreBLEU Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
from functools import lru_cache


class BaseTokenizer:
    """A base dummy tokenizer to derive from."""

    def signature(self):
        """
        Returns a signature for the tokenizer.
        :return: signature string
        """
        return "none"

    def __call__(self, line):
        """
        Tokenizes an input line with the tokenizer.
        :param line: a segment to tokenize
        :return: the tokenized line
        """
        return line


class TokenizerRegexp(BaseTokenizer):
    def signature(self):
        return "re"

    def __init__(self):
        self._re = [
            # language-dependent part (assuming Western languages)
            (re.compile(r"([\{-\~\[-\` -\&\(-\+\:-\@\/])"), r" \1 "),
            # tokenize period and comma unless preceded by a digit
            (re.compile(r"([^0-9])([\.,])"), r"\1 \2 "),
            # tokenize period and comma unless followed by a digit
            (re.compile(r"([\.,])([^0-9])"), r" \1 \2"),
            # tokenize dash when preceded by a digit
            (re.compile(r"([0-9])(-)"), r"\1 \2 "),
            # one space only between words
            # NOTE: Doing this in Python (below) is faster
            # (re.compile(r'\s+'), r' '),
        ]

    @lru_cache(maxsize=2**16)
    def __call__(self, line):
        """Common post-processing tokenizer for `13a` and `zh` tokenizers.
        :param line: a segment to tokenize
        :return: the tokenized line
        """
        for _re, repl in self._re:
            line = _re.sub(repl, line)

        # no leading or trailing spaces, single space within words
        # return ' '.join(line.split())
        # This line is changed with regards to the original tokenizer (seen above) to return individual words
        return line.split()


class Tokenizer13a(BaseTokenizer):
    def signature(self):
        return "13a"

    def __init__(self):
        self._post_tokenizer = TokenizerRegexp()

    @lru_cache(maxsize=2**16)
    def __call__(self, line):
        """Tokenizes an input line using a relatively minimal tokenization
        that is however equivalent to mteval-v13a, used by WMT.
        :param line: a segment to tokenize
        :return: the tokenized line
        """

        # language-independent part:
        line = line.replace("<skipped>", "")
        line = line.replace("-\n", "")
        line = line.replace("\n", " ")

        if "&" in line:
            line = line.replace("&quot;", '"')
            line = line.replace("&amp;", "&")
            line = line.replace("&lt;", "<")
            line = line.replace("&gt;", ">")

        return self._post_tokenizer(f" {line} ")


# Copyright 2017 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Python implementation of BLEU and smooth-BLEU.

This module provides a Python implementation of BLEU and smooth-BLEU.
Smooth BLEU is computed following the method outlined in the paper:
Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic
evaluation metrics for machine translation. COLING 2004.
"""

import collections
import math


def _get_ngrams(segment, max_order):
    """Extracts all n-grams upto a given maximum order from an input segment.

    Args:
      segment: text segment from which n-grams will be extracted.
      max_order: maximum length in tokens of the n-grams returned by this
          methods.

    Returns:
      The Counter containing all n-grams upto max_order in segment
      with a count of how many times each n-gram occurred.
    """
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(0, len(segment) - order + 1):
            ngram = tuple(segment[i : i + order])
            ngram_counts[ngram] += 1
    return ngram_counts


def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    """Computes BLEU score of translated segments against one or more references.

    Args:
      reference_corpus: list of lists of references for each translation. Each
          reference should be tokenized into a list of tokens.
      translation_corpus: list of translations to score. Each translation
          should be tokenized into a list of tokens.
      max_order: Maximum n-gram order to use when computing BLEU score.
      smooth: Whether or not to apply Lin et al. 2004 smoothing.

    Returns:
      3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
      precisions and brevity penalty.
    """
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0
    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = (matches_by_order[i] + 1.0) / (possible_matches_by_order[i] + 1.0)
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = float(matches_by_order[i]) / possible_matches_by_order[i]
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1.0 / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = float(translation_length) / reference_length

    if ratio > 1.0:
        bp = 1.0
    else:
        bp = math.exp(1 - 1.0 / ratio)

    bleu = geo_mean * bp

    return (bleu, precisions, bp, ratio, translation_length, reference_length)



================================================
FILE: flashrag/evaluator/evaluator.py
================================================
import os
from flashrag.evaluator.metrics import BaseMetric


class Evaluator:
    """Evaluator is used to summarize the results of all metrics."""

    def __init__(self, config):
        self.config = config
        self.save_dir = config["save_dir"]

        self.save_metric_flag = config["save_metric_score"]
        self.save_data_flag = config["save_intermediate_data"]
        self.metrics = [metric.lower() for metric in self.config["metrics"]]

        self.avaliable_metrics = self._collect_metrics()

        self.metric_class = {}  
        for metric in self.metrics:
            if metric in self.avaliable_metrics:
                self.metric_class[metric] = self.avaliable_metrics[metric](self.config)
            else:
                print(f"{metric} has not been implemented!")
                raise NotImplementedError

    def _collect_metrics(self):
        """Collect all classes based on ```BaseMetric```."""

        def find_descendants(base_class, subclasses=None):
            if subclasses is None:
                subclasses = set()

            direct_subclasses = base_class.__subclasses__()
            for subclass in direct_subclasses:
                if subclass not in subclasses:
                    subclasses.add(subclass)
                    find_descendants(subclass, subclasses)
            return subclasses

        avaliable_metrics = {}
        for cls in find_descendants(BaseMetric):
            metric_name = cls.metric_name
            avaliable_metrics[metric_name] = cls
        return avaliable_metrics

    def evaluate(self, data):
        """Calculate all metric indicators and summarize them."""

        result_dict = {}
        for metric in self.metrics:
            try:
                metric_result, metric_scores = self.metric_class[metric].calculate_metric(data)
                result_dict.update(metric_result)

                for metric_score, item in zip(metric_scores, data):
                    item.update_evaluation_score(metric, metric_score)
            except Exception as e:
                print(f"Error in {metric}: {e}")
                continue

        if self.save_metric_flag:
            self.save_metric_score(result_dict)

        if self.save_data_flag:
            self.save_data(data)

        return result_dict

    def save_metric_score(self, result_dict, file_name="metric_score.txt"):
        save_path = os.path.join(self.save_dir, file_name)
        with open(save_path, "w", encoding="utf-8") as f:
            for k, v in result_dict.items():
                f.write(f"{k}: {v}\n")

    def save_data(self, data, file_name="intermediate_data.json"):
        """Save the evaluated data, including the raw data and the score of each data
        sample on each metric."""

        save_path = os.path.join(self.save_dir, file_name)

        data.save(save_path)



================================================
FILE: flashrag/evaluator/metrics.py
================================================
import re
import numpy as np
import warnings
from collections import Counter
from flashrag.evaluator.utils import normalize_answer


class BaseMetric:
    """`BaseMetric` serves as the base object of all metrics. Implemented metric should
    inherit this class.
    """

    metric_name = "base"

    def __init__(self, config):
        self.config = config
        self.dataset_name = config["dataset_name"]

    def calculate_metric(self, data):
        """Get the total score of this metric and score for each sample.

        Args:
            data object: it contains basic information and generated information.

        Returns:
            (metric_score: dict, metric_score_list: list)
            metric_score: such as ``{'em': 0.53}``.
            metric_score_list: score for each sample.

        """
        return {}, []

    def get_dataset_answer(self, data):
        if any(choice == [] for choice in data.choices):
            golden_answers_list = data.golden_answers
        else:
            # multi-choice dataset
            all_choices_list = data.choices
            golden_choice_idx_list = data.golden_answers
            golden_answers_list = [
                [choices[idx] for idx in idx_list]
                for choices, idx_list in zip(all_choices_list, golden_choice_idx_list)
            ]

        return golden_answers_list


class F1_Score(BaseMetric):
    """Token-level F1 score"""

    metric_name = "f1"

    def __init__(self, config):
        super().__init__(config)

    def token_level_scores(self, prediction: str, ground_truths: list):
        final_metric = {"f1": 0, "precision": 0, "recall": 0}
        if isinstance(ground_truths, str):
            ground_truths = [ground_truths]
        for ground_truth in ground_truths:
            normalized_prediction = normalize_answer(prediction)
            normalized_ground_truth = normalize_answer(ground_truth)

            if normalized_prediction in ["yes", "no", "noanswer"] and normalized_prediction != normalized_ground_truth:
                continue
            if (
                normalized_ground_truth in ["yes", "no", "noanswer"]
                and normalized_prediction != normalized_ground_truth
            ):
                continue
            prediction_tokens = normalized_prediction.split()
            ground_truth_tokens = normalized_ground_truth.split()
            common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
            num_same = sum(common.values())
            if num_same == 0:
                continue
            precision = 1.0 * num_same / len(prediction_tokens)
            recall = 1.0 * num_same / len(ground_truth_tokens)
            f1 = (2 * precision * recall) / (precision + recall)
            for k in ["f1", "precision", "recall"]:
                final_metric[k] = max(eval(k), final_metric[k])
        return final_metric

    def calculate_metric(self, data):
        pred_list = data.pred
        golden_answers_list = self.get_dataset_answer(data)

        metric_score_list = [
            self.token_level_scores(pred, golden_answers)["f1"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        f1 = sum(metric_score_list) / len(metric_score_list)
        return {"f1": f1}, metric_score_list


class Recall_Score(F1_Score):
    """Token-level Recall score"""

    metric_name = "recall"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        pred_list = data.pred
        golden_answers_list = self.get_dataset_answer(data)
        metric_score_list = [
            self.token_level_scores(pred, golden_answers)["recall"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        precision = sum(metric_score_list) / len(metric_score_list)
        return {"recall": precision}, metric_score_list


class Precision_Score(F1_Score):
    """Token-level Precision score"""

    metric_name = "precision"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        pred_list = data.pred
        golden_answers_list = self.get_dataset_answer(data)
        metric_score_list = [
            self.token_level_scores(pred, golden_answers)["precision"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        precision = sum(metric_score_list) / len(metric_score_list)
        return {"precision": precision}, metric_score_list


class ExactMatch(BaseMetric):
    r"""Exact match measure whether the predicted answer is completely consistent
    with the standard answer.

    """

    metric_name = "em"

    def __init__(self, config):
        super().__init__(config)
        self.is_regex = self.dataset_name == "curatedtrec"

    def calculate_em(self, prediction: str, golden_answers: list) -> float:
        if isinstance(golden_answers, str):
            golden_answers = [golden_answers]
        normalized_prediction = normalize_answer(prediction)
        score = 0.0
        for golden_answer in golden_answers:
            if self.is_regex:
                print("Consider answer as regex!")
                golden_answer = re.compile(golden_answer, re.IGNORECASE)
                match = re.fullmatch(golden_answer, normalized_prediction)
                if match is not None:
                    score = 1.0
                    break
            else:
                golden_answer = normalize_answer(golden_answer)
                if golden_answer == normalized_prediction:
                    score = 1.0
                    break
        return score

    def calculate_metric(self, data):
        pred_list = data.pred
        golden_answers_list = self.get_dataset_answer(data)

        metric_score_list = [
            self.calculate_em(pred, golden_answers) for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        em_score = sum(metric_score_list) / len(metric_score_list)

        return {"em": em_score}, metric_score_list


class Sub_ExactMatch(BaseMetric):
    r"""Sub-Exact match measure whether the predicted answer contains the standard answer."""

    metric_name = "acc"

    def __init__(self, config):
        super().__init__(config)
        self.is_regex = self.dataset_name == "curatedtrec"

    def calculate_sub_em(self, prediction: str, golden_answers: list) -> float:
        if isinstance(golden_answers, str):
            golden_answers = [golden_answers]
        normalized_prediction = normalize_answer(prediction)
        score = 0.0
        for golden_answer in golden_answers:
            if self.is_regex:
                print("Consider answer as regex!")
                golden_answer = re.compile(golden_answer, re.IGNORECASE)
                match = re.search(golden_answer, normalized_prediction)
                if match is not None:
                    score = 1.0
                    break
            else:
                golden_answer = normalize_answer(golden_answer)
                if golden_answer in normalized_prediction:
                    score = 1.0
                    break
        return score

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_sub_em(pred, golden_answers) for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        sub_em_score = sum(metric_score_list) / len(metric_score_list)

        return {"acc": sub_em_score}, metric_score_list


class Retrieval_Recall(BaseMetric):
    r"""The recall of the top-k retreived passages, we measure if any of the passage contain the answer string."""

    metric_name = "retrieval_recall"

    def __init__(self, config):
        super().__init__(config)
        self.topk = config["metric_setting"]["retrieval_recall_topk"]

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        retrieve_docs = data.retrieval_result
        recall_score_list = []
        for doc_list, golden_answers in zip(retrieve_docs, golden_answers_list):
            if len(doc_list) < self.topk:
                warnings.warn(f"Length of retrieved docs is smaller than topk ({self.topk})")
            doc_list = [doc["contents"] for doc in doc_list[: self.topk]]
            hit_list = []
            for doc in doc_list:
                for golden_answer in golden_answers:
                    if normalize_answer(golden_answer) in normalize_answer(doc):
                        hit_list.append(True)
                        break
                else:
                    hit_list.append(False)
            score = 1 if any(hit_list) else 0
            recall_score_list.append(score)
        recall_score = sum(recall_score_list) / len(recall_score_list)

        return {f"retrieval_recall_top{self.topk}": recall_score}, recall_score_list


class Retrieval_Precision(BaseMetric):
    r"""The precision of the top-k retreived passages, we measure if any of the passage contain the answer string."""

    metric_name = "retrieval_precision"

    def __init__(self, config):
        super().__init__(config)
        self.topk = config["metric_setting"]["retrieval_recall_topk"]

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        retrieve_docs = data.retrieval_result
        precision_score_list = []
        for doc_list, golden_answers in zip(retrieve_docs, golden_answers_list):
            if len(doc_list) < self.topk:
                warnings.warn(f"Length of retrieved docs is smaller than topk ({self.topk})")
            doc_list = [doc["contents"] for doc in doc_list[: self.topk]]
            hit_list = []
            for doc in doc_list:
                for golden_answer in golden_answers:
                    if normalize_answer(golden_answer) in normalize_answer(doc):
                        hit_list.append(True)
                        break
                else:
                    hit_list.append(False)
            score = sum(hit_list) / len(hit_list)
            precision_score_list.append(score)
        precision_score = sum(precision_score_list) / len(precision_score_list)

        return {f"retrieval_precision_top{self.topk}": precision_score}, precision_score_list


class Rouge_Score(BaseMetric):
    metric_name = "rouge_score"
    cached_scores = {}
    
    def __init__(self, config):
        super().__init__(config)
        from rouge import Rouge

        self.scorer = Rouge()

    def calculate_rouge(self, pred, golden_answers):
        if (pred, tuple(golden_answers)) in self.cached_scores:
            return self.cached_scores[(pred, tuple(golden_answers))]
        output = {}
        for answer in golden_answers:
            scores = self.scorer.get_scores(pred, answer)
            for key in ["rouge-1", "rouge-2", "rouge-l"]:
                if key not in output:
                    output[key] = []
                output[key].append(scores[0][key]["f"])
        for k, v in output.items():
            output[k] = max(v)

        self.cached_scores[(pred, tuple(golden_answers))] = output
        return output




class Rouge_1(Rouge_Score):
    metric_name = "rouge-1"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-1"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"rouge-1": score}, metric_score_list


class Rouge_2(Rouge_Score):
    metric_name = "rouge-2"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-2"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"rouge-2": score}, metric_score_list


class Rouge_L(Rouge_Score):
    metric_name = "rouge-l"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-l"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"rouge-l": score}, metric_score_list



class ZH_Rouge_Score(BaseMetric):
    metric_name = "zh_rouge_score"
    cached_scores = {}
    
    def __init__(self, config):
        super().__init__(config)
        from rouge_chinese import Rouge

        self.scorer = Rouge()

    def calculate_rouge(self, pred, golden_answers):
        import jieba
        if (pred, tuple(golden_answers)) in self.cached_scores:
            return self.cached_scores[(pred, tuple(golden_answers))]
        output = {}
        pred = ' '.join(jieba.cut(pred))
        for answer in golden_answers:
            answer = ' '.join(jieba.cut(answer))
            scores = self.scorer.get_scores(pred, answer)
            for key in ["rouge-1", "rouge-2", "rouge-l"]:
                if key not in output:
                    output[key] = []
                output[key].append(scores[0][key]["f"])
        for k, v in output.items():
            output[k] = max(v)

        self.cached_scores[(pred, tuple(golden_answers))] = output
        return output




class ZH_Rouge_1(ZH_Rouge_Score):
    metric_name = "zh_rouge-1"

    def __init__(self, config):
        super().__init__(config)
        

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-1"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"zh_rouge-1": score}, metric_score_list


class ZH_Rouge_2(ZH_Rouge_Score):
    metric_name = "zh_rouge-2"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-2"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"zh_rouge-2": score}, metric_score_list


class ZH_Rouge_L(ZH_Rouge_Score):
    metric_name = "zh_rouge-l"

    def __init__(self, config):
        super().__init__(config)

    def calculate_metric(self, data):
        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        metric_score_list = [
            self.calculate_rouge(pred, golden_answers)["rouge-l"]
            for pred, golden_answers in zip(pred_list, golden_answers_list)
        ]
        score = sum(metric_score_list) / len(metric_score_list)

        return {"zh_rouge-l": score}, metric_score_list




class BLEU(BaseMetric):
    metric_name = "bleu"

    def __init__(self, config):
        super().__init__(config)
        from ._bleu import Tokenizer13a

        self.tokenizer = Tokenizer13a()
        self.max_order = config["metric_setting"].get("bleu_max_order", 4)
        self.smooth = config["metric_setting"].get("bleu_smooth", False)

    def calculate_metric(self, data):
        from ._bleu import compute_bleu

        golden_answers_list = self.get_dataset_answer(data)
        pred_list = data.pred

        pred_list = [self.tokenizer(pred) for pred in pred_list]
        golden_answers_list = [
            [self.tokenizer(ans) for ans in golden_answers] for golden_answers in golden_answers_list
        ]
        score = compute_bleu(
            reference_corpus=golden_answers_list,
            translation_corpus=pred_list,
            max_order=self.max_order,
            smooth=self.smooth,
        )
        (total_bleu, precisions, bp, ratio, translation_length, reference_length) = score

        score_list = []
        for pred, golden_answers in zip(pred_list, golden_answers_list):
            pred = [pred]
            golden_answers = [golden_answers]
            score = compute_bleu(
                reference_corpus=golden_answers,
                translation_corpus=pred,
                max_order=self.max_order,
                smooth=self.smooth,
            )
            (bleu, precisions, bp, ratio, translation_length, reference_length) = score
            score_list.append(bleu)

        return {"bleu": total_bleu}, score_list


class LLMJudge(BaseMetric):
    metric_name = "llm_judge"
    JUDGE_PROMPT = """
    You will be given a user_question and system_answer couple.
    Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.
    Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.

    Provide your feedback as follows:

    Feedback:::
    Total rating: (your rating, as a float between 0 and 10)

    Now here are the question and answer.

    Question: {question}
    Answer: {answer}

    Feedback:::
    Total rating: """

    def __init__(self, config):
        super().__init__(config)
        if "llm_judge_setting" in config["metric_setting"]:
            llm_setting = config["metric_setting"]["llm_judge_setting"]
        else:
            assert False, "No available LLM settings!"
        # TODO: integrate generator class
        llm_name = llm_setting["model_name"]
        if "model_path" not in llm_setting:
            model_path = config["model2path"].get(llm_name, None)
        else:
            model_path = llm_setting["model_path"]
        if model_path is None:
            assert False, "None model path "

        from transformers import pipeline

        self.llm_pipeline = pipeline("text2text-generation", model=model_path, device=0)

    def extract_judge_score(answer: str, split_str: str = "Total rating:") -> int:
        try:
            if split_str in answer:
                rating = answer.split(split_str)[1]
            else:
                rating = answer
            digit_groups = [el.strip() for el in re.findall(r"\d+(?:\.\d+)?", rating)]
            return float(digit_groups[0])
        except Exception as e:
            print(e)
            return 0

    def calculate_metric(self, data):
        question_list = data.question
        pred_list = data.pred

        judge_input_prompt = [self.JUDGE_PROMPT.format(question=q, answer=a) for q, a in zip(question_list, pred_list)]
        judge_output = self.llm_pipeline(judge_input_prompt, max_new_tokens=100, batch_size=8)
        judge_output = [item["generated_text"] for item in judge_output]

        metric_score_list = [self.extract_judge_score(o) for o in judge_output]
        # rescale score
        metric_score_list = [score / 10 + 1 for score in metric_score_list]

        score = sum(metric_score_list) / len(metric_score_list)

        return {"llm_judge_score": score}, metric_score_list


class CountToken(BaseMetric):
    metric_name = "input_tokens"

    def __init__(self, config):
        super().__init__(config)
        tokenizer_name = config["metric_setting"].get("tokenizer_name", None)
        is_hf_tokenizer = True
        from flashrag.utils.constants import OPENAI_MODEL_DICT

        if tokenizer_name is None or tokenizer_name in OPENAI_MODEL_DICT:
            # use gpt4 tokenizer
            import tiktoken

            if tokenizer_name is None:
                tokenizer_name = "gpt-4"
            tokenizer = tiktoken.encoding_for_model(tokenizer_name)
            is_hf_tokenizer = False
        else:
            from transformers import AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        self.tokenizer = tokenizer
        self.is_hf_tokenizer = is_hf_tokenizer

    def calculate_metric(self, data):
        input_prompts = data.prompt
        if self.is_hf_tokenizer:
            token_counts = [len(self.tokenizer.tokenize(text)) for text in input_prompts]
        else:
            token_counts = [len(self.tokenizer.encode(text)) for text in input_prompts]
        avg_tokens = sum(token_counts) / len(token_counts)

        return {"avg_input_tokens": avg_tokens}, token_counts

class GAOKAOMM_Accuracy(BaseMetric):
    metric_name = 'gaokao_acc'
    def __init__(self, config):
        super().__init__(config)
    
    def calculate_metric(self, data):
        metric_dict = {}
        acc_list = []
        for item in data:
            golden_answers = item.golden_answers
            golden_answers = [i.lower() for i in golden_answers]
            golden_answer = "".join(golden_answers)
            pred = item.pred.lower()
            subject = item.subject

            question_type = item.question_type
            if question_type == 'single_choice':
                acc = 1.0 if pred == golden_answer else 0.0
            else:
                if pred == golden_answer:
                    acc = 1.0
                elif pred in golden_answer:
                    acc = 0.5
                else:
                    acc = 0.0
            acc_list.append(acc)
            if subject not in metric_dict:
                metric_dict[subject] = []
            metric_dict[subject].append(acc)
        for key, value in metric_dict.items():
            metric_dict[key] = np.mean(value)
        
        metric_dict['avg_score'] = np.mean(acc_list)
        return metric_dict, acc_list 
                



================================================
FILE: flashrag/evaluator/utils.py
================================================
import re
import string


def normalize_answer(s):
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))




================================================
FILE: flashrag/generator/__init__.py
================================================
from flashrag.generator.generator import *
from flashrag.generator.multimodal_generator import *
from flashrag.generator.openai_generator import *
from flashrag.generator.utils import *



================================================
FILE: flashrag/generator/fid.py
================================================
# Source: FiD official repo: https://github.com/facebookresearch/FiD
# This software is released under Creative Commons public licenses.

import torch
import torch.nn as nn
import transformers
import types
import torch.nn.functional as F
from torch.nn import CrossEntropyLoss
import numpy as np

class FiDT5(transformers.T5ForConditionalGeneration):
    def __init__(self, config):
        super().__init__(config)
        self.wrap_encoder()

    def forward_(self, **kwargs):
        if 'input_ids' in kwargs:
            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)
        if 'attention_mask' in kwargs:
            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)
        
        return super(FiDT5, self).forward(
            **kwargs
        )

    # input ids : bs, n, seq_len -> bs, n*seq_len
    # We need to resize as B x (N * L) instead of (B * N) x L here
    # because the T5 forward method uses the input tensors to infer
    # dimensions used in the decoder.
    # EncoderWrapper resizes the inputs as (B * N) x L.
    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        if input_ids != None:
            # (bs, n, seq_len) -> (bs, n*seq_len) 
            # inputs might have already be resized in the generate method
            if input_ids.dim() == 3:
                self.encoder.n_passages = input_ids.size(1)
                input_ids = input_ids.view(input_ids.size(0), -1)
        if attention_mask != None:
            attention_mask = attention_mask.view(attention_mask.size(0), -1)
        #print(input_ids.shape)
        return super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

    def generate(self, input_ids, attention_mask, **kwargs):
        # input ids - bs, n, seq_len -> bs, n*seq_len
        self.encoder.n_passages = input_ids.size(1)
        return super().generate(
            input_ids=input_ids.view(input_ids.size(0), -1),
            attention_mask=attention_mask.view(attention_mask.size(0), -1),
            **kwargs,
        )

    def wrap_encoder(self, use_checkpoint=False):
        """
        Wrap T5 encoder to obtain a Fusion-in-Decoder model.
        """
        self.encoder = EncoderWrapper(self.encoder)
    
 
    def unwrap_encoder(self):
        """
        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.
        """
        self.encoder = self.encoder.encoder
        block = []
        for mod in self.encoder.block:
            block.append(mod.module)
        block = nn.ModuleList(block)
        self.encoder.block = block
    
    def load_t5(self, state_dict):
        self.unwrap_encoder()
        self.load_state_dict(state_dict)
        self.wrap_encoder() 

    def set_checkpoint(self, use_checkpoint):
        """
        Enable or disable checkpointing in the encoder.
        See https://pytorch.org/docs/stable/checkpoint.html
        """
        for mod in self.encoder.encoder.block:
            mod.use_checkpoint = use_checkpoint

    def tie_weights(self):
        pass  

class CheckpointWrapper(torch.nn.Module):
    """
    Wrapper replacing None outputs by empty tensors, which allows the use of
    checkpointing.
    """
    def __init__(self, module, use_checkpoint=False):
        super().__init__()
        self.module = module
        self.use_checkpoint = use_checkpoint

    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):
        if self.use_checkpoint and self.training:
            kwargs = {k: v for k, v in kwargs.items() if v is not None}
            def custom_forward(*inputs):
                output = self.module(*inputs, **kwargs)
                empty = torch.tensor(
                    [],
                    dtype=torch.float,
                    device=output[0].device,
                    requires_grad=True)
                output = tuple(x if x is not None else empty for x in output)
                return output

            output = torch.utils.checkpoint.checkpoint(
                custom_forward,
                hidden_states,
                attention_mask,
                position_bias
            )
            output = tuple(x if x.size() != 0 else None for x in output)
        else:
            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)
        return output

def apply_checkpoint_wrapper(t5stack, use_checkpoint):
    """
    Wrap each block of the encoder to enable checkpointing.
    """
    block = []
    for mod in t5stack.block:
        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)
        block.append(wrapped_mod)
    block = nn.ModuleList(block)
    t5stack.block = block

    
class FiDBart(transformers.BartForConditionalGeneration):
    def __init__(self, config):
        super().__init__(config)
        self.wrap_encoder()        
    
    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        
        if input_ids != None:
            # (bs, n, seq_len) -> (bs, n*seq_len) 
            # inputs might have already be resized in the generate method
            if input_ids.dim() == 3:
                self.model.encoder.n_passages = input_ids.size(1)
                input_ids = input_ids.view(input_ids.size(0), -1)

        if attention_mask != None:
            attention_mask = attention_mask.view(attention_mask.size(0), -1) 
        
        return super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)

    def generate(self, input_ids, attention_mask, **kwargs):
        self.model.encoder.n_passages = input_ids.size(1)
        return super().generate(
            input_ids=input_ids.view(input_ids.size(0), -1),
            attention_mask=attention_mask.view(attention_mask.size(0),-1),
            **kwargs)

    def wrap_encoder(self):
        """
        Wrap T5 encoder to obtain a Fusion-in-Decoder model.
        """
        self.model.encoder = EncoderWrapper(self.model.encoder)
        
    def unwrap_encoder(self):
        """
        Unwrap Fusion-in-Decoder encoder, useful to load bart weights.
        """
        self.model.encoder = self.model.encoder.encoder
        block = []
        for mod in self.model.encoder.layers:
            block.append(mod)
        block = nn.ModuleList(block)
        self.model.encoder.layers = block
        
    def load_pretrained_model(self, state_dict):
        self.unwrap_encoder()
        self.load_state_dict(state_dict) 
        self.wrap_encoder() 
    def tie_weights(self):
        pass  

class EncoderWrapper(torch.nn.Module):
    def __init__(self, encoder,use_checkpoint=False):
        super().__init__()
        self.encoder = encoder
        
        try:
            self.main_input_name = encoder.main_input_name
        except:
            pass
        apply_checkpoint_wrapper(self.encoder, use_checkpoint)
    
    def forward(self, input_ids=None, attention_mask=None,**kwargs):
        bsz, total_length = input_ids.shape
        passage_length = total_length // self.n_passages
        # total_input
        input_ids = input_ids.view(bsz*self.n_passages, passage_length)
        attention_mask = attention_mask.view(bsz*self.n_passages, passage_length)
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, **kwargs) 
        outputs.last_hidden_state = outputs.last_hidden_state.view(bsz, self.n_passages*passage_length, -1)
        return outputs 



================================================
FILE: flashrag/generator/generator.py
================================================
from typing import List
from copy import deepcopy
import warnings
from tqdm import tqdm
from tqdm.auto import trange
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    T5ForConditionalGeneration,
    BartForConditionalGeneration,
    AutoConfig,
)
from flashrag.generator.utils import resolve_max_tokens
from flashrag.utils import get_device


class BaseGenerator:
    """`BaseGenerator` is a base object of Generator model."""

    def __init__(self, config):
        self._config = config
        self.update_config()

    @property
    def config(self):
        return self._config

    @config.setter
    def config(self, config_data):
        self._config = config_data
        self.update_config()
    
    def update_config(self):
        self.update_base_setting()
        self.update_additional_setting()
    def update_base_setting(self):
        self.model_name = self._config["generator_model"]
        self.model_path = self._config["generator_model_path"]

        self.max_input_len = self._config["generator_max_input_len"]
        self.batch_size = self._config["generator_batch_size"]
        self.device = self._config["device"]
        self.gpu_num = self._config['gpu_num']
        self.generation_params = self._config["generation_params"]
    
    def update_additional_setting(self):
        pass

    def generate(self, input_list: list) -> List[str]:
        """Get responses from the generater.

        Args:
            input_list: it contains input texts, each item represents a sample.

        Returns:
            list: contains generator's response of each input sample.
        """
        pass


class EncoderDecoderGenerator(BaseGenerator):
    """Class for encoder-decoder model"""

    def __init__(self, config):
        super().__init__(config)
        model_config = AutoConfig.from_pretrained(self.model_path)
        arch = model_config.architectures[0].lower()
        if "t5" in arch or 'fusionindecoder' in arch:
            if self.fid:
                from flashrag.generator.fid import FiDT5
                self.model = FiDT5.from_pretrained(self.model_path)
            else:
                self.model = T5ForConditionalGeneration.from_pretrained(self.model_path)
        else:
            if self.fid:
                assert False, "FiD only support T5"
            self.model = BartForConditionalGeneration.from_pretrained(self.model_path)
        self.model.cuda()
        self.model.eval()
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
    def update_additional_setting(self):
        self.fid = self._config["use_fid"]

    def encode_passages(self, batch_text_passages: List[List[str]]):
        import torch
        # need size: [batch_size, passage_num, passage_len]
        passage_ids, passage_masks = [], []
        for text_passages in batch_text_passages:
            p = self.tokenizer(
                text_passages,
                max_length=self.max_input_len,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            passage_ids.append(p['input_ids'][None])
            passage_masks.append(p['attention_mask'][None])

        passage_ids = torch.cat(passage_ids, dim=0)
        passage_masks = torch.cat(passage_masks, dim=0)
        return passage_ids, passage_masks.bool()

    def generate(self, input_list: List, batch_size=None, **params):
        if isinstance(input_list, str):
            input_list = [input_list]
        if batch_size is None:
            batch_size = self.batch_size

        generation_params = deepcopy(self.generation_params)
        generation_params.update(params)

        # deal stop params
        stop_sym = None
        if "stop" in generation_params:
            from flashrag.generator.stop_word_criteria import StopWordCriteria

            stop_sym = generation_params.pop("stop")
            stopping_criteria = [
                StopWordCriteria(
                    tokenizer=self.tokenizer,
                    prompts=input_list,
                    stop_words=stop_sym,
                )
            ]
            generation_params["stopping_criteria"] = stopping_criteria

        generation_params = resolve_max_tokens(params, generation_params, prioritize_new_tokens=True)

        responses = []
        for idx in trange(0, len(input_list), batch_size, desc="Generation process: "):
            batched_prompts = input_list[idx : idx + batch_size]
            if self.fid:
                # assume each input in input_list is a list, contains K string
                input_ids, attention_mask = self.encode_passages(batched_prompts)
                inputs = {
                    "input_ids": input_ids.to(self.device),
                    "attention_mask": attention_mask.to(self.device),
                }
            else:
                inputs = self.tokenizer(
                    batched_prompts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_input_len,
                ).to(self.device)

            # TODO: multi-gpu inference
            import torch
            with torch.inference_mode():
                if self.fid:
                    if 'max_new_tokens' in generation_params:
                        max_new_tokens = generation_params.pop('max_new_tokens')
                    else:
                        max_new_tokens = 32

                    outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.pad_token_id, decoder_start_token_id=self.tokenizer.pad_token_id)
                else:
                    outputs = self.model.generate(**inputs, **generation_params)
            outputs = self.tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )

            responses += outputs

        return responses


class VLLMGenerator(BaseGenerator):
    """Class for decoder-only generator, based on vllm."""

    def __init__(self, config):
        super().__init__(config)
        
        from vllm import LLM
        if self.use_lora:
            self.model = LLM(
                self.model_path,
                tensor_parallel_size = self.tensor_parallel_size,
                gpu_memory_utilization = self.gpu_memory_utilization,
                enable_lora = True,
                max_lora_rank = 64,
                max_logprobs = 32016,
                max_model_len = self.max_model_len
            )
        else:
            self.model = LLM(
                self.model_path,
                tensor_parallel_size = self.tensor_parallel_size,
                gpu_memory_utilization = self.gpu_memory_utilization,
                max_logprobs = 32016,
                max_model_len = self.max_model_len
            )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
    def update_additional_setting(self):
        if "gpu_memory_utilization" not in self._config:
            self.gpu_memory_utilization = 0.85
        else:
            self.gpu_memory_utilization = self._config["gpu_memory_utilization"]
        if self.gpu_num != 1 and self.gpu_num % 2 != 0:
            self.tensor_parallel_size = self.gpu_num - 1
        else:
            self.tensor_parallel_size = self.gpu_num

        self.lora_path = None if "generator_lora_path" not in self._config else self._config["generator_lora_path"]
        self.use_lora = False
        if self.lora_path is not None:
            self.use_lora = True
        self.max_model_len = self._config['generator_max_input_len']

    def generate(
        self,
        input_list: List[str],
        return_raw_output=False,
        return_scores=False,
        **params,
    ):
        from vllm import SamplingParams

        if isinstance(input_list, str):
            input_list = [input_list]

        generation_params = deepcopy(self.generation_params)
        generation_params.update(params)
        if "do_sample" in generation_params:
            do_sample_flag = generation_params.pop("do_sample")
            if not do_sample_flag:
                generation_params["temperature"] = 0
        generation_params["seed"] = self._config["seed"]

        # handle param conflict
        generation_params = resolve_max_tokens(params, generation_params, prioritize_new_tokens=False)

        # fix for llama3
        if "stop" in generation_params:
            generation_params["stop"].append("<|eot_id|>")
            generation_params["include_stop_str_in_output"] = True
        else:
            generation_params["stop"] = ["<|eot_id|>"]

        if return_scores:
            if "logprobs" not in generation_params:
                generation_params["logprobs"] = 100

        sampling_params = SamplingParams(**generation_params)

        if self.use_lora:
            from vllm.lora.request import LoRARequest

            outputs = self.model.generate(
                input_list,
                sampling_params,
                lora_request=LoRARequest("lora_module", 1, self.lora_path),
            )
        else:
            outputs = self.model.generate(input_list, sampling_params)

        if return_raw_output:
            base_output = outputs
        else:
            generated_texts = [
                [c.text for c in output.outputs] if len(output.outputs) > 1 else output.outputs[0].text
                for output in outputs
            ]
            base_output = generated_texts
        if return_scores:
            scores = []
            for output in outputs:
                for single_output in output.outputs:
                    if single_output.logprobs:
                        token_probs = [np.exp(list(score_dict.values())[0].logprob) 
                                      for score_dict in single_output.logprobs]
                        output_scores.append(token_probs)
                    else:
                        output_scores.append([])
                if len(output_scores) == 1:
                    scores.append(output_scores[0])
                else:
                    scores.append(output_scores)
            return base_output, scores
        else:
            return base_output


class HFCausalLMGenerator(BaseGenerator):
    """Class for decoder-only generator, based on hf."""

    def __init__(self, config, model=None):
        super().__init__(config)
        self.model, self.tokenizer = self._load_model(model=model)
        if self.lora_path is not None:
            self.use_lora = True
            self.model.load_adapter(self.lora_path)

    def update_additional_setting(self):
        self.lora_path = None if "generator_lora_path" not in self._config else self._config["generator_lora_path"]
        self.use_lora = False

    def _load_model(self, model=None):
        r"""Load model and tokenizer for generator."""
        if model is None:
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype="auto",
                device_map="auto",
                trust_remote_code=True,
            )
        else:
            model.to(self.device)
        model.eval()
        tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        if "qwen" not in self.model_name:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"

        return model, tokenizer

    def add_new_tokens(self, token_embedding_path, token_name_func=lambda idx: f"[ref{idx+1}]"):
        import torch
        del self.model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            trust_remote_code=True,
        )
        # get original embedding weight matrix
        embedding_layer = self.model.get_input_embeddings()
        embedding_weights = embedding_layer.weight
        original_vocab_size, embedding_dim = embedding_weights.shape

        new_tokens_weights = torch.load(token_embedding_path)
        new_tokens_length = new_tokens_weights.shape[0]

        # expand vocabulary
        new_tokens = [token_name_func(idx) for idx in range(new_tokens_length)]
        self.tokenizer.add_tokens(new_tokens)

        # create new embedding matrix
        new_vocab_size = original_vocab_size + new_tokens_length
        new_embedding_weights = torch.zeros(new_vocab_size, embedding_dim)

        # copy original embeddings to the new weights
        new_embedding_weights[:original_vocab_size, :] = embedding_weights

        # append virtual token embeddings to the new weights
        for token, embedding in zip(new_tokens, new_tokens_weights):
            token_id = self.tokenizer.convert_tokens_to_ids(token)
            new_embedding_weights[token_id] = embedding

        # update the embedding table
        # note: we should avoid using the function resize_token_embeddings() because this function will also change the lm_head of the model
        embedding_layer.weight.data = new_embedding_weights
        self.model.eval()
        self.model.cuda()

    def generate(
        self,
        input_list: List[str],
        batch_size=None,
        return_scores=False,
        return_dict=False,
        **params,
    ):
        """Generate batches one by one. The generated content needs to exclude input."""

        if isinstance(input_list, str):
            input_list = [input_list]
        if batch_size is None:
            batch_size = self.batch_size

        generation_params = deepcopy(self.generation_params)
        generation_params.update(params)

        # deal stop params
        stop_sym = None
        if "stop" in generation_params:
            from flashrag.generator.stop_word_criteria import StopWordCriteria

            stop_sym = generation_params.pop("stop")
            stopping_criteria = [
                StopWordCriteria(
                    tokenizer=self.tokenizer,
                    prompts=input_list,
                    stop_words=stop_sym,
                )
            ]
            generation_params["stopping_criteria"] = stopping_criteria

        generation_params = resolve_max_tokens(params, generation_params, prioritize_new_tokens=True)

        # set eos token for llama
        if "llama" in self.model_name.lower():
            extra_eos_tokens = [
                self.tokenizer.eos_token_id,
                self.tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            ]
            if "eos_token_id" in generation_params:
                generation_params["eos_token_id"].extend(extra_eos_tokens)
            else:
                generation_params["eos_token_id"] = extra_eos_tokens

        responses = []
        scores = []
        generated_token_ids = []
        generated_token_logits = []

        import torch
        for idx in trange(0, len(input_list), batch_size, desc="Generation process: "):
            with torch.inference_mode():
                torch.cuda.empty_cache()
                batched_prompts = input_list[idx : idx + batch_size]
                inputs = self.tokenizer(
                    batched_prompts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_input_len,
                ).to(self.model.device)
                outputs = self.model.generate(
                    **inputs,
                    output_scores=True,
                    return_dict_in_generate=True,
                    **generation_params,
                )

                generated_ids = outputs.sequences
                logits = torch.stack(outputs.scores, dim=1).softmax(-1)
                generated_ids = generated_ids[:, inputs["input_ids"].shape[-1] :]
                gen_score = torch.gather(logits, 2, generated_ids[:, :, None]).squeeze(-1).cpu().tolist()
                scores.extend(gen_score)

            # get additinoal info
            if return_dict:
                batch_generated_token_ids = generated_ids.detach().cpu()
                batch_generated_token_logits = (
                    torch.cat(
                        [token_scores.unsqueeze(1) for token_scores in outputs.scores],
                        dim=1,
                    )
                    .detach()
                    .cpu()
                )
                if batch_generated_token_ids.shape[1] < generation_params["max_new_tokens"]:
                    real_batch_size, num_generated_tokens = batch_generated_token_ids.shape
                    padding_length = generation_params["max_new_tokens"] - num_generated_tokens
                    padding_token_ids = torch.zeros(
                        (real_batch_size, padding_length),
                        dtype=batch_generated_token_ids.dtype,
                    ).fill_(self.tokenizer.pad_token_id)
                    padding_token_logits = torch.zeros(
                        (
                            real_batch_size,
                            padding_length,
                            batch_generated_token_logits.shape[-1],
                        ),
                        dtype=batch_generated_token_logits.dtype,
                    )
                    batch_generated_token_ids = torch.cat([batch_generated_token_ids, padding_token_ids], dim=1)
                    batch_generated_token_logits = torch.cat(
                        [batch_generated_token_logits, padding_token_logits],
                        dim=1,
                    )
                generated_token_ids.append(batch_generated_token_ids)
                generated_token_logits.append(batch_generated_token_logits)

            for i, generated_sequence in enumerate(outputs.sequences):
                input_ids = inputs["input_ids"][i]
                text = self.tokenizer.decode(
                    generated_sequence,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False,
                )
                if input_ids is None:
                    prompt_length = 0
                else:
                    prompt_length = len(
                        self.tokenizer.decode(
                            input_ids,
                            skip_special_tokens=True,
                            clean_up_tokenization_spaces=False,
                        )
                    )
                new_text = text[prompt_length:]

                if stop_sym is not None:
                    strip_stopword = True
                    # Find the first occurrence of any stop word
                    lower_stop_index = len(new_text)  # Default to end of text
                    for sym in stop_sym:
                        stop_index = new_text.find(sym)
                        if stop_index != -1:
                            # Adjust stop index based on whether we're stripping the stop word
                            stop_index += 0 if strip_stopword else len(sym)
                            lower_stop_index = min(stop_index, lower_stop_index)

                    # Cut the text at the first stop word found (if any)
                    new_text = new_text[:lower_stop_index]

                responses.append(new_text.strip())

        if return_dict:
            generated_token_ids = torch.cat(generated_token_ids, dim=0)
            generated_token_logits = torch.cat(generated_token_logits, dim=0)
            return {
                "generated_token_ids": generated_token_ids,
                "generated_token_logits": generated_token_logits,
                "responses": responses,
                "scores": scores,
            }

        if return_scores:
            return responses, scores
        else:
            return responses


    def cal_gen_probs(self, prev, next):
        import torch
        input_ids = self.tokenizer.encode(prev, add_special_tokens=False)
        target_ids = self.tokenizer.encode(next, add_special_tokens=False)
        context_ids = input_ids + target_ids
        context_tensor = torch.tensor([context_ids]).to(self.device)
        with torch.inference_mode():
            outputs = self.model(context_tensor)
            logits = outputs.logits
            logits = logits[0, len(input_ids) - 1 : len(context_ids) - 1, :]
            logits = logits.to(torch.float32).detach().cpu()
            # softmax to normalize
            probs = torch.softmax(logits, dim=-1)
            # obtain probs of target_ids
            target_probs = probs[range(len(target_ids)), target_ids].numpy()

        return logits, target_probs


class FastChatGenerator(HFCausalLMGenerator):
    def __init__(self, config, model=None):
        super().__init__(config)

    def _load_model(self, model=None):
        r"""Load model and tokenizer for generator."""

        def get_gpu_memory():
            """Get available memory for each GPU."""
            import torch
            gpu_memory = []
            for gpu_id in range(self.gpu_num):
                with torch.cuda.device(gpu_id):
                    device = torch.cuda.current_device()
                    gpu_properties = torch.cuda.get_device_properties(device)
                    total_memory = gpu_properties.total_memory / (1024**3)
                    allocated_memory = torch.cuda.memory_allocated() / (1024**3)
                    available_memory = total_memory - allocated_memory
                    gpu_memory.append(available_memory)
            return gpu_memory

        if model is None:
            from fastchat.model import load_model

            if "gpu_memory_utilization" not in self._config:
                gpu_memory_utilization = 0.85
            else:
                gpu_memory_utilization = self._config["gpu_memory_utilization"]
            max_gpu_memory = None
            import torch
            self.gpu_num = torch.cuda.device_count()
            if self.gpu_num > 1:
                available_gpu_memory = get_gpu_memory()
                max_gpu_memory = str(int(min(available_gpu_memory) * gpu_memory_utilization)) + "GiB"

            model, tokenizer = load_model(
                self.model_path,
                device=get_device(),
                num_gpus=self.gpu_num,
                max_gpu_memory=max_gpu_memory,
                load_8bit=False,
                cpu_offloading=False,
                debug=False,
            )

        else:
            model.cuda()
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        model.eval()

        tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        if "qwen" not in self.model_name:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"

        return model, tokenizer



================================================
FILE: flashrag/generator/multimodal_generator.py
================================================
from typing import List
import os
import json
import importlib
from copy import deepcopy
import warnings
import math
from tqdm import tqdm
from tqdm.auto import trange
import numpy as np
import torch
from transformers import AutoProcessor, AutoTokenizer, AutoModel
from abc import abstractmethod
import json
import os
import importlib
import base64
from io import BytesIO
from flashrag.generator.utils import convert_image_to_base64, process_image, resolve_max_tokens, process_image_pil

class BaseMultiModalGenerator:
    """`BaseMultiModalGenerator` is a base object of Generator model."""

    def __init__(self, config):
        self.model_name = config["generator_model"]
        self.model_path = config["generator_model_path"]

        self.max_input_len = config["generator_max_input_len"]
        self.batch_size = config["generator_batch_size"]
        self.device = config["device"]
        self.gpu_num = torch.cuda.device_count()
        self.config = config
        self.generation_params = config["generation_params"]
    
    def generate(self, input_list: list) -> List[str]:
        """
        input_list: A list contains of messages, each message is a list, like:
        [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": "image1_path"},
                    {"type": "image", "image": "image2_path"},
                    {"type": "text", "text": "Describe this image."},
                ],
            }
        ]

        
        The content of each dict can be a str(pure text as input) or list(for multimodal input).
        """
        
        pass

class BaseInferenceEngine:
    def __init__(self, model_path, device='cpu', max_input_len=4096):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.max_input_len = max_input_len
        self._load_model()

    @abstractmethod
    def _load_model(self):
        pass

    @abstractmethod
    @torch.inference_mode(mode=True)
    def generate(self, input_list: list, batch_size=None, **params):
        pass

class Qwen2VLInferenceEngine(BaseInferenceEngine):
    def _load_model(self):
        from transformers import Qwen2VLForConditionalGeneration
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            self.model_path,
            torch_dtype='auto',
            device_map='auto',
            trust_remote_code=True
        ).eval()
        min_pixels = 3136
        max_pixels = 12845056
        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True, min_pixels=min_pixels, max_pixels=max_pixels)
        self.processor.tokenizer.model_max_length = self.max_input_len
        self.tokenizer = self.processor.tokenizer
    @torch.inference_mode(mode=True)
    def generate(self, input_list, **params):
        # convert image to base64
        for messages in input_list:
            for message in messages:
                if isinstance(message['content'], list):
                    for content_dict in message['content']:
                        if content_dict['type'] == 'image':
                            content_dict['image'] = convert_image_to_base64(content_dict['image'])

        from qwen_vl_utils import process_vision_info
        texts = [self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False) for messages in input_list]
        image_inputs, video_inputs = process_vision_info(input_list)    
        inputs = self.processor(text=texts, images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt").to(self.model.device)
        # print(inputs)
        # print(inputs['input_ids'].shape,inputs['attention_mask'].shape,inputs['pixel_values'].shape,inputs['image_grid_thw'].shape)
        outputs = self.model.generate(
            **inputs,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            **params
        )
        generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)]
        output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        return output_text

class InternVL2InferenceEngine(BaseInferenceEngine):
    def _load_model(self):
        import torch
        gpu_num = torch.cuda.device_count()
        self.model = AutoModel.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            device_map='auto' if gpu_num <= 1 else self.split_model(),
            trust_remote_code=True
        ).eval()
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        self.tokenizer.model_max_length = self.max_input_len
        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id 


    def split_model(self):
        # get model name
        with open(os.path.join(self.model_path, 'config.json')) as f:
            config = json.load(f)
        num_layers = config['llm_config']['num_hidden_layers']
        device_map = {}
        world_size = torch.cuda.device_count()
        # Since the first GPU will be used for ViT, treat it as half a GPU.
        num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
        num_layers_per_gpu = [num_layers_per_gpu] * world_size
        num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
        layer_cnt = 0
        for i, num_layer in enumerate(num_layers_per_gpu):
            for j in range(num_layer):
                device_map[f'language_model.model.layers.{layer_cnt}'] = i
                layer_cnt += 1
        device_map['vision_model'] = 0
        device_map['mlp1'] = 0
        device_map['language_model.model.tok_embeddings'] = 0
        device_map['language_model.model.embed_tokens'] = 0
        device_map['language_model.output'] = 0
        device_map['language_model.model.norm'] = 0
        device_map['language_model.lm_head'] = 0
        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0

        return device_map

    def build_transform(self, input_size):
        import torchvision.transforms as T
        from torchvision.transforms.functional import InterpolationMode
        IMAGENET_MEAN = (0.485, 0.456, 0.406)
        IMAGENET_STD = (0.229, 0.224, 0.225)
        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
        transform = T.Compose([
            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
            T.ToTensor(),
            T.Normalize(mean=MEAN, std=STD)
        ])
        return transform

    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):
        best_ratio_diff = float('inf')
        best_ratio = (1, 1)
        area = width * height
        for ratio in target_ratios:
            target_aspect_ratio = ratio[0] / ratio[1]
            ratio_diff = abs(aspect_ratio - target_aspect_ratio)
            if ratio_diff < best_ratio_diff:
                best_ratio_diff = ratio_diff
                best_ratio = ratio
            elif ratio_diff == best_ratio_diff:
                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                    best_ratio = ratio
        return best_ratio

    def dynamic_preprocess(self, image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
        orig_width, orig_height = image.size
        aspect_ratio = orig_width / orig_height

        # calculate the existing image aspect ratio
        target_ratios = set(
            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
            i * j <= max_num and i * j >= min_num)
        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

        # find the closest aspect ratio to the target
        target_aspect_ratio = self.find_closest_aspect_ratio(
            aspect_ratio, target_ratios, orig_width, orig_height, image_size)

        # calculate the target width and height
        target_width = image_size * target_aspect_ratio[0]
        target_height = image_size * target_aspect_ratio[1]
        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

        # resize the image
        resized_img = image.resize((target_width, target_height))
        processed_images = []
        for i in range(blocks):
            box = (
                (i % (target_width // image_size)) * image_size,
                (i // (target_width // image_size)) * image_size,
                ((i % (target_width // image_size)) + 1) * image_size,
                ((i // (target_width // image_size)) + 1) * image_size
            )
            # split the image
            split_img = resized_img.crop(box)
            processed_images.append(split_img)
        assert len(processed_images) == blocks
        if use_thumbnail and len(processed_images) != 1:
            thumbnail_img = image.resize((image_size, image_size))
            processed_images.append(thumbnail_img)
        return processed_images

    def load_image(self, image, input_size=448, max_num=12):
        transform = self.build_transform(input_size=input_size)
        images = self.dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
        pixel_values = [transform(image) for image in images]
        pixel_values = torch.stack(pixel_values)
        return pixel_values
    
    @torch.inference_mode(mode=True)
    def generate(self, input_list, **params):
        import torch
        # TODO: Currently only support single image batch or multi-image without batch 
        # convert input format to internvl2
        final_prompt_list = [] # each is a str
        final_image_list = [] # each is a list
        system_message = None
        for messages in input_list:
            # parse each query
            for item in messages:
                if item['role'] == 'system':
                    assert isinstance(item['content'], str)
                    system_message = item['content']
                else:
                    if isinstance(item['content'], str):
                        # pure text input
                        final_prompt_list.append(item['content'])
                        final_image_list.append([])
                    else:
                        # multimodal input
                        image_list = [d['image'] for d in item['content'] if d['type'] == 'image']
                        text = [d['text'] for d in item['content'] if d['type'] == 'text'][0]
                        final_prompt_list.append(text)
                        final_image_list.append(image_list)
        if system_message is not None:
            self.model.system_message = system_message
        
        final_image_list = [[self.load_image(img, max_num=12).to(self.model.dtype).to(self.model.device) for img in image_list] for image_list in final_image_list]

        if all([len(image_list) ==1 for image_list in final_image_list]):
            torch.cuda.empty_cache()
            # batch inference with single image
            final_image_list = [image_list[0] for image_list in final_image_list]
            pixel_values = torch.cat(final_image_list, dim=0)
            num_patches_list = [img.size(0) for img in final_image_list]
            final_prompt_list = [f'<image>\n{text}' for text in final_prompt_list]
            outputs = self.model.batch_chat(
                self.tokenizer,
                pixel_values,
                final_prompt_list,
                num_patches_list=num_patches_list,
                generation_config=params,
                history=None,
                return_history=None,
            )
            return outputs
        else:
            # do single item inference with multi image 
            outputs = []
            for image_list, prompt in zip(final_image_list, final_prompt_list):
                torch.cuda.empty_cache()
                pixel_values = torch.cat(image_list, dim=0)
                num_patches_list = [img.size(0) for img in image_list]
                prompt_prefix = ""
                for i in range(len(image_list)):
                    prompt_prefix += f'Image-{i+1}: <image>\n'
                prompt = prompt_prefix + prompt

                output = self.model.chat(
                    self.tokenizer,
                    pixel_values,
                    prompt,
                    num_patches_list=num_patches_list,
                    generation_config=params,
                    history=None,
                    return_history=None,
                )
                outputs.append(output)
            return outputs

class LlavaInferenceEngine(BaseInferenceEngine):
    def _load_model(self):
        from transformers import AutoProcessor
        with open(os.path.join(self.model_path, "config.json"), "r") as f:
            config = json.load(f)
            model_type = config['architectures'][0]
        model_type = getattr(importlib.import_module('transformers'), model_type)
        self.model = model_type.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map='auto',
            trust_remote_code=True
        ).eval()
        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.processor.tokenizer.padding_side = 'left'
        self.tokenizer = self.processor.tokenizer
        self.processor.patch_size = self.model.config.vision_config.patch_size
        self.processor.vision_feature_select_strategy = self.model.config.vision_feature_select_strategy
        self.image_token = "<image>"

    @torch.inference_mode(mode=True)
    def generate(self, input_list, **params):
        # add special tokens
        new_input_list = []
        visual_list = []
        for messages in input_list:
            new_messages = []
            for message in messages:
                item_visual_list = []
                if isinstance(message['content'],list):
                    # remove all image
                    image_list = [item['image'] for item in message['content'] if item['type'] == 'image']
                    item_visual_list.extend(image_list)
                    text_content = [item['text'] for item in message['content'] if item['type'] == 'text'][0]
                    image_tokens = " ".join([self.image_token]*len(image_list))
                    text_content = f"{image_tokens}\n{text_content}"
                    new_messages.append({"role": message['role'], "content": text_content})
                else:
                    new_messages.append(message)
                visual_list.append(item_visual_list)
            new_input_list.append(new_messages)
        visual_list = sum(visual_list, [])
        texts = self.tokenizer.apply_chat_template(new_input_list, tokenize=False, add_generation_prompt=True)
        inputs = self.processor(text=texts, images=visual_list, padding=True, truncation=True, max_length=self.max_input_len, return_tensors='pt').to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            **params
        )
        generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)]
        output_text = self.tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=True)
        return output_text


class HFModelInferenceEngineFactory:
    _engine_map = {
        'qwen': Qwen2VLInferenceEngine,
        'llava': LlavaInferenceEngine,
        'internvl': InternVL2InferenceEngine,
    }

    @staticmethod
    def get_engine(model_path, device='cpu', **kwargs):
        config_file_path = os.path.join(model_path, 'config.json')
        with open(config_file_path, "r") as f:
            model_config = json.load(f)
        model_arch = model_config['architectures'][0]
    
        for engine_name, engine_class in HFModelInferenceEngineFactory._engine_map.items():
            if engine_name in model_arch.lower():
                return engine_class(model_path, device, **kwargs)
            
        raise ValueError(f"Model {model_path} is not supported!")
      
    
class HFMultiModalGenerator(BaseMultiModalGenerator):
    def __init__(self, config):
        super().__init__(config)
        self.config = config
        self.model_name = config['generator_model']
        self.model_path = config['generator_model_path']
        
        self.inference_engine = HFModelInferenceEngineFactory.get_engine(
            model_path=self.model_path,
            device=self.device,
            max_input_len=self.max_input_len
        )

    @torch.inference_mode(mode=True)
    def generate(
        self,
        input_list: list,
        batch_size=None,
        **params
    ):
        # solve params
        if not isinstance(input_list[0], list):
            input_list = [input_list]
        if batch_size is None:
            batch_size = self.batch_size
        generation_params = deepcopy(self.generation_params)
        generation_params.update(params)
        if 'temperature' not in generation_params:
            generation_params['temperature'] = 0
        if 'do_sample' not in generation_params:
            generation_params['do_sample'] = True if generation_params['temperature'] > 0 else False
        if generation_params['do_sample'] == False:
            generation_params['temperature'] = 0
        # deal stop params
        stop_sym = None
        if "stop" in generation_params:
            from flashrag.generator.stop_word_criteria import StopWordCriteria

            stop_sym = generation_params.pop("stop")
            stopping_criteria = [
                StopWordCriteria(
                    tokenizer=self.tokenizer,
                    prompts=input_list,
                    stop_words=stop_sym,
                )
            ]
            generation_params["stopping_criteria"] = stopping_criteria

        generation_params = resolve_max_tokens(params, generation_params, prioritize_new_tokens=True)

        # preprocess input list
        from PIL import Image
        for messages in input_list:
            for message in messages:
                if isinstance(message['content'], list):
                    for content_dict in message['content']:
                        if content_dict['type'] == 'image':
                            content_dict['image'] = process_image_pil(content_dict['image'])

        output_responses = []
        for idx in trange(0, len(input_list), batch_size, desc='Generation process: '):
            torch.cuda.empty_cache()
            batch_prompts = input_list[idx: idx+batch_size]
            output_responses.extend(self.inference_engine.generate(batch_prompts, **generation_params))
        return output_responses












================================================
FILE: flashrag/generator/openai_generator.py
================================================
import os
from typing import List, Union
from copy import deepcopy
import warnings
from tqdm import tqdm
import numpy as np
import threading
import asyncio
from openai import AsyncOpenAI, AsyncAzureOpenAI
import tiktoken

_background_loop = None

def get_background_loop():
    global _background_loop
    if _background_loop is None:
        _background_loop = asyncio.new_event_loop()
        t = threading.Thread(target=lambda: _background_loop.run_forever(), daemon=True)
        t.start()
    return _background_loop

class OpenaiGenerator:
    """Class for api-based openai models"""

    def __init__(self, config):
        self._config = config
        self.update_config()
        
        # load openai client
        if "api_type" in self.openai_setting and self.openai_setting["api_type"] == "azure":
            del self.openai_setting["api_type"]
            self.client = AsyncAzureOpenAI(**self.openai_setting)
        else:
            self.client = AsyncOpenAI(**self.openai_setting)
        try:
            self.tokenizer = tiktoken.encoding_for_model(self.model_name)
        except Exception as e:
            print("Warning: ", e)
            warnings.warn("This model is not supported by tiktoken. Use gpt-3.5-turbo instead.")
            self.tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')
    @property
    def config(self):
        return self._config

    @config.setter
    def config(self, config_data):
        self._config = config_data
        self.update_config()
    
    def update_config(self):
        self.update_base_setting()
        self.update_additional_setting()

    def update_base_setting(self):
        self.model_name = self._config["generator_model"]
        self.batch_size = self._config["generator_batch_size"]
        self.generation_params = self._config["generation_params"]

        self.openai_setting = self._config["openai_setting"]
        if self.openai_setting["api_key"] is None:
            self.openai_setting["api_key"] = os.getenv("OPENAI_API_KEY")

    def update_additional_setting(self):
        pass
    
    async def _get_response(self, messages: Union[list, str], mode: str = 'chat', **params):
        print(messages)
        print("----asd-asd-a--------")
        if mode == 'chat':
            response = await self.client.chat.completions.create(
                model=self.model_name, messages=messages, **params
            )
            if not response.choices:
                raise ValueError("No choices returned from API.")
            return response.choices[0]
        else:
            response = await self.client.completions.create(
                model=self.model_name, prompt=messages, **params
            )
            if not response.choices:
                raise ValueError("No choices returned from API.")
            return response.choices[0]

    async def _get_batch_response(self, input_list: List[List], batch_size, mode, **params):
        tasks = [self._get_response(messages, mode, **params) for messages in input_list]
        all_results = []
        for idx in tqdm(range(0, len(tasks), batch_size), desc="Generation process: "):
            batch_tasks = tasks[idx: idx + batch_size]
            batch_results = await asyncio.gather(*batch_tasks)
            all_results.extend(batch_results)
        return all_results

    async def _generate_async(self, input_list: List, batch_size=None, return_scores=False, **params) -> List[str]:
        if isinstance(input_list, dict):
            input_list = [[input_list]]
        elif isinstance(input_list[0], dict):
            input_list = [input_list]
        if isinstance(input_list[0], list):
            mode = 'chat'
        else:
            mode = 'completion'

        if batch_size is None:
            batch_size = self.batch_size

        generation_params = deepcopy(self.generation_params)
        generation_params.update(params)
        generation_params.pop("do_sample", None)

        max_tokens = params.pop("max_tokens", None) or params.pop("max_new_tokens", None)
        if max_tokens is not None:
            generation_params["max_tokens"] = max_tokens
        else:
            generation_params["max_tokens"] = generation_params.get(
                "max_tokens", generation_params.pop("max_new_tokens", None)
            )
        generation_params.pop("max_new_tokens", None)

        if return_scores:
            generation_params["logprobs"] = True
            warnings.warn("Set logprobs to True to get generation scores.")


        results = await self._get_batch_response(input_list, batch_size, mode, **generation_params)

        response_texts = []
        scores = []
        for res in results:
            if mode == 'chat':
                text = res.message.content
            else:
                text = res.text
            if 'stop' in generation_params and not any([text.endswith(stop_str) for stop_str in generation_params['stop']]):
                # current openai api not support including stop, so need to add the stop token
                text += generation_params['stop'][0]
            response_texts.append(text)
            if return_scores:
                try:
                    score = np.exp([item.logprob for item in res.logprobs.content])
                    scores.append(score)
                except:
                    warnings.warn('Fail to get logprobs in openai generation!')
                    scores.append(None)
        return (response_texts, scores) if return_scores else response_texts

    # ----------------- åŒæ­¥åŒ…è£…æŽ¥å£ -----------------
    def generate(self, input_list: List, batch_size=None, return_scores=False, **params) -> List[str]:
        loop = get_background_loop()
        future = asyncio.run_coroutine_threadsafe(
            self._generate_async(input_list, batch_size=batch_size, return_scores=return_scores, **params),
            loop
        )
        return future.result()



================================================
FILE: flashrag/generator/stop_word_criteria.py
================================================
"""
Created by Nestor Demeure.
This software is released under the Apache License 2.0.
"""

from typing import List
import torch
from transformers import StoppingCriteria, AutoTokenizer


class StopWordCriteria(StoppingCriteria):
    """
    A stopping criteria that halts the text generation process if any specified stop word is encountered.

    Inspired by https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/9
    And: https://github.com/outlines-dev/outlines/blob/main/outlines/generate/api.py
    """

    def __init__(self, tokenizer: AutoTokenizer, prompts: List[str], stop_words: List[str] = [], check_every: int = 1):
        """
        Initializes the StopWordCriteria with the necessary parameters for checking stop words during text generation.

        Parameters:
            tokenizer (AutoTokenizer): The tokenizer for encoding prompts and stop words.
            prompts (List[str]): Initial prompts used for generation, needed to determine where generated text begins.
            stop_words (List[str]): Words that trigger the stopping of generation when detected.
            check_every (int): Frequency of checking for stop words in the token stream (a performance optimization, use 1 to cut it out).
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.input_sizes = [self.tokenizer.encode(prompt, return_tensors="pt").size(-1) for prompt in prompts]
        self.stop_words = stop_words
        self.max_stop_word_size = max(
            (self.tokenizer.encode(word, return_tensors="pt").size(-1) for word in stop_words), default=0
        )
        self.check_every = check_every

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        """
        Determines whether to stop generation based on the presence of stop words.

        Stops if a stop word is found in *all* batch elements *and* the sequence length is a multiple of `check_every`.
        Note: Delay in stopping may occur if `check_every > 1`.

        Parameters:
            input_ids (torch.LongTensor): Generated token IDs.
            scores (torch.FloatTensor): Generation scores for each token. Not used here.

        Returns:
            bool: True to stop generation, False to continue.
        """
        batch_size, seq_len = input_ids.shape

        # Skip check if no stop words are defined or it is not yet time to check
        if (len(self.stop_words) == 0) or (seq_len % self.check_every != 0):
            return False

        for i in range(batch_size):
            # Calculate starting index for new tokens
            prompt_size = self.input_sizes[i]
            max_new_tokens = (2 * self.max_stop_word_size) + self.check_every
            latest_tokens = input_ids[i, prompt_size:][-max_new_tokens:]

            # Check for stop words in the decoded text
            if not any(
                word in self.tokenizer.decode(latest_tokens, skip_special_tokens=True) for word in self.stop_words
            ):
                return False  # Continue generation if any batch item lacks stop words

        return True  # Stop generation if all conditions are met

    def extract_answers(self, input_ids: torch.LongTensor, strip_stopword: bool = True) -> List[str]:
        """
        Extracts generated answers by removing prompts and optionally stopping at the first stop word.

        Parameters:
            input_ids (torch.LongTensor): Generated token IDs.
            strip_stopword (bool): Determines whether the stop word is removed from the output.

        Returns:
            List[str]: Extracted answers, with or without stop words.
        """
        batch_size, _ = input_ids.shape
        result = []

        for i in range(batch_size):
            # Decode generated tokens to text, excluding the prompt
            prompt_size = self.input_sizes[i]
            answer_tokens = input_ids[i, prompt_size:]
            answer_text = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)

            # Find the first occurrence of any stop word
            lower_stop_index = len(answer_text)  # Default to end of text
            for word in self.stop_words:
                stop_index = answer_text.find(word)
                if stop_index != -1:
                    # Adjust stop index based on whether we're stripping the stop word
                    stop_index += 0 if strip_stopword else len(word)
                    lower_stop_index = min(stop_index, lower_stop_index)

            # Cut the text at the first stop word found (if any)
            answer_text = answer_text[:lower_stop_index]
            result.append(answer_text)

        return result



================================================
FILE: flashrag/generator/utils.py
================================================
import warnings
import os

def resolve_max_tokens(params: dict, generation_params: dict, prioritize_new_tokens: bool = False) -> dict:
    """
    Resolve and validate max_tokens parameters from both params and generation_params.

    Args:
        params: Dictionary containing user-provided parameters
        generation_params: Dictionary containing generation-specific parameters
        prioritize_new_tokens: If True, max_new_tokens takes precedence over max_tokens
                             If False, max_tokens takes precedence (default behavior)

    Returns:
        Updated generation_params dictionary
    """

    def get_token_params(param_dict: dict) -> tuple:
        """Extract max_tokens and max_new_tokens from a parameter dictionary."""
        return (param_dict.pop("max_tokens", None), param_dict.pop("max_new_tokens", None))

    def resolve_tokens(max_tokens: int, max_new_tokens: int) -> int:
        """
        Resolve between max_tokens and max_new_tokens values based on priority.
        Returns the resolved token value or None if no valid value found.
        """
        # If either value is None, return the non-None value
        if max_tokens is None:
            return max_new_tokens
        if max_new_tokens is None:
            return max_tokens

        # Both values exist but are different
        if max_tokens != max_new_tokens:
            if prioritize_new_tokens:
                warnings.warn(
                    f"max_tokens ({max_tokens}) and max_new_tokens ({max_new_tokens}) "
                    f"are different. Using max_new_tokens value as it has priority."
                )
                return max_new_tokens
            else:
                warnings.warn(
                    f"max_tokens ({max_tokens}) and max_new_tokens ({max_new_tokens}) "
                    f"are different. Using max_tokens value as it has priority."
                )
                return max_tokens

        # Both values are equal
        return max_tokens

    # Try to resolve from params first, then fall back to generation_params
    max_tokens, max_new_tokens = get_token_params(params)
    final_max_tokens = resolve_tokens(max_tokens, max_new_tokens)

    # If no valid tokens found in params, try generation_params
    if final_max_tokens is None:
        max_tokens, max_new_tokens = get_token_params(generation_params)
        final_max_tokens = resolve_tokens(max_tokens, max_new_tokens)

    generation_params.pop("max_new_tokens", None)
    generation_params.pop("max_tokens", None)
    if final_max_tokens is not None:
        if prioritize_new_tokens:
            generation_params["max_new_tokens"] = final_max_tokens
        else:
            generation_params["max_tokens"] = final_max_tokens
    return generation_params


def convert_image_to_base64(image):
    from PIL import Image
    from io import BytesIO
    import base64
    if isinstance(image, Image.Image):
        image = image.convert('RGB')
        buffer = BytesIO()
        image.save(buffer, format='JPEG')
        image_bs64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return f'data:image/jpeg;base64,{image_bs64}'
    return image

def process_image_pil(image):
    from PIL import Image
    if isinstance(image, Image.Image):
        return image
    elif isinstance(image, str):
        return load_image_from_source(image)
    else:
        raise ValueError("Image must be a PIL Image or a string path to an image file")

def process_image(content_dict):
    from PIL import Image
    image = content_dict.get('image')
    if isinstance(image, Image.Image):
        content_dict['image'] = convert_image_to_base64(image)
    elif isinstance(image, str):
        content_dict['image'] = convert_image_to_base64(load_image_from_source(image))

def load_image_from_source(image_path):
    from PIL import Image
    import requests
    if os.path.exists(image_path):
        return Image.open(image_path).convert('RGB')
    else:
        response = requests.get(image_path, stream=True)
        response.raise_for_status()
        return Image.open(response.raw).convert('RGB')


================================================
FILE: flashrag/judger/__init__.py
================================================
from flashrag.judger.judger import *


================================================
FILE: flashrag/judger/judger.py
================================================
from typing import cast, List
import json
from tqdm.auto import trange
from collections import Counter
import numpy as np
import torch
import faiss
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from flashrag.retriever.utils import load_model, pooling
from flashrag.dataset import Dataset


class BaseJudger:
    """Base object of Judger, used for judging whether to retrieve"""

    def __init__(self, config):
        self.config = config
        self.name = config["judger_name"] if "judger_name" in config else None
        self.judger_config = config["judger_config"] if "judger_config" in config else {}
        self.device = config["device"]

    def run(self, item) -> str:
        """Get judgement result.

        Args:
            item: dataset item, contains question, retrieval result...

        Returns:
            judgement: bool, whether to retreive
        """
        pass

    def batch_run(self, dataset, batch_size=None) -> List[str]:
        return [self.run(item) for item in dataset]


class SKRJudger(BaseJudger):
    """Implementation for SKR-knn
    Paper link: https://aclanthology.org/2023.findings-emnlp.691.pdf
    """

    def __init__(self, config):
        super().__init__(config)
        self.model_path = self.judger_config["model_path"]
        self.training_data_path = self.judger_config["training_data_path"]
        self.encoder, self.tokenizer = load_model(model_path=self.model_path, use_fp16=False)
        self.topk = self.judger_config["topk"] if "topk" in self.judger_config else 5
        self.batch_size = self.judger_config["batch_size"] if "batch_size" in config else 64
        self.max_length = self.judger_config["max_length"] if "max_length" in config else 128

        with open(self.training_data_path, "r") as f:
            self.training_data = json.load(f)
        # count number of pos & neg samples in training data
        self.training_data_counter = Counter([item["judgement"].strip() for item in self.training_data])
        self.training_pos_num = self.training_data_counter["ir_better"]
        self.training_neg_num = self.training_data_counter["ir_worse"]
        self.training_data_num = sum(self.training_data_counter.values())

        # encode training question into faiss
        training_questions = [item["question"] for item in self.training_data]
        all_embeddings = self.encode(training_questions)
        faiss_index = faiss.index_factory(all_embeddings.shape[-1], "Flat", faiss.METRIC_L2)
        faiss_index.add(all_embeddings)
        self.faiss = faiss_index

    @torch.inference_mode(mode=True)
    def encode(self, contents: list):
        inputs = self.tokenizer(
            contents,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=self.max_length,
        ).to("cuda")
        output = self.encoder(**inputs, return_dict=True)
        embeddings = pooling(output.pooler_output, output.last_hidden_state, inputs["attention_mask"], "pooler")

        embeddings = cast(torch.Tensor, embeddings)
        embeddings = torch.nn.functional.normalize(embeddings, dim=-1).detach()

        all_embeddings = embeddings.cpu().numpy()
        # all_embeddings = np.concatenate(all_embeddings, axis=0)
        all_embeddings = all_embeddings.astype(np.float32)

        return all_embeddings

    def judge(self, dataset):
        if isinstance(dataset, Dataset):
            questions = dataset.question
        elif isinstance(dataset, list):
            questions = dataset
        elif isinstance(dataset, str):
            questions = [dataset]
        else:
            raise TypeError("dataset must be a Dataset or a list of str.")
        
        all_judgements = []
        for start_idx in range(0, len(questions), self.batch_size):
            batch_question = questions[start_idx : start_idx + self.batch_size]
            batch_emb = self.encode(batch_question)
            scores, batch_idxs = self.faiss.search(batch_emb, k=self.topk)

            for idxs in batch_idxs:
                topk_samples = [self.training_data[idx]["judgement"].strip() for idx in idxs]
                topk_counter = Counter(topk_samples)

                # count number of pos & neg samples in topk
                ir_better_num = topk_counter["ir_better"]
                ir_worse_num = topk_counter["ir_worse"]
                topk_delta = ir_better_num - ir_worse_num

                training_data_delta = self.training_pos_num - self.training_neg_num

                # provide judgments based on the formula in the paper
                if training_data_delta < 0:
                    if topk_delta < 0 and topk_delta <= int(training_data_delta * self.topk / self.training_data_num):
                        judgement = False
                    else:
                        judgement = True
                else:
                    if topk_delta > 0 and topk_delta >= int(training_data_delta * self.topk / self.training_data_num):
                        judgement = True
                    else:
                        judgement = False

                all_judgements.append(judgement)

        return all_judgements


class AdaptiveJudger(BaseJudger):
    """Implementation for Adaptive-RAG
    Paper link: https://aclanthology.org/2024.naacl-long.389.pdf
    """

    def __init__(self, config):
        super().__init__(config)
        self.model_path = self.judger_config["model_path"]
        self.batch_size = self.judger_config.get("batch_size", 16)
        self.max_length = self.judger_config.get("max_length", 512)

        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model.eval()
        self.model.cuda()

    @torch.inference_mode(mode=True)
    def judge(self, dataset):
        if isinstance(dataset, Dataset):
            questions = dataset.question
        elif isinstance(dataset, list):
            questions = dataset
        elif isinstance(dataset, str):
            questions = [dataset]
        else:
            raise TypeError("dataset must be a Dataset or a list of str.")
            
        questions = [q.strip() for q in questions]

        all_preds = []
        for idx in trange(0, len(questions), self.batch_size, desc="Judger process: "):
            batch_input = questions[idx : idx + self.batch_size]
            batch_input = self.tokenizer(
                batch_input,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt",
            ).to(self.model.device)

            scores = self.model.generate(
                **batch_input, return_dict_in_generate=True, output_scores=True, max_length=self.max_length
            ).scores[0]

            probs = (
                torch.nn.functional.softmax(
                    torch.stack(
                        [
                            scores[:, self.tokenizer("A").input_ids[0]],
                            scores[:, self.tokenizer("B").input_ids[0]],
                            scores[:, self.tokenizer("C").input_ids[0]],
                        ]
                    ),
                    dim=0,
                )
                .detach()
                .cpu()
                .numpy()
            )

            preds_labels = np.argmax(probs, 0)
            label_to_option = {
                0: "A",
                1: "B",
                2: "C",
            }
            preds = [label_to_option[pred] for pred in preds_labels]
            all_preds.extend(preds)

        return all_preds



================================================
FILE: flashrag/pipeline/__init__.py
================================================
from flashrag.pipeline.mm_pipeline import *
from flashrag.pipeline.pipeline import *
from flashrag.pipeline.branching_pipeline import REPLUGPipeline, SuRePipeline
from flashrag.pipeline.active_pipeline import IterativePipeline, SelfRAGPipeline, FLAREPipeline, SelfAskPipeline, IRCOTPipeline, RQRAGPipeline
from flashrag.pipeline.reasoning_pipeline import *
from flashrag.pipeline.ReaRAG_utils import *



================================================
FILE: flashrag/pipeline/branching_pipeline.py
================================================
import itertools
from typing import List
import re
from tqdm import tqdm
import numpy as np
from transformers import LogitsProcessorList
from flashrag.utils import get_retriever, get_generator
from flashrag.pipeline import BasicPipeline
from flashrag.prompt import PromptTemplate


class REPLUGPipeline(BasicPipeline):
    def __init__(self, config, prompt_template=None, retriever=None, generator=None):
        from flashrag.pipeline.replug_utils import load_replug_model

        super().__init__(config, prompt_template)
        # load specify model for REPLUG
        if generator is None:
            model = load_replug_model(config["generator_model_path"])
            generator = get_generator(config, model=model)
        if retriever is None:
            retriever = get_retriever(config)
        self.generator = generator
        self.retriever = retriever
    def build_single_doc_prompt(self, question: str, doc_list: List[str]):
        return [self.prompt_template.get_string(question=question, formatted_reference=doc) for doc in doc_list]

    def format_reference(self, doc_item):
        content = doc_item["contents"]
        title = content.split("\n")[0]
        text = "\n".join(content.split("\n")[1:])
        return f"Document(Title: {title}): {text}"

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        import torch
        from flashrag.pipeline.replug_utils import REPLUGLogitsProcessor

        input_query = dataset.question
        retrieval_results, doc_scores = self.retriever.batch_search(input_query, return_score=True)
        dataset.update_output("retrieval_result", retrieval_results)
        dataset.update_output("doc_scores", doc_scores)

        pred_answer_list = []
        # each doc has a prompt
        for item in tqdm(dataset, desc="Inference: "):
            docs = [self.format_reference(doc_item) for doc_item in item.retrieval_result]
            prompts = self.build_single_doc_prompt(question=item.question, doc_list=docs)

            scores = torch.tensor(item.doc_scores, dtype=torch.float32).to(self.device)
            output = self.generator.generate(
                prompts, batch_size=len(docs), logits_processor=LogitsProcessorList([REPLUGLogitsProcessor(scores)])
            )
            # the output of the batch is same
            output = output[0]
            pred_answer_list.append(output)

        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)

        return dataset


class SuRePipeline(BasicPipeline):
    def __init__(self, config, prompt_template=None, retriever=None, generator=None):
        super().__init__(config, prompt_template)
        self.config = config
        if generator is None:
            generator = get_generator(config)
        if retriever is None:
            retriever = get_retriever(config)
        self.generator = generator
        self.retriever = retriever

        self.load_prompts()

    def load_prompts(self):
        # prompt for candidates generation
        P_CAN_INSTRUCT = (
            "Below are {N} passages related to the question at the end. After reading"
            "the passages, provide two correct candidates for the answer to the"
            "question at the end. Each answer should be in the form: (a) xx, (b)"
            "yy, and should not exceed 3 words for each candidate.\n\n"
            "{reference}"
            "Question: {question}\n"
            "Answer:"
        )

        # prompt for candidate-conditioned summarization
        P_SUM_INSTRUCT = (
            "Reference:\n{reference}\n"
            "Your job is to act as a professional writer. You need to write a"
            "good-quality passage that can support the given prediction about the"
            "question only based on the information in the provided supporting passages.\n"
            "Now, let's start. After you write, please write [DONE] to indicate you"
            "are done. Do not write a prefix (e.g., 'Response:') while writing a passage.\n"
            "Question: {question}\n"
            "Prediction: {pred}\n"
            "Passage:"
        )

        # prompt for instance-wise validation
        P_VAL_INSTRUCT = (
            "Question: {question}\n"
            "Prediction: {pred}\n"
            "Passage: {summary}\n"
            "Does the passage correctly support the prediction? Choices: [True,False].\n"
            "Answer:"
        )

        # prompt for pair-wise ranking
        P_RANK_INSTRUCT = (
            "Question: Given the following passages, determine which one provides a"
            "more informative answer to the subsequent question.\n"
            "Passage 1: {summary1}\n"
            "Passage 2: {summary2}\n"
            "Target Question: {question}\n"
            "Your Task:\n"
            "Identify which passage (Passage 1 or Passage 2) is more relevant and"
            "informative to answer the question at hand. Choices: [Passage 1,Passage 2].\n"
            "Answer:"
        )

        self.P_CAN_TEMPLATE = PromptTemplate(self.config, "", P_CAN_INSTRUCT)
        self.P_SUM_TEMPLATE = PromptTemplate(self.config, "", P_SUM_INSTRUCT)
        self.P_VAL_TEMPLATE = PromptTemplate(self.config, "", P_VAL_INSTRUCT)
        self.P_RANK_TEMPLATE = PromptTemplate(self.config, "", P_RANK_INSTRUCT)

    @staticmethod
    def format_ref(titles, texts):
        formatted_ref = ""
        idx = 1
        for title, text in zip(titles, texts):
            formatted_ref += f"Passage #{idx} Title: {title}\n"
            formatted_ref += f"Passage #{idx} Text: {text}\n"
            formatted_ref += "\n"
            idx += 1
        return formatted_ref

    @staticmethod
    def parse_candidates(model_response):
        """Parse candidates from model response"""
        model_response = model_response.strip("\n").strip()
        # r'\([a-z]\) ([^,]+)'
        candidates = re.findall("\((\w+)\)\s*([^()]+)", model_response)
        candidates = [cand[1].split("\n")[0].strip() for cand in candidates]
        # post-process
        candidates = [cand.replace(",", "").strip() for cand in candidates]
        return candidates

    @staticmethod
    def parse_validation(model_response):
        """Parse model's validation result into score based on the paper formula"""
        model_response = model_response.strip().lower()
        if "true" in model_response:
            return 1
        else:
            return 0

    @staticmethod
    def parse_ranking(model_response):
        """Parse model's pair ranking result into score"""
        model_response = model_response.strip().lower()
        if "passage 1" in model_response:
            score = 1
        elif "passage 2" in model_response:
            score = 0
        else:
            score = 0.5
        return score

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        input_query = dataset.question

        retrieval_results, doc_scores = self.retriever.batch_search(input_query, return_score=True)
        dataset.update_output("retrieval_result", retrieval_results)

        pred_answer_list = []
        for item in tqdm(dataset, desc="Pipeline runing: "):
            retrieval_result = item.retrieval_result
            doc_num = len(retrieval_result)
            # format all docs
            for doc_item in retrieval_result:
                if "title" not in doc_item or "text" not in doc_item:
                    doc_item["title"] = doc_item["contents"].split("\n")[0]
                    doc_item["text"] = "\n".join(doc_item["contents"].split("\n")[1:])
            formatted_ref = self.format_ref(
                titles=[i["title"] for i in retrieval_result], texts=[i["text"] for i in retrieval_result]
            )
            # get candidates

            input_prompt = self.P_CAN_TEMPLATE.get_string(
                N=doc_num, formatted_reference=formatted_ref, question=item.question
            )
            output = self.generator.generate([input_prompt])[0]
            candidates = self.parse_candidates(output)
            item.update_output("candidates", candidates)

            if len(candidates) == 0:
                print("No valid predictions!")
                pred = ""
                pred_answer_list.append(pred)
                continue

            # get summarization for each candidate
            input_prompts = [
                self.P_SUM_TEMPLATE.get_string(question=item.question, pred=cand, formatted_reference=formatted_ref)
                for cand in candidates
            ]

            all_summary = self.generator.generate(input_prompts)
            item.update_output("all_summary", all_summary)

            # instance-wise validation
            input_prompts = [
                self.P_VAL_TEMPLATE.get_string(question=item.question, pred=cand, summary=summary)
                for cand, summary in zip(candidates, all_summary)
            ]
            val_results = self.generator.generate(input_prompts)
            val_scores = [self.parse_validation(res) for res in val_results]
            item.update_output("val_scores", val_scores)

            # pair-wise ranking
            summary_num = len(all_summary)
            score_matrix = np.zeros((summary_num, summary_num))
            iter_idxs = list(itertools.permutations(range(summary_num), 2))
            input_prompts = [
                self.P_RANK_TEMPLATE.get_string(
                    question=item.question, summary1=all_summary[idx_tuple[0]], summary2=all_summary[idx_tuple[1]]
                )
                for idx_tuple in iter_idxs
            ]
            ranking_output = self.generator.generate(input_prompts)
            ranking_scores = [self.parse_ranking(res) for res in ranking_output]
            for idx_tuple, score in zip(iter_idxs, ranking_scores):
                score_matrix[idx_tuple[0], idx_tuple[1]] = score
            ranking_scores = score_matrix.sum(axis=1).squeeze().tolist()  # ranking score for each summary
            item.update_output("ranking_scores", ranking_scores)

            # combine two scores as the final score for each summary
            if not isinstance(ranking_scores, list):
                ranking_scores = [ranking_scores]
            if not isinstance(val_scores, list):
                val_scores = [val_scores]
            total_scores = [x + y for x, y in zip(val_scores, ranking_scores)]

            best_idx = np.argmax(total_scores)
            pred = candidates[best_idx]
            pred_answer_list.append(pred)

        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)

        return dataset



================================================
FILE: flashrag/pipeline/mm_pipeline.py
================================================
from flashrag.evaluator import Evaluator
from flashrag.utils import get_retriever, get_generator

class BasicMultiModalPipeline:
    """Base object of all multimodal pipelines. A pipeline includes the overall process of RAG.
    If you want to implement a pipeline, you should inherit this class.
    """

    def __init__(self, config, prompt_template=None):
        from flashrag.prompt import MMPromptTemplate
        self.config = config
        self.device = config["device"]
        self.retriever = None
        self.evaluator = Evaluator(config)
        if prompt_template is None:
            prompt_template = MMPromptTemplate(config)
        self.prompt_template = prompt_template

    def run(self, dataset, pred_process_fun=None):
        """The overall inference process of a RAG framework."""
        pass

    def evaluate(self, dataset, do_eval=True, pred_process_func=None):
        """The evaluation process after finishing overall generation"""

        if pred_process_func is not None:
            dataset = pred_process_func(dataset)

        if do_eval:
            # evaluate & save result
            eval_result = self.evaluator.evaluate(dataset)
            print(eval_result)

        return 


class MMSequentialPipeline(BasicMultiModalPipeline):
    PERFORM_MODALITY_DICT = {
        'text': ['text'],
        'image': ['image']
    }
    def __init__(self, config, prompt_template=None, retriever=None, generator=None):
        super().__init__(config, prompt_template)
        self.generator = get_generator(config) if generator is None else generator
        self.retriever = get_retriever(config) if retriever is None else retriever
    
    def naive_run(self, dataset, do_eval=True, pred_process_func=None):
        input_prompts = [
            self.prompt_template.get_string(item) for item in dataset
        ]
        
        dataset.update_output("prompt", input_prompts)

        pred_answer_list = self.generator.generate(input_prompts)
        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_func=pred_process_func)

        return dataset
    
    def run(self, dataset, do_eval=True, perform_modality_dict=PERFORM_MODALITY_DICT, pred_process_func=None):
        if None not in dataset.question:
            text_query_list = dataset.question
        else:
            text_query_list = dataset.text
        image_query_list = dataset.image

        # perform retrieval
        retrieval_result = []
        for modal in perform_modality_dict.get('text', []):
            retrieval_result.append(
                self.retriever.batch_search(text_query_list, target_modal=modal)
            )
        for modal in perform_modality_dict.get('image', []):
            retrieval_result.append(
                self.retriever.batch_search(image_query_list, target_modal=modal)
           )
        retrieval_result = [sum(group, []) for group in zip(*retrieval_result)]

        dataset.update_output("retrieval_result", retrieval_result)

        input_prompts = [
            self.prompt_template.get_string(item) for item in dataset
        ]
        
        dataset.update_output("prompt", input_prompts)

        pred_answer_list = self.generator.generate(input_prompts)
        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_func=pred_process_func)

        return dataset
        
        


================================================
FILE: flashrag/pipeline/pipeline.py
================================================
from flashrag.evaluator import Evaluator
from flashrag.dataset.utils import split_dataset, merge_dataset
from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger
from flashrag.prompt import PromptTemplate


class BasicPipeline:
    """Base object of all pipelines. A pipeline includes the overall process of RAG.
    If you want to implement a pipeline, you should inherit this class.
    """

    def __init__(self, config, prompt_template=None):
        self.config = config
        self.device = config["device"]
        self.retriever = None
        self.evaluator = Evaluator(config)
        self.save_retrieval_cache = config["save_retrieval_cache"]
        if prompt_template is None:
            prompt_template = PromptTemplate(config)
        self.prompt_template = prompt_template

    def run(self, dataset):
        """The overall inference process of a RAG framework."""
        pass

    def evaluate(self, dataset, do_eval=True, pred_process_fun=None):
        """The evaluation process after finishing overall generation"""

        if pred_process_fun is not None:
            dataset = pred_process_fun(dataset)

        if do_eval:
            # evaluate & save result
            eval_result = self.evaluator.evaluate(dataset)
            print(eval_result)

        # save retrieval cache
        if self.save_retrieval_cache:
            self.retriever._save_cache()

        return dataset


class SequentialPipeline(BasicPipeline):
    def __init__(self, config, prompt_template=None, retriever=None, generator=None):
        """
        inference stage:
            query -> pre-retrieval -> retriever -> post-retrieval -> generator
        """

        super().__init__(config, prompt_template)
        if generator is None:
            self.generator = get_generator(config)
        else:
            self.generator = generator

        if retriever is None:
            self.retriever = get_retriever(config)
        else:
            self.retriever = retriever

        # TODO: add rewriter module

        self.use_fid = config["use_fid"]

        if config["refiner_name"] is not None:
            self.refiner = get_refiner(config, self.retriever, self.generator)
        else:
            self.refiner = None

    def naive_run(self, dataset, do_eval=True, pred_process_fun=None):
        # direct generation without RAG
        input_prompts = [self.prompt_template.get_string(question=q) for q in dataset.question]
        dataset.update_output("prompt", input_prompts)

        pred_answer_list = self.generator.generate(input_prompts)
        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        input_query = dataset.question
        retrieval_results = self.retriever.batch_search(input_query)
        dataset.update_output("retrieval_result", retrieval_results)

        if self.refiner:
            input_prompt_flag = self.refiner.input_prompt_flag
            if "llmlingua" in self.refiner.name and input_prompt_flag:
                # input prompt
                input_prompts = [
                    self.prompt_template.get_string(question=q, retrieval_result=r)
                    for q, r in zip(dataset.question, dataset.retrieval_result)
                ]
                dataset.update_output("prompt", input_prompts)
                input_prompts = self.refiner.batch_run(dataset)
            else:
                # input retrieval docs
                refine_results = self.refiner.batch_run(dataset)
                dataset.update_output("refine_result", refine_results)
                input_prompts = [
                    self.prompt_template.get_string(question=q, formatted_reference=r)
                    for q, r in zip(dataset.question, refine_results)
                ]

        else:
            if not self.use_fid:
                input_prompts = [
                    self.prompt_template.get_string(question=q, retrieval_result=r)
                    for q, r in zip(dataset.question, dataset.retrieval_result)
                ]

        if self.use_fid:
            print("Use FiD generation")
            input_prompts = []
            for item in dataset:
                q = item.question
                docs = item.retrieval_result
                input_prompts.append([q + " " + doc['contents'] for doc in docs])
        dataset.update_output("prompt", input_prompts)

        # delete used refiner to release memory
        if self.refiner:
            del self.refiner
        pred_answer_list = self.generator.generate(input_prompts)
        dataset.update_output("pred", pred_answer_list)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)

        return dataset


class ConditionalPipeline(BasicPipeline):
    def __init__(self, config, prompt_template=None, retriever=None, generator=None):
        """
        inference stage:
            query -> judger -> sequential pipeline or naive generate
        """

        super().__init__(config, prompt_template)

        self.judger = get_judger(config)
        if generator is None:
            self.generator = get_generator(config)
        if retriever is None:
            self.retriever = get_retriever(config)
        self.generator = generator
        self.retriever = retriever

        self.sequential_pipeline = SequentialPipeline(
            config, prompt_template, retriever=self.retriever, generator=self.generator
        )

        self.zero_shot_templete = PromptTemplate(
            config=config,
            system_prompt="Answer the question based on your own knowledge. \
                            Only give me the answer and do not output any other words.",
            user_prompt="Question: {question}",
        )

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        # judge_result: list of bool element, representing whether to use retrieval
        judge_result = self.judger.judge(dataset)
        dataset.update_output("judge_result", judge_result)

        # split dataset based on judge_result
        dataset_split = split_dataset(dataset, judge_result)
        pos_dataset, neg_dataset = dataset_split[True], dataset_split[False]

        pos_dataset = self.sequential_pipeline.run(pos_dataset, do_eval=False)
        self.sequential_pipeline.prompt_template = self.zero_shot_templete
        neg_dataset = self.sequential_pipeline.naive_run(neg_dataset, do_eval=False)

        # merge datasets into original format
        dataset = merge_dataset(dataset_split, judge_result)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)

        return dataset


class AdaptivePipeline(BasicPipeline):
    def __init__(
        self,
        config,
        norag_template=None,
        single_hop_prompt_template=None,
        multi_hop_prompt_template=None,
        retriever = None,
        generator = None
    ):
        super().__init__(config)
        # load adaptive classifier as judger
        self.judger = get_judger(config)

        if generator is None:
            generator = get_generator(config)
        if retriever is None:
            retriever = get_retriever(config)

        self.generator = generator
        self.retriever = retriever

        # Load three pipeline for three types of query: naive/single-hop/multi-hop
        from flashrag.pipeline import IRCOTPipeline

        if norag_template is None:
            norag_templete = PromptTemplate(
                config=config,
                system_prompt="Answer the question based on your own knowledge. Only give me the answer and do not output any other words.",
                user_prompt="Question: {question}",
            )
        self.norag_pipeline = SequentialPipeline(
            config,
            prompt_template=norag_templete,
            retriever=retriever,
            generator=generator,
        )

        self.single_hop_pipeline = SequentialPipeline(
            config,
            prompt_template=single_hop_prompt_template,
            retriever=retriever,
            generator=generator,
        )

        self.multi_hop_pipeline = IRCOTPipeline(
            config, prompt_template=multi_hop_prompt_template, retriever=retriever, generator=generator, max_iter=5
        )

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        # judge_result: choice result representing which pipeline to use(e.g. A, B, C)
        judge_result = self.judger.judge(dataset)
        dataset.update_output("judge_result", judge_result)

        # split dataset based on judge_result
        dataset_split = split_dataset(dataset, judge_result)
        for symbol, symbol_dataset in dataset_split.items():
            if symbol == "A":
                symbol_dataset = self.norag_pipeline.naive_run(symbol_dataset, do_eval=False)
            elif symbol == "B":
                symbol_dataset = self.single_hop_pipeline.run(symbol_dataset, do_eval=False)
            elif symbol == "C":
                symbol_dataset = self.multi_hop_pipeline.run(symbol_dataset, do_eval=False)
            else:
                assert False, "Unknown symbol!"

        # merge datasets into original format
        dataset = merge_dataset(dataset_split, judge_result)

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)

        return dataset



================================================
FILE: flashrag/pipeline/ReaRAG_utils.py
================================================
import re
from termcolor import colored

class AgentUtilsBase():
    def __init__(self):
        pass

    def truncate(self, tokenizer, tokenized_prompt, model_max_length):
        half = int(model_max_length/2)
        prompt = tokenizer.decode(tokenized_prompt[:half], skip_special_tokens=False) + \
                tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=False)
        
        return prompt

    def preprocess_query(self, query):
        if "'" in query and '"' in query:
            query = query.replace("'", "\\'").replace('"', '\\"')
        return query

    def extract_code(self, text: str) -> str:
        triple_match = re.search(r'```[^\n]*\n(.+?)```', text, re.DOTALL)
        single_match = re.search(r'`([^`]*)`', text, re.DOTALL)
        if triple_match:
            return triple_match.group(1)
        elif single_match:
            return single_match.group(1)
        return text
    
    def postprocess_agent_response(self, response):
        """
        Implement the postprocess_agent_response function
        """
        raise NotImplementedError("postprocess_agent_response function must be implemented in a subclass")

class AgentUtils(AgentUtilsBase):
    def __init__(self):
        super().__init__()

    def parse_reasoning_steps(self, text: str):
        """
        Parse a string containing Thought/Action/Observation steps (including multi-line)
        and return a list of dictionaries of the form:
        
        [
            {
                "1": {
                    "Thought": "...",
                    "Action": "...",  # Only content inside backticks (if present)
                    "Observation": "..."
                }
            },
            {
                "2": {
                    "Thought": "...",
                    "Action": "...",
                    "Observation": "..."
                }
            },
            ...
        ]
        """
        # Regex pattern to match lines that start with "Thought X:", "Action X:", or "Observation X:".
        pattern = re.compile(r'^(Thought|Action|Observation)\s+(\d+):', re.MULTILINE)

        # This dictionary will accumulate:
        # data_dict[step_number] = {"Thought": ..., "Action": ..., "Observation": ...}
        data_dict = {}

        # We'll track the current label (Thought/Action/Observation) and step number
        current_label = None
        current_step = None
        last_pos = 0

        # Find all pattern occurrences in the text
        matches = list(pattern.finditer(text))

        for match in matches:
            # If we already have a label in progress, we can record its content
            if current_label is not None:
                # Slice the text from the last match's end to the start of this new match
                content = text[last_pos:match.start()].strip()
                # Store that content in data_dict
                data_dict[current_step][current_label] = content

            # Extract the new label and step
            label = match.group(1)       # "Thought", "Action", or "Observation"
            step = match.group(2)        # e.g. "1", "2", "3"

            # Ensure a dict for this step
            if step not in data_dict:
                data_dict[step] = {"Thought": None, "Action": None, "Observation": None}

            # Update current label/step
            current_label = label
            current_step = step
            # We'll slice from here next time
            last_pos = match.end()

        # Handle the final block after the last match
        if current_label is not None:
            content = text[last_pos:].strip()
            data_dict[current_step][current_label] = content

        # Post-process:
        #  - For each step, extract only the text inside triple backticks for "Action".
        for step_number in data_dict:
            action_text = data_dict[step_number]["Action"]
            if action_text:
                # Extract content inside triple backticks
                data_dict[step_number]["Action"] = self.extract_code(action_text)

        # Convert our dictionary to the desired list-of-dicts structure
        structured_data = []
        for step_number in sorted(data_dict.keys(), key=lambda x: int(x)):
            structured_data.append({step_number: data_dict[step_number]})

        return structured_data
    
    def postprocess_agent_response(self, response):
        """
        Extract Thought and Action, then extract the dict from Action.

        Return:
        - thought: str
        - action: dict
        - is_valid: bool
        """
        parsed_codes = self.parse_reasoning_steps(response)

        thoughts = []
        actions = []
        for steps in parsed_codes:
            for step_idx, step in steps.items():
                thought = f"Thought {step_idx}: {step['Thought']}"
                action = eval(self.extract_code(step['Action']))

                assert "function" in action, f"Action does not contain 'function' key: {action}"
                assert "parameters" in action, f"Action does not contain 'parameters' key: {action}"

                thoughts.append(thought)
                actions.append(action)

        return thoughts, actions
    
def print_code(codes):
    for idx, step in enumerate(codes):
        print(f"{colored(step['thought'], 'blue')}")
        print(colored(f"Action {idx+1}:\n```\n{step['action']}\n```", 'red'))
        print(colored(f"Observation {idx+1}: {step['observation']}", 'yellow'))
        # print(f"{'-'*60}")



================================================
FILE: flashrag/pipeline/reasoning_pipeline.py
================================================
import re
from tqdm import tqdm
from typing import List, Tuple, Dict, Optional, Union
import math
import json
import numpy as np
import copy
# from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
from flashrag.utils import get_retriever, get_generator, selfask_pred_parse, ircot_pred_parse
from flashrag.pipeline import BasicPipeline
from flashrag.pipeline.ReaRAG_utils import AgentUtils
from flashrag.dataset.utils import get_batch_dataset, merge_batch_dataset
from flashrag.prompt import PromptTemplate
from flashrag.prompt import get_generate_final_answer_message,get_generate_intermediate_answer_message,get_generate_subquery_message
from flashrag.utils.utils import extract_between,extract_between_all


class ReasoningPipeline(BasicPipeline):
    system_prompt=""
    user_prompt=(
        "The User asks a question, and the Assistant solves it.\n"
        "The Assistant first thinks about the reasoning process in the mind and then provides the User with the final answer.\n"
        "The output format of reasoning process and final answer are enclosed within <think> </think> and <answer> </answer> tags, "
        "respectively, i.e., \"<think> reasoning process here </think>\\n\\n<answer> final answer here </answer>\".\n"
        "During the thinking process, the Assistant can perform searching for uncertain knowledge if necessary with the format of "
        "\"<|begin_of_query|> search query (only keywords) here <|end_of_query|>\". **A query must involve only a single triple**.\n"
        "Then, the system will provide the Assistant with helpful information with the format of "
        "\"<|begin_of_documents|> ...search results... <|end_of_documents|>\".\n\n"
        "User:{question}\n"
        "Assistant: <think>"
    )


    def __init__(self, config, 
        prompt_template=None, 
        max_retrieval_num=5, 
        begin_of_query_token="<|begin_of_query|>",
        end_of_query_token="<|end_of_query|>",
        begin_of_documents_token="<|begin_of_documents|>",
        end_of_documents_token="<|end_of_documents|>",
        begin_of_answer_token="<answer>",
        end_of_answer_token='</answer>',
        retriever=None, 
        generator=None,
    ):
        if prompt_template is None:
            prompt_template = PromptTemplate(
                config=config,
                system_prompt=self.system_prompt,
                user_prompt=self.user_prompt
            )
        super().__init__(config, prompt_template)

        if generator is None:
            self.generator = get_generator(config)
        else:
            self.generator = generator
        if retriever is None:
            self.retriever = get_retriever(config)
        else:
            self.retriever = retriever

        self.max_retrieval_num = max_retrieval_num

        self.begin_of_query_token = begin_of_query_token
        self.end_of_query_token = end_of_query_token
        self.begin_of_documents_token = begin_of_documents_token
        self.end_of_documents_token = end_of_documents_token
        self.begin_of_answer_token = begin_of_answer_token
        self.end_of_answer_token = end_of_answer_token

        self.stop_tokens = ["<|im_end|>", "<|endoftext|>", self.end_of_answer_token, self.end_of_query_token]
    
    def _retrieved_docs_to_string(self, retrieved_docs: List[Dict]):
        format_doc_string = ""
        for idx, doc in enumerate(retrieved_docs):
            contents = doc['contents']
            title = contents.split('\n')[0]
            text = '\n'.join(contents.split('\n')[1:])
            doc_string = f"Title: {title} Text: {text}"
            doc_string = re.sub(r'^\d+\s+', '', doc_string)
            format_doc_string += f'({idx+1}){doc_string}\n'
        format_doc_string = f'\n\n{self.begin_of_documents_token}\n{format_doc_string}\n{self.end_of_documents_token}\n\n'
        return format_doc_string

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        prompts = [self.prompt_template.get_string(question=question) for question in dataset.question]
        dataset.update_output('prompt', prompts)
        dataset.update_output('finish_flag', [False] * len(prompts))
        dataset.update_output('retrieval_results', [{} for _ in range(len(prompts))])
        dataset.update_output('retrieved_times', [0] * len(prompts))

        # Logic of reasoning
        for current_step_idx in range(self.max_retrieval_num + 1):
            exist_items = [item for item in dataset if item.finish_flag == False]
            exist_prompts = [item.prompt for item in exist_items]
            
            print(f"Current step: {current_step_idx}, exist_items: {len(exist_items)}")

            if len(exist_items) == 0:
                print("All prompts are finished")
                break
            if current_step_idx == self.max_retrieval_num:
                print("Max retrieval number reached")
                for item in exist_items:
                    item.pred = 'No valid answer found'
                    item.finish_flag = True
                    item.finish_reason = 'Reach max retrieval number'
                break

            step_outputs = self.generator.generate(exist_prompts, stop=self.stop_tokens)
            step_query_list = [] # store generated queries for retrieval

            # parse each sample's step output
            for item, step_output in zip(exist_items, step_outputs):
                item.prompt = item.prompt + step_output.strip()
                if self.end_of_answer_token in step_output and (step_output.endswith(self.end_of_answer_token) or step_output.endswith("<|endoftext|>")):
                    item.pred = str(extract_between(step_output, self.begin_of_answer_token, self.end_of_answer_token))
                    item.finish_flag = True
                    item.finish_reason = "Finished"
                
                elif self.begin_of_query_token in step_output and step_output.endswith(self.end_of_query_token):
                    query = extract_between(step_output, self.begin_of_query_token, self.end_of_query_token)
                    if query is not None:
                        step_query_list.append({'item': item, 'query': query})
                    else:
                        item.pred = 'No valid answer found'
                        item.finish_flag = True
                        item.finish_reason = 'Query instruction error'

                else:
                    item.pred = step_output.strip()
                    item.finish_flag = True
                    item.finish_reason = 'Normal finish without answer pattern'
                
            # do retrieval and add retrieved docs to prompt
            if len(step_query_list) > 0:
                retrieved_docs = self.retriever.batch_search([it['query'] for it in step_query_list])
                for it, item_retrieved_docs in zip(step_query_list, retrieved_docs):
                    item = it['item']
                    query = it['query']
                    item.retrieval_results[item.retrieved_times] = {'query': query,'docs': copy.copy(item_retrieved_docs)}
                    #item.retrieved_docs += [item_retrieved_docs]
                    format_doc_string = self._retrieved_docs_to_string(item_retrieved_docs)
                    item.prompt += format_doc_string
                    item.retrieved_times += 1

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset


class SimpleDeepSearcherPipeline(ReasoningPipeline):
    system_prompt=""
    user_prompt=(
        "You are a reasoning assistant with the ability to perform web searches to help "
        "you answer the user's question accurately. You have special tools:\n\n"
        "- To perform a search: write <|begin_search_query|> your query here <|end_search_query|>.\n"
        "Then, the system will search and analyze relevant web pages, then provide you with helpful information in the format <|begin_search_result|> ...search results... <|end_search_result|>.\n\n"
        "Whenever you encounter a topic, fact, or piece of information you are uncertain about or need further details on, please perform a search to gather more accurate, up-to-date, or specific information. You can repeat the search process multiple times if necessary. The maximum number of search attempts is limited to {MAX_SEARCH_LIMIT}.\n\n"
        "Once you have all the information you need, continue your reasoning.\n\n"
        "Remember:\n"
        "- Use <|begin_search_query|> to request a web search and end with <|end_search_query|>.\n"
        "- When done searching, continue your reasoning.\n"
        "- Do not generate <|begin_search_result|> and <|end_search_result|> tags yourself.\n\n"
        "Please answer the following question. You should think step by step to solve it.\n\n"
        "Provide your final answer in the format \\boxed{{YOUR_ANSWER}}.\n\n"
        "Question:\n{question}\n\n"
    )
    def __init__(self, config,
        prompt_template=None, 
        max_retrieval_num=10, 
        begin_of_query_token="<|begin_search_query|>",
        end_of_query_token="<|end_search_query|>",
        begin_of_documents_token="<|begin_search_result|>",
        end_of_documents_token="<|end_search_result|>",
        begin_of_answer_token=None,
        end_of_answer_token=None,
        retriever=None, 
        generator=None,
    ):
        super().__init__(config,
                         prompt_template=prompt_template,
                         max_retrieval_num=max_retrieval_num, 
                         begin_of_query_token=begin_of_query_token,
                         end_of_query_token=end_of_query_token,
                         begin_of_documents_token=begin_of_documents_token,
                         end_of_documents_token=end_of_documents_token,
                         begin_of_answer_token=begin_of_answer_token,
                         end_of_answer_token=end_of_answer_token,
                         retriever=retriever, 
                         generator=generator,
                         )
        self.stop_tokens = ["<|im_end|>", "<|endoftext|>", self.end_of_query_token]

    def truncate_prompt(self, prompt ,max_len):
            assert isinstance(prompt, str)
            try:
                tokenized_prompt = self.prompt_template._get_tokenizer().encode(prompt, truncation=False, return_tensors="pt").input_ids[0]
            except:
                tokenized_prompt = self.prompt_template._get_tokenizer().encode(prompt, truncation=False, return_tensors="pt")[0]

            if len(tokenized_prompt) >= max_len:
                print(f"The doc length is greater than the maximum length ({len(tokenized_prompt)} > {max_len}) and has been truncated!")
                half = int(max_len / 2) - 20
                prompt = self.prompt_template._get_tokenizer().decode(tokenized_prompt[:half], skip_special_tokens=True) + \
                        self.prompt_template._get_tokenizer().decode(tokenized_prompt[-half:], skip_special_tokens=True)
            return prompt

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        prompts = [self.prompt_template.get_string(MAX_SEARCH_LIMIT=self.max_retrieval_num, question=question) for question in dataset.question]
        dataset.update_output('prompt', prompts)
        dataset.update_output('finish_flag', [False] * len(prompts))
        dataset.update_output('retrieval_results', [{} for _ in range(len(prompts))])
        dataset.update_output('retrieved_times', [0] * len(prompts))
        
        for current_step_idx in range(self.max_retrieval_num + 1):
            exist_items = [item for item in dataset if item.finish_flag == False]
            exist_prompts = [self.prompt_template.truncate_prompt(item.prompt) for item in exist_items]
            
            print(f"Current step: {current_step_idx}, exist_items: {len(exist_items)}")

            if len(exist_items) == 0:
                print("All prompts are finished")
                break
            if current_step_idx == self.max_retrieval_num:
                print("Max retrieval number reached")
                for item in exist_items:
                    item.pred = 'No valid answer found'
                    item.finish_flag = True
                    item.finish_reason = 'Reach max retrieval number'
                break

            step_outputs = self.generator.generate(exist_prompts, stop=self.stop_tokens)
            step_query_list = [] # store generated queries for retrieval

            # parse each sample's step output
            for item, step_output in zip(exist_items, step_outputs):
                item.prompt = item.prompt + step_output.strip()
                if "\\boxed" in step_output:
                    item.pred = str(extract_between(step_output, "\\boxed{", "}"))
                    item.finish_flag = True
                    item.finish_reason = "Finished"
                elif self.begin_of_query_token in step_output and step_output.endswith(self.end_of_query_token):
                    query = extract_between(step_output, self.begin_of_query_token, self.end_of_query_token)
                    if query is not None:
                        step_query_list.append({'item': item, 'query': query})
                    else:
                        item.pred = 'No valid answer found'
                        item.finish_flag = True
                        item.finish_reason = 'Query instruction error'
                else:
                    item.pred = step_output.strip()
                    item.finish_flag = True
                    item.finish_reason = 'Normal finish without answer pattern'

            if len(step_query_list) > 0:
                retrieved_docs = self.retriever.batch_search([it['query'] for it in step_query_list])
                for it, item_retrieved_docs in zip(step_query_list, retrieved_docs):
                    item = it['item']
                    query = it['query']
                    item.retrieval_results[item.retrieved_times] = {'query': query,'docs': copy.copy(item_retrieved_docs)}
                    format_doc_string = self.truncate_prompt(self._retrieved_docs_to_string(item_retrieved_docs),3000)
                    item.prompt += format_doc_string
                    item.retrieved_times += 1

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset  

class SearchR1Pipeline(ReasoningPipeline):
    system_prompt=""
    user_prompt=(
        "Answer the given question. "
        "You must conduct reasoning inside <think> and </think> first every time you get new information. "
        "After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. "
        "You can search as many times as your want. "
        "If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: {question}\n"
    )
    
    def __init__(self, config,
        prompt_template=None, 
        max_retrieval_num=5, 
        begin_of_query_token="<search>",
        end_of_query_token="</search>",
        begin_of_documents_token="<information>",
        end_of_documents_token="</information>",
        begin_of_answer_token="<answer>",
        end_of_answer_token='</answer>',
        retriever=None, 
        generator=None,
    ):
        super().__init__(config,
                         prompt_template=prompt_template,
                         max_retrieval_num=max_retrieval_num, 
                         begin_of_query_token=begin_of_query_token,
                         end_of_query_token=end_of_query_token,
                         begin_of_documents_token=begin_of_documents_token,
                         end_of_documents_token=end_of_documents_token,
                         begin_of_answer_token=begin_of_answer_token,
                         end_of_answer_token=end_of_answer_token,
                         retriever=retriever, 
                         generator=generator,
                         )
        self.stop_tokens = ["</search>", " </search>", "</search>\n", " </search>\n", "</search>\n\n", " </search>\n\n", "</answer>", "</answer>\n", "</answer>\n\n","<|endoftext|>","<|im_end|>"]

    def _retrieved_docs_to_string(self, retrieved_docs: List[Dict]):
        format_doc_string = ""
        for idx, doc in enumerate(retrieved_docs):
            contents = doc['contents']
            title = contents.split('\n')[0]
            text = '\n'.join(contents.split('\n')[1:])
            format_doc_string += f"Doc {idx+1}(Title: {title}) {text}\n"
        format_doc_string = f'\n\n{self.begin_of_documents_token}\n{format_doc_string}\n{self.end_of_documents_token}\n\n'
        return format_doc_string       
    
    
class AutoRefinePipeline(SearchR1Pipeline):
    system_prompt=""
    user_prompt = (
        "You are a helpful assistant who is good at answering questions with multi-turn search engine calling. "
        "To answer questions, you must first reason through the available information using <think> and </think>. "
        "If you identify missing knowledge, you may issue a search request using <search> query </search> at any time. "
        "The retrieval system will provide you with the three most relevant documents enclosed in<documents> and </documents>. "
        "After each search, you need to summarize and refine the existing documents in <refine> and </refine>. You may send multiple search requests if needed. "
        "Once you have sufficient information, provide a concise final answer using <answer> and </answer>.\n"
        "<user> Question: {question} </user>"
    )
    
    
    def __init__(self, config,
        prompt_template=None, 
        max_retrieval_num=5, 
        begin_of_query_token="<search>",
        end_of_query_token="</search>",
        begin_of_documents_token="<documents>",
        end_of_documents_token="</documents>",
        begin_of_answer_token="<answer>",
        end_of_answer_token='</answer>',
        retriever=None, 
        generator=None,
    ):
        super().__init__(config,
                         prompt_template=prompt_template,
                         max_retrieval_num=max_retrieval_num, 
                         begin_of_query_token=begin_of_query_token,
                         end_of_query_token=end_of_query_token,
                         begin_of_documents_token=begin_of_documents_token,
                         end_of_documents_token=end_of_documents_token,
                         begin_of_answer_token=begin_of_answer_token,
                         end_of_answer_token=end_of_answer_token,
                         retriever=retriever, 
                         generator=generator,
                         )
        self.stop_tokens = ["</search>", " </search>", "</search>\n", " </search>\n", "</search>\n\n", " </search>\n\n", "</answer>", "</answer>\n", "</answer>\n\n","<|endoftext|>","<|im_end|>"]    
            
            
class O2SearcherPipeline(ReasoningPipeline):
    system_prompt=(
        "As a expert researcher, provide comprehensive key findings for open-ended queries and precise answers to other specific questions. "
        "Each time you receive new information, you MUST first engage in reasoning within the <think> and </think> tags. "
        "After reasoning, if you realize that you lack certain knowledge, you can invoke a SEARCH action with distinct queries (one to five) using the <search>\n<query>QUERY</query>\n<query>QUERY</query>\n</search> format to obtain relevant learnings, "
        "which will be presented between the <learnings> and </learnings> tags.\n "
        "You are allowed to perform searches as many times as necessary. If you determine that no additional external knowledge is required, you can directly present the output within the <answer> and </answer> tags."
    )
    user_prompt='''Initial query:{question}'''
    error_prompt='''The response you attempted before is invalid. If you plan to execute actions like SEARCH, you need to enclose the SEARCH queries within the <search> and </search> tags. Furthermore, the required queries for the SEARCH action should be placed between the <query> and </query> tags. Moreover, if you wish to present the final output for the initial query, you must wrap the result within the <answer> and </answer> tags.
'''
    extra_prompt='''Search learnings: <learnings>{learning_str}</learnings>.
'''
    
    def __init__(self, config,
        prompt_template=None, 
        max_retrieval_num=5, 
        begin_of_query_token="<query>",
        end_of_query_token="</query>",
        begin_of_search_token="<search>",
        end_of_search_token="</search>",
        begin_of_documents_token="<learnings>",
        end_of_documents_token="</learnings>",
        begin_of_answer_token="<answer>",
        end_of_answer_token='</answer>',
        retriever=None, 
        generator=None,
    ):
        super().__init__(config,
                         prompt_template=prompt_template,
                         max_retrieval_num=max_retrieval_num, 
                         begin_of_query_token=begin_of_query_token,
                         end_of_query_token=end_of_query_token,
                         begin_of_documents_token=begin_of_documents_token,
                         end_of_documents_token=end_of_documents_token,
                         begin_of_answer_token=begin_of_answer_token,
                         end_of_answer_token=end_of_answer_token,
                         retriever=retriever, 
                         generator=generator,
                         )
        self.begin_of_search_token = begin_of_search_token
        self.end_of_search_token = end_of_search_token
        self.tokenizer = self.prompt_template._get_tokenizer()
        self.stop_tokens = ["</search>\n", "</search>", "</search>\n\n", "</answer>", "</answer>\n", "</answer>\n\n", "></search"]
        
    def run(self, dataset, do_eval=True, pred_process_fun=None):
        # prompts = [self.prompt_template.get_string(question=question) for question in dataset.question]
        messagess = [[{'role':'system','content':self.system_prompt},{'role':'user','content':self.user_prompt.format(question=question)}] for question in dataset.question]
        prompts = [self.prompt_template.get_string(messages=messages) for messages in messagess]
        dataset.update_output('messages',messagess)
        dataset.update_output('prompt', prompts)
        dataset.update_output('finish_flag', [False] * len(prompts))
        dataset.update_output('retrieval_results', [{} for _ in range(len(prompts))])
        dataset.update_output('retrieved_times', [0] * len(prompts))

        # Logic of reasoning
        for current_step_idx in range(self.max_retrieval_num + 1):
            exist_items = [item for item in dataset if item.finish_flag == False]
            exist_prompts = [item.prompt for item in exist_items]
            
            print(f"Current step: {current_step_idx}, exist_items: {len(exist_items)}")

            if len(exist_items) == 0:
                print("All prompts are finished")
                break
            if current_step_idx == self.max_retrieval_num:
                print("Max retrieval number reached")
                for item in exist_items:
                    item.pred = 'No valid answer found'
                    item.finish_flag = True
                    item.finish_reason = 'Reach max retrieval number'
                break

            step_outputs = self.generator.generate(exist_prompts, stop=self.stop_tokens)
            step_query_list = [] # store generated queries for retrieval

            # parse each sample's step output
            for item, step_output in zip(exist_items, step_outputs):
                item.prompt = item.prompt + step_output.strip()
                if self.end_of_answer_token in step_output and ((self.end_of_answer_token in step_output) or step_output.endswith("<|endoftext|>")):
                    item.pred = str(extract_between(step_output, self.begin_of_answer_token, self.end_of_answer_token))
                    item.finish_flag = True
                    item.finish_reason = "Finished"
                
                elif self.begin_of_search_token in step_output:
                    if "</search>" not in step_output:
                        step_output += '>'
                    queries = extract_between(step_output, self.begin_of_search_token, self.end_of_search_token)
                    if queries:
                        queries = extract_between_all(queries, self.begin_of_query_token, self.end_of_query_token)                    
                    if queries is not None:
                        item.messages.append({'role':'user','content':step_output.strip()})
                        item.prompt = self.prompt_template.get_string(messages=item.messages)
                        step_query_list.append({'item': item, 'queries': queries, 'item_query_num': len(queries)})
                    else:
                        item.pred = 'No valid answer found'
                        item.finish_flag = True
                        item.finish_reason = 'Query instruction error'

                else:
                    item.pred = step_output.strip()
                    item.messages.append({'role':'user','content':self.error_prompt})
                    item.prompt = self.prompt_template.get_string(messages=item.messages)
                    # item.finish_flag = False
                    # item.finish_reason = 'Normal finish without answer pattern'
                
            # do retrieval and add retrieved docs to prompt
            if len(step_query_list) > 0:
                # print(dataset.question)
                _retrieved_docs = self.retriever.batch_search(sum([it['queries'] for it in step_query_list],[]))
                retrieved_docs = []
                doc_index = 0
                for it in step_query_list:
                    item=it['item']
                    item_query_num = it['item_query_num']
                    item_retrieved_docs = sum(_retrieved_docs[doc_index:doc_index+item_query_num],[])
                    doc_index += item_query_num
                    retrieved_docs.append(item_retrieved_docs)
                for it, item_retrieved_docs in zip(step_query_list, retrieved_docs):
                    item = it['item']
                    queries = it['queries']
                    item.retrieval_results[item.retrieved_times] = {'queries': queries,'docs': copy.copy(item_retrieved_docs)}
                    # print(item_retrieved_docs)
                    learning_str = self._retrieved_docs_to_string(item_retrieved_docs)
                    extra_info = self.extra_prompt.format(learning_str=learning_str)
                    item.messages.append({'role':'user','content':extra_info})
                    item.prompt = self.prompt_template.get_string(messages=item.messages)
                    item.retrieved_times += 1

        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset


class ReaRAGPipeline(BasicPipeline):
    rearag_system_prompt = rearag_system_prompt = '''Your task is to solve a question answering task. To improve your solving accuracy, please conduct reasoning process interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action are in the form of function, there are two types:

# Available functions:
(1) search
{
    "name": "search",
    "description": "It can help you find useful information through the internet or local knowledge base. You can use this tool to access external knowledge",
    "parameters": {
        "type": "object",
        "properties": {
            "query": {
                "description": "what you want to search"
            }
        },
        "required": [
            "query"
        ]
    }
}

(2) finish
{
    "name": "finish",
    "description": "You can use this function to make a conclusion from the reasoning process and give the final answer. The reasoining process is completed after this `finish` function is called",
    "parameters": {
        "type": "object",
        "properties": {
            "answer": {
                "description": "the final answer"
            }
        },
        "required": [
            "answer"
        ]
    }
}

Please follow the format strictly.
'''
    user_prompt = '''{question}'''
    long_ans_prompt = "If the Question is comparison type, do not refer the given Context, else, answer the Question based on the given Contexts.\n\nContext: {}\n\nQuestion: {}\n\nAnswer:"
    extract_short_ans_prompt = '''The Reference Answer is the final answer to the question. It's the final deterministic answer, your task is to give concise version of it. Only give me the short answer and do not output any other words.
[Question]
{question}
[Reference answer]
{reference_ans}

Only give me the short answer and do not output any other words. For yes, or no answer, only answer it short. Give the shortest answer possible.
'''

    def __init__(self,config,
        prompt_template=None,
        max_iter_num=15,
        begin_of_function_token="```",
        end_of_function_token="```",
        retriever=None, 
        generator=None,):
        if prompt_template is None:
            prompt_template = PromptTemplate(
                config=config,
                system_prompt=self.rearag_system_prompt,
                user_prompt=self.user_prompt
            )
        super().__init__(config, prompt_template)

        if generator is None:
            self.generator = get_generator(config)
        else:
            self.generator = generator
        if retriever is None:
            self.retriever = get_retriever(config)
        else:
            self.retriever = retriever
        self.max_iter_num = max_iter_num
        self.begin_of_function_token = begin_of_function_token
        self.end_of_function_token = end_of_function_token
        self.AgentUtils = AgentUtils()
        self.sample_generate_params = {
            'temperature': 0,
            'top_p': 0.85,
            'stop': ["<|user|>", "<|observation|>", "<|assistant|>"],
        }
        self.rag_generate_params = {
            'top_p': 0.7,
            'temperature': 0,
            'stop': ["<|user|>", "<|endoftext|>", "<|assistant|>"]
        }
        
    def template_doc_and_tempquery(self, retrieved_docs: List[Dict], question:str):
        chunks = []
        for _ in retrieved_docs:
            if _ not in chunks:
                chunks.append(_['contents'])
        prompt = self.long_ans_prompt.format('\n\n'.join(chunks), question)
        return prompt
            
            
    def small_batch_run(self, dataset, do_eval=True, pred_process_fun=None):
        dataset.messages = [[{"role": "system", "content": self.rearag_system_prompt},
                            {"role": "user", "content": question},
                            {"role": "assistant", "content": ''},] for question in dataset.question]
        prompts = [self.prompt_template.get_string(messages=messages) for messages in dataset.messages]
        dataset.update_output('messages',dataset.messages)
        dataset.update_output('prompt',prompts)
        dataset.update_output('finish_flag',[False]*len(prompts))
        dataset.update_output('status',['start']*len(prompts))
        dataset.update_output('retrieval_results',[{} for _ in range(len(prompts))])
        dataset.update_output('retrieved_times',[0]*len(prompts))
        
        for current_step_idx in range(self.max_iter_num + 1):
            exist_items = [item for item in dataset if item.finish_flag == False]
            exist_prompts = [item.prompt for item in exist_items]
            print(f"Current step: {current_step_idx}, exist_items: {len(exist_items)}")

            if len(exist_items) == 0:
                print("All prompts are finished")
                break
            if current_step_idx == self.max_iter_num:
                print("Max retrieval number reached")
                for item in exist_items:
                    item.pred = "No valid answer found"
                    item.finish_flag = True
                    item.finish_reason = "Reach max retrieval number"
                break
            
            step_outputs = self.generator.generate(exist_prompts,**self.sample_generate_params)
            # print(step_outputs)           
            
            step_query_list = []
            
            for item, step_output in zip(exist_items,step_outputs):
                try:
                    thoughts, actions = self.AgentUtils.postprocess_agent_response(step_output)
                except Exception as e:
                    print(f"Error in postprocess_agent_response: {e}")
                    item.status = "repeat"
                    continue
                try:
                    thought, action = thoughts[0], actions[0]
                except:
                    item.status = "repeat"
                    continue
                action_type = action["function"]
                if action_type not in ["search","finish"]:
                    item.status = "repeat"
                    continue
                if action_type == "finish":
                    reference_ans = action["parameters"]["answer"]
                    prompt = self.extract_short_ans_prompt.format(question=item.question,reference_ans=reference_ans)
                    final_message = [{"role":"user", "content":prompt}]
                    prompt = self.prompt_template.get_string(messages=final_message)
                    final_answer = self.generator.generate(prompt,**self.rag_generate_params)[0]
                    item.finish_flag = True
                    item.pred = final_answer
                    item.finish_reason = "Finished"
                if action_type == "search":
                    item.status = "search"
                    query = action["parameters"]["query"]
                    step_query_list.append({'item': item, 'query': query})
                    item.messages[-1] = {"role": "assistant", "content": step_output}
            
            # å¤„ç†repeatçŠ¶æ€çš„items
            repeat_items = [item for item in exist_items if item.status == "repeat"]
            for item in repeat_items:
                # é‡ç½®æ¶ˆæ¯åˆ°åˆå§‹çŠ¶æ€
                item.messages = [{"role": "system", "content": self.rearag_system_prompt},
                               {"role": "user", "content": item.question},
                               {"role": "assistant", "content": ''}]
                item.prompt = self.prompt_template.get_string(messages=item.messages)
                item.status = "repeat"

            if len(step_query_list) > 0:
                retrieved_docs = self.retriever.batch_search([it['query'] for it in step_query_list])
                temp_prompts = []
                for it, item_retrieved_docs in zip(step_query_list, retrieved_docs):
                    item = it['item']
                    query = it['query']
                    item.retrieval_results[item.retrieved_times] = {'query': query,'docs': copy.copy(item_retrieved_docs)}
                    #item.retrieved_docs += [item_retrieved_docs]
                    temp_prompt = self.template_doc_and_tempquery(item_retrieved_docs,query)
                    temp_prompts.append(temp_prompt)
                    item.retrieved_times += 1
                
                observations = self.generator.generate(temp_prompts)
                
                for observation, it in zip(observations, step_query_list):
                    item = it['item']
                    item.messages.append({"role": "observation", "content": observation})
                    item.messages.append({"role": "assistant", "content": ''})
                    item.prompt = self.prompt_template.get_string(messages=item.messages)
        
        dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset
                    
    def run(self, dataset, do_eval=True, pred_process_fun=None, batch_size=128):
        """æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®é›†
        
        Args:
            dataset: è¾“å…¥æ•°æ®é›†
            do_eval: æ˜¯å¦è¿›è¡Œè¯„ä¼°
            pred_process_fun: é¢„æµ‹ç»“æžœå¤„ç†å‡½æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            å¤„ç†åŽçš„å®Œæ•´æ•°æ®é›†
        """
        processed_datasets = []
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®
        for batch_dataset in get_batch_dataset(dataset,batch_size):
            processed_batch = self.small_batch_run(
                batch_dataset, 
                do_eval=False,  # å…ˆä¸è¿›è¡Œè¯„ä¼°
                pred_process_fun=pred_process_fun
            )
            processed_datasets.append(processed_batch)
        
        # åˆå¹¶æ‰€æœ‰å¤„ç†åŽçš„æ‰¹æ¬¡
        merged_dataset = merge_batch_dataset(processed_datasets)
        
        # åœ¨åˆå¹¶åŽçš„å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°
        if do_eval:
            merged_dataset = self.evaluate(merged_dataset, pred_process_fun=pred_process_fun)
            
        return merged_dataset
                    

            
class CoRAGPipeline(BasicPipeline):
    extract_short_ans_prompt = '''The Reference Answer is the final answer to the question. It's the final deterministic answer, your task is to give concise version of it. Only give me the short answer and do not output any other words.
[Question]
{question}
[Reference answer]
{reference_ans}

Only give me the short answer and do not output any other words. For yes, or no answer, only answer it short. Give the shortest answer possible.
'''
    def __init__(self,config,
                 max_iter_num=4, 
                 prompt_template=None,
                 retriever=None, 
                 generator=None,):
        super().__init__(config, prompt_template)
        if generator is None:
            self.generator = get_generator(config)
        else:
            self.generator = generator
        if retriever is None:
            self.retriever = get_retriever(config)
        else:
            self.retriever = retriever
        self.task_desc = config['task_desc']
        self.max_iter_num = max_iter_num
        self.generate_subquery_params = {
            'temperature': 0,
            'do_sample': False,
            'max_tokens': 64,
            # 'n':15,
        }
        self.generate_intermediate_answer_params = {
            'temperature': 0,
            'do_sample': False,
            'max_tokens': 64,
        }
        self.generate_final_answer_params = {
            'temperature': 0,
            'do_sample': False,
            'max_tokens': 64,
            'skip_special_tokens': True,
        }

    def run(self, dataset, do_eval=True, pred_process_fun=None):
        question = dataset.question
        dataset.update_output('sub_queries',[[] for _ in range(len(question))])
        dataset.update_output('sub_answers',[[] for _ in range(len(question))])
        dataset.update_output('sub_retrieval_results',[{} for _ in range(len(question))])

        dataset.update_output('subquery_messages',[[] for _ in range(len(question))])
        dataset.update_output('intermediate_answer_messages',[[] for _ in range(len(question))])
        dataset.update_output('final_answer_messages',[[] for _ in range(len(question))])

        dataset.update_output('subquery_prompts',[[] for _ in range(len(question))])
        dataset.update_output('intermediate_answer_prompts',[[] for _ in range(len(question))])
        dataset.update_output('final_answer_prompts',[[] for _ in range(len(question))])
        dataset.update_output('finish_flag',[False]*len(question))
        # æœ€å¤§è¿­ä»£æ¬¡æ•°

        for now_iter_num in range(self.max_iter_num):
            # å…ˆå¯¹äºŽæ¯ä¸ªquestionï¼Œç”Ÿæˆä¸€ä¸ªsubquery
            now_items = [item for item in dataset if item.finish_flag == False]
            subquery_messages =  [get_generate_subquery_message(item.question, item.sub_queries, item.sub_answers, self.task_desc) for item in now_items]

            # å…ˆä½¿ç”¨generatorç”Ÿæˆsubquery
            for item, subquery_message in zip(now_items, subquery_messages):
                item.subquery_messages.append(subquery_message)
                item.subquery_prompts.append(self.prompt_template.get_string(messages=item.subquery_messages[-1]))
            subquery_responses = self.generator.generate([item.subquery_prompts[-1] for item in now_items],**self.generate_subquery_params)
            for item, subquery_response in zip(now_items, subquery_responses):
                item.sub_queries.append(self._normalize_subquery(subquery_response))

            temp_retrieval_results = self.retriever.batch_search([item.sub_queries[-1] for item in now_items])

            for item, temp_retrieval_result in zip(now_items, temp_retrieval_results):
                item.sub_retrieval_results[now_iter_num] = temp_retrieval_result

            intermediate_answer_messages = [get_generate_intermediate_answer_message(item.sub_queries[-1], item.sub_retrieval_results[now_iter_num]) for item in now_items]
            intermediate_answer_prompts = [self.prompt_template.get_string(messages=intermediate_answer_message) for intermediate_answer_message in intermediate_answer_messages]

            for item, intermediate_answer_message, intermediate_answer_prompt in zip(now_items, intermediate_answer_messages, intermediate_answer_prompts):
                item.intermediate_answer_messages.append(intermediate_answer_message)
                item.intermediate_answer_prompts.append(intermediate_answer_prompt)
            
            intermediate_answer_responses = self.generator.generate(intermediate_answer_prompts,**self.generate_intermediate_answer_params)
            for item, intermediate_answer_response in zip(now_items, intermediate_answer_responses):
                item.sub_answers.append(intermediate_answer_response)

        # ä½¿ç”¨generatorç”Ÿæˆfinal_answer
        final_answer_messages = [get_generate_final_answer_message(item.question, item.sub_queries, item.sub_answers,self.task_desc,sum(list(item.sub_retrieval_results.values()),[])) for item in dataset]
        final_answer_prompts = [self.prompt_template.get_string(messages=final_answer_message) for final_answer_message in final_answer_messages]

        final_answer_responses = self.generator.generate(final_answer_prompts,**self.generate_final_answer_params)
        dataset.update_output('pred',final_answer_responses)


        if do_eval:
            dataset = self.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)
        return dataset
        
    def _normalize_subquery(self,subquery: str) -> str:
        subquery = subquery.strip()
        if subquery.startswith('"') and subquery.endswith('"'):
            subquery = subquery[1:-1]
        if subquery.startswith('Intermediate query'):
            subquery = re.sub(r'^Intermediate query \d+: ', '', subquery)
        return subquery



================================================
FILE: flashrag/pipeline/replug_utils.py
================================================
# Source: https://github.com/IntelLabs/fastRAG/blob/main/fastrag/generators/replug.py
# The release is licensed under the Apache License 2.0

import warnings
from typing import List, Optional, Union
import transformers
from transformers import (
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    GenerationMixin,
    LogitsProcessor,
    LogitsProcessorList,
    StoppingCriteriaList,
)
import torch
import torch.nn as nn
import torch.distributed as dist
from transformers.generation.stopping_criteria import validate_stopping_criteria
from transformers.generation.utils import (
    SampleDecoderOnlyOutput,
    SampleEncoderDecoderOutput,
    SampleOutput,
)


class REPLUG_Generation(GenerationMixin):
    """Implementing REPLUG-based sampling text generation."""

    def sample(
        self,
        input_ids: torch.LongTensor,
        logits_processor: Optional[LogitsProcessorList] = None,
        stopping_criteria: Optional[StoppingCriteriaList] = None,
        logits_warper: Optional[LogitsProcessorList] = None,
        max_length: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[Union[int, List[int]]] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_scores: Optional[bool] = None,
        return_dict_in_generate: Optional[bool] = None,
        synced_gpus: bool = False,
        streamer: Optional["BaseStreamer"] = None,
        **model_kwargs,
    ) -> Union[SampleOutput, torch.LongTensor]:
        # init values
        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
        if max_length is not None:
            warnings.warn(
                "`max_length` is deprecated in this function, use"
                " `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.",
                UserWarning,
            )
            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)
        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()
        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id
        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None
        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores
        output_attentions = (
            output_attentions if output_attentions is not None else self.generation_config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states
        )
        return_dict_in_generate = (
            return_dict_in_generate
            if return_dict_in_generate is not None
            else self.generation_config.return_dict_in_generate
        )

        # init attention / hidden states / scores tuples
        scores = () if (return_dict_in_generate and output_scores) else None
        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
        cross_attentions = () if (return_dict_in_generate and output_attentions) else None
        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None

        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
        if return_dict_in_generate and self.config.is_encoder_decoder:
            encoder_attentions = model_kwargs["encoder_outputs"].get("attentions") if output_attentions else None
            encoder_hidden_states = (
                model_kwargs["encoder_outputs"].get("hidden_states") if output_hidden_states else None
            )

        # keep track of which sequences are already finished
        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)

        this_peer_finished = False  # used by synced_gpus only
        # auto-regressive generation
        while True:
            if synced_gpus:
                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                # The following logic allows an early break if all peers finished generating their sequence
                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)
                # send 0.0 if we finished, 1.0 otherwise
                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)
                # did all peers finish? the reduced sum will be 0.0 then
                if this_peer_finished_flag.item() == 0.0:
                    break

            # prepare model inputs
            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)

            # forward pass to get next token
            outputs = self(
                **model_inputs,
                return_dict=True,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
            )

            if synced_gpus and this_peer_finished:
                continue  # don't waste resources running the code we don't need

            next_token_logits = outputs.logits[:, -1, :]

            # pre-process distribution
            ### REPLUG - document weighting is done via REPLUGLogitsProcessor
            next_token_scores = logits_processor(input_ids, next_token_logits)
            next_token_scores = logits_warper(input_ids, next_token_scores)

            # Store scores, attentions and hidden_states when required
            if return_dict_in_generate:
                if output_scores:
                    scores += (next_token_scores,)
                if output_attentions:
                    decoder_attentions += (
                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)
                    )
                    if self.config.is_encoder_decoder:
                        cross_attentions += (outputs.cross_attentions,)

                if output_hidden_states:
                    decoder_hidden_states += (
                        (outputs.decoder_hidden_states,) if self.config.is_encoder_decoder else (outputs.hidden_states,)
                    )

            ### REPLUG
            # Sample from the normalized "logits", assuming the REPLUG processor was used!
            probs = nn.functional.softmax(next_token_scores, dim=-1)
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)
            # Lock same next-token for all examples in batch
            next_tokens[:] = next_tokens[0]

            # finished sentences should have their next token be a padding token
            if eos_token_id is not None:
                if pad_token_id is None:
                    raise ValueError("If `eos_token_id` is defined, make sure that `pad_token_id` is defined.")
                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)

            # update generated ids, model inputs, and length for next step
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
            if streamer is not None:
                streamer.put(next_tokens.cpu())
            model_kwargs = self._update_model_kwargs_for_generation(
                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
            )

            # if eos_token was found in one sentence, set sentence to finished
            if eos_token_id_tensor is not None:
                unfinished_sequences = unfinished_sequences.mul(
                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
                )

                # stop when each sentence is finished
                if unfinished_sequences.max() == 0:
                    this_peer_finished = True

            # stop if we exceed the maximum length
            if stopping_criteria(input_ids, scores):
                this_peer_finished = True

            if this_peer_finished and not synced_gpus:
                break

        if streamer is not None:
            streamer.end()

        if return_dict_in_generate:
            if self.config.is_encoder_decoder:
                return SampleEncoderDecoderOutput(
                    sequences=input_ids,
                    scores=scores,
                    encoder_attentions=encoder_attentions,
                    encoder_hidden_states=encoder_hidden_states,
                    decoder_attentions=decoder_attentions,
                    cross_attentions=cross_attentions,
                    decoder_hidden_states=decoder_hidden_states,
                )
            else:
                return SampleDecoderOnlyOutput(
                    sequences=input_ids,
                    scores=scores,
                    attentions=decoder_attentions,
                    hidden_states=decoder_hidden_states,
                )
        else:
            return input_ids


class REPLUGLogitsProcessor(LogitsProcessor):
    """
    Merge logits of different docs in one batch.

    Reference: fastRAG
    """

    def __init__(self, doc_scores: torch.FloatTensor):
        self.num_docs = doc_scores.shape[0]
        # normalize
        doc_scores /= doc_scores.sum()
        self.doc_scores = torch.unsqueeze(doc_scores, 1)  # k*1

    def __call__(self, input_ids, scores):
        # doc_score: k*1, scores: k*vocab_size
        replug_scores = self.doc_scores * scores
        replug_scores = replug_scores.sum(dim=0)  # 1*vocab_size
        replug_scores = torch.tile(replug_scores, (self.num_docs, 1))  # k*vocab_size
        return replug_scores


def load_replug_model(name_or_path):
    class HF_REPLUG:
        "Creates a HF model that inherits from REPLUG_Generation class"

        def __new__(cls, name_or_path, **kwargs):
            return factory(name_or_path).from_pretrained(name_or_path, **kwargs)

    def factory(name_or_path):
        loadedConfig = AutoConfig.from_pretrained(name_or_path)
        try:
            pretrained_class_object = getattr(transformers, loadedConfig.architectures[0])
            if pretrained_class_object not in MODEL_FOR_CAUSAL_LM_MAPPING.values():
                raise ValueError(f"Model {pretrained_class_object} is not used for causal LM generation.")
        except AttributeError:
            raise ValueError("Transformers architecture unknown.")

        class HF(pretrained_class_object, REPLUG_Generation):
            """Wrapper around HuggingFace transformers with REPLUG generation."""

            _keys_to_ignore_on_load_unexpected = [r"cls"]
            _tied_weights_keys = ["lm_head.weight"]

        return HF

    return HF_REPLUG(name_or_path)



================================================
FILE: flashrag/prompt/__init__.py
================================================
from flashrag.prompt.base_prompt import *
from flashrag.prompt.mm_prompt import *
from flashrag.prompt.coRAG_prompt import *



================================================
FILE: flashrag/prompt/base_prompt.py
================================================
from transformers import AutoTokenizer, AutoConfig
import tiktoken
import warnings

class PromptTemplate:
    placeholders = ["reference", "question"]
    base_system_prompt = (
        "Answer the question based on the given document."
        "Only give me the answer and do not output any other words."
        "\nThe following are given documents.\n\n{reference}"
    )
    base_user_prompt = "Question: {question}"

    def __init__(self, config, system_prompt="", user_prompt="", reference_template=None, enable_chat=True):

        self.config = config
        self.is_openai = config["framework"] == "openai"
        self.max_input_len = config['generator_max_input_len']
        if not self.is_openai:
            self.generator_path = config["generator_model_path"]
            model_config = AutoConfig.from_pretrained(self.generator_path, trust_remote_code=True)
            model_name = model_config._name_or_path.lower()
            self.is_chat = False
            if "chat" in model_name or "instruct" in model_name or config['is_reasoning']:
                self.is_chat = True
        else:
            self.is_chat = True
            self.enable_chat = True

        if len(system_prompt) == 0 and len(user_prompt) == 0:
            system_prompt = self.base_system_prompt
            user_prompt = self.base_user_prompt
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt
        self.enable_chat = enable_chat
        self.reference_template = reference_template
        self.tokenizer = None

        # self._check_placeholder()
    def _get_tokenizer(self):
        if self.tokenizer is None:
            if self.is_openai:
                try:
                    self.tokenizer = tiktoken.encoding_for_model(self.config['generator_model'])
                except Exception as e:
                    print("Error: ", e)
                    warnings.warn("This model is not supported by tiktoken. Use gpt-3.5-turbo instead.")
                    self.tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')
            else:
                self.tokenizer = AutoTokenizer.from_pretrained(self.generator_path, trust_remote_code=True)
        return self.tokenizer


    def _check_placeholder(self):
        # check placeholder in prompt
        for holder in self.placeholders:
            flag = False
            for prompt in [self.system_prompt, self.user_prompt]:
                if f"{holder}" in prompt:
                    print(f"Find `{holder}` in template")
                    flag = True
                    break
            if not flag and holder != "reference":
                assert False

    def truncate_prompt(self, prompt):
        if self.is_openai:
            if self.enable_chat:
                truncated_messages = []
                total_tokens = 0
                assert isinstance(prompt, list)
                for message in prompt:
                    role_content = message['content']
                    encoded_message = self._get_tokenizer().encode(role_content)

                    if total_tokens + len(encoded_message) <= self.max_input_len:
                        truncated_messages.append(message)
                        total_tokens += len(encoded_message)
                    else:
                        print(f"The input text length is greater than the maximum length ({total_tokens + len(encoded_message)} > {self.max_input_len}) and has been truncated!")
                        remaining_tokens = self.max_input_len - total_tokens
                        truncated_message = self._get_tokenizer().decode(encoded_message[:remaining_tokens])
                        message['content'] = truncated_message
                        truncated_messages.append(message)
                        break
            else:
                assert isinstance(prompt, str)
                tokenized_prompt = self._get_tokenizer().encode(prompt,allowed_special={'<|endoftext|>'})
                half = int(self.max_input_len / 2)
                truncated_messages = self._get_tokenizer().decode(tokenized_prompt[:half]) + self._get_tokenizer().decode(tokenized_prompt[-half:])

            return truncated_messages

        else:
            assert isinstance(prompt, str)
            try:
                tokenized_prompt = self._get_tokenizer().encode(prompt, truncation=False, return_tensors="pt").input_ids[0]
            except:
                tokenized_prompt = self._get_tokenizer().encode(prompt, truncation=False, return_tensors="pt")[0]

            if len(tokenized_prompt) >= self.max_input_len:
                print(f"The input text length is greater than the maximum length ({len(tokenized_prompt)} > {self.max_input_len}) and has been truncated!")
                half = int(self.max_input_len / 2) - 20
                prompt = self._get_tokenizer().decode(tokenized_prompt[:half], skip_special_tokens=True) + \
                        self._get_tokenizer().decode(tokenized_prompt[-half:], skip_special_tokens=True)
            return prompt



    def get_string(self, question=None, retrieval_result=None, formatted_reference=None, previous_gen=None, messages=None, **params):
        if messages is not None:
            if isinstance(messages, str):
                return self.truncate_prompt(messages)
            if self.is_chat and self.enable_chat:
                if self.is_openai:
                    self.truncate_prompt(messages)
                else:
                    prompt = self._get_tokenizer().apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    return self.truncate_prompt(prompt)
            else:
                prompt = "\n\n".join(
                    [message['content'] for message in messages if message['content']]
                )
                return self.truncate_prompt(prompt)

        if formatted_reference is None:
            if retrieval_result is not None:
                formatted_reference = self.format_reference(retrieval_result)
            else:
                formatted_reference = ""

        input_params = {"question": question, "reference": formatted_reference}
        input_params.update(**params)

        system_prompt = self.system_prompt.format(**input_params)
        user_prompt = self.user_prompt.format(**input_params)

        if self.is_chat and self.enable_chat:
            input = []
            if system_prompt != "":
                input.append({"role": "system", "content": system_prompt})
            if user_prompt != "":
                input.append({"role": "user", "content": user_prompt})
            if not self.is_openai:
                try:
                    input = self._get_tokenizer().apply_chat_template(input, tokenize=False, add_generation_prompt=True)
                except:
                    print("Warning: the generator tokenizer not support `apply_chat_template`")
                    input = system_prompt + '\n\n' + user_prompt
        else:
            input = "\n\n".join([prompt for prompt in [system_prompt, user_prompt] if prompt != ""])

        if previous_gen is not None and previous_gen not in ["", " "]:
            if self.is_openai:
                if self.enable_chat:
                    input.append({"role": 'assistant', 'content': previous_gen})
                else:    
                    input += f'<|endoftext|>{previous_gen}'
                
            else:
                input += previous_gen

        return self.truncate_prompt(input)

    def get_string_with_varying_examplars(
        self,
        question,
        retrieval_result=None,
        formatted_reference=None,
        previous_gen=None,
        examplars=[],
        tokenizer=None,
        max_length=2048,
        **params,
    ):
        """
        Select the maximum number of examplars that can be placed in the prompt
        """

        final_examplars = None
        num = len(examplars)
        while len(examplars) > 0:
            for num in range(len(examplars), 0, -1):
                possible_prompt = self.get_string(
                    question=question,
                    retrieval_result=retrieval_result,
                    formatted_reference=formatted_reference,
                    previous_gen=previous_gen,
                    examplars="\n\n".join(examplars[:num]),
                    **params,
                )

                possible_prompt_tokens = self._get_tokenizer().encode(possible_prompt)
                if len(possible_prompt_tokens) <= max_length:
                    final_examplars = examplars[:num]
                    break
            if final_examplars is None:
                examplars = examplars[1:]
            else:
                break
        if final_examplars is None:
            final_examplars = []

        final_prompt = self.get_string(
            question=question,
            retrieval_result=retrieval_result,
            formatted_reference=formatted_reference,
            previous_gen=previous_gen,
            examplars="\n\n".join(final_examplars[:num]),
            **params,
        )

        return final_prompt

    def format_reference(self, retrieval_result):
        format_reference = ""
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item["contents"]
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            if self.reference_template is not None:
                format_reference += self.reference_template.format(idx=idx, title=title, text=text)
            else:
                format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference



================================================
FILE: flashrag/prompt/coRAG_prompt.py
================================================
from typing import List, Dict, Optional

def get_generate_subquery_message(query: str, past_subqueries: List[str], past_subanswers: List[str], task_desc: str) -> List[Dict]:
    assert len(past_subqueries) == len(past_subanswers)
    past = ''
    for idx in range(len(past_subqueries)):
        past += f"""Intermediate query {idx+1}: {past_subqueries[idx]}
Intermediate answer {idx + 1}: {past_subanswers[idx]}\n"""
    past = past.strip()

    prompt = f"""You are using a search engine to answer the main query by iteratively searching the web. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search engine may not understand complex questions.

## Previous intermediate queries and answers
{past or 'Nothing yet'}

## Task description
{task_desc}

## Main query to answer
{query}

Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else."""

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages


def get_generate_intermediate_answer_message(subquery: str, documents: List[Dict]) -> List[Dict]:
    context = ''
    for idx, doc in enumerate(documents):
        context += f"""{doc['contents']}\n\n"""

    prompt = f"""Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond "No relevant information found" if the documents do not contain useful information.

## Documents
{context.strip()}

## Query
{subquery}

Respond with a concise answer only, do not explain yourself or output anything else."""

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages


def get_generate_final_answer_message(
        query: str, past_subqueries: List[str], past_subanswers: List[str], task_desc: str,
        documents: Optional[List[Dict]] = None
) -> List[Dict]:

    assert len(past_subqueries) == len(past_subanswers)
    past = ''
    for idx in range(len(past_subqueries)):
        past += f"""Intermediate query {idx+1}: {past_subqueries[idx]}
Intermediate answer {idx+1}: {past_subanswers[idx]}\n"""
    past = past.strip()

    context = ''
    if documents:
        for idx, doc in enumerate(documents):
            context += f"""Doc {idx}: {doc['contents']}\n\n"""

    prompt = f"""Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.

## Documents
{context.strip()}

## Intermediate queries and answers
{past or 'Nothing yet'}

## Task description
{task_desc}

## Main query
{query}

Respond with an appropriate answer only, do not explain yourself or output anything else. """

    messages: List[Dict] = [
        {'role': 'user', 'content': prompt}
    ]
    return messages



================================================
FILE: flashrag/prompt/mm_prompt.py
================================================
import re

class MMPromptTemplate:
    BASE_USER_PROMPT = '{reference}\nBased on the above examples, answer the following question. Only give me the final choices.\nQuestion: {question}\nAnswer: '
    def __init__(self, config, system_prompt=None, user_prompt=None):
        self.config = config
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt if user_prompt is not None else self.BASE_USER_PROMPT
    def get_string(self, item):
        question = item.question if item.question is not None else item.text
        question_image = item.image
        # retrieval_result = item.retrieval_result
        try:
            retrieval_result = item.retrieval_result
        except:
            retrieval_result = []

        messages = []
        if self.system_prompt is not None:
            messages.append({"role": "system", "content": self.system_prompt})
        reference_str = ""
        content_list = []
        for idx, item in enumerate(retrieval_result):
            # item is multimodal data or raw text
            if 'image' not in item:
                # raw text item
                reference_str += f'Example {idx+1}: {item["contents"]}\n'
            else:
                content_list.append({'type': 'image', 'image': item['image']})
                reference_str += f'Example {idx+1}: {item["text"]}\n'
        content_list.append({'type': 'image', 'image': question_image})
        content_list.append({'type': 'text', 'text': self.user_prompt.format(question=question, reference=reference_str)})
        messages.append({"role": "user", "content": content_list})
        return messages


class GAOKAOMMPromptTemplate(MMPromptTemplate):
    BASE_USER_PROMPT = "è¯·ä½ åšä¸€é“{subject}é€‰æ‹©é¢˜\nè¯·ä½ ç»“åˆæ–‡å­—å’Œå›¾ç‰‡ä¸€æ­¥ä¸€æ­¥æ€è€ƒ,å¹¶å°†æ€è€ƒè¿‡ç¨‹å†™åœ¨ã€è§£æžã€‘å’Œ<eoe>ä¹‹é—´ã€‚{instruction}\nä¾‹å¦‚ï¼š{example}\nè¯·ä½ ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ä½œç­”ã€‚\nä½ å¯ä»¥å‚è€ƒä¸€äº›çŸ¥è¯†: {reference}ã€‚é¢˜ç›®å¦‚ä¸‹ï¼š{question}"
    INSTRUCTION_DICT = {
        'single_choice': 'ä½ å°†ä»ŽAï¼ŒBï¼ŒCï¼ŒDç­‰é€‰é¡¹ä¸­é€‰å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œå¹¶å†™åœ¨ã€ç­”æ¡ˆã€‘å’Œ<eoa>ä¹‹é—´ã€‚',
        'multiple_choice': 'ä½ å°†ä»ŽAï¼ŒBï¼ŒCï¼ŒDç­‰é€‰é¡¹ä¸­é€‰å‡ºæ‰€æœ‰ç¬¦åˆé¢˜æ„çš„ç­”æ¡ˆï¼Œå¹¶å†™åœ¨ã€ç­”æ¡ˆã€‘å’Œ<eoa>ä¹‹é—´ã€‚'
    }
    EXAMPLE_DICT = {
        'single_choice': 'ã€ç­”æ¡ˆã€‘: A <eoa>\nå®Œæ•´çš„é¢˜ç›®å›žç­”çš„æ ¼å¼å¦‚ä¸‹ï¼š\nã€è§£æžã€‘ ... <eoe>\nã€ç­”æ¡ˆã€‘ ... <eoa>',
        'multiple_choice': 'ã€ç­”æ¡ˆã€‘ AB <eoa>\nå®Œæ•´çš„é¢˜ç›®å›žç­”çš„æ ¼å¼å¦‚ä¸‹ï¼š\nã€è§£æžã€‘ ... <eoe>\nã€ç­”æ¡ˆã€‘... <eoa>'
    }
    def __init__(self, config, system_prompt=None, user_prompt=None):
        self.config = config
        self.system_prompt = system_prompt
        if user_prompt is None:
            self.user_prompt = self.BASE_USER_PROMPT
        else:
            self.user_prompt = user_prompt

    def get_string(self, item):
        question = item.question if item.question is not None else item.text
        question_image = item.image
        question_type = item.question_type
        subject = item.subject
        
        instruction = self.INSTRUCTION_DICT[question_type]
        example = self.EXAMPLE_DICT[question_type]

        messages = []
        if self.system_prompt is not None:
            messages.append({"role": "system", "content": self.system_prompt})
        content_list = []
        if '{reference}' not in self.user_prompt:
            user_prompt = self.user_prompt.format(question=question, instruction=instruction, example=example, subject=subject)
        else:
            retrieval_result = item.retrieval_result
            reference_str = ""
            for idx, item in enumerate(retrieval_result):
                # item is multimodal data or raw text
                if 'image' not in item:
                    # raw text item
                    reference_str += f'å‚è€ƒå†…å®¹ {idx+1}: {item["contents"]}\n'
                else:
                    content_list.append({'type': 'image', 'image': item['image']})
                    reference_str += f'å‚è€ƒå†…å®¹ {idx+1}: {item["text"]}, æ ‡å‡†ç­”æ¡ˆ: {item["golden_answers"][0]}\n'
            user_prompt = self.user_prompt.format(question=question, reference=reference_str, instruction=instruction, example=example, subject=subject)

        content_list.append({'type': 'image', 'image': question_image})
        content_list.append({'type': 'text', 'text': user_prompt})
        messages.append({"role": "user", "content": content_list})
        return messages


class MathVistaPromptTemplate:
    BASE_USER_PROMPT_FREE_FORM = (
        "You are an AI assistant designed to solve mathematical and visual reasoning problems. "
        "Below is a question that requires you to analyze the given information and provide a detailed answer.\n\n"
        "### Reference Information:\n{reference}\n\n"
        "### Question:\n{question}\n\n"
        "### Instructions:\n"
        "Only give me the final answer.\n\n"
        "### Answer:\n"
    )

    BASE_USER_PROMPT_MULTI_CHOICE = (
        "You are an AI assistant designed to solve mathematical and visual reasoning problems. "
        "Below is a multiple-choice question that requires you to analyze the given information and select the correct choice.\n\n"
        "### Reference Information:\n{reference}\n\n"
        "### Question:\n{question}\n\n"
        "### Instructions:\n"
        "Only give me the correct option letter, e.g., A, B, C, D.\n\n"
        "### Answer:\n"
    )

    def __init__(self, config, system_prompt=None, user_prompt_free_form=None, user_prompt_multi_choice=None):
        self.config = config
        self.system_prompt = system_prompt
        self.user_prompt_free_form = user_prompt_free_form if user_prompt_free_form is not None else self.BASE_USER_PROMPT_FREE_FORM
        self.user_prompt_multi_choice = user_prompt_multi_choice if user_prompt_multi_choice is not None else self.BASE_USER_PROMPT_MULTI_CHOICE

    def get_string(self, item, use_retrieval_image=False, use_retrieval_text=False, use_question_image=False):
        question = item.question if item.question is not None else item.text
        question_image = item.image
        question_type = item.question_type  # 'free_form' or 'multi_choice'

        messages = []
        if self.system_prompt is not None:
            messages.append({"role": "system", "content": self.system_prompt})

        content_list = []
        reference_str = ""

        # Add retrieval results as reference (if available)
        try:
            retrieval_result = item.retrieval_result
        except:
            retrieval_result = []
            
        for idx, item in enumerate(retrieval_result):
            if 'image' not in item:
                reference_str += f'Example {idx + 1}: {item["contents"]}\n'
            else:
                content_list.append({'type': 'image', 'image': item['image']})
                reference_str += f'Example {idx + 1}: {item["text"]}\n'

        # Add question image (if available)
        if question_image:
            content_list.append({'type': 'image', 'image': question_image})

        # Format the user prompt based on question type
        if question_type == 'free_form':
            user_prompt = self.user_prompt_free_form.format(question=question, reference=reference_str)
        elif question_type == 'multi_choice':
            user_prompt = self.user_prompt_multi_choice.format(question=question, reference=reference_str)
        else:
            raise ValueError(f"Unsupported question type: {question_type}")

        # Add the formatted user prompt
        content_list.append({'type': 'text', 'text': user_prompt})
        messages.append({"role": "user", "content": content_list})

        return messages
    



================================================
FILE: flashrag/prompt/selfask_examplars.py
================================================
SELF_ASK_PROMPT_SINGLE_HOP = """Given the following question, answer it by providing follow up questions and intermediate answers. If intermediate questions are not necessarry, answer the question directly. You are provided with evidence that can help you arrive at the answer before the question.
#
Context1: The Big Red One: Fuller was a World War II veteran and served with the 1st Infantry Division, which is nicknamed "The Big Red One" for the red numeral "1" on the division's shoulder patch. He received the Silver Star, Bronze Star, and Purple Heart during his service.
Question: how did the big red one get its name
Are follow up questions needed here: No.
So the final answer is: its shoulder patch
#
Context1: Module:Location map/data/Cayman Islands: Module:Location map/data/Cayman Islands is a location map definition used to overlay markers and labels on an equirectangular projection map of Cayman
Question: where are the cayman islands on the map
Are follow up questions needed here: No.
So the final answer is: western Caribbean Sea
#
Context1: Korean War | Combatants, Summary, Years, Map ... - Britannica: After more than a million combat casualties had been suffered on both sides, the fighting ended in July 1953 with Korea still divided into two hostile states. Negotiations in 1954 produced no further agreement, and the front line has been accepted ever since as the de facto boundary between North and South Korea.
Question: who won the war between north korea and south korea
Are follow up questions needed here: No.
So the final answer is: technically still at war
#
Context1: It's Always Sunny in Philadelphia (season 13): The thirteenth season of the American comedy television series It's Always Sunny in Philadelphia premiered on FXX on September 5, 2018.
Question: when does it's always sunny in philadelphia season 13 start
Are follow up questions needed here: No.
So the final answer is: September 5, 2018
#
Context1: You've Got a Friend in Me: "You've Got a Friend in Me" is a song by Randy Newman. Used as the theme song for the 1995 Disney/Pixar animated film Toy Story, it has since become a major ...
Question: who sang you got a friend in me from toy story
Are follow up questions needed here: No.
So the final answer is: Randy Newman
#
Context1: Timeline of space exploration: This is a timeline of space exploration which includes notable achievements, first accomplishments and milestones in humanity's exploration of outer space.
Question: when was the first person sent to space
Are follow up questions needed here: No.
So the final answer is: 12 April 1961
#"""


SELF_ASK_PROMPT_MULTI_HOP = """Given the following question, answer it by providing follow up questions and intermediate answers. If intermediate questions are not necessarry, answer the question directly. You are provided with evidence that can help you arrive at the answer before the question.
#
Context1: Xawery Å»uÅ‚awski: Polish-Russian War (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Å»uÅ‚awski based on the novel Polish-Russian War under the white-red flag by Dorota MasÅ‚owska. So the answer is Xawery Å»uÅ‚awski.
Context2: Xawery Å»uÅ‚awski: Xawery Å»uÅ‚awski ; National Film School in ÅÃ³dÅº Â· 1995â€“present Â· Maria Strzelecka Â· 2.
Question: Who is the mother of the director of film Polish-Russian War (Film)?
Are follow up questions needed here: Yes.
Follow up: Who is the director of the film Polish-Russian War (Film)?
Intermediate answer: The director of the film Polish-Russian War is Xawery Å»uÅ‚awski.
Follow up: Who is the mother of Xawery Å»uÅ‚awski?
Intermediate answer: The mother of Xawery Å»uÅ‚awski is MaÅ‚gorzata Braunek.
So the final answer is: Rick Scott MaÅ‚gorzata Braunek.
#
Context1: 2003: Blind Shaft (Chinese: ç›²äº•; pinyin: MÃ¡ngjÇng) is a 2003 film about a pair of brutal con artists operating in the illegal coal mines of present-day northern China. So the answer is 2003.
Context2: December 2, 1932: Release and reception. The Mask of Fu Manchu opened in New York on December 2, 1932. The film cost a total of $338,000 and had worldwide rentals of $625,000. It had a profit of $62,000. So the answer is December 2, 1932.
Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
Are follow up questions needed here: Yes.
Follow up: When did Blind Shaft come out?
Intermediate answer: Blind Shaft came out in 2003.
Follow up: When did The Mask Of Fu Manchu come out?
Intermediate answer: The Mask Of Fu Manchu came out in 1932.
So the final answer is: The Mask Of Fu Manchu.
#
Context1: John V, Prince of Anhalt-Zerbst: John was the second (but eldest surviving) son of Ernest I, Prince of Anhalt-Dessau, by his wife Margarete, daughter of Henry I, Duke of MÃ¼nsterberg-Oels, and granddaughter of George of PodÄ›brady, King of Bohemia.
Context2: 12 June 1516: Ernest I, Prince of Anhalt-Dessau (died Dessau, 12 June 1516), was a German prince of the House of Ascania and ruler of the principality of Anhalt-Dessau. So the answer is 12 June 1516.
Question: When did John V, Prince Of Anhalt-Zerbst's father die?
Are follow up questions needed here: Yes.
Follow up: Who is the father of John V, Prince Of Anhalt-Zerbst?
Intermediate answer: The father of John V, Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.
Follow up: When did Ernest I, Prince of Anhalt-Dessau die?
Intermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.
So the final answer is: 12 June 1516
#
Context1: El extraÃ±o viaje: El extraÃ±o viaje (English: The Strange Voyage) is a 1964 Spanish black drama film directed by Fernando FernÃ¡n GÃ³mez.
Context2: Love in Pawn: Love in Pawn is a 1953 British comedy film directed by Charles Saunders and starring Bernard Braden, Barbara Kelly and Jeannie Carson.
Context3: 28 August 1921: Fernando FernÃ¡ndez GÃ³mez (28 August 1921 â€“ 21 November 2007) better known as Fernando FernÃ¡n GÃ³mez was a Spanish actor, screenwriter, film director, theater director and member of the Royal Spanish Academy for seven years. So the answer is 28 August 1921.
Context4: Charles Saunders (director): Charles Joel Saunders (8 April 1904 â€“ 20 April 1997) was an English film director and screenwriter who began in the industry as a film editor, and who also contributed to television.
Question: Which film has the director who was born later, El ExtraÃ±o Viaje or Love In Pawn?
Are follow up questions needed here: Yes.
Follow up: Who is the director of El ExtraÃ±o Viaje?
Intermediate answer: The director of El ExtraÃ±o Viaje is Fernando FernÃ¡n GÃ³mez.
Follow up: Who is the director of Love in Pawn?
Intermediate answer: The director of Love in Pawn is Charles Saunders.
Follow up: When was Fernando FernÃ¡n GÃ³mez born?
Intermediate answer: Fernando FernÃ¡n GÃ³mez was born on 28 August 1921.
Follow up: When was Charles Saunders (director) born?
Intermediate answer: Charles Saunders was born on 8 April 1904.
So the final answer is: El ExtraÃ±o Viaje.
#
Context1: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 â€“ 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.
Context2: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 â€“ 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.
Question: Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt's father-in-law?
Are follow up questions needed here: Yes.
Follow up: Who is the husband of Catherine of Pomerania, Countess Palatine of Neumarkt?
Intermediate answer: The husband of Catherine of Pomerania, Countess Palatine of Neumarkt is John, Count Palatine of Neumarkt.
Follow up: Who is the father of John, Count Palatine of Neumarkt?
Intermediate answer: The father of John, Count Palatine of Neumarkt is Rupert III of the Palatinate.
So the final answer is: Rupert III of the Palatinate.
#
Context1: Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Crimen a las tres. Directed by, Luis Saslavsky.
Context2: Elio Petri: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. So the answer is Elio Petri.
Context3: March 20, 1995: Luis Saslavsky (April 21, 1903 â€“ March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. So the answer is March 20, 1995.
Context4: Elio Petri: Final years. In 1981, Petri visited Geneva to direct Arthur Miller\'s new play The American Clock, with Marcello Mastroianni playing the lead role. Petri died of cancer on 10 November 1982. He was 53 years old.
Question: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?
Are follow up questions needed here: Yes.
Follow up: Who is the director of Crimen a las tres?
Intermediate answer: The director of Crimen a las tres is Luis Saslavsky.
Follow up: Who is the director of The Working Class Goes to Heaven?
Intermediate answer: The director of The Working Class Goes to Heaven is Elio Petri.
Follow up: When did Luis Saslavsky die?
Intermediate answer: Luis Saslavsky died on March 20, 1995.
Follow up: When did Elio Petri die?
Intermediate answer: Elio Petri died on 10 November 1982.
So the final answer is: The Working Class Goes to Heaven
#"""



================================================
FILE: flashrag/refiner/__init__.py
================================================
from flashrag.refiner.refiner import *
from flashrag.refiner.kg_refiner import *


================================================
FILE: flashrag/refiner/kg_refiner.py
================================================
import os
import json
from tqdm import tqdm
import numpy as np
import torch
import torch.nn.functional as F
from flashrag.refiner import BaseRefiner
from flashrag.prompt import PromptTemplate
from flashrag.retriever.encoder import Encoder, STEncoder
from flashrag.utils import hash_object


class KGTraceRefiner(BaseRefiner):
    def __init__(self, config, retriever=None, generator=None):
        super().__init__(config)
        self.config = config
        self.input_prompt_flag = False

        default_setting = {
            "num_examplars": 3,
            "max_chain_length": 4,
            "topk_triple_select": 5,  # num of candidate triples
            "num_choices": 20,
            "min_triple_prob": 1e-4,
            "num_beams": 5,  # number of selected prob at each step of constructing chain
            "num_chains": 20,  # number of generated chains
            "n_context": 5,  # number of used chains in generation
            "context_type": "triples",  # triples/triple-doc
            "triple_save_path": os.path.join(config["save_dir"], "save_triples.json"),
            "triple_load_path": None,
        }
        if "trace_config" in config and config["trace_config"] is not None:
            default_setting.update(config["trace_config"])
        self.kg_setting = default_setting

        self.num_examplars = self.kg_setting["num_examplars"]
        self.max_chain_length = self.kg_setting["max_chain_length"]
        self.topk_triple_select = self.kg_setting["topk_triple_select"]
        self.num_beams = self.kg_setting["num_beams"]
        self.num_chains = self.kg_setting["num_chains"]
        self.num_choices = self.kg_setting["num_choices"]
        self.min_triple_prob = self.kg_setting["min_triple_prob"]
        self.n_context = self.kg_setting["n_context"]
        self.context_type = self.kg_setting["context_type"]
        self.triple_save_path = self.kg_setting["triple_save_path"]
        self.triple_load_path = self.kg_setting["triple_load_path"]

        # set necessary component
        if retriever is None:
            print("Load new retriever")
            from flashrag.utils import get_retriever

            self.retriever = get_retriever(config)
        else:
            self.retriever = retriever
        if generator is None:
            print("Load new generator")
            from flashrag.utils import get_generator

            self.generator = get_generator(config)
        else:
            self.generator = generator

        # load demonstrations
        if config["retrieval_method"] != "e5":
            self.encoder = Encoder(
                model_name="e5",
                model_path=config["model2path"]["e5"],
                pooling_method="mean",
                max_length=256,
                use_fp16=True,
            )
        else:
            self.encoder = self.retriever.encoder

        # load demonstrations for generating triples and reasoning chain
        if config["dataset_name"].lower() == "hotpotqa":
            from flashrag.prompt.trace_examplars import (
                TRIPLE_EXAMPLARS_HOTPOTQA,
                GENERATING_CHAIN_EXAMPLARS_HOTPOTQA,
                FINAL_CHAIN_EXAMPLARS_HOTPOTQA,
            )

            self.triple_examplars = TRIPLE_EXAMPLARS_HOTPOTQA
            self.final_chain_examplars = FINAL_CHAIN_EXAMPLARS_HOTPOTQA
            self.generating_chain_examplars = GENERATING_CHAIN_EXAMPLARS_HOTPOTQA
        elif config["dataset_name"].lower() == "musique":
            from flashrag.prompt.trace_examplars import (
                TRIPLE_EXAMPLARS_MUSIQUE,
                GENERATING_CHAIN_EXAMPLARS_MUSIQUE,
                FINAL_CHAIN_EXAMPLARS_MUSIQUE,
            )

            self.triple_examplars = TRIPLE_EXAMPLARS_MUSIQUE
            self.final_chain_examplars = FINAL_CHAIN_EXAMPLARS_MUSIQUE
            self.generating_chain_examplars = GENERATING_CHAIN_EXAMPLARS_MUSIQUE
        elif "2wiki" in config["dataset_name"].lower():
            from flashrag.prompt.trace_examplars import (
                TRIPLE_EXAMPLARS_2WIKIMULTIHOPQA,
                GENERATING_CHAIN_EXAMPLARS_2WIKIMULTIHOPQA,
                FINAL_CHAIN_EXAMPLARS_2WIKIMULTIHOPQA,
            )

            self.triple_examplars = TRIPLE_EXAMPLARS_2WIKIMULTIHOPQA
            self.final_chain_examplars = FINAL_CHAIN_EXAMPLARS_2WIKIMULTIHOPQA
            self.generating_chain_examplars = GENERATING_CHAIN_EXAMPLARS_2WIKIMULTIHOPQA
        else:
            # use hotpotqa examplars
            from flashrag.prompt.trace_examplars import (
                TRIPLE_EXAMPLARS_HOTPOTQA,
                GENERATING_CHAIN_EXAMPLARS_HOTPOTQA,
                FINAL_CHAIN_EXAMPLARS_HOTPOTQA,
            )

            self.triple_examplars = TRIPLE_EXAMPLARS_HOTPOTQA
            self.final_chain_examplars = FINAL_CHAIN_EXAMPLARS_HOTPOTQA
            self.generating_chain_examplars = GENERATING_CHAIN_EXAMPLARS_HOTPOTQA

        triple_examplars_text_list = [f"title: {item['title']} text: {item['text']}" for item in self.triple_examplars]
        self.triple_examplars_embeddings = self.encoder.encode(triple_examplars_text_list, is_query=False)
        self.triple_examplars_embeddings = torch.tensor(self.triple_examplars_embeddings)

        chain_examplars_text_list = [item["question"] for item in self.final_chain_examplars]
        self.chain_examplars_embeddings = self.encoder.encode(chain_examplars_text_list, is_query=True)
        self.chain_examplars_embeddings = torch.tensor(self.chain_examplars_embeddings)

        if self.triple_load_path is not None:
            with open(self.triple_load_path, "r") as f:
                self.extracted_doc_triples = json.load(f)
        else:
            self.extracted_doc_triples = {}  # id: triples
        self.token_id_to_choice_map = None

    def get_examplars_for_triple(self, doc_list, batch_size=64):
        # load demonstrations for each doc
        doc_examplars = []

        doc_text_list = []
        for doc_content in doc_list:
            title = doc_content.split("\n")[0]
            text = "\n".join(doc_content.split("\n")[1:])
            doc_text_list.append(f"title: {title} text: {text}")

        doc_embeddings = []
        for idx in range(0, len(doc_text_list), batch_size):
            batch_data = doc_text_list[idx : idx + batch_size]
            batch_embedding = self.encoder.encode(batch_data, is_query=True)
            doc_embeddings.append(batch_embedding)
        doc_embeddings = np.concatenate(doc_embeddings, axis=0)
        doc_embeddings = torch.tensor(doc_embeddings)

        similarities = torch.matmul(doc_embeddings, self.triple_examplars_embeddings.T)
        examplars_rank = torch.argsort(similarities, dim=1, descending=True)
        for i, _ in enumerate(doc_list):
            rank = examplars_rank[i].tolist()
            examplars = [self.triple_examplars[idx] for idx in rank[: self.num_examplars]]
            examplars = [
                "Title: {}\nText: {}\nKnowledge Triples: {}".format(
                    example["title"], example["text"], example["triples"]
                )
                for example in examplars
            ]
            doc_examplars.append(examplars)

        return doc_examplars

    def get_examplars_for_reasoning_chain(self, all_query):
        generating_chain_examplars = []
        final_chain_examplars = []
        query_embeddings = self.encoder.encode(all_query, is_query=True)
        query_embeddings = torch.tensor(query_embeddings)

        similarities = torch.matmul(query_embeddings, self.chain_examplars_embeddings.T)
        examplars_rank = torch.argsort(similarities, dim=1, descending=True)
        for i, _ in enumerate(all_query):
            rank = examplars_rank[i].tolist()
            examplars = [self.final_chain_examplars[idx] for idx in rank[: self.num_examplars]]
            final_chain_examplars.append(examplars)
            examplars = [self.generating_chain_examplars[idx] for idx in rank[: self.num_examplars]]
            generating_chain_examplars.append(examplars)
        return generating_chain_examplars, final_chain_examplars

    def parse_triple_output(self, doc_list, output_list):
        def parse_model_output(triples_text: str):
            import re

            results = []
            for one_triple_text in re.findall(r"<([^>]*)>", triples_text):
                pieces = one_triple_text.rsplit(";", maxsplit=2)
                if len(pieces) != 3:
                    print(f'Something wrong with this triple: "{one_triple_text}", Skip this triple!')
                    continue
                head, relation, tail = pieces
                results.append((head.strip(), relation.strip(), tail.strip()))
            return results

        # parse model_outputs
        results = []
        for j, (doc_content, generated_content) in enumerate(zip(doc_list, output_list)):
            triples = parse_model_output(generated_content)  # [(head, relation, tail)]
            triples_in_one_document = []
            title = doc_content.split("\n")[0]
            text = "\n".join(doc_content.split("\n")[1:])
            for head, relation, tail in triples:
                # if head.lower() != title.lower():
                #     if head.lower() not in text.lower():
                #         head = title

                triples_in_one_document.append(
                    {
                        "head": head,
                        "relation": relation,
                        "tail": tail,
                    }
                )
            results.append(triples_in_one_document)
        return results

    def extract_document_triples(self, queries, retrieval_results):
        """
        Extract triples from documents associated with each query, handling duplicates and generating prompts for LLM processing.
        """

        # deduplicate documents and map document IDs to their contents and associated queries
        unique_docs = {}
        doc_queries = {}
        for query, docs in zip(queries, retrieval_results):
            for doc in docs:
                # if 'id' not in doc:
                #     doc['id'] = hash_object(doc['contents'])
                doc["id"] = hash_object(doc["contents"])
                doc_id = doc["id"]
                if doc_id not in self.extracted_doc_triples:
                    unique_docs[doc_id] = doc["contents"]
                    doc_queries.setdefault(doc_id, []).append(query)

        # prepare data structures for document processing
        doc_ids = list(unique_docs.keys())
        doc_id_mapping = {doc_id: index for index, doc_id in enumerate(doc_ids)}
        docs_content = [unique_docs[doc_id] for doc_id in doc_ids]

        if len(docs_content) > 0:
            # obtain exemplars for each document
            doc_examplars = self.get_examplars_for_triple(docs_content)

            # construct prompts for triple extraction using an LLM
            # prompts for extracting triples from documents
            system_prompt = (
                "Given a title and a text, extract all the knowledge triples in the form of <title; relation; tail entity>, "
                "where title is the provided title, tail entity is a phrase in the text and relation denotes a description of the relation "
                "between the title and the tail entity. \n\nThe followings are some examples: \n\n{examplars}"
            )
            user_prompt = "Title: {title}\nText: {text}\nKnowledge Triples: "
            prompt_template = PromptTemplate(config=self.config, system_prompt=system_prompt, user_prompt=user_prompt)
            prompts = [
                prompt_template.get_string_with_varying_examplars(
                    question="",
                    examplars=examplars,
                    title=doc.split("\n")[0],
                    text="\n".join(doc.split("\n")[1:]),
                    tokenizer=self.generator.tokenizer,
                    max_length=2048,
                )
                for doc, examplars in zip(docs_content, doc_examplars)
            ]

            # generate triples via the language model
            outputs = self.generator.generate(prompts, max_tokens=512)
            triples = self.parse_triple_output(docs_content, outputs)

        # reconstruct triples for the original query-documents structure
        all_doc_triples = []
        for query_retrieval_result in retrieval_results:
            query_triples = []
            for doc in query_retrieval_result:
                doc_id = doc["id"]
                if doc_id in doc_id_mapping:
                    triple_set = triples[doc_id_mapping[doc_id]]
                    self.extracted_doc_triples[doc_id] = triple_set
                elif doc_id in self.extracted_doc_triples:
                    triple_set = self.extracted_doc_triples[doc_id]
                else:
                    raise AssertionError("Document ID not found during triple extraction.")
                query_triples.append(triple_set)
            all_doc_triples.append(query_triples)

        return all_doc_triples

    def convert_candidate_triples_to_choices(self, candidates):
        return "\n".join(
            ["A. no need for additional knowledge triples"]
            + ["{}. {}".format(chr(ord("B") + k), triple) for k, triple in enumerate(candidates)]
        )

    def build_prompt_for_reasoning_chain(
        self,
        hop,
        question,
        existing_triples,
        candidate_triples,
        generating_chain_examplars=[],
        final_chain_examplars=[],
        use_demonstration=True,
    ):

        base_instruction = (
            "Select the next knowledge triple that extends an existing set of knowledge triples to form a coherent reasoning path capable of answering a specified question. "
            "If the current reasoning path is sufficient to answer the question, simply output A. Please only output the choice for the next knowledge triple."
        )

        if use_demonstration and len(generating_chain_examplars) > 0:
            demonstration_instruction = (
                "\n\nThe followings are some examples of coherent reasoning paths capable of answering the specified question "
                f"and how the {hop}-th knowledge triples in these paths are selected:\n\n"
            )

            # deal with examplars
            examplars = []
            for i, (rp_examplar, grp_examplar) in enumerate(zip(final_chain_examplars, generating_chain_examplars)):
                if len(grp_examplar) < hop + 1:
                    continue
                examplar = "coherent reasoning path: {}\nquestion: {}\n".format(
                    rp_examplar["chains"], rp_examplar["question"]
                )
                examplar += "The {}-th triple in the reasoning path is selected as:\n".format(hop + 1)
                one_step_item = grp_examplar[hop]
                examplar += "existing knowledge triples: {}\nquestion: {}\ncandidate knowledge triples:\n{}\nthe next possible triple is:{}\n".format(
                    ", ".join(one_step_item["triples"]),
                    one_step_item["question"],
                    "\n".join(one_step_item["candidate_triples"]),
                    one_step_item["answer"],
                )
                examplars.append(examplar)
                if len(examplars) >= self.num_examplars:
                    break

            system_prompt = base_instruction + " " + demonstration_instruction + "{examplars}"
        else:
            system_prompt = base_instruction + "\n\n"

        user_prompt = (
            "The {hop}-th triple in the reasoning path is selected as:\nexisting knowledge triples: {existing_triples}\n"
            "question: {question}\ncandidate knowledge triples:\n{candidate_triples}\nthe next possible triple is:"
        )

        prompt_template = PromptTemplate(config=self.config, system_prompt=system_prompt, user_prompt=user_prompt)
        prompt = prompt_template.get_string_with_varying_examplars(
            hop=hop + 1,
            question=question,
            examplars=examplars,
            existing_triples=", ".join(existing_triples),
            candidate_triples=self.convert_candidate_triples_to_choices(candidate_triples),
            tokenizer=self.generator.tokenizer,
            max_length=2048,
        )

        return prompt

    def get_answer_token_indices(self, tokenizer, num_choices, token_ids):
        """Obtain the index of token corresponsding to the option"""
        if self.token_id_to_choice_map is None:
            self.token_id_to_choice_map = {}
            choices = [chr(ord("A") + i) for i in range(num_choices + 1)]
            for choice in choices:
                self.token_id_to_choice_map[tokenizer.encode(choice, add_special_tokens=False)[0]] = choice
                self.token_id_to_choice_map[tokenizer.encode(" {}".format(choice), add_special_tokens=False)[-1]] = (
                    choice
                )

        answer_token_indices = torch.zeros((token_ids.shape[0],), dtype=token_ids.dtype).fill_(token_ids.shape[1] - 1)
        for i in range(token_ids.shape[0]):
            for j in range(token_ids.shape[1]):
                if token_ids[i, j].item() in self.token_id_to_choice_map:
                    answer_token_indices[i] = j
                    break

        return answer_token_indices

    def get_reasoning_chain(self, all_query, all_doc_triples, triple_to_doc_ids):
        all_generating_chain_examplars, all_final_chain_examplars = self.get_examplars_for_reasoning_chain(all_query)
        all_chain_results = []

        for query, doc_triples, generating_chain_examplars, final_chain_examplars, triple_doc_id_list in tqdm(
            zip(
                all_query, all_doc_triples, all_generating_chain_examplars, all_final_chain_examplars, triple_to_doc_ids
            ),
            total=len(all_query),
            desc="Generating reasoning chain for query",
        ):
            # run single item
            flatten_triples = sum(doc_triples, [])  # all triples
            flatten_doc_ids = sum(
                [
                    [doc_id for _ in single_doc_triple]
                    for doc_id, single_doc_triple in zip(triple_doc_id_list, doc_triples)
                ],
                [],
            )
            num_total_triples = len(flatten_triples)
            triple_text = [
                "<{}; {}; {}>".format(triple_item["head"], triple_item["relation"], triple_item["tail"])
                for triple_item in flatten_triples
            ]
            triple_embeddings = torch.tensor(self.encoder.encode(triple_text, is_query=False))

            paths = [[]]  # each list contains index of triples to format a reasoning path
            paths_scores = [1.0]
            paths_finished = [False]

            for j in range(self.max_chain_length):
                if np.sum(paths_finished) == self.num_chains:
                    break

                # triple rank: concatenation of question and selected triples as query
                path_queries = [
                    "knowledge triples: {}\nquestion: {}".format(" ".join([triple_text[idx] for idx in path]), query)
                    for path in paths
                ]
                path_query_embeddings = torch.tensor(self.encoder.encode(path_queries, is_query=True))

                path_triples_similarities = torch.matmul(path_query_embeddings, triple_embeddings.T)
                candidate_triples_mask = torch.ones_like(path_triples_similarities)
                for k, path in enumerate(paths):
                    # mask the chosen triples
                    candidate_triples_mask[k, path] = 0.0
                path_triples_similarities = path_triples_similarities - 10000 * (1.0 - candidate_triples_mask)
                topk_most_relevant_triples_indices = torch.topk(
                    path_triples_similarities, k=min(self.topk_triple_select, num_total_triples), dim=1
                )[1].tolist()

                # construct prompts for selecting triple in reasoning chain
                # for each path, create an input to LLM
                input_prompts = []
                exisiting_triples = [
                    [triple_text[idx] for idx in path] for path in paths
                ]  # each item represents existing triples path
                candidate_triples = [
                    [triple_text[idx] for idx in candidate_triples_indices]
                    for candidate_triples_indices in topk_most_relevant_triples_indices
                ]  # each item represents candidate triples in a path
                for triples, candidates in zip(exisiting_triples, candidate_triples):
                    prompt = self.build_prompt_for_reasoning_chain(
                        hop=j,
                        question=query,
                        existing_triples=triples,
                        candidate_triples=candidates,
                        generating_chain_examplars=generating_chain_examplars,
                        final_chain_examplars=final_chain_examplars,
                        use_demonstration=True,
                    )
                    input_prompts.append(prompt)

                # get generated result
                torch.cuda.empty_cache()
                generate_output = self.generator.generate(input_prompts, max_tokens=32, return_dict=True)
                generated_token_ids, generated_token_logits = (
                    generate_output["generated_token_ids"],
                    generate_output["generated_token_logits"],
                )

                answer_token_indices = self.get_answer_token_indices(
                    self.generator.tokenizer, self.num_choices, generated_token_ids
                )
                answer_token_logits = generated_token_logits.gather(
                    1, answer_token_indices[:, None, None].expand(-1, -1, generated_token_logits.shape[-1])
                )
                answer_token_logits = answer_token_logits.squeeze(1)

                choices_token_ids_list = list(self.token_id_to_choice_map.keys())
                choices_list = [
                    self.token_id_to_choice_map[token_id] for token_id in choices_token_ids_list
                ]  # ['A','B','C',..], may be duplication
                answer_token_probs = F.softmax(answer_token_logits[:, choices_token_ids_list], dim=1)

                new_paths, new_paths_scores, new_paths_finished = [], [], []
                topk_choices_probs, topk_choices_indices = torch.topk(
                    answer_token_probs, k=self.num_beams, dim=1
                )  # for each path, select choice token in topk probs
                for i in range(len(paths)):
                    if paths_finished[i]:
                        new_paths.append(paths[i])
                        new_paths_scores.append(paths_scores[i])
                        new_paths_finished.append(True)
                        continue
                    if torch.all(torch.isnan(topk_choices_probs[i])):
                        print(
                            "No choice in generated results! generated text: {}".format(
                                self.generator.tokenizer.decode(generated_token_ids[i])
                            )
                        )
                        new_paths.append(paths[i])
                        new_paths_scores.append(paths_scores[i])
                        new_paths_finished.append(False)
                        continue
                    for b in range(self.num_beams):
                        if (
                            torch.isnan(topk_choices_probs[i, b])
                            or topk_choices_probs[i, b].item() < self.min_triple_prob
                        ):
                            continue
                        current_choice = choices_list[topk_choices_indices[i, b].item()]
                        if current_choice != "A" and (
                            ord(current_choice) - ord("B") >= len(topk_most_relevant_triples_indices[i])
                        ):
                            # generated invalid option
                            continue
                        new_paths_scores.append(paths_scores[i] * topk_choices_probs[i, b].item())
                        if current_choice == "A":
                            new_paths.append(paths[i] + [-1])
                            new_paths_finished.append(True)
                        else:
                            new_paths.append(
                                paths[i] + [topk_most_relevant_triples_indices[i][ord(current_choice) - ord("B")]]
                            )
                            new_paths_finished.append(False)

                assert len(new_paths) == len(new_paths_scores)
                assert len(new_paths) == len(new_paths_finished)
                new_paths_sorted_indices = sorted(
                    range(len(new_paths_scores)), key=lambda x: new_paths_scores[x], reverse=True
                )
                topk_new_paths_sorted_indices = new_paths_sorted_indices[: self.num_chains]
                paths = [new_paths[idx] for idx in topk_new_paths_sorted_indices]
                paths_scores = [new_paths_scores[idx] for idx in topk_new_paths_sorted_indices]
                paths_finished = [new_paths_finished[idx] for idx in topk_new_paths_sorted_indices]

            query_chain_result = [
                {
                    "triples": [triple_text[idx] for idx in path if idx >= 0],
                    "triple_doc_ids": [flatten_doc_ids[idx] for idx in path if idx >= 0],
                    "score": path_score,
                }
                for path, path_score in zip(paths, paths_scores)
            ]  # Contains paths with their scores

            # sort with scores
            query_chain_result.sort(key=lambda x: float(x["score"]), reverse=True)

            all_chain_results.append(query_chain_result)

        return all_chain_results

    def format_reference(self, retrieval_result):
        format_reference = ""
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item["contents"]
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            if self.reference_template is not None:
                format_reference += self.reference_template.format(idx=idx, title=title, text=text)
            else:
                format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference

    def batch_run(self, dataset):
        all_query = dataset.question
        retrieval_results = dataset.retrieval_result
        print("Begin extracting triples")
        all_doc_triples = self.extract_document_triples(all_query, retrieval_results)
        triple_to_doc_ids = [
            [doc_item["id"] for doc_item in query_retrieval_result] for query_retrieval_result in retrieval_results
        ]
        dataset.update_output("doc_triples", all_doc_triples)
        # save triples for re-use
        with open(self.triple_save_path, "w") as f:
            json.dump(self.extracted_doc_triples, f, indent=4)
        print("Finish extracting tiples")

        print("Begin generating reasoning chain")
        reasoning_chain_result = self.get_reasoning_chain(all_query, all_doc_triples, triple_to_doc_ids)
        dataset.update_output("reasoning_chain", reasoning_chain_result)
        print("Finish generating reasoning chain")

        # get refine result based on context type
        refine_result = []

        if self.context_type == "triples":
            for query_chain_result in reasoning_chain_result:
                # query_chain_list: reasoning chain for a query
                query_chain_result = query_chain_result[: self.n_context]
                all_triple_text = []
                for chain in query_chain_result:
                    triples = chain["triples"]
                    for triple in triples:
                        triple = triple.replace("<", "").replace(">", "").replace(";", "", 2)
                        if triple not in all_triple_text:
                            all_triple_text.append(triple)
                refine_text = "\n".join(["{}. {}".format(i + 1, text) for i, text in enumerate(all_triple_text)])
                refine_result.append(refine_text)

        elif self.context_type == "triple-doc":
            for query_chain_result, query_retrieval_results in zip(reasoning_chain_result, retrieval_results):

                query_chain_result = query_chain_result[: self.n_context]
                chains_doc_id_count_dict = {}

                for chain in query_chain_result:
                    for triple, doc_id in zip(chain["triples"], chain["triple_doc_ids"]):
                        chains_doc_id_count_dict[doc_id] = chains_doc_id_count_dict.get(doc_id, 0) + 1

                ranked_chains_doc_id = sorted(chains_doc_id_count_dict.items(), key=lambda x: x[1], reverse=True)
                query_doc_to_idx = {doc_item["id"]: idx for idx, doc_item in enumerate(query_retrieval_results)}
                final_doc_idx = [query_doc_to_idx[doc_id] for doc_id in ranked_chains_doc_id]
                final_doc_list = [query_retrieval_results[idx] for idx in final_doc_idx]
                refine_text = self.format_reference(final_doc_list)
                refine_result.append(refine_text)

        else:
            assert False

        return refine_result



================================================
FILE: flashrag/refiner/refiner.py
================================================
from typing import List
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from flashrag.retriever.encoder import Encoder
from tqdm import tqdm
import re
import torch
import numpy as np

class BaseRefiner:
    r"""Base object of Refiner method"""

    def __init__(self, config):
        self.config = config
        self.name = config["refiner_name"]
        self.model_path = config["refiner_model_path"]
        self.device = config["device"]
        self.input_prompt_flag = config["refiner_input_prompt_flag"] if "refiner_input_prompt_flag" in config else False

    def run(self, item) -> str:
        r"""Get refining result.

        Args:
            item: dataset item, contains question, retrieval result...

        Returns:
            str: refining result of this item
        """
        pass

    def batch_run(self, dataset, batch_size=None) -> List[str]:
        return [self.run(item) for item in dataset]


class LLMLinguaRefiner(BaseRefiner):
    """Implementation for (Long)LLMLingua."""

    def __init__(self, config):
        super().__init__(config)
        default_config = {
            'use_llmlingua2': False,
            "rate": 0.55,
            "condition_in_question": "after_condition",
            "reorder_context": "sort",
            "dynamic_context_compression_ratio": 0.3,
            "condition_compare": True,
            "context_budget": "+100",
            "rank_method": "longllmlingua",
        }
        if "llmlingua_config" in config and config["llmlingua_config"] is not None:
            self.compress_config = config["llmlingua_config"]
        else:
            self.compress_config = default_config

        from flashrag.refiner.llmlingua_compressor import PromptCompressor

        if 'use_llmlingua2' in self.compress_config:
            use_llmlingua2 = self.compress_config.pop('use_llmlingua2')
        else:
            use_llmlingua2 = False
        self.refiner = PromptCompressor(model_name=self.model_path, use_llmlingua2=use_llmlingua2)

    def format_reference(self, retrieval_result):
        format_reference = ""
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item["contents"]
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference

    def batch_run(self, dataset):
        output = []
        for item in tqdm(dataset, desc="Refining process: "):
            question = item.question
            # TODO: suit more cases
            if self.input_prompt_flag:
                input_prompt = item.prompt
                prompt_split = input_prompt.split("\n\n")
                # need fixed format prompt: instr + demon(retrieval results) + question
                instruction, question = prompt_split[0], prompt_split[-1]
                demonstration = "\n".join(prompt_split[1:-1])
                item_output = self.refiner.compress_prompt(
                    [i for i in demonstration.split("\n") if i != ""],
                    instruction=instruction,
                    question=question,
                    **self.compress_config,
                )
            else:
                retrieval_result = item.retrieval_result
                docs = self.format_reference(retrieval_result).split("\n")
                docs = [i for i in docs if i != ""]
                item_output = self.refiner.compress_prompt(
                    docs, instruction="", question=question, **self.compress_config
                )
            output.append(item_output["compressed_prompt"])
        return output


class SelectiveContextRefiner(BaseRefiner):
    """Implementation for Selective Context"""

    def __init__(self, config):
        super().__init__(config)
        from flashrag.refiner.selective_context_compressor import SelectiveContext

        default_config = {"reduce_ratio": 0.5}

        self.refiner = SelectiveContext(model_type="gpt2", model_path=self.model_path, lang="en")
        if "sc_config" in config and config["sc_config"] is not None:
            self.compress_config = config["sc_config"]
        else:
            self.compress_config = default_config

    def format_reference(self, retrieval_result):
        format_reference = ""
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item["contents"]
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference

    def batch_run(self, dataset):
        # only use text
        all_inputs = []
        for item in dataset:
            retrieval_result = item.retrieval_result
            all_inputs.append(self.format_reference(retrieval_result))

        output = []
        for text in tqdm(all_inputs, desc="Refining process: "):
            compress_text, _ = self.refiner(text, **self.compress_config)
            output.append(compress_text)
        return output


class ExtractiveRefiner(BaseRefiner):
    """Implementation for Extractive compressor.
    Using retrieval method to select sentences or other granularity data.
    """

    def __init__(self, config):
        super().__init__(config)
        # number of keeping sentences
        self.topk = config["refiner_topk"]
        self.pooling_method = config["refiner_pooling_method"]
        self.encode_max_length = config["refiner_encode_max_length"]
        self.mini_batch_size = config['refiner_mini_batch_size'] if 'refiner_mini_batch_size' in config else 256
        # load model
        self.encoder = Encoder(
            model_name=self.name, 
            model_path=self.model_path, 
            pooling_method=self.pooling_method, 
            max_length=self.encode_max_length, 
            use_fp16=True
        )

    def batch_run(self, dataset, batch_size=16):
        questions = dataset.question
        # only use text
        retrieval_results = dataset.retrieval_result
        retrieval_results = [
            ["\n".join(doc_item["contents"].split("\n")[1:]) for doc_item in item_result]
            for item_result in retrieval_results
        ]

        # split into sentences: [[sent1, sent2,...], [...]]
        sent_lists = [
            [i.strip() for i in re.split(r"(?<![A-Za-z]\.)(?<=[.!?])\s+", " ".join(res)) if len(i.strip()) > 5]
            for res in retrieval_results
        ]
        score_lists = []  # matching scores, size == sent_lists
        for idx in tqdm(range(0, len(questions), batch_size), desc="Refining process: "):
            batch_questions = questions[idx : idx + batch_size]
            batch_sents = sent_lists[idx : idx + batch_size]
            question_embs = self.encoder.encode(batch_questions, is_query=True)
            
            flatten_batch_sents = sum(batch_sents, [])
            sent_embs = []
            for s_index in tqdm(range(0, len(flatten_batch_sents), self.mini_batch_size), desc='Sentence encoding..,'):
                mini_batch_sents = flatten_batch_sents[s_index:s_index+self.mini_batch_size]
                mini_sent_embs = self.encoder.encode(mini_batch_sents, is_query=False)
                sent_embs.append(mini_sent_embs)
            sent_embs = np.concatenate(sent_embs, axis=0)

            scores = question_embs @ sent_embs.T
            start_idx = 0
            for row_score, single_list in zip(scores, batch_sents):
                row_score = row_score.tolist()
                score_lists.append(row_score[start_idx : start_idx + len(single_list)])
                start_idx += len(single_list)

        # select topk sents
        retain_lists = []
        for sent_scores, sent_list in zip(score_lists, sent_lists):
            assert len(sent_scores) == len(sent_list)
            if len(sent_scores) < self.topk:
                retain_lists.append(sent_list)
                continue

            topk_idxs = torch.topk(torch.Tensor(sent_scores), min(self.topk, len(sent_scores))).indices.tolist()
            retain_lists.append([sent_list[idx] for idx in sorted(topk_idxs) if idx < len(sent_list)])

        return [" ".join(sents) for sents in retain_lists]


class AbstractiveRecompRefiner(BaseRefiner):
    """Implementation for Abstractive RECOMP compressor:
    RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation.
    """

    def __init__(self, config):
        super().__init__(config)

        self.max_input_length = config["refiner_max_input_length"]
        self.max_output_length = config["refiner_max_output_length"]

        # load model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_path)
        self.model.cuda()
        self.model.eval()

    def batch_run(self, dataset, batch_size=2):
        # only use text
        retrieval_results = dataset.retrieval_result
        retrieval_results = [
            ["\n".join(doc_item["contents"].split("\n")[1:]) for doc_item in item_result]
            for item_result in retrieval_results
        ]

        # input processing in recomp training format
        format_inputs = [
            "Question: {question}\n Document: {document}\n Summary: ".format(
                question=item.question, document="\n".join(docs)
            )
            for item, docs in zip(dataset, retrieval_results)
        ]

        results = []
        for idx in tqdm(range(0, len(format_inputs), batch_size), desc="Refining process: "):
            batch_inputs = format_inputs[idx : idx + batch_size]
            batch_inputs = self.tokenizer(
                batch_inputs, return_tensors="pt", padding=True, truncation=True, max_length=self.max_input_length
            ).to(self.device)

            batch_outputs = self.model.generate(**batch_inputs, max_length=self.max_output_length)

            batch_outputs = self.tokenizer.batch_decode(
                batch_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )

            results.extend(batch_outputs)

        return results



================================================
FILE: flashrag/refiner/selective_context_compressor.py
================================================
# Implementation of Selective-Context, modified from official repo: https://github.com/liyucheng09/Selective_Context
# Licensed under The MIT License


import re
from typing import List, Tuple
import spacy
import numpy as np
import os
import sys

sys.path.append("..")
from dataclasses import dataclass
from nltk.tokenize import word_tokenize
import time
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer


@dataclass
class LexicalUnits:
    unit_type: str
    text: List[str]
    self_info: List[float] = None

    def __add__(self, other):
        assert self.unit_type == other.unit_type, "Cannot add two different unit types"
        return LexicalUnits(self.unit_type, self.text + other.text, self.self_info + other.self_info)

    def __radd__(self, other):
        if other == 0:
            return self
        return NotImplementedError()

    def add_to_head(self, token, self_info):
        return LexicalUnits(self.unit_type, [token] + self.text, [self_info] + self.self_info)

    def add_to_tail(self, token, self_info):
        return LexicalUnits(self.unit_type, self.text + [token], self.self_info + [self_info])


class SelectiveContext:

    def __init__(self, model_type="gpt2", model_path="openai-community/gpt2", lang="en"):

        self.model_type = model_type
        self.model_path = model_path
        self.lang = lang

        # this means we calculate self-information sentence by sentence
        self.sent_level_self_info = True

        self._prepare_phrase_tokenizer()
        self.sent_tokenize_pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s"
        self.phrase_mask_token = ""
        self.sent_mask_token = "<...some content omitted.>"

        self._prepare_model()

    def _prepare_phrase_tokenizer(self):
        # we use space to tokenize sentence into phrases
        # for English, we should use `spacy.load("en_core_web_sm").add_pipe('merge_noun_chunks')`
        # for Chinese, use `nlp = spacy.load('zh_core_web_sm')`` directly
        lang = self.lang
        if lang == "en":
            self.nlp = spacy.load("en_core_web_lg", disable=["ner"])
            self.nlp.add_pipe("merge_noun_chunks")
        elif lang == "zh":
            self.nlp = spacy.load("zh_core_web_sm", disable=["ner"])

    def _prepare_model(self):
        # Load tokenizer
        if self.lang == "zh":
            self.tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        elif self.lang == "en":
            self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_path)
        else:
            raise NotImplementedError()

        if self.model_type == "gpt2":
            if self.lang == "zh":
                self.model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
            else:
                self.model = GPT2LMHeadModel.from_pretrained(self.model_path)
            self.model.to("cuda")
            self.model.eval()

            print("model loaded")

            self.max_token_length = self.model.config.n_positions
            self.get_self_information = self._get_self_info_via_gpt2

        elif self.model_type == "curie":
            global openai
            import openai

            self.max_token_length = 2048

            self.get_self_information = self._get_self_info_via_curie

    def get_self_information(self, text: str) -> Tuple[List[str], List[float]]:
        # it takes text as input, and return a list of words and a list of self-information scores
        raise NotImplementedError

    def _get_self_info_via_gpt2(self, text: str) -> Tuple[List[str], List[float]]:
        if self.lang == "en":
            text = f"<|endoftext|>{text}"
        elif self.lang == "zh":
            text = f"[CLS]{text}"
        with torch.inference_mode(mode=True):
            encoding = self.tokenizer(
                text, max_length=1024, truncation=True, add_special_tokens=False, return_tensors="pt"
            )
            encoding = encoding.to(self.model.device)
            outputs = self.model(**encoding)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            self_info = -torch.log(probs)

        input_ids = encoding["input_ids"]
        input_ids_expaned = input_ids[:, 1:].unsqueeze(-1)

        tokens = [self.tokenizer.decode(token_) for token_ in input_ids.squeeze().tolist()[1:]]
        return tokens, self_info[:, :-1].gather(-1, input_ids_expaned).squeeze(-1).squeeze(0).tolist()

    def _get_self_info_via_curie(self, text: str) -> Tuple[List[str], List[float]]:
        num_retry = 3
        openai.api_key = os.environ["OPENAI_API_KEY"]

        for _ in range(num_retry):
            try:
                r = openai.Completion.create(
                    model="curie",
                    prompt=f"<|endoftext|>{text}",
                    max_tokens=0,
                    temperature=0,
                    echo=True,
                    logprobs=0,
                )
                break
            except Exception as e:
                print(e)
                time.sleep(1)

        result = r["choices"][0]
        tokens, logprobs = result["logprobs"]["tokens"][1:], result["logprobs"]["token_logprobs"][1:]

        assert len(tokens) == len(logprobs), f"Expected {len(tokens)} logprobs, got {len(logprobs)}"

        self_info = [-logprob for logprob in logprobs]
        return tokens, self_info

    def _lexical_unit(self, sents):

        if self.sent_level_self_info:
            sent_self_info = []
            all_noun_phrases = []
            all_noun_phrases_info = []
            all_tokens = []
            all_token_self_info = []

            for sent in sents:
                # print(sent)
                tokens, self_info = self.get_self_information(sent)
                sent_self_info.append(np.mean(self_info))

                all_tokens.extend(tokens)
                all_token_self_info.extend(self_info)

                noun_phrases, noun_phrases_info = self._calculate_lexical_unit(tokens, self_info)

                # We need to add a space before the first noun phrase for every sentence except the first one
                if len(all_noun_phrases) != 0:
                    noun_phrases[0] = f" {noun_phrases[0]}"
                all_noun_phrases.extend(noun_phrases)
                all_noun_phrases_info.extend(noun_phrases_info)

            return [
                LexicalUnits("sent", text=sents, self_info=sent_self_info),
                LexicalUnits("phrase", text=all_noun_phrases, self_info=all_noun_phrases_info),
                LexicalUnits("token", text=all_tokens, self_info=all_token_self_info),
            ]

    def _calculate_lexical_unit(self, tokens, self_info):
        def _unit_info(tokens, self_info, units):
            current_unit_idx = 0
            current_position = 0
            unit_self_info = [[] for _ in range(len(units))]

            for idx, (token, info) in enumerate(zip(tokens, self_info)):
                current_position += len(token)
                if current_position == len(units[current_unit_idx]):
                    unit_self_info[current_unit_idx].append(info)
                    current_position = current_position - len(units[current_unit_idx])
                    current_unit_idx += 1
                elif current_position > len(units[current_unit_idx]):
                    counter_ = 1
                    current_position = current_position - len(units[current_unit_idx])
                    current_unit_idx += 1
                    while current_position >= len(units[current_unit_idx]):
                        counter_ += 1
                        current_position = current_position - len(units[current_unit_idx])
                        current_unit_idx += 1
                        if current_unit_idx >= len(units):
                            break
                    partial_info = info / counter_
                    for _ in range(counter_):
                        unit_self_info[(current_unit_idx - 1) - _].append(partial_info)
                else:
                    if token == " ":
                        continue
                    unit_self_info[current_unit_idx].append(info)

            unit_self_info_ = [np.mean(info) for info in unit_self_info]
            return unit_self_info_

        def _noun_phrases(sent):
            noun_phrases = []
            doc = self.nlp(sent)
            for index, chunk in enumerate(doc):
                if index == 0:
                    noun_phrases.append(chunk.text)
                else:
                    noun_phrases.append(doc[index - 1].whitespace_ + chunk.text)
            return noun_phrases

        if self.sent_level_self_info:
            # in this case, the self_info is for each sentence
            # we only need to calculate the self_info for each phrase

            sent = "".join(tokens)
            # noun_phrases = [chunk.text for chunk in self.nlp(sent).noun_chunks]
            noun_phrases = _noun_phrases(sent)
            # noun_phrases[-1] = noun_phrases[-1] + ' '
            noun_phrases_info = _unit_info(tokens, self_info, noun_phrases)

            return noun_phrases, noun_phrases_info

    def beautify_context(self, context: str) -> str:
        context = re.sub(r"\s+", " ", context)
        return context

    def self_info_mask(self, sents: List[str], self_info: List[float], mask_level):
        # mask_level: mask sentences, phrases, or tokens
        sents_after_mask = []
        masked_sents = []

        self.ppl_threshold = np.nanpercentile(self_info, self.mask_ratio * 100)

        for sent, info in zip(sents, self_info):
            if info < self.ppl_threshold:
                masked_sents.append(sent)
                sents_after_mask.append(self.mask_a_sent(sent, mask_level))
            else:
                sents_after_mask.append(sent)
        masked_context = " ".join(sents_after_mask) if mask_level == "sent" else "".join(sents_after_mask)

        return masked_context, masked_sents

    def mask_a_sent(self, sent, level):
        if level == "phrase":
            return self.phrase_mask_token
        elif level == "sent":
            if self.keep_leading_word:
                leading_few_words = " ".join(word_tokenize(sent)[: self.num_lead_words]) + " "
            else:
                leading_few_words = ""
            return leading_few_words + self.mask_token
        elif level == "token":
            return ""

    def __call__(self, text: str, reduce_ratio: float = 0.5, reduce_level: str = "phrase") -> List[str]:
        context = self.beautify_context(text)

        self.mask_ratio = reduce_ratio

        sents = re.split(self.sent_tokenize_pattern, context)
        sents = [sent.strip() for sent in sents if sent.strip()]

        # You want the reduce happen at sentence level, phrase level, or token level?
        assert reduce_level in [
            "sent",
            "phrase",
            "token",
        ], f"reduce_level should be one of ['sent', 'phrase', 'token'], got {reduce_level}"
        sent_lus, phrase_lus, token_lus = self._lexical_unit(sents)
        # print(phrase_lus, '^^^^')
        lexical_level = {"sent": sent_lus, "phrase": phrase_lus, "token": token_lus}

        # context is the reduced context, masked_sents denotes what context has been filtered out
        context, masked_sents = self.self_info_mask(
            lexical_level[reduce_level].text, lexical_level[reduce_level].self_info, reduce_level
        )
        return context, masked_sents



================================================
FILE: flashrag/retriever/__init__.py
================================================
from flashrag.retriever.retriever import *
from flashrag.retriever.reranker import *
from flashrag.retriever.utils import *


================================================
FILE: flashrag/retriever/__main__.py
================================================
from . import index_builder

if __name__ == "__main__":
    index_builder.main()



================================================
FILE: flashrag/retriever/encoder.py
================================================
from typing import List, Union
import os
import json
import torch
import numpy as np
from tqdm import tqdm
from flashrag.retriever.utils import load_model, pooling, parse_query, parse_image
from flashrag.utils import get_device


class Encoder:
    """
    Encoder class for encoding queries using a specified model.

    Attributes:
        model_name (str): The name of the model.
        model_path (str): The path to the model.
        pooling_method (str): The method used for pooling.
        max_length (int): The maximum length of the input sequences.
        use_fp16 (bool): Whether to use FP16 precision.
        instruction (str): Additional instructions for parsing queries.

    Methods:
        encode(query_list: List[str], is_query=True) -> np.ndarray:
            Encodes a list of queries into embeddings.
    """

    def __init__(self, model_name, model_path, pooling_method, max_length, use_fp16=True, instruction=None, silent=False):
        self.model_name = model_name
        self.model_path = model_path
        self.pooling_method = pooling_method
        self.max_length = max_length
        self.use_fp16 = use_fp16
        self.instruction = instruction
        self.silent = silent
        self.gpu_num = torch.cuda.device_count()
        self.model, self.tokenizer = load_model(model_path=model_path, use_fp16=use_fp16)

    @torch.inference_mode()
    def single_batch_encode(self, query_list: Union[List[str], str], is_query=True) -> np.ndarray:
        query_list = parse_query(self.model_name, query_list, self.instruction, is_query)

        inputs = self.tokenizer(
            query_list, max_length=self.max_length, padding=True, truncation=True, return_tensors="pt"
        )
        inputs = {k: v.to(get_device()) for k, v in inputs.items()}

        if "T5" in type(self.model).__name__ or (
            isinstance(self.model, torch.nn.DataParallel) and "T5" in type(self.model.module).__name__
        ):
            # T5-based retrieval model
            decoder_input_ids = torch.zeros((inputs["input_ids"].shape[0], 1), dtype=torch.long).to(
                inputs["input_ids"].device
            )
            output = self.model(**inputs, decoder_input_ids=decoder_input_ids, return_dict=True)
            query_emb = output.last_hidden_state[:, 0, :]

        else:
            output = self.model(**inputs, return_dict=True)
            pooler_output = output.get("pooler_output", None)
            last_hidden_state = output.get("last_hidden_state", None)
            query_emb = pooling(pooler_output, last_hidden_state, inputs["attention_mask"], self.pooling_method)
        if "dpr" not in self.model_name:
            query_emb = torch.nn.functional.normalize(query_emb, dim=-1)
        query_emb = query_emb.detach().cpu().numpy()
        query_emb = query_emb.astype(np.float32, order="C")
        return query_emb

    @torch.inference_mode()
    def encode(self, query_list: List[str], batch_size=64, is_query=True) -> np.ndarray:
        query_emb = []
        for i in tqdm(range(0, len(query_list), batch_size), desc="Encoding process: ", disable=self.silent):
            query_emb.append(self.single_batch_encode(query_list[i : i + batch_size], is_query))
        query_emb = np.concatenate(query_emb, axis=0)
        return query_emb

    @torch.inference_mode()
    def multi_gpu_encode(self, query_list: Union[List[str], str], batch_size=64, is_query=True) -> np.ndarray:
        if self.gpu_num > 1:
            self.model = torch.nn.DataParallel(self.model)
        query_emb = self.encode(query_list, batch_size, is_query)
        return query_emb


class STEncoder:
    """
    STEncoder class for encoding queries using SentenceTransformers.

    Attributes:
        model_name (str): The name of the model.
        model_path (str): The path to the model.
        max_length (int): The maximum length of the input sequences.
        use_fp16 (bool): Whether to use FP16 precision.
        instruction (str): Additional instructions for parsing queries.

    Methods:
        encode(query_list: List[str], batch_size=64, is_query=True) -> np.ndarray:
            Encodes a list of queries into embeddings.
        multi_gpu_encode(query_list: List[str], is_query=True, batch_size=None) -> np.ndarray:
            Encodes a list of queries into embeddings using multiple GPUs.
    """

    def __init__(self, model_name, model_path, max_length, use_fp16, instruction, silent=False):
        import torch
        from sentence_transformers import SentenceTransformer

        self.model_name = model_name
        self.model_path = model_path
        self.max_length = max_length
        self.use_fp16 = use_fp16
        self.instruction = instruction
        self.silent = silent
        self.model = SentenceTransformer(
            model_path, trust_remote_code=True, model_kwargs={"torch_dtype": torch.float16 if use_fp16 else torch.float}
        )

    @torch.inference_mode()
    def encode(self, query_list: Union[List[str], str], batch_size=64, is_query=True) -> np.ndarray:
        query_list = parse_query(self.model_name, query_list, self.instruction, is_query)
        query_emb = self.model.encode(
            query_list,
            batch_size=batch_size,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=not self.silent,
        )
        query_emb = query_emb.astype(np.float32, order="C")

        return query_emb

    @torch.inference_mode()
    def multi_gpu_encode(self, query_list: Union[List[str], str], batch_size=None, is_query=True) -> np.ndarray:
        query_list = parse_query(self.model_name, query_list, self.instruction, is_query)
        pool = self.model.start_multi_process_pool()
        query_emb = self.model.encode_multi_process(
            query_list,
            pool,
            normalize_embeddings=True,
            batch_size=batch_size,
            show_progress_bar=not self.silent,
        )
        self.model.stop_multi_process_pool(pool)
        query_emb = query_emb.astype(np.float32, order="C")

        return query_emb


class ClipEncoder:
    """ClipEncoder class for encoding queries using CLIP."""

    def __init__(self, model_name, model_path, silent=False):

        self.model_name = model_name
        self.model_path = model_path
        self.load_clip_model()
        self.silent = silent

    def load_clip_model(self):
        from transformers import AutoModel, AutoProcessor

        with open(os.path.join(self.model_path, "config.json")) as f:
            config = json.load(f)
        model_type = config.get("architectures", [None])[0]
        self.model_type = model_type

        if model_type == "CLIPModel" or model_type == "ChineseCLIPModel":
            self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True)
            self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
            # set model max length for chineseclipmodel
        elif model_type.endswith("CLIPModel"):
            self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True)
            self.processor = None
        else:
            raise NotImplementedError(f"Unsupported model type: {model_type}")

        self.model.eval()
        self.model.cuda()

        # set model max length for model that not specified in config.json
        if self.processor is not None and self.processor.tokenizer.model_max_length > 100000:
            try:
                model_max_length = config["text_config"]["max_position_embeddings"]
            except:
                model_max_length = 512
            self.processor.tokenizer.model_max_length = model_max_length

    @torch.inference_mode()
    def single_batch_encode(self, query_list: Union[List[str], str], modal="image") -> np.ndarray:
        encode_func_dict = {
            "text": self.encode_text,
            "image": self.encode_image,
        }
        return encode_func_dict[modal](query_list)

    @torch.inference_mode()
    def encode(self, query_list: List[str], batch_size=64, modal="image") -> np.ndarray:
        if not isinstance(query_list, list):
            query_list = [query_list]
        query_emb = []
        for i in tqdm(range(0, len(query_list), batch_size), desc="Encoding process: ", disable=self.silent):
            query_emb.append(self.single_batch_encode(query_list[i : i + batch_size], modal))
        query_emb = np.concatenate(query_emb, axis=0)
        return query_emb

    @torch.inference_mode()
    def multi_gpu_encode(self, query_list: Union[List[str], str], batch_size=64, is_query=True) -> np.ndarray:
        if self.gpu_num > 1:
            self.model = torch.nn.DataParallel(self.model)
        query_emb = self.encode(query_list, batch_size, is_query)
        return query_emb

    @torch.inference_mode()
    def encode_image(self, image_list: List) -> np.ndarray:
        # Each item in image_list: PIL Image, local path, or URL
        if self.model_type == "CLIPModel" or self.model_type == "ChineseCLIPModel":
            # need handle image
            image_list = [parse_image(image) for image in image_list]
            inputs = self.processor(images=image_list, return_tensors="pt")
            inputs = {k: v.cuda() for k, v in inputs.items()}
            image_emb = self.model.get_image_features(**inputs)
            image_emb = image_emb / image_emb.norm(p=2, dim=-1, keepdim=True)
            image_emb = image_emb.detach().cpu().numpy().astype(np.float32)
        elif self.model_type.endswith("CLIPModel"):
            image_emb = self.model.encode_image(image_list)
        else:
            raise NotImplementedError(f"Unsupported model type: {self.model_type}")
        return image_emb

    @torch.inference_mode()
    def encode_text(self, text_list: List[str]) -> np.ndarray:
        # Each item in image_list: PIL Image, local path, or URL
        if self.model_type == "CLIPModel" or self.model_type == "ChineseCLIPModel":
            inputs = self.processor(
                text=text_list,
                padding=True,
                truncation=True,
                return_tensors="pt",
            )
            inputs = {k: v.cuda() for k, v in inputs.items()}
            text_emb = self.model.get_text_features(**inputs)
            text_emb = text_emb / text_emb.norm(p=2, dim=-1, keepdim=True)
            text_emb = text_emb.detach().cpu().numpy().astype(np.float32)
        elif self.model_type.endswith("CLIPModel"):
            text_emb = self.model.encode_text(
                text_list,
                padding=True,
                truncation=True,
            )
        else:
            raise NotImplementedError(f"Unsupported model type: {self.model_type}")
        return text_emb



================================================
FILE: flashrag/retriever/index_builder.py
================================================
import re
import time
from concurrent.futures import ThreadPoolExecutor

import faiss
import json
import warnings
import numpy as np
from typing import Dict, List
import shutil
import subprocess
import argparse
import datasets
import torch
from tqdm import tqdm
from flashrag.retriever.utils import load_model, load_corpus, pooling, set_default_instruction, judge_zh
from transformers import AutoTokenizer, AutoModelForMaskedLM

import os
import multiprocessing

cores = str(multiprocessing.cpu_count())
os.environ["RAYON_NUM_THREADS"] = cores


class Index_Builder:
    r"""A tool class used to build an index used in retrieval."""

    def __init__(
            self,
            retrieval_method,
            model_path,
            corpus_path,
            save_dir,
            max_length,
            batch_size,
            use_fp16,
            n_postings=1000,
            centroid_fraction=0.2,
            min_cluster_size=2,
            summary_energy=0.4,
            batched_indexing=10000,
            corpus_embedded_path=None,
            pooling_method=None,
            instruction=None,
            faiss_type=None,
            embedding_path=None,
            save_embedding=False,
            faiss_gpu=False,
            use_sentence_transformer=False,
            bm25_backend="bm25s",
            index_modal="all",
            nknn=0,
    ):
        self.retrieval_method = retrieval_method.lower()
        self.model_path = model_path
        self.corpus_path = corpus_path
        self.corpus_embedded_path = corpus_embedded_path
        self.save_dir = save_dir
        self.max_length = max_length
        self.batch_size = batch_size
        self.use_fp16 = use_fp16
        self.instruction = instruction
        self.faiss_type = faiss_type if faiss_type is not None else "Flat"
        self.embedding_path = embedding_path
        self.save_embedding = save_embedding
        self.faiss_gpu = faiss_gpu
        self.use_sentence_transformer = use_sentence_transformer
        self.bm25_backend = bm25_backend
        self.index_modal = index_modal
        self.n_postings = n_postings
        self.centroid_fraction = centroid_fraction
        self.min_cluster_size = min_cluster_size
        self.summary_energy = summary_energy
        self.batched_indexing = batched_indexing
        self.nknn = nknn

        # judge if the retrieval model is clip
        self.is_clip = ("clip" in self.retrieval_method) or (self.model_path is not None and "clip" in self.model_path)
        if not self.is_clip:
            try:
                with open(os.path.join(self.model_path, "config.json")) as f:
                    config = json.load(f)
                model_type = config.get("architectures", [None])[0]
                self.is_clip = "clip" in model_type.lower()
            except:
                pass
        if self.is_clip:
            print("Use clip model!")

        # config pooling method
        if pooling_method is None:
            try:
                # read pooling method from 1_Pooling/config.json
                pooling_config = json.load(open(os.path.join(self.model_path, "1_Pooling/config.json")))
                for k, v in pooling_config.items():
                    if k.startswith("pooling_mode") and v == True:
                        pooling_method = k.split("pooling_mode_")[-1]
                        if pooling_method == "mean_tokens":
                            pooling_method = "mean"
                        elif pooling_method == "cls_token":
                            pooling_method = "cls"
                        else:
                            # raise warning: not implemented pooling method
                            warnings.warn(f"Pooling method {pooling_method} is not implemented.", UserWarning)
                            pooling_method = "mean"
                        break
            except:
                print(f"Pooling method not found in {self.model_path}, use default pooling method (mean).")
                # use default pooling method
                pooling_method = "mean"
        else:
            if pooling_method not in ["mean", "cls", "pooler"]:
                raise ValueError(f"Invalid pooling method {pooling_method}.")
        self.pooling_method = pooling_method

        self.gpu_num = torch.cuda.device_count()
        # prepare save dir
        print(self.save_dir)
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
        else:
            if not self._check_dir(self.save_dir):
                warnings.warn("Some files already exists in save dir and may be overwritten.", UserWarning)

        self.embedding_save_path = os.path.join(self.save_dir, f"emb_{self.retrieval_method}.memmap")

        self.corpus = load_corpus(self.corpus_path)

        print("Finish loading...")

    @staticmethod
    def _check_dir(dir_path):
        r"""Check if the dir path exists and if there is content."""

        if os.path.isdir(dir_path):
            if len(os.listdir(dir_path)) > 0:
                return False
        else:
            os.makedirs(dir_path, exist_ok=True)
        return True

    def build_index(self):
        r"""Constructing different indexes based on selective retrieval method."""
        if self.retrieval_method == "bm25":
            if self.bm25_backend == "pyserini":
                self.build_bm25_index_pyserini()
            elif self.bm25_backend == "bm25s":
                self.build_bm25_index_bm25s()
            else:
                assert False, "Invalid bm25 backend!"
        elif self.retrieval_method == "splade":
            from seismic import SeismicIndex
            self.build_seismic_index()
        else:
            self.build_dense_index()

    def build_seismic_index(self):
        """Build Seismic index after saving documents in required JSONL format using batch processing."""

        r"""Full Command:
        # Use "splade" (sparse embedding neural model) as retrieval method to trigger sysmic index costruction.
        python -m flashrag.retriever.index_builder \ # builder
        --retrieval_method splade \ # Model name to trigger seismic index (splade only available)
        --model_path retriever/splade-v3 \ # Local path or repository path are both supported.
        --corpus_embedded_path data/ms_marco/ms_marco_embedded_corpus.jsonl \  # Use cached embedded corpus if corpus is already available ins seismic expected format
        --corpus_path data/ms_marco/ms_marco_corpus.jsonl \ # Corpus path in format {id, contents} jsonl file to be embedded if not already built
        --save_dir indexes/ \ # save index directory
        --use_fp16 \ # tell to use fp16 for splade model
        --max_length 512 \ # max tokens for each document
        --batch_size 4 \ # batch size for splade model (Suggested between 2 and 24 for 16GB VRAM)
        --n_postings 1000 \ # seismic number of posting lists
        --centroid_fraction 0.2 \ # seismic centroids
        --min_cluster_size 2 \ # seismic min cluster
        --summary_energy 0.4 \ # seismic energy
        --batched_indexing 10000 # seismic batch
        --nknn 32
        """

        if self.pooling_method != 'max':
            print(
                f'Pooling method: {self.pooling_method.upper()} not supported on sparse neural retrieval models. fallback to: MAX.')
        # Load document encoder model (only splade i currently implemented at the moment)
        tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        model = AutoModelForMaskedLM.from_pretrained(self.model_path)
        # Use half precision
        if self.use_fp16:
            model = model.half()
        # Use more devices if available
        if torch.cuda.device_count() > 1:
            print(f"Using {torch.cuda.device_count()} GPUs")
            model = torch.nn.DataParallel(model)

        # Load to cuda
        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
        model.eval()

        # Create file for embedded corpus
        corpus_name = os.path.splitext(os.path.basename(self.corpus_path))[0]
        output_path = os.path.join(self.save_dir, f"{corpus_name}_{self.retrieval_method}_embedded.jsonl")

        # Init vars
        total_docs = len(self.corpus)
        processed_docs = 0
        start_time = time.time()
        last_update = start_time

        if self.corpus_embedded_path:
            print("Using cached corpus in seismic format.")
            if self.nknn > 0:
                index = SeismicIndex.build(
                    self.corpus_embedded_path,
                    n_postings=self.n_postings,
                    centroid_fraction=self.centroid_fraction,
                    min_cluster_size=self.min_cluster_size,
                    summary_energy=self.summary_energy,
                    batched_indexing=self.batched_indexing,
                    num_threads=int(cores),
                    nknn=self.nknn
                )
            else:
                index = SeismicIndex.build(
                    self.corpus_embedded_path,
                    n_postings=self.n_postings,
                    centroid_fraction=self.centroid_fraction,
                    min_cluster_size=self.min_cluster_size,
                    summary_energy=self.summary_energy,
                    batched_indexing=self.batched_indexing,
                    num_threads=int(cores)
                )
            index.save(os.path.join(self.save_dir, f"{corpus_name}_{self.retrieval_method}"))
            return index


        with open(output_path, 'w') as f_out, ThreadPoolExecutor(max_workers=1) as pool:
            # create progress bar
            progress_bar = tqdm(
                desc="Processing Documents",
                total=total_docs,
                unit="doc",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )

            batch = []
            for doc in self.corpus:
                # Load batch
                batch.append(doc)

                # On batch ready process batch
                if len(batch) >= self.batch_size:
                    self._process_batch(batch, f_out, model, tokenizer, pool)
                    processed_docs += len(batch)
                    batch = []

                    # Progress updates
                    current_time = time.time()
                    if processed_docs % 100 == 0 or (current_time - last_update) > 5.0:
                        elapsed = current_time - start_time
                        docs_per_sec = processed_docs / elapsed
                        remaining = (total_docs - processed_docs) / docs_per_sec

                        progress_bar.set_postfix({
                            'speed': f"{docs_per_sec:.1f} docs/s",
                            'ETA': f"{remaining / 60:.1f} min"
                        })
                        progress_bar.update(100 if processed_docs % 100 == 0 else processed_docs % 100)
                        last_update = current_time

            # Handle final leftover batch
            if batch:
                self._process_batch(batch, f_out, model, tokenizer, pool)
                progress_bar.update(len(batch))

            progress_bar.close()

        if self.nknn > 0:
            index = SeismicIndex.build(
                output_path,
                n_postings=self.n_postings,
                centroid_fraction=self.centroid_fraction,
                min_cluster_size=self.min_cluster_size,
                summary_energy=self.summary_energy,
                batched_indexing=self.batched_indexing,
                num_threads=int(cores),
                nknn=self.nknn
            )
        else:
            # Create index on embedded corpus file and save it
            index = SeismicIndex.build(
                output_path,
                n_postings=self.n_postings,
                centroid_fraction=self.centroid_fraction,
                min_cluster_size=self.min_cluster_size,
                summary_energy=self.summary_energy,
                batched_indexing=self.batched_indexing,
                num_threads=int(cores)
            )

        index.save(os.path.join(self.save_dir, f"{corpus_name}_{self.retrieval_method}"))
        return index

    def _process_batch(self, batch, f_out, model, tokenizer, pool):
        # Get embeddings
        texts = [doc['contents'] for doc in batch]
        vectors = self._get_sparse_embedding(texts, model, tokenizer)

        # create json for the batch
        def save(docs, embs):
            for doc, vector in zip(docs, embs):
                if 'vector' not in doc:
                    doc['vector'] = vector
                f_out.write(json.dumps({
                    "id": str(doc['id']),
                    "contents": doc['contents'],
                    "vector": doc['vector']
                }) + "\n")
        pool.submit(save, batch, vectors)

    def _get_sparse_embedding(self, texts: List[str], model, tokenizer) -> List[Dict[str, float]]:
        """Generate sparse embeddings for a batch of texts."""
        inputs = tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=self.max_length,
            add_special_tokens=True
        ).to(model.device)

        with torch.no_grad():
            logits = model(**inputs).logits  # [batch_size, seq_len, vocab_size]
            attention_mask = inputs["attention_mask"].unsqueeze(-1)

            scores = torch.log1p(torch.relu(logits)) * attention_mask

            v_repr = torch.max(scores, dim=1)[0]  # [batch_size, vocab_size]

            # Move to CPU (it seems much faster)
            v_repr = v_repr.cpu()
            nonzero_mask = v_repr > 1e-4

            # Get sparse values and indices in batch
            batch_indices, token_indices = torch.nonzero(nonzero_mask, as_tuple=True)
            token_scores = v_repr[batch_indices, token_indices]

            # Convert once all token IDs to strings (batched)
            unique_token_ids = torch.unique(token_indices)
            token_id_to_token = {
                idx.item(): tok for idx, tok in zip(
                    unique_token_ids, tokenizer.convert_ids_to_tokens(unique_token_ids.tolist())
                )
            }

            # Build final embeddings
            from collections import defaultdict
            embeddings = defaultdict(dict)
            for b_idx, t_idx, score in zip(batch_indices, token_indices, token_scores):
                embeddings[b_idx.item()][token_id_to_token[t_idx.item()]] = round(score.item(), 4)

            # Convert to list for each document
            return [embeddings[i] for i in range(len(texts))]

    @staticmethod
    def get_tokens_and_weights(sparse_embedding, tokenizer):
        token_weight_dict = {}
        for i in range(len(sparse_embedding.indices)):
            token = tokenizer.decode([sparse_embedding.indices[i]])
            weight = sparse_embedding.values[i]
            token_weight_dict[token] = round(weight, 4)

        # Sort the dictionary by weights
        token_weight_dict = dict(sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True))
        return token_weight_dict

    @staticmethod
    def clean_text(self, text: str) -> str:
        """Preprocess the text by removing special characters that seems to cause some problems in seismic index build resulting in a parse error."""
        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags
        text = re.sub(r'\s+', ' ', text)  # Normalize multiple spaces
        text = text.strip()  # Remove leading/trailing whitespaces
        return text

    def build_bm25_index_pyserini(self):
        """Building BM25 index based on Pyserini library.

        Reference: https://github.com/castorini/pyserini/blob/master/docs/usage-index.md#building-a-bm25-index-direct-java-implementation
        """

        # to use pyserini pipeline, we first need to place jsonl file in the folder
        self.save_dir = os.path.join(self.save_dir, "bm25")
        os.makedirs(self.save_dir, exist_ok=True)
        temp_dir = self.save_dir + "/temp"
        temp_file_path = temp_dir + "/temp.jsonl"
        os.makedirs(temp_dir, exist_ok=True)

        if self.corpus_path.endswith(".jsonl"):
            shutil.copyfile(self.corpus_path, temp_file_path)
            # check if the language is chinese
            with open(self.corpus_path, 'r', encoding='utf-8') as file:
                first_item = json.loads(file.readline())
                contents = first_item.get("contents", "")  # èŽ·å– contents å­—æ®µ
                zh_flag = judge_zh(contents)
        elif self.corpus_path.endswith(".parquet"):
            corpus = datasets.load_dataset('parquet', data_files=self.corpus_path, split="train")
            new_corpus = [{'id': idx, 'contents': text} for idx, text in enumerate(corpus['text'])]
            contents = new_corpus[0]['contents']
            zh_flag = judge_zh(contents)
            with open(temp_file_path, 'w', encoding='utf-8') as f:
                for item in new_corpus:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
        else:
            raise NotImplementedError

        print("Start building bm25 index...")
        pyserini_args = [
            "--collection",
            "JsonCollection",
            "--input",
            temp_dir,
            "--index",
            self.save_dir,
            "--generator",
            "DefaultLuceneDocumentGenerator",
            "--threads",
            "1",
        ]

        if zh_flag:
            print("Use chinese bm25 mode")
            pyserini_args.append("--language")
            pyserini_args.append("zh")

        subprocess.run(["python", "-m", "pyserini.index.lucene"] + pyserini_args)

        shutil.rmtree(temp_dir)

        print("Finish!")

    def build_bm25_index_bm25s(self):
        """Building BM25 index based on bm25s library."""

        import bm25s
        import Stemmer

        self.save_dir = os.path.join(self.save_dir, "bm25")
        os.makedirs(self.save_dir, exist_ok=True)

        corpus = load_corpus(self.corpus_path)
        # TODO: BM25s not support chinese well
        is_zh = judge_zh(corpus[0]['contents'])
        if is_zh:
            tokenizer = bm25s.tokenization.Tokenizer(stopwords='zh')
        else:
            stemmer = Stemmer.Stemmer("english")
            tokenizer = bm25s.tokenization.Tokenizer(stopwords='en', stemmer=stemmer)

        corpus_text = corpus["contents"]
        corpus_tokens = tokenizer.tokenize(corpus_text, return_as='tuple')
        retriever = bm25s.BM25(corpus=corpus, backend="numba")
        retriever.index(corpus_tokens)
        retriever.save(self.save_dir, corpus=None)
        tokenizer.save_vocab(self.save_dir)
        tokenizer.save_stopwords(self.save_dir)

        print("Finish!")

    def _load_embedding(self, embedding_path, corpus_size, hidden_size):
        all_embeddings = np.memmap(embedding_path, mode="r", dtype=np.float32).reshape(corpus_size, hidden_size)
        return all_embeddings

    def _save_embedding(self, all_embeddings):
        memmap = np.memmap(self.embedding_save_path, shape=all_embeddings.shape, mode="w+", dtype=all_embeddings.dtype)
        length = all_embeddings.shape[0]
        # add in batch
        save_batch_size = 10000
        if length > save_batch_size:
            for i in tqdm(range(0, length, save_batch_size), leave=False, desc="Saving Embeddings"):
                j = min(i + save_batch_size, length)
                memmap[i:j] = all_embeddings[i:j]
        else:
            memmap[:] = all_embeddings

    def encode_all(self):
        encode_data = [item["contents"] for item in self.corpus]
        if self.gpu_num > 1:
            print("Use multi gpu!")
            self.batch_size = self.batch_size * self.gpu_num
            all_embeddings = self.encoder.multi_gpu_encode(encode_data, batch_size=self.batch_size, is_query=False)
        else:
            all_embeddings = self.encoder.encode(encode_data, batch_size=self.batch_size, is_query=False)

        return all_embeddings

    def encode_all_clip(self):
        if self.index_modal == "all":
            modal_dict = {"text": None, "image": None}
        else:
            modal_dict = {self.index_modal: None}
        for modal, _ in modal_dict.items():
            encode_data = [item[modal] for item in self.corpus]
            if self.gpu_num > 1:
                print("Use multi gpu!")
                self.batch_size = self.batch_size * self.gpu_num
                all_embeddings = self.encoder.multi_gpu_encode(encode_data, batch_size=self.batch_size, modal=modal)
            else:
                all_embeddings = self.encoder.encode(encode_data, batch_size=self.batch_size, modal=modal)
            modal_dict[modal] = all_embeddings

        all_embeddings = np.concatenate(list(modal_dict.values()), axis=0)
        return all_embeddings

    @torch.no_grad()
    def build_dense_index(self):
        """Obtain the representation of documents based on the embedding model(BERT-based) and
        construct a faiss index.
        """

        if self.is_clip:
            from flashrag.retriever.encoder import ClipEncoder

            self.encoder = ClipEncoder(
                model_name=self.retrieval_method,
                model_path=self.model_path,
            )
            hidden_size = self.encoder.model.projection_dim

        elif self.use_sentence_transformer:
            from flashrag.retriever.encoder import STEncoder

            self.encoder = STEncoder(
                model_name=self.retrieval_method,
                model_path=self.model_path,
                max_length=self.max_length,
                use_fp16=self.use_fp16,
                instruction=self.instruction,
            )
            hidden_size = self.encoder.model.get_sentence_embedding_dimension()
        else:
            from flashrag.retriever.encoder import Encoder

            self.encoder = Encoder(
                model_name=self.retrieval_method,
                model_path=self.model_path,
                pooling_method=self.pooling_method,
                max_length=self.max_length,
                use_fp16=self.use_fp16,
                instruction=self.instruction,
            )
            hidden_size = self.encoder.model.config.hidden_size

        if self.embedding_path is not None:
            corpus_size = len(self.corpus)
            all_embeddings = self._load_embedding(self.embedding_path, corpus_size, hidden_size)
        else:
            all_embeddings = self.encode_all_clip() if self.is_clip else self.encode_all()
            if self.save_embedding:
                self._save_embedding(all_embeddings)
            del self.corpus

        # build index
        if self.is_clip:
            if self.index_modal == "all":
                assert all_embeddings.shape[0] % 2 == 0
                text_embedding = all_embeddings[: len(all_embeddings) // 2, :]
                image_embedding = all_embeddings[len(all_embeddings) // 2:, :]
                text_index_save_path = os.path.join(
                    self.save_dir, f"{self.retrieval_method}_{self.faiss_type}_text.index"
                )
                self.save_faiss_index(text_embedding, self.faiss_type, text_index_save_path)

                image_index_save_path = os.path.join(
                    self.save_dir, f"{self.retrieval_method}_{self.faiss_type}_image.index"
                )
                self.save_faiss_index(image_embedding, self.faiss_type, image_index_save_path)
            else:
                self.index_save_path = os.path.join(
                    self.save_dir, f"{self.retrieval_method}_{self.faiss_type}_{self.index_modal}.index"
                )
                self.save_faiss_index(all_embeddings, self.faiss_type, self.index_save_path)
        else:
            self.index_save_path = os.path.join(self.save_dir, f"{self.retrieval_method}_{self.faiss_type}.index")
            if os.path.exists(self.index_save_path):
                print("The index file already exists and will be overwritten.")
            self.save_faiss_index(all_embeddings, self.faiss_type, self.index_save_path)
        print("Finish!")

    def save_faiss_index(
            self,
            all_embeddings,
            faiss_type,
            index_save_path,
    ):
        # build index
        print("Creating index")
        dim = all_embeddings.shape[-1]
        faiss_index = faiss.index_factory(dim, faiss_type, faiss.METRIC_INNER_PRODUCT)

        if self.faiss_gpu:
            co = faiss.GpuMultipleClonerOptions()
            co.useFloat16 = True
            co.shard = True
            faiss_index = faiss.index_cpu_to_all_gpus(faiss_index, co)
            if not faiss_index.is_trained:
                faiss_index.train(all_embeddings)
            faiss_index.add(all_embeddings)
            faiss_index = faiss.index_gpu_to_cpu(faiss_index)
        else:
            if not faiss_index.is_trained:
                faiss_index.train(all_embeddings)
            faiss_index.add(all_embeddings)

        faiss.write_index(faiss_index, index_save_path)


import argparse


def main():
    parser = argparse.ArgumentParser(description="Creating index.")

    # Basic parameters
    parser.add_argument("--retrieval_method", type=str)
    parser.add_argument("--model_path", type=str, default=None)
    parser.add_argument("--corpus_path", type=str)
    parser.add_argument("--corpus_embedded_path", type=str, default=None)
    parser.add_argument("--save_dir", default="indexes/", type=str)

    # Parameters for building dense index
    parser.add_argument("--max_length", type=int, default=180)
    parser.add_argument("--batch_size", type=int, default=512)
    parser.add_argument("--use_fp16", default=False, action="store_true")
    parser.add_argument("--pooling_method", type=str, default=None)
    parser.add_argument("--instruction", type=str, default=None)
    parser.add_argument("--faiss_type", default=None, type=str)
    parser.add_argument("--embedding_path", default=None, type=str)
    parser.add_argument("--save_embedding", action="store_true", default=False)
    parser.add_argument("--faiss_gpu", default=False, action="store_true")
    parser.add_argument("--sentence_transformer", action="store_true", default=False)
    parser.add_argument("--bm25_backend", default="pyserini", choices=["bm25s", "pyserini"])

    # Parameters for build multi-modal retriever index
    parser.add_argument("--index_modal", type=str, default="all", choices=["text", "image", "all"])

    # New arguments for seismic index
    parser.add_argument("--n_postings", type=int, default=1000)
    parser.add_argument("--centroid_fraction", type=float, default=0.2)
    parser.add_argument("--min_cluster_size", type=int, default=2)
    parser.add_argument("--summary_energy", type=float, default=0.4)
    parser.add_argument("--nknn", type=int, default=0)
    parser.add_argument("--batched_indexing", type=int, default=10000)

    args = parser.parse_args()

    index_builder = Index_Builder(
        retrieval_method=args.retrieval_method,
        model_path=args.model_path,
        corpus_path=args.corpus_path,
        save_dir=args.save_dir,
        max_length=args.max_length,
        batch_size=args.batch_size,
        use_fp16=args.use_fp16,
        pooling_method=args.pooling_method,
        instruction=args.instruction,
        faiss_type=args.faiss_type,
        embedding_path=args.embedding_path,
        save_embedding=args.save_embedding,
        faiss_gpu=args.faiss_gpu,
        use_sentence_transformer=args.sentence_transformer,
        bm25_backend=args.bm25_backend,
        index_modal=args.index_modal,
        n_postings=args.n_postings,
        centroid_fraction=args.centroid_fraction,
        min_cluster_size=args.min_cluster_size,
        summary_energy=args.summary_energy,
        batched_indexing=args.batched_indexing,
        corpus_embedded_path=args.corpus_embedded_path,
        nknn=args.nknn
    )
    index_builder.build_index()


if __name__ == "__main__":
    main()



================================================
FILE: flashrag/retriever/reranker.py
================================================
from typing import List
import torch
import warnings
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from flashrag.retriever.encoder import Encoder


class BaseReranker:
    r"""Base object for all rerankers."""

    def __init__(self, config):
        self.config = config
        self.reranker_model_name = config["rerank_model_name"]
        self.reranker_model_path = config["rerank_model_path"]
        self.topk = config["rerank_topk"]
        self.max_length = config["rerank_max_length"]
        self.batch_size = config["rerank_batch_size"]
        self.device = config["device"]

    def get_rerank_scores(self, query_list: List[str], doc_list: List[str], batch_size):
        """Return flatten list of scores for each (query,doc) pair
        Args:
            query_list: List of N queries
            doc_list:  Nested list of length N, each element corresponds to K documents of a query

        Return:
            [score(q1,d1), score(q1,d2),... score(q2,d1),...]
        """
        all_scores = []
        return all_scores

    @torch.inference_mode(mode=True)
    def rerank(self, query_list, doc_list, batch_size=None, topk=None):
        r"""Rerank doc_list."""
        if batch_size is None:
            batch_size = self.batch_size
        if topk is None:
            topk = self.topk
        if isinstance(query_list, str):
            query_list = [query_list]
        if not isinstance(doc_list[0], list):
            doc_list = [doc_list]

        assert len(query_list) == len(doc_list)
        if topk > min([len(docs) for docs in doc_list]):
            warnings.warn("The number of doc returned by the retriever is less than the topk.")

        # get doc contents
        doc_contents = []
        for docs in doc_list:
            if all([isinstance(doc, str) for doc in docs]):
                doc_contents.append([doc for doc in docs])
            else:
                doc_contents.append([doc["contents"] for doc in docs])

        all_scores = self.get_rerank_scores(query_list, doc_contents, batch_size)
        assert len(all_scores) == sum([len(docs) for docs in doc_list])

        # sort docs
        start_idx = 0
        final_scores = []
        final_docs = []
        for docs in doc_list:
            doc_scores = all_scores[start_idx : start_idx + len(docs)]
            doc_scores = [float(score) for score in doc_scores]
            sort_idxs = np.argsort(doc_scores)[::-1][:topk]
            start_idx += len(docs)

            final_docs.append([docs[idx] for idx in sort_idxs])
            final_scores.append([doc_scores[idx] for idx in sort_idxs])

        return final_docs, final_scores


class CrossReranker(BaseReranker):
    def __init__(self, config):
        super().__init__(config)
        self.tokenizer = AutoTokenizer.from_pretrained(self.reranker_model_path)
        self.ranker = AutoModelForSequenceClassification.from_pretrained(self.reranker_model_path, num_labels=1)
        self.ranker.eval()
        self.ranker.to(self.device)

    @torch.inference_mode(mode=True)
    def get_rerank_scores(self, query_list, doc_list, batch_size):
        # flatten all pairs
        all_pairs = []
        for query, docs in zip(query_list, doc_list):
            all_pairs.extend([[query, doc] for doc in docs])
        all_scores = []
        for start_idx in tqdm(range(0, len(all_pairs), batch_size), desc="Reranking process: "):
            pair_batch = all_pairs[start_idx : start_idx + batch_size]

            inputs = self.tokenizer(
                pair_batch, padding=True, truncation=True, return_tensors="pt", max_length=self.max_length
            ).to(self.device)
            batch_scores = (
                self.ranker(**inputs, return_dict=True)
                .logits.view(
                    -1,
                )
                .float()
                .cpu()
            )
            all_scores.extend(batch_scores)

        return all_scores


class BiReranker(BaseReranker):
    def __init__(self, config):
        super().__init__(config)
        self.encoder = Encoder(
            model_name=self.reranker_model_name,
            model_path=self.reranker_model_path,
            pooling_method=config["rerank_pooling_method"],
            max_length=self.max_length,
            use_fp16=config["rerank_use_fp16"],
        )

    def get_rerank_scores(self, query_list, doc_list, batch_size):
        query_emb = []
        for start_idx in range(0, len(query_list), batch_size):
            query_batch = query_list[start_idx : start_idx + batch_size]
            batch_emb = self.encoder.encode(query_batch, is_query=True)
            query_emb.append(batch_emb)
        query_emb = np.concatenate(query_emb, axis=0)

        flat_doc_list = sum(doc_list, [])
        doc_emb = []
        for start_idx in range(0, len(flat_doc_list), batch_size):
            doc_batch = flat_doc_list[start_idx : start_idx + batch_size]
            batch_emb = self.encoder.encode(doc_batch, is_query=False)
            doc_emb.append(batch_emb)
        doc_emb = np.concatenate(doc_emb, axis=0)

        scores = query_emb @ doc_emb.T  # K*L
        all_scores = []
        score_idx = 0
        for idx, doc in enumerate(doc_list):
            all_scores.extend(scores[idx, score_idx : score_idx + len(doc)])
            score_idx += len(doc)

        return all_scores



================================================
FILE: flashrag/retriever/retriever.py
================================================
import json
import os
import time
import requests

os.environ["TOKENIZERS_PARALLELISM"] = "false"
import warnings
from typing import List, Dict, Union
import functools
from tqdm import tqdm
import faiss
import copy
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from flashrag.utils import get_reranker, get_device
from flashrag.retriever.utils import load_corpus, load_docs, convert_numpy, judge_image, judge_zh
from flashrag.retriever.encoder import Encoder, STEncoder, ClipEncoder
import torch

if get_device() == "cpu":
    faiss.omp_set_num_threads(1)

def cache_manager(func):
    """
    Decorator used for retrieving document cache.
    With the decorator, The retriever can store each retrieved document as a file and reuse it.
    """

    @functools.wraps(func)
    def wrapper(self, query=None, num=None, return_score=False):
        if num is None:
            num = self.topk
        if self.use_cache:
            if isinstance(query, str):
                new_query_list = [query]
            else:
                new_query_list = query

            no_cache_query = []
            cache_results = []
            for new_query in new_query_list:
                if new_query in self.cache:
                    cache_res = self.cache[new_query]
                    if len(cache_res) < num:
                        warnings.warn(f"The number of cached retrieval results is less than topk ({num})")
                    cache_res = cache_res[:num]
                    # separate the doc score
                    doc_scores = [item["score"] for item in cache_res]
                    cache_results.append((cache_res, doc_scores))
                else:
                    cache_results.append(None)
                    no_cache_query.append(new_query)

            if no_cache_query != []:
                # use batch search without decorator
                no_cache_results, no_cache_scores = self._batch_search_with_rerank(no_cache_query, num, True)
                no_cache_idx = 0
                for idx, res in enumerate(cache_results):
                    if res is None:
                        assert new_query_list[idx] == no_cache_query[no_cache_idx]
                        cache_results[idx] = (
                            no_cache_results[no_cache_idx],
                            no_cache_scores[no_cache_idx],
                        )
                        no_cache_idx += 1

            results, scores = (
                [t[0] for t in cache_results],
                [t[1] for t in cache_results],
            )

        else:
            results, scores = func(self, query=query, num=num, return_score=True)

        if self.save_cache:
            # merge result and score
            save_results = results.copy()
            save_scores = scores.copy()
            if isinstance(query, str):
                query = [query]
                if "batch" not in func.__name__:
                    save_results = [save_results]
                    save_scores = [save_scores]
            for new_query, doc_items, doc_scores in zip(query, save_results, save_scores):
                for item, score in zip(doc_items, doc_scores):
                    item["score"] = score
                self.cache[new_query] = doc_items

        if return_score:
            return results, scores
        else:
            return results

    return wrapper


def rerank_manager(func):
    """
    Decorator used for reranking retrieved documents.
    """

    @functools.wraps(func)
    def wrapper(self, query, num=None, return_score=False):
        results, scores = func(self, query=query, num=num, return_score=True)
        if self.use_reranker:
            results, scores = self.reranker.rerank(query, results)
            if "batch" not in func.__name__:
                results = results[0]
                scores = scores[0]
        if return_score:
            return results, scores
        else:
            return results

    return wrapper


class BaseRetriever:
    """Base object for all retrievers."""

    def __init__(self, config):
        self._config = config
        self.update_config()

    @property
    def config(self):
        return self._config

    @config.setter
    def config(self, config_data):
        self._config = config_data
        self.update_config()

    def update_config(self):
        self.update_base_setting()
        self.update_additional_setting()

    def update_base_setting(self):
        self.retrieval_method = self._config["retrieval_method"]
        self.topk = self._config["retrieval_topk"]

        self.index_path = self._config["index_path"]
        self.corpus_path = self._config["corpus_path"]

        self.save_cache = self._config["save_retrieval_cache"]
        self.use_cache = self._config["use_retrieval_cache"]
        self.cache_path = self._config["retrieval_cache_path"]

        self.use_reranker = self._config["use_reranker"]
        if self.use_reranker:
            self.reranker = get_reranker(self._config)
        else:
            self.reranker = None

        if self.save_cache:
            self.cache_save_path = os.path.join(self._config["save_dir"], "retrieval_cache.json")
            self.cache = {}
        if self.use_cache:
            assert self.cache_path is not None
            with open(self.cache_path, "r") as f:
                self.cache = json.load(f)
        self.silent = self._config["silent_retrieval"] if "silent_retrieval" in self._config else False

    def update_additional_setting(self):
        pass

    def _save_cache(self):
        self.cache = convert_numpy(self.cache)

        def custom_serializer(obj):
            if isinstance(obj, np.float32):
                return float(obj)
            raise TypeError(f"Type {type(obj)} not serializable")

        with open(self.cache_save_path, "w") as f:
            json.dump(self.cache, f, indent=4, default=custom_serializer)

    def _search(self, query: str, num: int, return_score: bool) -> List[Dict[str, str]]:
        r"""Retrieve topk relevant documents in corpus.

        Return:
            list: contains information related to the document, including:
                contents: used for building index
                title: (if provided)
                text: (if provided)

        """

        pass

    def _batch_search(self, query, num, return_score):
        pass

    def search(self, *args, **kwargs):
        return self._search(*args, **kwargs)

    def batch_search(self, *args, **kwargs):
        return self._batch_search(*args, **kwargs)


class BaseTextRetriever(BaseRetriever):
    """Base text retriever."""

    def __init__(self, config):
        super().__init__(config)

    @cache_manager
    @rerank_manager
    def search(self, *args, **kwargs):
        return self._search(*args, **kwargs)

    @cache_manager
    @rerank_manager
    def batch_search(self, *args, **kwargs):
        return self._batch_search(*args, **kwargs)

    @rerank_manager
    def _batch_search_with_rerank(self, *args, **kwargs):
        return self._batch_search(*args, **kwargs)

    @rerank_manager
    def _search_with_rerank(self, *args, **kwargs):
        return self._search(*args, **kwargs)


class BM25Retriever(BaseTextRetriever):
    r"""BM25 retriever based on pre-built pyserini index."""

    def __init__(self, config, corpus=None):
        super().__init__(config)
        self.load_model_corpus(corpus)

    def update_additional_setting(self):
        self.backend = self._config["bm25_backend"]

    def load_model_corpus(self, corpus):
        if self.backend == "pyserini":
            # Warning: the method based on pyserini will be deprecated
            from pyserini.search.lucene import LuceneSearcher

            self.searcher = LuceneSearcher(self.index_path)
            self.contain_doc = self._check_contain_doc()
            if not self.contain_doc:
                if corpus is None:
                    self.corpus = load_corpus(self.corpus_path)
                else:
                    self.corpus = corpus
            self.max_process_num = 8
               
        elif self.backend == "bm25s":
            import Stemmer
            import bm25s

            self.corpus = load_corpus(self.corpus_path)
            is_zh = judge_zh(self.corpus[0]["contents"])

            self.searcher = bm25s.BM25.load(self.index_path, mmap=True, load_corpus=False)
            if is_zh:
                self.tokenizer = bm25s.tokenization.Tokenizer(stopwords="zh")
                self.tokenizer.load_stopwords(self.index_path)
                self.tokenizer.load_vocab(self.index_path)
            else:
                stemmer = Stemmer.Stemmer("english")
                self.tokenizer = bm25s.tokenization.Tokenizer(stopwords="en", stemmer=stemmer)
                self.tokenizer.load_stopwords(self.index_path)
                self.tokenizer.load_vocab(self.index_path)

            self.searcher.corpus = self.corpus
            self.searcher.backend = "numba"

        else:
            assert False, "Invalid bm25 backend!"

    def _check_contain_doc(self):
        r"""Check if the index contains document content"""
        return self.searcher.doc(0).raw() is not None

    def _search(self, query: str, num: int = None, return_score=False) -> List[Dict[str, str]]:
        if num is None:
            num = self.topk
        if self.backend == "pyserini":
            is_zh = judge_zh(query)
            if is_zh:
                self.searcher.set_language("zh")
            hits = self.searcher.search(query, num)
            if len(hits) < 1:
                if return_score:
                    return [], []
                else:
                    return []

            scores = [hit.score for hit in hits]
            if len(hits) < num:
                warnings.warn("Not enough documents retrieved!")
            else:
                hits = hits[:num]

            if self.contain_doc:
                all_contents = [json.loads(self.searcher.doc(hit.docid).raw())["contents"] for hit in hits]
                results = [
                    {
                        "id": hit.docid, 
                        "title": content.split("\n")[0].strip('"'),
                        "text": "\n".join(content.split("\n")[1:]),
                        "contents": content,
                    }
                    for content, hit in zip(all_contents, hits)
                ]
            else:
                results = load_docs(self.corpus, [hit.docid for hit in hits])
        elif self.backend == "bm25s":
            import bm25s

            # query_tokens = self.tokenizer.tokenize([query], return_as="tuple", update_vocab=False)
            query_tokens = bm25s.tokenize([query])
            results, scores = self.searcher.retrieve(query_tokens, k=num)
            results = list(results[0])
            scores = list(scores[0])
        else:
            assert False, "Invalid bm25 backend!"

        if return_score:
            return results, scores
        else:
            return results

    def _batch_search(self, query, num: int = None, return_score=False):
        if self.backend == "pyserini":
            # TODO: modify batch method
            results = []
            scores = []
            for _query in query:
                item_result, item_score = self._search(_query, num, True)
                results.append(item_result)
                scores.append(item_score)
        elif self.backend == "bm25s":
            import bm25s

            # query_tokens = self.tokenizer.tokenize(query, return_as="tuple", update_vocab=False)
            query_tokens = bm25s.tokenize(query)
            results, scores = self.searcher.retrieve(query_tokens, k=num)
        else:
            assert False, "Invalid bm25 backend!"
        results = results.tolist() if isinstance(results, np.ndarray) else results
        scores = scores.tolist() if isinstance(scores, np.ndarray) else scores
        if return_score:
            return results, scores
        else:
            return results


class DenseRetriever(BaseTextRetriever):
    r"""Dense retriever based on pre-built faiss index."""

    def __init__(self, config: dict, corpus=None):
        super().__init__(config)

        self.load_corpus(corpus)
        self.load_index()
        self.load_model()

    def load_corpus(self, corpus):
        if corpus is None:
            self.corpus = load_corpus(self.corpus_path)
        else:
            self.corpus = corpus

    def load_index(self):
        if self.index_path is None or not os.path.exists(self.index_path):
            raise Warning(f"Index file {self.index_path} does not exist!")
        self.index = faiss.read_index(self.index_path)
        if self.use_faiss_gpu:
            co = faiss.GpuMultipleClonerOptions()
            co.useFloat16 = True
            co.shard = True
            self.index = faiss.index_cpu_to_all_gpus(self.index, co=co)

    def update_additional_setting(self):
        self.query_max_length = self._config["retrieval_query_max_length"]
        self.pooling_method = self._config["retrieval_pooling_method"]
        self.use_fp16 = self._config["retrieval_use_fp16"]
        self.batch_size = self._config["retrieval_batch_size"]
        self.instruction = self._config["instruction"]

        self.retrieval_model_path = self._config["retrieval_model_path"]
        self.use_st = self._config["use_sentence_transformer"]
        self.use_faiss_gpu = self._config["faiss_gpu"]

    def load_model(self):
        if self.use_st:
            self.encoder = STEncoder(
                model_name=self.retrieval_method,
                model_path=self._config["retrieval_model_path"],
                max_length=self.query_max_length,
                use_fp16=self.use_fp16,
                instruction=self.instruction,
                silent=self.silent,
            )
        else:
            # check pooling method
            self._check_pooling_method(self.retrieval_model_path, self.pooling_method)
            self.encoder = Encoder(
                model_name=self.retrieval_method,
                model_path=self.retrieval_model_path,
                pooling_method=self.pooling_method,
                max_length=self.query_max_length,
                use_fp16=self.use_fp16,
                instruction=self.instruction,
            )

    def _check_pooling_method(self, model_path, pooling_method):
        try:
            # read pooling method from 1_Pooling/config.json
            pooling_config = json.load(open(os.path.join(model_path, "1_Pooling/config.json")))
            for k, v in pooling_config.items():
                if k.startswith("pooling_mode") and v == True:
                    detect_pooling_method = k.split("pooling_mode_")[-1]
                    if detect_pooling_method == "mean_tokens":
                        detect_pooling_method = "mean"
                    elif detect_pooling_method == "cls_token":
                        detect_pooling_method = "cls"
                    else:
                        # raise warning: not implemented pooling method
                        warnings.warn(f"Pooling method {detect_pooling_method} is not implemented.", UserWarning)
                        detect_pooling_method = "mean"
                    break
        except:
            detect_pooling_method = None

        if detect_pooling_method is not None and detect_pooling_method != pooling_method:
            warnings.warn(
                f"Pooling method in model config file is {detect_pooling_method}, but the input is {pooling_method}. Please check carefully."
            )

    def _search(self, query: str, num: int = None, return_score=False):
        if num is None:
            num = self.topk
        query_emb = self.encoder.encode(query)
        scores, idxs = self.index.search(query_emb, k=num)
        scores = scores.tolist()
        idxs = idxs[0]
        scores = scores[0]

        results = load_docs(self.corpus, idxs)
        if return_score:
            return results, scores
        else:
            return results

    def _batch_search(self, query: List[str], num: int = None, return_score=False):
        if isinstance(query, str):
            query = [query]
        if num is None:
            num = self.topk
        batch_size = self.batch_size

        results = []
        scores = []
        emb = self.encoder.encode(query, batch_size=batch_size, is_query=True)
        scores, idxs = self.index.search(emb, k=num)
        scores = scores.tolist()
        idxs = idxs.tolist()

        flat_idxs = [idx for sublist in idxs for idx in sublist]
        results = load_docs(self.corpus, flat_idxs)
        results = [results[i * num : (i + 1) * num] for i in range(len(idxs))]

        if return_score:
            return results, scores
        else:
            return results


class MultiModalRetriever(BaseRetriever):
    r"""Multi-modal retriever based on pre-built faiss index."""

    def __init__(self, config: dict, corpus=None):
        super().__init__(config)
        self.mm_index_dict = config[
            "multimodal_index_path_dict"
        ]  # {"text": "path/to/text_index", "image": "path/to/image_index"}
        self.index_dict = {"text": None, "image": None}
        for modal in ["text", "image"]:
            idx_path = self.mm_index_dict[modal]
            if idx_path is not None:
                self.index_dict[modal] = faiss.read_index(idx_path)
            if config["faiss_gpu"]:
                co = faiss.GpuMultipleClonerOptions()
                co.useFloat16 = True
                co.shard = True
                self.index_dict[modal] = faiss.index_cpu_to_all_gpus(self.index_dict[modal], co=co)
        if corpus is None:
            self.corpus = load_corpus(self.corpus_path)
        else:
            self.corpus = corpus
        self.topk = config["retrieval_topk"]
        self.batch_size = config["retrieval_batch_size"]

        self.encoder = ClipEncoder(
            model_name=self.retrieval_method, model_path=config["retrieval_model_path"], silent=self.silent
        )

    def _judge_input_modal(self, query):
        if not isinstance(query, str):
            return "image"
        else:
            if query.startswith("http") or query.endswith(".jpg") or query.endswith(".png"):
                return "image"
            else:
                return "text"

    def _search(self, query, target_modal: str = "text", num: int = None, return_score=False):
        if num is None:
            num = self.topk
        assert target_modal in ["image", "text"]

        query_modal = (
            self._judge_input_modal(query) if not isinstance(query, list) else self._judge_input_modal(query[0])
        )
        if query_modal == "image" and isinstance(query, str):
            from PIL import Image

            if os.path.exists(query):
                query = Image.open(query)
            else:
                import requests

                query = Image.open(requests.get(query, stream=True).raw)

        query_emb = self.encoder.encode(query, modal=query_modal)

        scores, idxs = self.index_dict[target_modal].search(query_emb, k=num)
        scores = scores.tolist()
        idxs = idxs[0]
        scores = scores[0]

        results = load_docs(self.corpus, idxs)
        if return_score:
            return results, scores
        else:
            return results

    def _batch_search(self, query: List[str], target_modal: str = "text", num: int = None, return_score=False):
        if isinstance(query, str):
            query = [query]
        if num is None:
            num = self.topk
        batch_size = self.batch_size
        assert target_modal in ["image", "text"]

        query_modal = self._judge_input_modal(query[0])
        if query_modal == "image" and isinstance(query[0], str):
            from PIL import Image
            import requests

            if os.path.exists(query[0]):
                query = [Image.open(q) for q in query]
            else:
                query = [Image.open(requests.get(q, stream=True).raw) for q in query]

        results = []
        scores = []

        for start_idx in tqdm(range(0, len(query), batch_size), desc="Retrieval process: ", disable=self.silent):
            query_batch = query[start_idx : start_idx + batch_size]
            batch_emb = self.encoder.encode(query_batch, modal=query_modal)
            batch_scores, batch_idxs = self.index_dict[target_modal].search(batch_emb, k=num)

            batch_scores = batch_scores.tolist()
            batch_idxs = batch_idxs.tolist()

            flat_idxs = flat_idxs = [idx for sublist in batch_idxs for idx in sublist]
            batch_results = load_docs(self.corpus, flat_idxs)
            batch_results = [batch_results[i * num : (i + 1) * num] for i in range(len(batch_idxs))]

            scores.extend(batch_scores)
            results.extend(batch_results)

        if return_score:
            return results, scores
        else:
            return results


class MultiRetrieverRouter:
    def __init__(self, config):
        self.merge_method = config["multi_retriever_setting"].get("merge_method", "concat")  # concat/rrf/rerank
        self.final_topk = config["multi_retriever_setting"].get("topk", 5)
        self.retriever_list = self.load_all_retriever(config)
        self.config = config

        if self.merge_method == "rerank":
            config["multi_retriever_setting"]["rerank_topk"] = self.final_topk
            config["multi_retriever_setting"]["device"] = config["device"]
            self.reranker = get_reranker(config["multi_retriever_setting"])

    def load_all_retriever(self, config):
        retriever_config_list = config["multi_retriever_setting"]["retriever_list"]
        # use the same corpus for efficient memory usage
        all_corpus_dict = {}
        retriever_list = []
        for retriever_config in retriever_config_list:
            retrieval_method = retriever_config["retrieval_method"]
            print(f"Loading {retrieval_method} retriever...")
            retrieval_model_path = retriever_config["retrieval_model_path"]
            corpus_path = retriever_config["corpus_path"]

            if retrieval_method == "bm25":
                if corpus_path is None:
                    corpus = None
                else:
                    if corpus_path in all_corpus_dict:
                        corpus = all_corpus_dict[corpus_path]
                    else:
                        corpus = load_corpus(corpus_path)
                        all_corpus_dict[corpus_path] = corpus
                retriever = BM25Retriever(retriever_config, corpus)
            else:
                if corpus_path in all_corpus_dict:
                    corpus = all_corpus_dict[corpus_path]
                else:
                    corpus = load_corpus(corpus_path)
                    all_corpus_dict[corpus_path] = corpus

                # judge modality
                from transformers import AutoConfig

                try:
                    model_config = AutoConfig.from_pretrained(retrieval_model_path)
                    arch = model_config.architectures[0]
                    print("arch: ", arch)
                    if "clip" in arch.lower():
                        retriever = MultiModalRetriever(retriever_config, corpus)
                    else:
                        retriever = DenseRetriever(retriever_config, corpus)
                except:
                    retriever = DenseRetriever(retriever_config, corpus)

            retriever_list.append(retriever)

        return retriever_list

    def add_source(self, result: Union[list, tuple], retriever):
        retrieval_method = retriever.retrieval_method
        corpus_path = retriever.corpus_path
        is_multimodal = isinstance(retriever, MultiModalRetriever)
        # for naive search, result is a list of dict, each repr a doc
        # for batch search, result is a list of list, each repr a doc list(per query)
        for item in result:
            if isinstance(item, list):
                for _item in item:
                    _item["source"] = retrieval_method
                    _item["corpus_path"] = corpus_path
                    _item["is_multimodal"] = is_multimodal
            else:
                item["source"] = retrieval_method
                item["corpus_path"] = corpus_path
                item["is_multimodal"] = is_multimodal
        return result

    def _search_or_batch_search(self, query: Union[str, list], target_modal, num, return_score, method, retriever_list):
        if num is None:
            num = self.final_topk

        result_list = []
        score_list = []

        def process_retriever(retriever):
            is_multimodal = isinstance(retriever, MultiModalRetriever)
            params = {"query": query, "return_score": return_score}

            if is_multimodal:
                params["target_modal"] = target_modal

            if method == "search":
                output = retriever.search(**params)
            else:
                output = retriever.batch_search(**params)

            if return_score:
                result, score = output
            else:
                result = output
                score = None

            result = self.add_source(result, retriever)
            return result, score

        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_retriever = {
                executor.submit(process_retriever, retriever): retriever for retriever in retriever_list
            }
            for future in as_completed(future_to_retriever):
                try:
                    result, score = future.result()
                    result_list.extend(result)
                    if score is not None:
                        score_list.extend(score)
                except Exception as e:
                    print(f"Error processing retriever {future_to_retriever[future]}: {e}")
        result_list, score_list = self.reorder(result_list, score_list, retriever_list)
        result_list, score_list = self.post_process_result(query, result_list, score_list, num)
        if return_score:
            return result_list, score_list
        else:
            return result_list

    def reorder(self, result_list, score_list, retriever_list):
        """
        batch_search:
        original result like: [[bm25-q1-d1, bm25-q1-d2],[bm25-q2-d1, bm25-q2-d2], [e5-q1-d1, e5-q1-d2], [e5-q2-d1, e5-q2-d2]]
        reorder to: [[bm25-q1-d1, bm25-q1-d2, e5-q1-d1, e5-q1-d2], [bm25-q2-d1,bm25-q2-d2, e5-q2-d1, e5-q2-d2]]

        navie search:
        original result like: [bm25-d1, bm25-d2, e5-d1, e5-d2]

        """

        retriever_num = len(retriever_list)
        query_num = len(result_list) // retriever_num
        assert query_num * retriever_num == len(result_list)

        if isinstance(result_list[0], dict):
            return result_list, score_list

        final_result = []
        final_score = []
        for q_idx in range(query_num):
            final_result.append(sum([result_list[q_idx + r_idx * query_num] for r_idx in range(retriever_num)], []))
            if score_list != []:
                final_score.append(sum([score_list[q_idx + r_idx * query_num] for r_idx in range(retriever_num)], []))
        return final_result, final_score

    def post_process_result(self, query: Union[str, list], result_list, score_list, num):
        # based on self.merge_method
        if self.merge_method == "concat":
            # remove duplicate doc
            if isinstance(result_list[0], dict):
                exist_id = set()
                for idx, doc in enumerate(result_list):
                    if doc["id"] not in exist_id:
                        exist_id.add(doc["id"])
                    else:
                        result_list.remove(doc)
                        if score_list != []:
                            score_list.remove(idx)
            else:
                for query_idx, query_doc_list in enumerate(result_list):
                    exist_id = set()
                    for doc_idx, doc in enumerate(query_doc_list):
                        if doc["id"] not in exist_id:
                            exist_id.add(doc["id"])
                        else:
                            query_doc_list.remove(doc)
                            if score_list != []:
                                score_list[query_idx].remove(doc_idx)
            return result_list, score_list
        elif self.merge_method == "rrf":
            if (isinstance(result_list[0], dict) and len(set([doc["corpus_path"] for doc in result_list])) > 1) or (
                isinstance(result_list[0], list) and len(set([doc["corpus_path"] for doc in result_list[0]])) > 1
            ):
                warnings.warn(
                    "Using multiple corpus may lead to conflicts in DOC IDs, which may result in incorrect rrf results!"
                )
            if isinstance(result_list[0], dict):
                result_list, score_list = self.rrf_merge([result_list], num, k=60)
                result_list = result_list[0]
                score_list = score_list[0]
            else:
                result_list, score_list = self.rrf_merge(result_list, num, k=60)
            return result_list, score_list
        elif self.merge_method == "rerank":
            if isinstance(result_list[0], dict):
                query, result_list, score_list = [query], [result_list], [score_list]
            # parse the result of multimodal corpus
            for item_result in result_list:
                for item in item_result:
                    if item["is_multimodal"]:
                        item["contents"] = item["text"]
            # rerank all docs
            print(result_list)
            result_list, score_list = self.reranker.rerank(query, result_list, topk=num)
            if isinstance(query, str):
                result_list, score_list = result_list[0], score_list[0]
            return result_list, score_list
        else:
            raise NotImplementedError

    def rrf_merge(self, results, topk=10, k=60):
        """
        Perform Reciprocal Rank Fusion (RRF) on retrieval results.

        Args:
            results (list of list of dict): Retrieval results for multiple queries.
            topk (int): Number of top results to return per query.
            k (int): RRF hyperparameter to adjust rank contribution.

        Returns:
            list of list of dict: Fused results with topk highest scores per query.
        """
        fused_results = []
        fused_scores = []
        for query_results in results:
            # Initialize a score dictionary to accumulate RRF scores
            score_dict = {}
            retriever_result_dict = {}
            id2item = {}
            for item in query_results:
                source = item["source"]
                if source not in retriever_result_dict:
                    retriever_result_dict[source] = []
                retriever_result_dict[source].append(item["id"])
                id2item[item["id"]] = item

            # Calculate RRF scores for each document
            for retriever, retriever_result in retriever_result_dict.items():
                for rank, doc_id in enumerate(retriever_result, start=1):
                    if doc_id not in score_dict:
                        score_dict[doc_id] = 0
                    # Add RRF score for the document
                    score_dict[doc_id] += 1 / (k + rank)

            # Sort by accumulated RRF score
            sorted_results = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)

            # Keep only the topk results
            top_ids = [i[0] for i in sorted_results[:topk]]
            top_scores = [i[1] for i in sorted_results[:topk]]

            fused_results.append([id2item[id] for id in top_ids])
            fused_scores.append(top_scores)

        return fused_results, fused_scores

    def search(self, query, target_modal="text", num: Union[list, int, None] = None, return_score=False):
        # query: str or PIL.Image
        # judge query type: text or image
        if judge_image(query):
            retriever_list = [
                retriever for retriever in self.retriever_list if isinstance(retriever, MultiModalRetriever)
            ]
        else:
            retriever_list = self.retriever_list
        if target_modal == "image":
            # remove text retriever
            retriever_list = [retriever for retriever in retriever_list if isinstance(retriever, MultiModalRetriever)]

        return self._search_or_batch_search(
            query, target_modal, num, return_score, method="search", retriever_list=retriever_list
        )

    def batch_search(self, query, target_modal="text", num: Union[list, int, None] = None, return_score=False):
        # judge query type: text or image
        if not isinstance(query, list):
            query = [query]
        if target_modal == "image":
            self._retriever_list = [
                retriever for retriever in self.retriever_list if isinstance(retriever, MultiModalRetriever)
            ]
        else:
            self._retriever_list = self.retriever_list
        query_type_list = [judge_image(q) for q in query]
        if all(query_type_list):
            # all query is image
            if self.merge_method == "rerank":
                warnings.warn("merge_method is rerank, but all query is image, use default method `concat` instead")
                self.merge_method = "concat"
            retriever_list = [
                retriever for retriever in self._retriever_list if isinstance(retriever, MultiModalRetriever)
            ]

            return self._search_or_batch_search(
                query, target_modal, num, return_score, method="batch_search", retriever_list=retriever_list
            )
        elif all([not t for t in query_type_list]):
            # all query is text
            # if exist text retriever, don't use mm retriever for text-text search
            if any([isinstance(retriever, BaseTextRetriever) for retriever in self._retriever_list]):
                self._retriever_list = [
                    retriever for retriever in self._retriever_list if not isinstance(retriever, MultiModalRetriever)
                ]
            return self._search_or_batch_search(
                query, target_modal, num, return_score, method="batch_search", retriever_list=self._retriever_list
            )
        else:
            # query list is the mix of image and text
            if self.merge_method == "rerank":
                warnings.warn("merge_method is rerank, but some query is image, use default method `concat` instead")
                self.merge_method = "concat"
            image_query_idx = [i for i, t in enumerate(query_type_list) if t]
            image_query_list = [query[i] for i in image_query_idx]
            text_query_list = [q for q in query if q not in image_query_list]

            text_output = self._search_or_batch_search(
                text_query_list,
                target_modal,
                num,
                return_score,
                method="batch_search",
                retriever_list=self._retriever_list,
            )
            retriever_list = [
                retriever for retriever in self._retriever_list if isinstance(retriever, MultiModalRetriever)
            ]
            image_output = self._search_or_batch_search(
                text_query_list, target_modal, num, return_score, method="batch_search", retriever_list=retriever_list
            )

            # merge text output and image output
            if return_score:
                text_result, text_score = text_output
                image_result, image_score = image_output
                final_result = []
                final_score = []
                text_idx = 0
                image_idx = 0
                for idx in range(len(query)):
                    if idx not in image_query_idx:
                        final_result.append(text_result[text_idx])
                        final_score.append(text_score[text_idx])
                        text_idx += 1
                    else:
                        final_result.append(image_result[image_idx])
                        final_score.append(image_score[image_idx])
                        image_idx += 1
                return final_result, final_score
            else:
                final_result = []
                text_idx = 0
                image_idx = 0
                for idx in range(len(query)):
                    if idx not in image_query_idx:
                        final_result.append(text_result[text_idx])
                        text_idx += 1
                    else:
                        final_result.append(image_result[image_idx])
                        image_idx += 1
                return final_result


class SparseRetriever(BaseTextRetriever):
    """Sparse embedding retriever supporting only SPLADE with Seismic backend for now."""

    def __init__(self, config):
        super().__init__(config)

        import multiprocessing
        self.cores = str(multiprocessing.cpu_count())
        os.environ["RAYON_NUM_THREADS"] = self.cores

        self.progress_bar = None

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.corpus = load_corpus(config["corpus_path"])
        self.tokenizer, self.model = self._load_sparse_model()

        self.id = 0

        self.update_additional_setting()

        self.seismic_query_cut = self.config["seismic_query_cut"]
        self.seismic_heap_factor = self.config["seismic_heap_factor"]
        self.index_max_tokens = self.config["seismic_max_tokens_length"]
        self._init_seismic_index()

    def update_additional_setting(self):
        """Load config shared for all the models supported"""
        self.query_max_length = self._config["retrieval_query_max_length"]
        self.use_fp16 = self._config["retrieval_use_fp16"]
        self.batch_size = self._config["retrieval_batch_size"]
        self.retrieval_model_path = self._config["retrieval_model_path"]
        self.pooling_method = self._config["retrieval_pooling_method"]

    def _init_seismic_index(self):
        """Initialize Seismic index."""
        from seismic import SeismicIndex  # Assuming this is available
        self.seismic_index = SeismicIndex.load(self.index_path)
        self.string_type = f'U{self.index_max_tokens}'  # For Seismic string dtype

    def _load_sparse_model(self):
        """Load tokenizer and model based on sparse type."""
        from transformers import AutoModelForMaskedLM, AutoTokenizer
        # Load model
        tokenizer = AutoTokenizer.from_pretrained(self.retrieval_model_path)
        model = AutoModelForMaskedLM.from_pretrained(self.retrieval_model_path)

        if self.use_fp16:
            model = model.half()

        # Use more gpus if available
        if torch.cuda.device_count() > 1:
            model = torch.nn.DataParallel(model, device_ids=self.config['gpu_id'].split(','))

        model = model.to(self.device)
        model.eval()
        return tokenizer, model

    def _encode(self, query):
        inputs = self.tokenizer(
            query,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=self.query_max_length,
            add_special_tokens=True
        ).to(self.model.device)

        with torch.no_grad():
            logits = self.model(**inputs).logits  # [batch_size, seq_len, vocab_size]
            attention_mask = inputs["attention_mask"].unsqueeze(-1)  # [batch, seq_len, 1]

            scores = torch.log1p(torch.relu(logits)) * attention_mask
            v_repr = torch.max(scores, dim=1)[0]  # [batch_size, vocab_size]

            # Move to CPU (it seems much faster)
            v_repr = v_repr.cpu()
            nonzero_mask = v_repr > 1e-4

            # Get sparse values and indices in batch
            batch_indices, token_indices = torch.nonzero(nonzero_mask, as_tuple=True)
            token_scores = v_repr[batch_indices, token_indices]

            # Convert once all token IDs to strings (batched)
            unique_token_ids = torch.unique(token_indices)
            token_id_to_token = {
                idx.item(): tok for idx, tok in zip(
                    unique_token_ids, self.tokenizer.convert_ids_to_tokens(unique_token_ids.tolist())
                )
            }

            # Build final embeddings
            from collections import defaultdict
            embeddings = defaultdict(dict)
            for b_idx, t_idx, score in zip(batch_indices, token_indices, token_scores):
                embeddings[b_idx.item()][token_id_to_token[t_idx.item()]] = round(score.item(), 4)

            # Convert to list for each document
            return [embeddings[i] for i in range(len(query))]

    def search(self, query: list, num: int = None, return_score=False) -> (List[Dict], List[float]):
        """Search using sparse vector."""
        num = num or self.topk

        query_vec = self._encode(query)
        results, scores = self._seismic_search(query_vec, num)

        if return_score:
            return results, scores
        else:
            return results

    def batch_search(self, query, num=None, return_score=False):
        """Search using sparse vector."""
        if isinstance(query, str):
            query = [query]

        if self.pooling_method != 'max':
            print(
                f'Pooling method: {self.pooling_method.upper()} not supported on sparse neural retrieval models. fallback to: MAX.')

        num = num or self.topk

        embeddings = []
        batch = []
        
        # Encode
        for i in range(len(query)):
            # Process batch
            batch.append(query[i])
            if len(batch) >= self.batch_size:
                query_vec = self._encode(batch)
                embeddings.extend(query_vec)
                batch = []

        if batch:
            query_vec = self._encode(batch)
            embeddings.extend(query_vec)

        # Search
        search_results = self._seismic_batch_search(embeddings, num)

        results = []
        scores = []
        for result in sorted(search_results, key=lambda e: int(e[0][0])):
            tmp_results = []
            tmp_scores = []

            for query_id, score, doc_id in result:
                tmp_results.append(self.corpus[int(doc_id)])
                tmp_scores.append(score)

            results.append(tmp_results)
            scores.append(tmp_scores)
        
        if return_score:
            return results, scores
        else:
            return results

    def _seismic_search(self, query_vec: List[Dict[str, float]], k: int) -> (List[Dict], List[float]):
        """Search using Seismic backend."""
        # Convert query to Seismic format
        results, scores = self.index_search(k, query_vec)
        return results[0], scores[0]

    def _seismic_batch_search(self, query_vecs: List[Dict[str, float]], k: int) -> (
            List[List[Dict]], List[List[float]]):
        """Batch search using Seismic backend (one query at a time)."""
        return self.index_search(k, query_vecs)

    def index_search(self, k, query_vec):
        max_len = max(len(query) for query in query_vec)
        pad_token = ""  # or whatever default is appropriate

        query_components = []
        query_values = []
        ids = []

        for query in query_vec:
            keys = list(query.keys())
            values = list(query.values())

            # Pad to max_len
            padded_keys = keys + [pad_token] * (max_len - len(keys))
            padded_values = values + [0.0] * (max_len - len(values))

            query_components.append(np.array(padded_keys, dtype='U30'))
            query_values.append(np.array(padded_values, dtype=np.float32))
            ids.append(self.id)
            self.id += 1

        ids = np.array(ids, dtype='U30')
        # Execute search
        search_results = self.seismic_index.batch_search(
            queries_ids=ids,  # Placeholder ID
            query_components=query_components,
            query_values=query_values,
            query_cut=self.seismic_query_cut,
            heap_factor=self.seismic_heap_factor,
            k=k,
            sorted=True,  # specified even if default value
            num_threads=int(self.cores)
        )
        return search_results

class SerperRetriever(BaseRetriever):
    """Retriever based on Google Serper API for web search."""

    def __init__(self, config):
        super().__init__(config)
        
        # Serper API specific configuration
        self.api_key = config["serper_api_key"]
        if not self.api_key:
            raise ValueError("serper_api_key is required in config")
        
        self.api_url = "https://google.serper.dev/search"
        self.search_type = config["serper_search_type"] if config["serper_search_type"] else "search"  # search, news, images, etc.
        self.location = config["serper_location"] if config["serper_location"] else None  # e.g., "United States"
        self.gl = config["serper_gl"] if config["serper_gl"] else None  # Country code, e.g., "us"
        self.hl = config["serper_hl"] if config["serper_hl"] else "en"  # Language, e.g., "en"
        
    def _search(self, query: str, num: int) -> List[Dict[str, str]]:
        """
        Retrieve top-k relevant documents using Google Serper API.
        
        Args:
            query: Search query string
            num: Number of results to return
            return_score: Whether to return relevance scores
            
        Returns:
            List of dictionaries containing search results with keys:
                - contents: The snippet/description
                - title: Page title
                - text: Full text (same as contents for web search)
                - url: Page URL
                - score: Relevance score (if return_score=True)
        """
        headers = {
            'X-API-KEY': self.api_key,
            'Content-Type': 'application/json'
        }
        
        payload = {
            'q': query,
            'num': num,
            'hl': self.hl
        }
        
        if self.location:
            payload['location'] = self.location
        if self.gl:
            payload['gl'] = self.gl
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            
            results = []
            
            # Parse organic results
            organic_results = data.get('organic', [])
            for idx, item in enumerate(organic_results[:num]):
                result = {
                    'title': item.get('title', ''),
                    'text': item.get('snippet', ''),
                    'url': item.get('link', ''),
                }
                
                results.append(result)
            
            return results
            
        except requests.exceptions.RequestException as e:
            print(f"Error calling Serper API: {e}")
            return []
        except Exception as e:
            print(f"Unexpected error in _search: {e}")
            return []
    
    def search(self, query: str, num: int = None) -> List[Dict[str, str]]:
        """
        Single search wrapper for SerperRetriever.
        """
        if num is None:
            num = self.topk
        return self._search(query, num)
    
    def _batch_search(self, query_list: List[str], num: int) -> List[List[Dict[str, str]]]:
        """
        Batch search for multiple queries.
        
        Args:
            query_list: List of query strings
            num: Number of results per query
            return_score: Whether to return relevance scores
            
        Returns:
            List of result lists, one for each query
        """
        results = []
        if num is None:
            num = self.topk
        
        for query in query_list:
            result = self._search(query, num)
            results.append(result)
            # Add a small delay to avoid rate limiting
            time.sleep(0.1)
        
        return results

    def batch_search(self, query_list: List[str], num: int = None):
        return self._batch_search(query_list, num)

def main():
# Example configuration
    config = {
        # Base retriever config
        "retrieval_method": "serper",
        "retrieval_topk": 10,
        "index_path": None,  # Not used for Serper
        "corpus_path": None,  # Not used for Serper
        "save_dir": "./output",
        
        # Serper specific config
        "serper_api_key": "your-api-key",
        "serper_search_type": "search",
        "serper_location": "United States",
        "serper_gl": "us",
        "serper_hl": "en"
    }
    from flashrag.config import Config
    config = Config("basic_config.yaml",config)
    retriever = SerperRetriever(config)

    # Batch search
    queries = ["Python programming", "Machine learning"]
    batch_results = retriever.batch_search(queries)
    print(batch_results)

if __name__ == "__main__":
    main()



================================================
FILE: flashrag/retriever/utils.py
================================================
import json
import os
import warnings
from typing import Dict, Any, Union, List, Dict
import numpy as np
import datasets
import re
import langid
from transformers import AutoTokenizer, AutoModel, AutoConfig
from flashrag.utils import get_device

_has_printed_instruction = False  # trigger instruction print once

def convert_numpy(obj: Union[Dict, list, np.ndarray, np.generic]) -> Any:
    """Recursively convert numpy objects in nested dictionaries or lists to native Python types."""
    if isinstance(obj, dict):
        return {k: convert_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy(i) for i in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()  # Convert numpy arrays to lists
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()  # Convert numpy scalars to native Python scalars
    elif isinstance(obj, np.float32):
        return float(obj)
    else:
        return obj  # Return the object as-is if it's neither a dict, list, nor numpy type


def judge_zh(input_str: str):
    assert isinstance(input_str, str), input_str
    if len(input_str) == 0:
        return False
    detect_result = langid.classify(input_str)
    if detect_result[0] == 'zh':
        return True
    else:
        return False
    #return bool(re.search(r'[\u4e00-\u9fff]', input_str))


def convert_numpy(obj: Union[Dict, list, np.ndarray, np.generic]) -> Any:
    """Recursively convert numpy objects in nested dictionaries or lists to native Python types."""
    if isinstance(obj, dict):
        return {k: convert_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy(i) for i in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()  # Convert numpy arrays to lists
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()  # Convert numpy scalars to native Python scalars
    elif isinstance(obj, np.float32):
        return float(obj)
    else:
        return obj  # Return the object as-is if it's neither a dict, list, nor numpy type
    
def load_model(model_path: str, use_fp16: bool = False):
    model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
    model.eval()
    model.to(get_device())
    if use_fp16:
        model = model.half()
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, trust_remote_code=True)

    return model, tokenizer


def pooling(pooler_output, last_hidden_state, attention_mask=None, pooling_method="mean"):
    if last_hidden_state is None and pooling_method in ['mean', 'cls']:
        warnings.warn('last_hidden_state is None, using pooler_output instead.')
        pooling_method = 'pooler'

    if pooling_method == "mean":
        last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)
        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
    elif pooling_method == "cls":
        return last_hidden_state[:, 0]
    elif pooling_method == "pooler":
        return pooler_output
    else:
        raise NotImplementedError("Pooling method not implemented!")


def set_default_instruction(model_name, is_query=True, is_zh=False):
    instruction = ""
    if "e5" in model_name.lower():
        if is_query:
            instruction = "query: "
        else:
            instruction = "passage: "

    if "bge" in model_name.lower():
        if is_query:
            if "zh" in model_name.lower() or is_zh:
                instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
            else:
                instruction = "Represent this sentence for searching relevant passages: "

    return instruction


def parse_query(model_name, query_list, instruction=None, is_query=True):
    """
    processing query for different encoders
    """
    global _has_printed_instruction

    if isinstance(query_list, str):
        query_list = [query_list]

    if instruction is not None:
        instruction = instruction.strip() + " "
    else:
        instruction = set_default_instruction(model_name, is_query=is_query, is_zh=judge_zh(query_list[0]))
    
    if not _has_printed_instruction:
        if instruction == "":
            warnings.warn('Instruction is not set')
        else:
            print(f"Use `{instruction}` as retreival instruction")
        _has_printed_instruction = True
        
    query_list = [instruction + query for query in query_list]

    return query_list


def load_corpus(corpus_path: str):
    if corpus_path.endswith(".jsonl"):
        corpus = datasets.load_dataset('json', data_files=corpus_path, split="train")
    elif corpus_path.endswith(".parquet"):
        corpus = datasets.load_dataset('parquet', data_files=corpus_path, split="train")
        corpus = corpus.cast_column('image', datasets.Image())
    else:
        raise NotImplementedError("Corpus format not supported!")
    if 'contents' not in corpus.features:
        try:
            print("No `contents` field found in corpus, using `text` instead.")
            corpus = corpus.map(lambda x: {"contents": x["text"]})
        except:
            warnings.warn("No `contents` & `text` field found in corpus.")
    return corpus

def read_jsonl(file_path):
    with open(file_path, "r") as f:
        while True:
            new_line = f.readline()
            if not new_line:
                return
            new_item = json.loads(new_line)

            yield new_item


def load_docs(corpus, doc_idxs: List[int]):
    results = [corpus[int(idx)] for idx in doc_idxs]

    return results


def parse_image(image):
    from PIL import Image

    if isinstance(image, str):
        if image.startswith("http"):
            import requests

            image = Image.open(requests.get(image, stream=True).raw)
        else:
            image = Image.open(image)
    return image


def judge_image(x):
    from PIL import Image
    if isinstance(x, str):
        if x.startswith("http"):
            return True
        if os.path.exists(x):
            return True
    elif isinstance(x, Image.Image):
        return True
    else:
        warnings.warn('image type not supported')
    return False
    


================================================
FILE: flashrag/utils/__init__.py
================================================
from flashrag.utils.utils import *
from flashrag.utils.pred_parse import *


================================================
FILE: flashrag/utils/constants.py
================================================
OPENAI_MODEL_DICT = {
    # chat
    "gpt-4o": "o200k_base",
    "gpt-4": "cl100k_base",
    "gpt-3.5-turbo": "cl100k_base",
    "gpt-3.5": "cl100k_base",  # Common shorthand
    "gpt-35-turbo": "cl100k_base",  # Azure deployment name
    # base
    "davinci-002": "cl100k_base",
    "babbage-002": "cl100k_base",
    # embeddings
    "text-embedding-ada-002": "cl100k_base",
    "text-embedding-3-small": "cl100k_base",
    "text-embedding-3-large": "cl100k_base",
    # DEPRECATED MODELS
    # text (DEPRECATED)
    "text-davinci-003": "p50k_base",
    "text-davinci-002": "p50k_base",
    "text-davinci-001": "r50k_base",
    "text-curie-001": "r50k_base",
    "text-babbage-001": "r50k_base",
    "text-ada-001": "r50k_base",
    "davinci": "r50k_base",
    "curie": "r50k_base",
    "babbage": "r50k_base",
    "ada": "r50k_base",
    # code (DEPRECATED)
    "code-davinci-002": "p50k_base",
    "code-davinci-001": "p50k_base",
    "code-cushman-002": "p50k_base",
    "code-cushman-001": "p50k_base",
    "davinci-codex": "p50k_base",
    "cushman-codex": "p50k_base",
    # edit (DEPRECATED)
    "text-davinci-edit-001": "p50k_edit",
    "code-davinci-edit-001": "p50k_edit",
    # old embeddings (DEPRECATED)
    "text-similarity-davinci-001": "r50k_base",
    "text-similarity-curie-001": "r50k_base",
    "text-similarity-babbage-001": "r50k_base",
    "text-similarity-ada-001": "r50k_base",
    "text-search-davinci-doc-001": "r50k_base",
    "text-search-curie-doc-001": "r50k_base",
    "text-search-babbage-doc-001": "r50k_base",
    "text-search-ada-doc-001": "r50k_base",
    "code-search-babbage-code-001": "r50k_base",
    "code-search-ada-code-001": "r50k_base",
    # open source
    "gpt2": "gpt2",
    "gpt-2": "gpt2",  # Maintains consistency with gpt-4
}



================================================
FILE: flashrag/utils/pred_parse.py
================================================
import re

def selfask_pred_parse(dataset):
    """Parsing the prediction results of self-ask format."""
    FINAL_ANSWER_PREFIX = "So the final answer is: "

    for item in dataset:
        pred = item.pred
        lines = pred.split("\n")
        answer = ""
        for line in lines:
            if FINAL_ANSWER_PREFIX in line:
                answer = line.split(FINAL_ANSWER_PREFIX)[1].strip()
                break
        item.update_output('raw_pred', pred)
        item.update_output('pred', answer)

    return dataset


def ircot_pred_parse(dataset):
    FINAL_ANSWER_PREFIX = "So the answer is:"
    for item in dataset:
        pred = item.pred
        if FINAL_ANSWER_PREFIX in pred:
            answer = pred.split(FINAL_ANSWER_PREFIX)[1].strip()
        else:
            answer = pred
        item.update_output('raw_pred', pred)
        item.update_output('pred', answer)
    return dataset


def basic_pred_parse(dataset):
    for item in dataset:
        pred = item.pred
        item.update_output('raw_pred', pred)
        item.update_output('pred', pred.split("\n")[0].strip())
    return dataset



def gaokaomm_pred_parse(dataset):
    """
    Extract choice answer from model output.

    Format of model_output that is expected:
    'single_choice': choice answer should be the last Capital Letter of the model_output, e.g.: "...ã€ç­”æ¡ˆã€‘ A <eoa>"
    'multi_choice': "...ã€ç­”æ¡ˆã€‘ ABD " or write the choice answers at the end of the model_output, e.g. "... ACD"
    """

    for item in dataset:
        model_output = item.pred
        question_type = item.question_type

        if question_type == 'single_choice':
            model_answer = ""
            temp = re.findall(r'[A-D]', model_output[::-1])
            if len(temp) != 0:
                model_answer = temp[0]

        elif question_type == 'multiple_choice':
            model_answer = []
            answer = ''
            content = re.sub(r'\s+', '', model_output)
            answer_index = content.find('ã€ç­”æ¡ˆã€‘')
            if answer_index > 0:
                temp = content[answer_index:]
                if len(re.findall(r'[A-D]', temp)) > 0:
                    for t in re.findall(r'[A-D]', temp):
                        answer += t
            else:
                temp = content[-10:]
                if len(re.findall(r'[A-D]', temp)) > 0:
                    for t in re.findall(r'[A-D]', temp):
                        answer += t
            if len(answer) != 0:
                model_answer.append(answer)
        model_answer = "".join(model_answer)

        item.update_output('raw_pred', model_output)
        item.update_output('pred', model_answer)

    return dataset


================================================
FILE: flashrag/utils/utils.py
================================================
import os
import re
import json
import importlib
from transformers import AutoConfig
from flashrag.dataset.dataset import Dataset
import torch

def get_dataset(config):
    """Load dataset from config."""
    SUPPORT_FILES = ["jsonl", "json", "parquet"]

    dataset_path = config["dataset_path"]
    all_split = config["split"]

    split_dict = {split: None for split in all_split}

    for split in all_split:
        exist_flag = 0
        for file_postfix in SUPPORT_FILES:
            split_path = os.path.join(dataset_path, f"{split}.{file_postfix}")
            if not os.path.exists(split_path):
                continue
            else:
                exist_flag = 1
                break
        if exist_flag == 0:
            continue
        else:
            print(f"Loading {split} dataset from: {split_path}...")
        if split in ["test", "val", "dev", "train"]:
            split_dict[split] = Dataset(
                config, split_path, sample_num=config["test_sample_num"], random_sample=config["random_sample"]
            )
        else:
            split_dict[split] = Dataset(config, split_path)

    return split_dict


def get_generator(config, **params):
    """Automatically select generator class based on config."""

    if config['framework'] == 'openai':
        return getattr(importlib.import_module("flashrag.generator"), "OpenaiGenerator")(config, **params)
    
    # judge multimodal model
    with open(os.path.join(config["generator_model_path"], "config.json"), "r") as f:
        model_config = json.load(f)
    arch = model_config['architectures'][0]
    if all(["vision" not in key for key in model_config.keys()]):
        is_mm = False
    else:
        is_mm = True
    
    if is_mm:
        return getattr(importlib.import_module("flashrag.generator"), "HFMultiModalGenerator")(config, **params)
    else:
        if config["framework"] == "vllm":
            return getattr(importlib.import_module("flashrag.generator"), "VLLMGenerator")(config, **params)
        elif config["framework"] == "fschat":
            return getattr(importlib.import_module("flashrag.generator"), "FastChatGenerator")(config, **params)
        elif config["framework"] == "hf":
            if "t5" in arch.lower() or "bart" in arch.lower():
                return getattr(importlib.import_module("flashrag.generator"), "EncoderDecoderGenerator")(config, **params)
            else:
                return getattr(importlib.import_module("flashrag.generator"), "HFCausalLMGenerator")(config, **params)
        else:
            raise NotImplementedError


def get_retriever(config):
    r"""Automatically select retriever class based on config's retrieval method

    Args:
        config (dict): configuration with 'retrieval_method' key

    Returns:
        Retriever: retriever instance
    """
    if config["use_multi_retriever"]:
        # must load special class for manage multi retriever
        return getattr(importlib.import_module("flashrag.retriever"), "MultiRetrieverRouter")(config)

    if config["retrieval_method"] == "bm25":
        return getattr(importlib.import_module("flashrag.retriever"), "BM25Retriever")(config)
    elif config["retrieval_method"] == "splade":
        return getattr(importlib.import_module("flashrag.retriever"), "SparseRetriever")(config)
    else:
        try:
            model_config = AutoConfig.from_pretrained(config["retrieval_model_path"])
            arch = model_config.architectures[0]
            if "clip" in arch.lower():
                return getattr(importlib.import_module("flashrag.retriever"), "MultiModalRetriever")(config)
            else:
                return getattr(importlib.import_module("flashrag.retriever"), "DenseRetriever")(config)
        except:
            return getattr(importlib.import_module("flashrag.retriever"), "DenseRetriever")(config)


def get_reranker(config):
    model_path = config["rerank_model_path"]
    # get model config
    model_config = AutoConfig.from_pretrained(model_path)
    arch = model_config.architectures[0]
    if "forsequenceclassification" in arch.lower():
        return getattr(importlib.import_module("flashrag.retriever"), "CrossReranker")(config)
    else:
        return getattr(importlib.import_module("flashrag.retriever"), "BiReranker")(config)


def get_judger(config):
    judger_name = config["judger_name"]
    if "skr" in judger_name.lower():
        return getattr(importlib.import_module("flashrag.judger"), "SKRJudger")(config)
    elif "adaptive" in judger_name.lower():
        return getattr(importlib.import_module("flashrag.judger"), "AdaptiveJudger")(config)
    else:
        assert False, "No implementation!"


def get_refiner(config, retriever=None, generator=None):
    # é¢„å®šä¹‰é»˜è®¤è·¯å¾„å­—å…¸
    DEFAULT_PATH_DICT = {
        "recomp_abstractive_nq": "fangyuan/nq_abstractive_compressor",
        "recomp:abstractive_tqa": "fangyuan/tqa_abstractive_compressor",
        "recomp:abstractive_hotpotqa": "fangyuan/hotpotqa_abstractive",
    }
    REFINER_MODULE = importlib.import_module("flashrag.refiner")

    refiner_name = config["refiner_name"]
    refiner_path = (
        config["refiner_model_path"]
        if config["refiner_model_path"] is not None
        else DEFAULT_PATH_DICT.get(refiner_name, None)
    )

    try:
        model_config = AutoConfig.from_pretrained(refiner_path)
        arch = model_config.architectures[0].lower()
        print(arch)
    except Exception as e:
        print("Warning", e)
        model_config, arch = "", ""

    if "recomp" in refiner_name:
        if model_config.model_type == "t5":
            refiner_class = "AbstractiveRecompRefiner"
        else:
            refiner_class = "ExtractiveRefiner"
    elif 'bert' in arch:
        refiner_class = "ExtractiveRefiner"
    elif 'T5' in arch or 'Bart' in arch:
        refiner_class = "AbstractiveRecompRefiner"
    elif "lingua" in refiner_name:
        refiner_class = "LLMLinguaRefiner"
    elif "selective-context" in refiner_name or "sc" in refiner_name:
        refiner_class = "SelectiveContextRefiner"
    elif "kg-trace" in refiner_name:
        return getattr(REFINER_MODULE, "KGTraceRefiner")(config, retriever, generator)
    else:
        raise ValueError("No implementation!")

    return getattr(REFINER_MODULE, refiner_class)(config)


def hash_object(o) -> str:
    """Returns a character hash code of arbitrary Python objects."""
    import hashlib
    import io
    import dill
    import base58

    m = hashlib.blake2b()
    with io.BytesIO() as buffer:
        dill.dump(o, buffer)
        m.update(buffer.getbuffer())
        return base58.b58encode(m.digest()).decode()

def extract_between(text: str, start_tag: str, end_tag: str):
    pattern = re.escape(start_tag) + r"(.*?)" + re.escape(end_tag)
    matches = re.findall(pattern, text, flags=re.DOTALL)
    if matches:
        return matches[-1].strip()
    return None

def extract_between_all(text:str, start_tag:str, end_tag:str):
    pattern = re.escape(start_tag) + r"(.*?)" + re.escape(end_tag)
    matches = re.findall(pattern, text, flags=re.DOTALL)
    if matches:
        return matches
    return None

def get_device() -> str:
    return "cuda" if torch.cuda.is_available() else "cpu"



================================================
FILE: scripts/build_index.sh
================================================
CUDA_VISIBLE_DEVICES=0 python -m flashrag.retriever.index_builder \
    --retrieval_method e5 \
    --model_path /model/e5-base-v2 \
    --corpus_path .. \
    --save_dir /index/ \
    --use_fp16 \
    --max_length 256 \
    --batch_size 512 \
    --pooling_method mean \
    --faiss_type Flat \
    --save_embedding


================================================
FILE: scripts/chunk_doc_corpus.py
================================================
"""Chunk documents from a Document Corpus JSONL file.

This script is used to chunk documents from a Document Corpus JSONL file,
via Chonkie.
"""

import argparse
import json
from tqdm import tqdm
import chonkie


def load_jsonl(file_path):
    documents = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            documents.append(json.loads(line))
    return documents


def save_jsonl(documents, file_path):
    with open(file_path, "w", encoding="utf-8") as f:
        for doc in documents:
            f.write(json.dumps(doc) + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chunk documents from a JSONL file.")
    parser.add_argument("--input_path", type=str, required=True, help="Path to input JSONL file")
    parser.add_argument("--output_path", type=str, required=True, help="Path to output JSONL file")
    parser.add_argument(
        "--chunk_by", default="token", choices=["token", "word", "sentence", "recursive"], help="Chunking method to use"
    )
    parser.add_argument("--chunk_size", default=512, type=int, help="Size of chunks")
    parser.add_argument("--tokenizer_name_or_path", default="o200k_base", type=str)

    args = parser.parse_args()

    # Load documents
    print("Loading documents...")
    documents = load_jsonl(args.input_path)

    # Initialize chunker
    if args.chunk_by == "token":
        chunker = chonkie.TokenChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
    elif args.chunk_by == "word":
        chunker = chonkie.TokenChunker(tokenizer="word", chunk_size=args.chunk_size)
    elif args.chunk_by == "sentence":
        chunker = chonkie.SentenceChunker(tokenizer_or_token_counter=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
    elif args.chunk_by == "recursive":
        chunker = chonkie.RecursiveChunker(
            tokenizer_or_token_counter=args.tokenizer_name_or_path, chunk_size=args.chunk_size, min_characters_per_chunk=1
        )
    else:
        raise ValueError(f"Invalid chunking method: {args.chunk_by}")

    # Process and chunk documents
    print("Chunking documents...")
    chunked_documents = []
    current_chunk_id = 0
    for doc in tqdm(documents):
        title, text = doc["contents"].split("\n", 1)
        chunks = chunker.chunk(text)
        for chunk in chunks:
            chunked_doc = {
                "id": current_chunk_id,
                "doc_id": doc["id"],
                "title": title,
                "contents": title + "\n" + chunk.text,
            }
            chunked_documents.append(chunked_doc)
            current_chunk_id += 1

    # Save chunked documents
    print("Saving chunked documents...")
    save_jsonl(chunked_documents, args.output_path)
    print(f"Done! Processed {len(documents)} documents into {len(chunked_documents)} chunks.")



================================================
FILE: scripts/preprocess_wiki.py
================================================
import argparse
from tqdm import tqdm
import re
import html
import os
import json
import subprocess
from pathlib import Path
import shutil
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Pool


def load_corpus(dir_path):
    def iter_files(path):
        """Walk through all files located under a root path."""
        if os.path.isfile(path):
            yield path
        elif os.path.isdir(path):
            for dirpath, _, filenames in os.walk(path):
                for f in filenames:
                    yield os.path.join(dirpath, f)
        else:
            raise RuntimeError("Path %s is invalid" % path)

    def read_jsonl_file(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                json_data = json.loads(line)
                corpus.append(json_data)

    all_files = [file for file in iter_files(dir_path)]
    corpus = []

    with ThreadPoolExecutor(max_workers=args.num_workers) as executor:
        for file_path in all_files:
            executor.submit(read_jsonl_file, file_path)

    return corpus


def basic_process(title, text):
    title = html.unescape(title)
    text = html.unescape(text)
    text = text.strip()

    if "(disambiguation)" in title.lower():
        return None, None
    if "(disambiguation page)" in title.lower():
        return None, None
    # Take out List/Index/Outline pages (mostly links)
    if re.match(r"(List of .+)|(Index of .+)|(Outline of .+)", title):
        return None, None
    if text.startswith("REDIRECT") or text.startswith("redirect"):
        return None, None
    if text.endswith(". References."):
        text = text[: -len(" References.")].strip()

    text = re.sub("\{\{cite .*?\}\}", " ", text, flags=re.DOTALL)
    text = text.replace(r"TABLETOREPLACE", " ")
    text = text.replace(r"'''", " ")
    text = text.replace(r"[[", " ")
    text = text.replace(r"]]", " ")
    text = text.replace(r"{{", " ")
    text = text.replace(r"}}", " ")
    text = text.replace("<br>", " ")
    text = text.replace("&quot;", '"')
    text = text.replace("&amp;", "&")
    text = text.replace("& amp;", "&")
    text = text.replace("nbsp;", " ")
    text = text.replace("formatnum:", "")

    # text = re.sub('<poem.*?</poem>', ' ', text, flags=re.DOTALL) # might have useful information?
    text = re.sub("<math.*?</math>", "", text, flags=re.DOTALL)
    text = re.sub("<chem.*?</chem>", "", text, flags=re.DOTALL)
    text = re.sub("<score.*?</score>", "", text, flags=re.DOTALL)

    # clean residual mess from xml dump that shouldn't have made its way here
    text = re.sub("\| ?item[0-9]?_?style= ?.*? ", " ", text)
    text = re.sub("\| ?col[0-9]?_?style= ?.*? ", " ", text)
    text = re.sub("\| ?row[0-9]?_?style= ?.*? ", " ", text)
    text = re.sub("\| ?style= ?.*? ", " ", text)
    text = re.sub("\| ?bodystyle= ?.*? ", " ", text)
    text = re.sub("\| ?frame_?style= ?.*? ", " ", text)
    text = re.sub("\| ?data_?style= ?.*? ", " ", text)
    text = re.sub("\| ?label_?style= ?.*? ", " ", text)
    text = re.sub("\| ?headerstyle= ?.*? ", " ", text)
    text = re.sub("\| ?list_?style= ?.*? ", " ", text)
    text = re.sub("\| ?title_?style= ?.*? ", " ", text)
    text = re.sub("\| ?ul_?style= ?.*? ", " ", text)
    text = re.sub("\| ?li_?style= ?.*? ", " ", text)
    text = re.sub("\| ?border-style= ?.*? ", " ", text)
    text = re.sub('\|? ?style=".*?"', "", text)
    text = re.sub('\|? ?rowspan=".*?"', "", text)
    text = re.sub('\|? ?colspan=".*?"', "", text)
    text = re.sub('\|? ?scope=".*?"', "", text)
    text = re.sub('\|? ?align=".*?"', "", text)
    text = re.sub('\|? ?valign=".*?"', "", text)
    text = re.sub('\|? ?lang=".*?"', "", text)
    text = re.sub('\|? ?bgcolor=".*?"', "", text)
    text = re.sub("\|? ?bg=\#[a-z]+", "", text)
    text = re.sub('\|? ?width=".*?"', "", text)
    text = re.sub("\|? ?height=[0-9]+", "", text)
    text = re.sub("\|? ?width=[0-9]+", "", text)
    text = re.sub("\|? ?rowspan=[0-9]+", "", text)
    text = re.sub("\|? ?colspan=[0-9]+", "", text)
    text = re.sub(r"[\n\t]", " ", text)
    text = re.sub("<.*?/>", "", text)
    text = re.sub("\|? ?align=[a-z]+", "", text)
    text = re.sub("\|? ?valign=[a-z]+", "", text)
    text = re.sub("\|? ?scope=[a-z]+", "", text)
    text = re.sub("&lt;ref&gt;.*?&lt;/ref&gt;", " ", text)
    text = re.sub("&lt;.*?&gt;", " ", text)
    text = re.sub("File:[A-Za-z0-9 ]+\.[a-z]{3,4}(\|[0-9]+px)?", "", text)
    text = re.sub("Source: \[.*?\]", "", text)
    text = text.replace("Country flag|", "country:")
    text = text.replace("flag|", "country:")
    text = text.replace("flagicon|", "country:")
    text = text.replace("flagcountry|", "country:")
    text = text.replace("Flagu|", "country:")
    text = text.replace("display=inline", "")
    text = text.replace("display=it", "")
    text = text.replace("abbr=on", "")
    text = text.replace("disp=table", "")

    title = title.replace("\n", " ").replace("\t", " ")

    return title, text


def split_list(lst, n):
    """Split a list into n roughly equal parts."""
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m) : (i + 1) * k + min(i + 1, m)] for i in range(n)]


def single_worker(docs):
    results = []
    for item in tqdm(docs):
        title, text = basic_process(item[0], item[1])
        if title is None:
            continue
        title = f'"{title}"'
        results.append((title, text))
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate clean wiki corpus file for indexing.")
    parser.add_argument("--dump_path", type=str)
    parser.add_argument(
        "--use_chonkie",
        type=bool,
        default=True,
        action="store_true",
    )
    parser.add_argument("--chunk_by", default="token", choices=["token", "sentence", "recursive", "100w"], type=str)
    parser.add_argument("--chunk_size", default=512, type=int)
    parser.add_argument("--tokenizer_name_or_path", default="o200k_base", type=str)
    parser.add_argument("--seg_size", default=None, type=int)
    parser.add_argument("--stride", default=None, type=int)
    parser.add_argument("--num_workers", default=4, type=int)
    parser.add_argument("--save_path", type=str, default="clean_corpus.jsonl")
    args = parser.parse_args()

    if args.use_chonkie:
        import chonkie
    else:
        assert args.chunk_by in ["100w", "sentence"], "Only supports sentence and 100w chunking without chonkie!"
        import spacy

        nlp = spacy.load("en_core_web_lg")

    # extract wiki dump
    temp_dir = os.path.join(Path(args.save_path).parent, "temp")
    os.makedirs(temp_dir)
    subprocess.run(
        [
            "python",
            "-m",
            "wikiextractor.WikiExtractor",
            "--json",
            "--filter_disambig_pages",
            "--quiet",
            "-o",
            temp_dir,
            "--process",
            str(args.num_workers),
            args.dump_path,
        ]
    )

    corpus = load_corpus(temp_dir)

    documents = {}
    # To avoid duplicate pages
    for item in tqdm(corpus):
        title = item["title"]
        text = item["text"]
        if title in documents:
            documents[title] += " " + text
        else:
            documents[title] = text

    print("Start pre-processing...")
    documents = list(documents.items())

    with Pool(processes=args.num_workers) as p:
        result_list = list(tqdm(p.imap(single_worker, split_list(documents, args.num_workers))))
    result_list = sum(result_list, [])

    all_title = [item[0] for item in result_list]
    all_text = [item[1] for item in result_list]

    print("Start chunking...")
    idx = 0
    clean_corpus = []

    if args.use_chonkie:
        print("Using Chonkie chunker...")
    else:
        print("Using default chunker...")

    if args.use_chonkie:
        # Initialize a Chonkie chunker, based on the chunk_by argument
        if args.chunk_by == "token":
            chunker = chonkie.TokenChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
        elif args.chunk_by == "sentence":
            chunker = chonkie.SentenceChunker(tokenizer_or_token_counter=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
        elif args.chunk_by == "recursive":
            chunker = chonkie.RecursiveChunker(
                tokenizer_or_token_counter=args.tokenizer_name_or_path, chunk_size=args.chunk_size, min_characters_per_chunk=1
            )
        elif args.chunk_by == "100w":
            chunker = chonkie.TokenChunker(tokenizer="word", chunk_size=100)
        else:
            raise ValueError(f"Invalid chunking method: {args.chunk_by}")

        # Chunk the text into segments, with chunker
        for title, text in tqdm(zip(all_title, all_text), total=len(all_text)):
            chunks = chunker.chunk(text)
            for chunk in chunks:
                clean_corpus.append({"title": title, "text": chunk.text})
    else:
        if args.chunk_by == "sentence":
            for doc in tqdm(nlp.pipe(all_text, n_process=args.num_workers, batch_size=2000), total=len(all_text)):
                title = all_title[idx]
                idx += 1
                sentences = [sent.text.strip() for sent in doc.sents]
                segments = []
                for i in range(0, len(sentences), args.stride):
                    segment = " ".join(sentences[i : i + args.seg_size])
                    segments.append(segment)
                    if i + args.seg_size >= len(sentences):
                        break
                for segment in segments:
                    text = segment.replace("\n", " ").replace("\t", " ")
                    clean_corpus.append({"title": title, "text": text})

        elif args.chunk_by == "100w":
            for doc in tqdm(nlp.pipe(all_text, n_process=args.num_workers, batch_size=2000), total=len(all_text)):
                title = all_title[idx]
                idx += 1
                segments = []
                word_count = 0
                segment_tokens = []
                for token in doc:
                    segment_tokens.append(token.text_with_ws)
                    if not token.is_space and not token.is_punct:
                        word_count += 1
                        if word_count == 100:
                            word_count = 0
                            segments.append("".join([token for token in segment_tokens]))
                            segment_tokens = []
                if word_count != 0:
                    for token in doc:
                        segment_tokens.append(token.text_with_ws)
                        if not token.is_space and not token.is_punct:
                            word_count += 1
                            if word_count == 100:
                                word_count = 0
                                segments.append("".join([token for token in segment_tokens]))
                                break
                if word_count != 0:
                    segments.append("".join([token for token in segment_tokens]))

                for segment in segments:
                    text = segment.replace("\n", " ").replace("\t", " ")
                    clean_corpus.append({"title": title, "text": text})

    shutil.rmtree(temp_dir)

    print("Start saving corpus...")
    with open(args.save_path, "w", encoding="utf-8") as f:
        for idx, item in enumerate(clean_corpus):
            title = f"\"{item['title']}\""
            item = {"id": idx, "title": title, "text": item["text"]}
            f.write(json.dumps(item) + "\n")
    print("Finish!")



================================================
FILE: .github/scripts/python/update_version.py
================================================
"""
This CLI script is used to update the version of the package. It is used by the
CI/CD pipeline to update the version of the package when a new release is made.

It uses argparse to parse the command line arguments, which are the new version
and the path to the package's __init__.py file.
"""

import argparse
from pathlib import Path
import re

def main():
    parser = argparse.ArgumentParser(
        description="Update the version of the package."
    )
    parser.add_argument(
        "--version",
        type=str,
        help="The new version of the package.",
        required=True,
    )
    parser.add_argument(
        "--path",
        type=Path,
        help="The path to the package's version file.",
    )
    args = parser.parse_args()
    with open("flashrag/version.py","r") as f:
        version = f.read().strip()
    version = version.split("=",1)[1].strip().replace("\"","")
    new_version = re.sub(r'dev\d+', f'dev{args.version}', version)
    with open(args.path, "w") as f:
        f.write(f"__version__ = \"{new_version}\"")
        

if __name__ == "__main__":
    main()


